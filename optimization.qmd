---
author:
  - name: Alec Loudenback
  - name: Yun-Tien Lee
---

# Optimization {#sec-optimization}

> Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise. - John Tukey, 1962

## Chapter Overview

Optimization lies at the heart of every quantitative finance workflow: calibrating pricing models, allocating portfolios, fitting yield curves, or matching liability cash flows all boil down to choosing inputs that minimize (or maximize) a well-defined metric. This chapter surveys the objective functions we typically optimize, contrasts gradient-based and gradient-free techniques, and illustrates how Julia's differentiable programming ecosystem accelerates financial calibration and model fitting.

```{julia}
#| echo: false
#| output: false
using Pkg
Pkg.activate("env/optimization")
Pkg.instantiate()
```

## Introduction

Local and global optimization: A **local optimum** value refers to a solution where the objective function (or cost function) has the best possible value in a neighborhood surrounding that solution. A **global optimum** value, on the other hand, is the best possible value of the objective function across the entire feasible domain. For smooth and convex functions, any local minimum is also a global minimum, and the negative gradient always points in a descent direction, making gradient-based methods extremely efficient for finding the optimal solution. Even for non-convex functions, the negative gradient provides valuable information about the direction to move toward improving the objective function value locally.

```{julia}
using CairoMakie
let
    f(x) = x^8 - 3x^4 + x
    xs = range(-1.5, 1.5; length=1001)
    ys = f.(xs)

    fig = Figure(resolution=(600, 400))
    ax = Axis(fig[1, 1],
        xlabel="x",
        ylabel="f(x)",
        title="Function with Local and Global Optima (over [-1.5, 1.5])",
        limits=(-1.5, 1.5, minimum(ys) - 0.5, maximum(ys) + 0.5),
    )

    lines!(ax, xs, ys, color=:blue)

    # Global minimum on the sampled grid
    i_global = argmin(ys)
    x_global_min = xs[i_global]
    y_global_min = ys[i_global]
    scatter!(ax, [x_global_min], [y_global_min],
        color=:red, markersize=10, label="Global min")

    # Detect local minima by comparing neighbors (simple discrete test)
    left_lower = ys[2:end-1] .< ys[1:end-2]
    right_lower = ys[2:end-1] .< ys[3:end]
    local_min_inds = findall(left_lower .& right_lower) .+ 1
    # Exclude the global index to highlight a different local minimum if present
    local_min_inds = filter(i -> i != i_global, local_min_inds)
    if !isempty(local_min_inds)
        i_local = first(local_min_inds)
        scatter!(ax, [xs[i_local]], [ys[i_local]],
            color=:black, markersize=10, label="Local min")
    end

    axislegend(ax, position=:rt)
    fig
end

```

Optimization techniques are incredibly important and find uses in many areas:

-   Machine learning relies on "training", which is essentially optimizing parameters to match target data.
-   Determining parameters for market consistent models where the modeled price is fit to market-observed prices.
-   Minimizing a certain risk outcome by optimizing asset allocations.
-   Maximizing risk-adjusted yield in a portfolio.

We will introduce some basic concepts and categories of optimization techniques in this chapter:

-   Objective and Loss Functions
-   Optimization Techniques
    -   Nonlinear Optimization
        -   Gradient Based Techniques
        -   Bracketing Methods
        -   Other Techniques
    -   Linear Optimization

## Objective and Loss Functions

Any optimization algorithm needs to know what is being optimized. We call this function the **objective function**`\index{objective function}`{=latex}. Usually, the objective function is something we want to minimize — though maximizing a function is mathematically equivalent to minimizing its negative. Concretely, if you want to maximize $f(x)$, you can minimize $-f(x)$. The **optimal point** (also known as the $\arg\min$) is simply the input value that produces the smallest objective value.

In many practical situations — especially those with noisy or uncertain data — the function we want to optimize is called a **loss function**`\index{loss function}`{=latex}. This term is common in statistical modeling and machine learning, where the aim is to measure how far a model's predictions deviate from real observations. For instance, the following squared loss function computes the sum of squared errors between a model's predictions and the targets:

```julia
loss(f, guess, target) = sum((f.(guess) .- target).^2)
```

Here, `guess` might represent model parameters (or input values), and `target` is the observed data. Minimizing this loss aligns the model's predictions with reality as closely as possible.

## Optimization Techniques

### Nonlinear Optimization

Nonlinear optimization refers to problems where the relationship between the inputs and outputs is not constrained to be a linear relationship, which is incredibly common in financial modeling.

In many financial settings, variables must obey certain bounds or relationships (e.g., portfolios cannot exceed a total budget, a particular risk measure must remain below a threshold). Handling constraints often involves methods like Lagrange multipliers, projected gradient, or analytic penalty functions. We won't cover these in depth here, but be aware that most general optimization libraries can handle constraints in nonlinear problems as well.

#### Gradient-Based Optimization

Gradient-based optimization`\index{gradient-based optimization}`{=latex} algorithms tend to utilize the gradient (i.e., multivariable derivative) in order to make the optimization substantially more efficient. The gradient is useful because it can tell you if you are at a stationary point (when the derivative is zero, the function may be at a maximum, minimum, or saddle point), but also because algorithms can be smarter about searching for a solution using the additional information. 

The **gradient** provides the direction of the steepest ascent of a function. Optimization algorithms often iteratively update parameters in the direction opposite to the gradient (for minimization problems), which tends to converge towards a local minimum (or maximum for maximization problems). Besides, computing the gradient is often computationally feasible and relatively inexpensive compared to other methods for determining function behavior, such as higher-order derivatives or finite-difference methods. Beyond just the direction, the magnitude (or norm) of the gradient also indicates how steep the function change is in that direction. This information is used to adjust step sizes in optimization algorithms, balancing between convergence speed and stability.

Calculating gradients in the context of computer algorithms is discussed at length in @sec-autodiff, but a quick recap of the available approaches:

-   Finite differences: evaluate the function at nearby points and determine the rates of change associated with each change in direction.
-   Analytic derivatives: A human-derived or computer tool (such as Mathematica) is able to analytically determine a derivative that is coded into the optimization algorithm.
-   Automatic differentiation (AD): elementary rules are applied to decompose elementary code operations into derivatives, allowing for very efficient computation of complex algorithms.

To compare the approaches, here is an example of determining the derivative of a simple function at a certain point:

```{julia}
using Zygote

# Define a differentiable function
f(x) = 3x^2 + 2x + 1
# Define an input value
x = 2.0
h = 1e-3

finite_diff = (f(x + h) - f(x - h)) / 2h
auto_diff = gradient(f, x)[1]
analytic_diff = 6x + 2

println("Value of f(x) at x=", f(x))
println("Gradient of f(x) at x=", finite_diff)
println("Gradient of f(x) at x=", auto_diff)
println("Gradient of f(x) at x=", analytic_diff)
```

1.  `finite_diff` uses finite differences. It needs multiple calls to `f` and is sensitive to the choice of `h` (too small amplifies round-off, too large smears the slope).
2.  `auto_diff` comes from automatic differentiation and matches machine precision for smooth code without symbolic work.
3.  `analytic_diff` is a closed-form derivative. When the algebra is manageable this remains the gold standard, but AD gives similar accuracy with far less manual effort.

##### Root finding

Root finding`\index{root finding}`{=latex}, also known as root approximation or root isolation, is the process of finding the values of the independent variable (usually denoted as $x$) for which a given function equals zero. In mathematical terms, if we have a function $f(x)$, root finding involves finding values of $x$ such that $f(x) = 0$.

There are various algorithms for root finding, each with its own advantages and disadvantages depending on the characteristics of the function and the requirements of the problem. One notable approach is Newton's method`\index{Newton's method}`{=latex}, an iterative method that uses the derivative or gradient of the function to approximate the root with increasing accuracy in each iteration.

We will again use a simple function to illustrate the process:

```{julia}
using Zygote

# Define a differentiable function
f(x) = 2x^2 - 3x + 1
# Define an initial value
x = 0.0
# tolerance of difference in value
tol = 1e-6
# maximum number of iteration of the algorithm
max_iter = 1000
iter = 0
while abs(f(x)) > tol && iter < max_iter
    x -= f(x) / gradient(f, x)[1]
    iter += 1
end
if iter == max_iter
    println("Warning: Maximum number of iterations reached.")
else
    println("Root found after ", iter, " iterations.")
end
print("Approximate root: ", x)
```

::: callout-tip
Newton's method converges quadratically near a simple root, but it can diverge if the initial guess is far away or if the derivative becomes very small. In practice we often bracket the solution first (to guarantee a sign change) and then switch to Newton once we are in a safe neighborhood.
:::

Although it might look different, **root-finding** (i.e. finding $x$ such that $g(x) = 0$) can be cast as a minimization problem by defining an objective function such as $g(x)^2$. In this view, driving $g(x)^2$ to zero compels $g(x)$ itself to be zero, so the methods and algorithmic ideas from minimization apply naturally to root-finding scenarios as well.

##### BFGS

BFGS`\index{BFGS algorithm}`{=latex} — named for Broyden, Fletcher, Goldfarb, and Shanno — is a popular member of the quasi-Newton`\index{quasi-Newton methods}`{=latex} family of optimization algorithms. Although it does require first-order gradient information, BFGS does not need the exact **Hessian** (i.e., second derivatives). Instead, it updates an approximation to the inverse Hessian at each step using the gradients from previous iterations. This extra curvature information allows BFGS to converge much faster than steepest descent in practice. Moreover, because it never explicitly forms the full Hessian matrix, it remains efficient for moderately sized problems. In finance and actuarial settings, BFGS can be especially useful for model calibration or parameter estimation tasks where one needs to handle nonlinear functions relatively quickly but cannot afford the computational overhead of second derivatives.

The following example uses the `Optim` package available in Julia to do the BFGS optimization. We don't illustrate the complete algorithm as it is a bit longer, but wanted to ensure that this workhorse of an algorithm was mentioned.

```{julia}
using Optim

# Define the objective function to minimize
function objective_function(x)
    return sum(abs2, x)
end

# Initial guess for the minimization
initial_x = [1.0]
# Perform optimization using BFGS method
result = optimize(objective_function, initial_x, BFGS())
# Extract the optimized solution
solution = result.minimizer
minimum_value = result.minimum

# Print the result
println("Optimized solution: x = ", solution)
println("Minimum value found: ", minimum_value)
```

#### Gradient-Free Optimization

This category includes algorithms that do not rely on gradients or derivative information. They often explore the objective function using heuristics or other types of probes to guide the search.

##### Bracketing Methods

A bracketed search algorithm is a technique used in optimization and numerical methods to confine or "bracket" a minimum or maximum of a function within a specified interval. The primary goal is to reduce the search space systematically until a satisfactory solution or range containing the optimal value is found.

For 1D minimization with bracketing, golden-section search and Brent's method are standard. The bisection code below is for root finding, not minimization.

```{julia}
function bisection_method(f, a, b; tol=1e-6, max_iter=100)
    """
    Bisection method to find a root of the function f(x) within the interval [a, b].

    Parameters:
    - f: Function to find the root of.
    - a, b: Initial interval [a, b] where the root is expected to be.
    - tol: Tolerance for the root (default is 1e-6).
    - max_iter: Maximum number of iterations allowed (default is 100).

    Returns:
    - root: Approximate root found within the tolerance.
    - iterations: Number of iterations taken to converge.
    """
    fa = f(a)
    fb = f(b)
    if fa * fb > 0
        error("The function values at the endpoints must have opposite signs.")
    end
    iterations = 0
    while (b - a) / 2 > tol && iterations < max_iter
        c = (a + b) / 2
        fc = f(c)
        if fc == 0
            return c, iterations
        end
        if fa * fc < 0
            b = c
            fb = fc
        else
            a = c
            fa = fc
        end
        iterations += 1
    end
    root = (a + b) / 2
    return root, iterations
end

# Define the function we want to find the root of
function f(x)
    return x^3 - 6x^2 + 11x - 6.1
end

# Initial interval [a, b] and tolerance
a = 0.5
b = 10
tolerance = 1e-6
# Apply the bisection method
root, iterations = bisection_method(f, a, b, tol=tolerance)

# Print results
println("Approximate root: ", root)
println("Iterations taken: ", iterations)
println("Function value at root: ", f(root))
```

A popular practical algorithm is called Brent's Method, which uses additional heuristics to accelerate the optimization routine in most cases.

#### Other Non-Gradient Based Optimization Techniques

##### Nelder-Mead simplex method

The Nelder-Mead simplex method`\index{Nelder-Mead method}`{=latex} is a popular optimization algorithm used for minimizing (or maximizing) nonlinear functions that are not necessarily differentiable. It's particularly useful when gradient-based methods cannot be applied. It is often used in low-dimensional problems due to its simplicity and robustness. We will use the Rosenbrock function, which can be useful in certain portfolio optimization problems, to illustrate the process.

```{julia}
using Optim

# Define the Rosenbrock function
rosenbrock(x) = (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2

# Initial guess for (x, y)
initial_guess = [-1.5, 2.0]

# Perform optimization using the Nelder-Mead method
result = optimize(rosenbrock, initial_guess, NelderMead())

# Extract results
optimal_point = Optim.minimizer(result)
minimum_value = Optim.minimum(result)

println("Optimal Point: ", optimal_point)
println("Minimum Value: ", minimum_value)
```

##### Simulated annealing

Simulated Annealing (SA)`\index{simulated annealing}`{=latex} is a probabilistic optimization technique inspired by the annealing process in metallurgy. It is used to find near-optimal solutions to optimization problems, particularly in cases where traditional gradient-based methods may get stuck in local minima/maxima. SA accepts worse solutions with a certain probability, allowing it to explore the search space more broadly initially and then gradually narrow down toward better solutions as it progresses. In this section the Rastrigin function is used to illustrate the process. The function can be useful for asset modeling.

```{julia}
using Random

Random.seed!(1234)

# Parameters
max_iterations = 1000 # Number of iterations
T₀ = 100.0 # Starting temperature
cooling_rate = 0.99 # Cooling rate (temperature multiplier)
bounds = (-5.12, 5.12) # Bounds for the search space
dimension = 5 # Number of dimensions in the search space

# Objective function: Rastrigin function
rastrigin(x) = 10length(x) + sum(xᵢ * xᵢ - 10cos(2π * xᵢ) for xᵢ in x)

# Random initialization within bounds
function initialize_solution()
    bounds[1] .+ (bounds[2] - bounds[1]) .* rand(dimension)
end

# Random perturbation within bounds
function perturb_solution(solution; step=0.1)
    perturbed = solution .+ step .* randn(length(solution))
    return clamp.(perturbed, bounds[1], bounds[2])
end

# Simulated Annealing main function
function simulated_annealing_min()
    cur_sol = initialize_solution()
    cur_f = rastrigin(cur_sol)
    best_sol = copy(cur_sol)
    best_f = cur_f
    cur_t = T₀

    for iteration in 1:max_iterations
        # Generate new candidate solution by perturbation
        cand_sol = perturb_solution(cur_sol)
        cand_f = rastrigin(cand_sol)

        # Acceptance probability (Metropolis criterion)
        ΔE = cand_f - cur_f
        if ΔE < 0 || rand() < exp(-ΔE / max(cur_t, eps()))
            cur_sol = cand_sol
            cur_f = cand_f
        end

        # Update best solution found so far
        if cur_f < best_f
            best_sol = copy(cur_sol)
            best_f = cur_f
        end

        # Decrease temperature
        cur_t *= cooling_rate
        if iteration % 100 == 0
            println("Iter $iteration: Best=$best_f, Temp=$cur_t")
        end
    end

    return best_sol, best_f
end

# Run the simulated annealing algorithm
best_solution, best_value = simulated_annealing_min()
println("Best Solution: ", best_solution)
println("Best Value (Minimum): ", best_value)
```

On this run the algorithm settles near the global minimizer at the origin. In practice we experiment with the number of iterations and cooling rate to balance runtime against solution quality; slower cooling schedules explore more broadly at the expense of speed.

##### Particle swarm optimization (PSO)

Particle swarm optimization (PSO)`\index{particle swarm optimization (PSO)}`{=latex} is a meta-heuristic optimization algorithm inspired by the social behavior of birds flocking or fish schooling. It is used to solve optimization problems by iteratively improving a candidate solution based on the velocity and position of particles (potential solutions) in the search space. The PSO algorithm differs from other methods in a key way, that instead of updating a single candidate solution at each iteration, we update a population (set) of candidate solutions, called a swarm. Each candidate solution in the swarm is called a particle. We think of a swarm as an apparently disorganized population of moving individuals that tend to cluster together while each individual seems to be moving in a random direction. The PSO algorithm aims to mimic the social behavior of animals and insects.

```{julia}
using Random, CairoMakie

# Define the Rosenbrock function
rosenbrock(x) = (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2

# PSO Implementation
function particle_swarm_optimization(
    objective, n_particles, n_iterations, bounds, dim; vmax=0.5)
    # Initialize particles
    span = bounds[2] - bounds[1]
    positions = [bounds[1] .+ span .* rand(dim) for _ in 1:n_particles]
    # small initial velocities
    velocities = [0.1 .* randn(dim) for _ in 1:n_particles]
    personal_best_positions = deepcopy(positions)
    personal_best_scores = [objective(p) for p in positions]
    best_idx = argmin(personal_best_scores)
    global_best_position = personal_best_positions[best_idx]
    global_best_score = minimum(personal_best_scores)

    # PSO parameters
    ω = 0.5            # Inertia weight
    c1, c2 = 2.0, 2.0  # Cognitive and social learning factors

    # Optimization loop
    for iter in 1:n_iterations
        for i in 1:n_particles
            # Update velocity
            r1, r2 = rand(), rand()
            velocities[i] .= clamp.(ω .* velocities[i] .+
                                    c1 .* r1 .* (personal_best_positions[i] .- positions[i]) .+
                                    c2 .* r2 .* (global_best_position .- positions[i]),
                -vmax, vmax)

            # Update position
            positions[i] .+= velocities[i]

            # Clamp positions within bounds
            positions[i] .= clamp.(positions[i], bounds[1], bounds[2])

            # Evaluate fitness
            score = objective(positions[i])
            if score < personal_best_scores[i]
                personal_best_positions[i] = deepcopy(positions[i])
                personal_best_scores[i] = score
            end

            if score < global_best_score
                global_best_position = deepcopy(positions[i])
                global_best_score = score
            end
        end
        if iter % 100 == 0
            println("Iteration $iter: Best Score = $global_best_score")
        end
    end
    return global_best_position, global_best_score
end

# Parameters
n_particles = 30
n_iterations = 100
bounds = (-2.0, 2.0)
dim = 2

# Run PSO
best_position, best_score = particle_swarm_optimization(
    rosenbrock, n_particles, n_iterations, bounds, dim)

println("Best Position: $best_position")
println("Best Score: $best_score")

# Visualization
x = -2.0:0.05:2.0
y = -1.0:0.05:3.0
Z = [(1 - xᵢ)^2 + 100 * (yᵢ - xᵢ^2)^2 for yᵢ in y, xᵢ in x]

# Heatmap and Scatter Plot
fig = Figure()
ax = Axis(fig[1, 1], title="PSO Optimization of Rosenbrock", xlabel="x", ylabel="y")
heatmap!(ax, x, y, Z, colormap=:viridis)
scatter!(ax, [best_position[1]], [best_position[2]],
    color=:red, markersize=10, label="Optimal")
axislegend(ax)
fig
```

The swarm converges close to the valley ridge of the Rosenbrock function. Plotting both the surface and the swarm's best point makes it easy to sanity-check whether particles collapsed onto a plausible solution.

##### Evolutionary Algorithm

An evolutionary algorithm (EA)`\index{evolutionary algorithm}`{=latex} is a family of optimization algorithms inspired by the principles of biological evolution. They are particularly useful for solving complex optimization problems where traditional gradient-based methods may struggle due to nonlinearity, multimodality, or high dimensionality of the search space.

The following shows an example to maximize population fitness in terms of an objective function, with common crossover and mutation processes throughout all generations.

```{julia}
using Random

Random.seed!(1234)

# Parameters
population_size = 50 # Number of individuals in the population
chromosome_length = 5 # Number of genes in each individual (dimensionality)
generations = 100 # Number of generations
mutation_rate = 0.1 # Probability of mutation
crossover_rate = 0.7 # Probability of crossover
bounds = (-5.12, 5.12) # Boundaries for each gene

# Target function: Rastrigin function
rastrigin(x) = 10length(x) + sum(xᵢ * xᵢ - 10cos(2π * xᵢ) for xᵢ in x)

# Initialize population randomly within bounds
function initialize_population()
    span = bounds[2] - bounds[1]
    [bounds[1] .+ span .* rand(chromosome_length)
     for _ in 1:population_size]
end

# Fitness function (negative because we are minimizing)
function fitness(individual)
    return -rastrigin(individual)
end

# Selection: Tournament selection
function tournament_selection(population, fitnesses)
    c1, c2 = rand(1:population_size, 2)
    if fitnesses[c1] > fitnesses[c2]
        return population[c1]
    else
        return population[c2]
    end
end

# Crossover: Single-point crossover
function crossover(parent1, parent2)
    if rand() < crossover_rate
        point = rand(1:chromosome_length)
        child1 = vcat(parent1[1:point], parent2[point+1:end])
        child2 = vcat(parent2[1:point], parent1[point+1:end])
        return child1, child2
    else
        return parent1, parent2
    end
end

# Mutation: Randomly change genes with some probability
function mutate(individual)
    for i in eachindex(individual)
        if rand() < mutation_rate
            individual[i] = bounds[1] + (bounds[2] - bounds[1]) * rand()
        end
    end
    individual
end

# Main Genetic Algorithm loop
function genetic_algorithm()
    population = initialize_population()
    best_individual = nothing
    best_fitness = -Inf

    for gen in 1:generations
        # Evaluate fitness
        fitnesses = [fitness(ind) for ind in population]

        # Find best individual in current population
        current_best = argmax(fitnesses)
        if fitnesses[current_best] > best_fitness
            best_fitness = fitnesses[current_best]
            best_individual = population[current_best]
        end

        # Generate new population
        new_population = Vector{Vector{Float64}}()
        while length(new_population) < population_size
            # Selection
            parent1 = tournament_selection(population, fitnesses)
            parent2 = tournament_selection(population, fitnesses)

            # Crossover
            child1, child2 = crossover(parent1, parent2)

            # Mutation
            child1 = mutate(child1)
            child2 = mutate(child2)

            # Add children to new population
            push!(new_population, child1, child2)
        end
        population = new_population[1:population_size]

        if gen % 100 == 0
            println("Generation $gen: Best Fitness = ", best_fitness)
        end
    end

    return best_individual, -best_fitness
end

# Run the genetic algorithm
best_solution, best_value = genetic_algorithm()
println("Best Solution: ", best_solution)
println("Best Value (Minimum): ", best_value)
```

Despite their simplicity, evolutionary algorithms can navigate rugged search spaces where gradients are unavailable or uninformative. For financial applications they appear in stress-test design and agent-based simulations where payoff surfaces are discontinuous.

##### Bayesian optimization

Bayesian Optimization (BO)`\index{Bayesian optimization}`{=latex} is a powerful technique for global optimization of expensive-to-evaluate black-box functions. It leverages probabilistic models to predict the objective function's behavior across the search space and uses these models to make informed decisions about where to evaluate the function next. This approach efficiently balances exploration (searching for promising regions) and exploitation (exploiting regions likely to yield optimal values), making it particularly suitable for optimization problems where function evaluations are costly, such as optimizing parameters of complex simulations.

```{julia}
using Random
using Surrogates
using CairoMakie

# reproducible seed
Random.seed!(12345)

# Forrester-like, multimodal objective function
f(x) = (6x - 2)^2 * sin(12x - 4)
lb, ub = 0.0, 1.0

# Sobol initialization
n_init = 6
x_init = sample(n_init, lb, ub, SobolSample())
y_init = f.(x_init)

# Kriging surrogate model
krig = Kriging(x_init, y_init, lb, ub; p=1.9)

# Surrogate optimization loop (Expected Improvement)
maxiters = 20
num_new_samples = 200
best_x, best_f = surrogate_optimize(
    f, EI(), lb, ub, krig, SobolSample();
    maxiters=maxiters, num_new_samples=num_new_samples
)

# Plotting
xs = collect(range(lb, ub, length=400))
ys_true = f.(xs)
ys_pred = krig.(xs)

fig = Figure(size=(800, 500))
ax = Axis(
    fig[1, 1],
    title="Surrogate Optimization (Kriging + EI)",
    xlabel="x", ylabel="f(x)"
)
# True function
lines!(ax, xs, ys_true, color=:black, linewidth=2, label="True f(x)")
# Surrogate prediction
lines!(ax, xs, ys_pred, color=:red, linestyle=:dash, linewidth=2,
    label="Kriging surrogate")
# Initial samples
scatter!(ax, x_init, y_init, color=:blue, markersize=10, label="Initial samples")
# Best found point
scatter!(ax, [best_x], [best_f], color=:green, markersize=12, label="Best found")
axislegend(ax, position=:rb)
fig
```

While gradient-free methods help optimize complex nonlinear or noisy functions, some financial and operational problems are best modeled with linear relationships. The next section shows how to set up such problems, including constraints`\index{constrained optimization}`{=latex}, using linear and integer programming.

### Linear optimization

Linear optimization`\index{linear programming}`{=latex}, also known as linear programming (LP), is a mathematical method for finding the best outcome in a mathematical model with linear relationships. It involves optimizing a linear objective function subject to a set of linear equality and inequality constraints. Linear programming has a wide range of applications across various fields, including operations research, economics, engineering, and logistics.

We will use linear optimization to solve the following problem, with $n$ the number of elements in $b$:

$$
\begin{aligned}
\max_{x} \quad & c \cdot x \\
\text{subject to} \quad & x \geq 0 \\
& A_i \cdot x \leq b_i \quad \forall i \in \{1, 2, \ldots, n\}
\end{aligned}
$$

```{julia}
using JuMP, GLPK, LinearAlgebra

# Define the objective coefficients
c = [1.0, 2.0, 3.0]
# Define the constraint matrix (A) and right-hand side (b)
A = [1.0 1.0 0.0;
    0.0 1.0 1.0]
b = [10.0, 20.0]
# Create a JuMP model
linear_model = Model(GLPK.Optimizer)
# Define decision variables
@variable(linear_model, x[1:3] >= 0)
# Define objective function
@objective(linear_model, Max, dot(c, x))
# Add constraints
@constraint(linear_model, constr[i=1:2], dot(A[i, :], x) <= b[i])
# Solve the optimization problem
optimize!(linear_model)

# Print results
println("Objective value: ", objective_value(linear_model))
println("Optimal solution:")
for i in 1:3
    println("\tx[$i] = ", value(x[i]))
end
```

#### Integer programming

Integer Programming (IP)`\index{integer programming}`{=latex} is a type of optimization problem where some or all of the variables are restricted to be integers. Although the problem definition seems similar to an LP, the complexity of solving an IP increases hugely as the solution space is not continuous but discrete.

Let us use IP to solve this problem. A factory produces two types of products $x_1$ and $x_2$ with the following details:

$$
\begin{aligned}
\max_{x} \quad & 40x_1 + 50x_2 \\
\text{subject to} \quad & x_1, x_2 \in \mathbb{Z} \\
& 4x_1 + 3x_2 \leq 200 \quad \text{(labor)} \\
& x_1 + 2x_2 \leq 40 \quad \text{(material)}
\end{aligned}
$$

```{julia}
# Import necessary packages
using JuMP, GLPK, MathOptInterface

# Create a model with the GLPK solver
model = Model(GLPK.Optimizer)

# Define decision variables (x₁ and x₂ are integers)
@variable(model, x₁ >= 0, Int)
@variable(model, x₂ >= 0, Int)

# Define the objective function (maximize profit)
@objective(model, Max, 40 * x₁ + 50 * x₂)

# Add constraints
@constraint(model, 4x₁ + 3x₂ <= 200)  # Labor constraint
@constraint(model, x₁ + 2x₂ <= 40)   # Material constraint

# Solve the model
optimize!(model)

# Check the solution status
if termination_status(model) == MathOptInterface.OPTIMAL
    println("Optimal solution found!")
    println("x₁ (Product x₁ units): ", value(x₁))
    println("x₂ (Product x₂ units): ", value(x₂))
    println("Maximum Profit: ", objective_value(model))
else
    println("No optimal solution found. Status: ", termination_status(model))
end
```

## Example: Model Fitting

In model fitting, the "best fitting curve" refers to the curve or function that best describes the relationship between the independent and dependent variables in the data. The goal of model fitting is to find the parameters of the chosen curve or function that minimize the difference between the observed data points and the values predicted by the model.

The process of finding the best fitting curve typically involves:

-   Choosing a model: Based on the nature of the data and the underlying relationship between the variables, a suitable model or family of models is selected.

-   Estimating parameters: Using the chosen model, one estimates the parameters that best describe the relationship between the variables. This is often done using optimization techniques such as least squares regression, maximum likelihood estimation, or Bayesian inference.

-   Evaluating the fit: Once the parameters are estimated, one evaluates the goodness of fit of the model by comparing the predicted values to the observed data. Common metrics for evaluating fit, or error functions, include the residual sum of squares, the coefficient of determination (R-squared), and visual inspection of the residuals.

-   Iterating if necessary: If the fit is not satisfactory, one may need to iterate on the model or consider alternative models until one finds a satisfactory fit to the data.

While we have shown general approaches like BFGS or gradient-free schemes, libraries such as `LsqFit` wrap these concepts into convenient functions. Under the hood, these packages may employ gradient‐based methods (including automatic differentiation) to refine parameters. Below is a demonstration:

```{julia}
using Random, LsqFit, CairoMakie

x_data = 0:0.1:10
y_data = 2 .* sin.(x_data) .+ 0.5 .* randn(length(x_data))
# Define the model function, using a polynomial function 
# to fit sinusoidal data for illustration purposes
curve_model(x, p) = p[1] * x .^ 5 + p[2] * x .^ 4 .+ p[3] * x .^ 3 .+
                    p[4] * x .^ 2 .+ p[5] * x .+ p[6]
# Initial parameter guess
p₀ = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
# Fit the model to the data
fit_result = curve_fit(curve_model, x_data, y_data, p₀)
# Extract the fitted parameters
params = coef(fit_result)
# Evaluate the model with the fitted parameters
y_fit = curve_model(x_data, params)
# Plot the data and the fitted curve
fig = Figure()
Axis(fig[1, 1],
    title="Curve Fitting - a polynomial guess-fitting a sinusoidal function")
scatter!(x_data, y_data, label="Data")
lines!(x_data, y_fit, label="Fitted Curve", linestyle=:dash, color=:red)
fig
```

The recovered parameters are the coefficients of the degree-5 polynomial. In market calibration these parameters would translate to, for example, seasonal components in demand or cyclical risk drivers in factor models.

## More Resources

The textbook "Algorithms For Optimization" (by Kochenderfer and Wheeler) is a comprehensive introduction to optimization and uses Julia for its examples.

In Julia, [Optimization.jl](https://github.com/SciML/Optimization.jl)`\index{Optimization.jl}`{=latex} provides a unified front-end for all kinds of general optimization problems. [JuMP.jl](https://jump.dev)`\index{JuMP.jl}`{=latex} provides a unified front-end in a specialized optimization mini-domain specific language.
