```{julia}
#| echo: false
#| output: false
using Pkg
Pkg.activate("env/compsci")
Pkg.instantiate()
```

# Elements of Computer Science

> “Fundamentally, computer science is a science of abstraction—creating the right model for a problem and devising the appropriate mechanizable techniques to solve it. Confronted with a problem, we must create an abstraction of that problem that can be represented and manipulated inside a computer. Through these manipulations, we try to find a solution to the original problem.” - Al Aho and Jeff Ullman (1992)

## In this section

Adapting computer science concepts to work for financial professionals. Concepts like computability, computational complexity, the language of algorithms and problem solving, looking for and using patterns, and adopting digital-first practices to automate the boring parts of the job.

## Computer Science for Financial Professionals

Computer science as a term can be a bit misleading because of the overwhelming association with the physical desktop or laptop machines that we call "computers". The discipline of computer science is much richer than consumer electronics: at it's core, computer science concerns itself with areas of reserach and answering tough questions:

-   **Algorithms and Optimization**. How can a problem be solved efficiently? How can that problem be solved *at all*? Given constraints, how can one find an optimal solution?
-   **Information Theory**. Given limited data, what *can* be known or inferred from it?
-   **Theory of Computation**. What sorts of questions are even answerable? Is an answer easy to computer or will resolving it require more resources than the entire known universe? Will a computation ever stop calculating?
-   **Data Structures**. How to encode, store, and use data? How does that data relate to each other and what are the trade-offs between different representations of that data?

For a reader in the twenty-first century we hope that's it's patently obvious how impactful the *applied* computer science has been as an end-user of the internet, artificial intelligence, computational photography, safety control systems, etc., etc. have been to our lives. It is a testament to the utlity of being able to harness some of the ideas of this science is. Many of the most impactful advances occur at the boundary between two disciplines. It's here in this chapter that we desire to bring together the financial discipline together with computer science and to provide the financial practitioner with the language and concepts to leverage some of computer science's most relevant ideas.

In this section, we will refer back to a problem called the travelling salesperson problem (TSP).

## Algorithms & Complexity

**Algorithms** is a general term for a process that transforms an input to an output. It's the dirty, down-to-earn implementation of a mathemetical function or process. Further, we should indicate that a process needs to be specified in sufficient detail to be able to call itself an algorithm versus a heuristic which does not indicate with enough detail how the process would unfold.

### Computational Complexity

We can characterize the computational complexity of a problem by looking at how long an algorithm takes to complete a task when given an input of size $n$. We can then compare two approaches to see which is computationally less complex for a given $n$.

::: callout-warn
Note that computational complexity isn't quite the same as how fast an algorithm will run on your computer, but it's a very good guide. Modern computer architectures can sometimes execute multiple instructions in a single cycle of the CPU making an algorithm that is, on paper, slower than another actually run faster in practice. Additionally, sometimes algorithms are able to substantially limit the number of *computations* to be performed, at the expense of using a lot more *memory* and thereby trading CPU usage with RAM usage.

You can think of computational complexity as a measure of how much work is to be performed. Sometimes the computer is able to perform certain kinds of work more efficiently.

Further, when we analyze an algorithm recall that ultimately our code gets translated into instructions for the computer hardware. Some instructions are implemented in a way that for any type of number (e.g. floating point), it doesn't matter if the number is `1.0` or `0.41582574300044717`, the operation will take the exact same time and number of instructions to execute (e.g. for the addition operation).

Sometimes a higher level operation is implemented in a way that takes many machine instructions. For exmaple, division instructions may require many CPU cycles when compared to multiplication or division. Sometimes this is an important distiction and sometimes not, but for this book we will ignore this level of analysis.
:::

#### Example: Sum of Consecutive Integers

Take for example the problem of determining the sum of integers from $1$ to $n$. We will explore three different algorithms and the associated computational complexity for them.

#### Constant Time

A mathematical proof can show a simple formula for the result. This allows us to compute the answer in **constant time**, which means that for any $n$, our algorithm is essentially the same amount of work.

```{julia}
nsum_constant(n) = n * (n + 1) ÷ 2
```

In this we see that we perform three operations: a mulitiplication, a sum, and a division, no matter what n is. If `n` is `10_000_000` we'd expect this to complete in about a single unit of time.

#### Linear Time

This algorithm performs a number of operations which grows in proportion with $n$ by individually summing up each element in $1$ through $n$:

```{julia}
function nsum_linear(n)
    result = 0
    for i in 1:n
        result += i
    end

    result
end
```

If $n$ were `10_000_000`, we'd expect it to run with roughly 10 million operations, or about 3 million times as many operations as the constant time version. We can say that this version of the alogrithm will take approximately $n$ steps to complete.

#### Quadratic Time

What if we were less efficient, and instead said that the operation $n + 42$ was to be implemented not as the basic addition of two numbers, but that we should *add one to* $n$ forty-two times? That is, we'll see that we add a second loop which increments our result by a unit instead of simply adding the current `i` to the running total `result`:

```{julia}
function nsum_quadratic(n)
    result = 0
    for i in 1:n # <1>
        for j in 1:i # <2>
            result += 1
        end
    end

    result
end
```

1.  The outer loop with iterator `i`.
2.  The inner loop with iterator `j`.

Breaking down the steps:

-   When `i` is `1` there is 1 addition in the inner loop
-   When `i` is `2` there are 2 additions in the inner loop
-   ...
-   When `i` is `n` there are `n` additions in the inner loop

Therefore, this computation takes $1 + ... + (n-2) + (n-1) + n$ steps to complete. We actually know that this simplifies down to our constant time formula $n * (n + 1) ÷ 2$ or $n^2 + n ÷ 2$ steps to complete.

#### Comparison

##### Big-O Notation

We can categorize the above implementations using a convention called **Big-O Notation**[^elements-of-compsci-1] which is a way of distilling and classifying computational complexity. We characterize the algorithms by the most significant term in the total number of operations. @tbl-complexity-sum shows for the examples constructed above what the description, order, and order of magnitude complexity is.

[^elements-of-compsci-1]: "Big-O", so named because of the "O" in used in $O(1)$. $O(n)$, etc. Not one of the sciences' more creative names.

| Function         | Computational Cost | Complexity Description | Big-O Order | Steps ($n=10,000$) |
|---------------|---------------|---------------|---------------|---------------|
| `nsum_constant`  | fixed              | Constant               | $O(1)$      | \~1                |
| `nsum_linear`    | $n$                | Linear                 | $O(n)$      | \~10,000           |
| `nsum_quadratic` | $n^2 + n ÷ 2$      | Quadratic              | $O(n^2)$    | \~100,000,000      |

: Complexity comparison for the three sample cases of summing integers from $1$ to $n$. {#tbl-complexity-sum}

@tbl-complexity-generic shows a comparison of a more extended set of complexity levels. For the most complex categories of problems, the cost to compute grows so fast that it boggles the mind. What sorts of problems fall into the most complex categories? $O(2^n)$, or exponential complexity, examples include the traveling salesman problem if solved with dynamic programming or the recursive approach to calculating the $nth$ Fibbonnaci number. The beastly $O(n!)$ algorithms include brute force solving the traveling salesman problem or enumerating all partitions of a set. In financial modeling, we may encounter these sorts of problems in portfolio optimization (using the brute-force approach of testing every potential combination assets to optimize a portfolio).

| Big-O Order         | $n=10$    | $n=1,000$  | $n=1,000,000$ |
|---------------------|-----------|------------|---------------|
| $O(1)$              | 1         | 1          | 1             |
| $O(n)$              | 10        | 1,000      | 1,000,000     |
| $O(n^2)$            | 100       | 1,000,000  | 10\^12        |
| $O(log(n))$         | 3         | 7          | 14            |
| $O(n\times log(n))$ | 30        | 7,000      | 14,000,000    |
| $O(2^n)$            | 1,024     | \~10\^300  | \~10\^301029  |
| $O(n!)$             | 3,628,800 | \~10\^2567 | \~10\^5565708 |

: Different Big-O Orders of Complexity {#tbl-complexity-generic}

::: callout-note
We care only about the most significant term because when $n$ is large, the most significant term tends to dominate. For example, in our quadratic time example which has $n^2 + n ÷ 2$ steps, if n is a large number like 10 million, then we see that it will result in:

$$
n^2 + n ÷ 2
(10^6)^2 + 10^6 ÷ 2
(10^12) + 5^6 
$$

$10^12$ is *significantly* more important than $5^6$ (sixty-four million times as important, to be precise).

Conversely, if n is small then we don't really care about computational complexity in general. This is why Big-O notation reduces the problem down to only the most significant complexity cost term.
:::

##### Empirical Results

```{julia}
using BenchmarkTools
@btime nsum_constant(10_000)
```

```{julia}
@btime nsum_linear(10_000)
```

```{julia}
@btime nsum_quadratic(10_000)
```

The preceding examples of constant, linear, and exponential times are *conceptually* correct but if we try to run them in practice we see that the description doesn't seem to hold at all for the linear time version, as it runs as quickly as the constant time version.

What happened was that the compiler was able to understand and optimize the linear version such that it effectively transformed it into the constant time version and avoid the iterative summation that we had written. For examples that are simple enough to use as a teaching problem, the compiler can often optimize different written code down to the same efficient machine code.

### Expected versus worst-case complexity

Another consideration is that there may be one approach which performs better in the majority of cases, at the expense of having very poor performance in specific cases. Sometimes we may risk those high cost cases if we expect the benefit to be worthwhile on the rest of the problem set.

### Complexity: Takeaways

The idea of algorithmic complexity is important because it grounds us in the harsh truth that some problems are *very* difficult to compute. It's in these cases that a lot of the creativity and domain specific heuristics can become the foremost consideration. We must remember to be thoughful about the design of our models and when searching for additional performance to look for the loops-within-loops or combinatorical explosions. It's often at this level, rather than micro-optimizations, that you can transform the performance of the overall model (unless the fundamental complexity of the problem at hand forbids it).

## Data Structures

**Data structures** is the art and science of how to represent data in discreate objects. There are



## Formal Verification

## The Discipline of Software Engineering

### Patterns