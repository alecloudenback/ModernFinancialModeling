[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Thinking for Actuaries and Financial Professionals",
    "section": "",
    "text": "Preface\nThis book is intended to enable practitioners and advanced students of financial disciplines to utilize the tools, language, and ideas of computational sciences in their own discipline."
  },
  {
    "objectID": "cover_draft.html",
    "href": "cover_draft.html",
    "title": "1  Draft of Cover",
    "section": "",
    "text": "Draft of Cover"
  },
  {
    "objectID": "intro.html#the-approach",
    "href": "intro.html#the-approach",
    "title": "Introduction",
    "section": "The approach",
    "text": "The approach\nThe authors of the book are practicing actuaries, but we intend for the content to be applicable to nearly all practitioners in the financial industry. The discussion and examples may have an orientation towards insurance topics, but the concepts and patterns are applicable to a wide variety of related disciplines.\nWe will pull from examples on both sides of the balance sheet: the left (assets) and right (liabilities). We may also take the liberty to, at times, abuse traditional accounting notions: a liability is just an asset with the obligor and obligee switched. When the accounting conventions are important (such as modeling a total balance sheet) we will be mindful in explaining the accounting perspective. In practice, this means that we’ll take examples that use examples of assets (fixed income, equity, derivatives) or liabilities (life insurance, annuities, long term care) and show that similar modeling techniques can be used for both."
  },
  {
    "objectID": "intro.html#what-you-will-learn",
    "href": "intro.html#what-you-will-learn",
    "title": "Introduction",
    "section": "What you will learn",
    "text": "What you will learn\nIt is our hope that with the help of this book, you will find it more efficient to discuss aspects of modeling with colleagues, borrow problem solving language from computer science, spot recurring structural patterns in problems that arise, and understand how best to make use of the “bicycle for your mind” in the context of financial modeling.\nIt is the experience of the authors that many professsionals that do complex modeling as a part of their work have gotten to be very proficient in spite of not having substantive formal training on problem solving, algorithms, or model architecture. This book serves to fill that gap and provide the “missing semester” (or “years of practical learning”!). After reading this book, we hope that you will appreciate the attributes of Microsoft Excel that made it so ubiquitous, but that you prefer to use a programming language for the ability to more naturally express the relevant abstractions which make your models simpler, faster, or more usuable by others."
  },
  {
    "objectID": "intro.html#prerequisites",
    "href": "intro.html#prerequisites",
    "title": "Introduction",
    "section": "Prerequisites",
    "text": "Prerequisites\nBasic experience with financial modeling is not strictly required, but it will benefit the reader to be familar so that the examples will not be attempting to teach both financial maths and computer science simultaneously.\nAdvanced financial maths (e.g. stochastic calculus) is not required. Indeed, this book is not oriented to the advanced technicalities of Wall Street “quants” and is instead directed at the multitudes of financial practitioners focused on producing results that are not measured in the microseconds of high-frequency trading.\nPrior programming experience is not required either: Chapter 5 introduces the basic syntax and concepts while Chapter 22 covers setting up your environment to follow along. For readers with background in programming, we recommend skimming Chapter 5 and reading in full the sections which have a  symbol in the margin, which is our way of highlighting Julia-specific content to be aware of.\n\n\n\n\n\n\nTODO\n\n\n\nCreate a venn diagram showing financial modeling at the intersection of statistics, financial math, computer science."
  },
  {
    "objectID": "intro.html#the-contents-of-this-book",
    "href": "intro.html#the-contents-of-this-book",
    "title": "Introduction",
    "section": "The Contents of This Book",
    "text": "The Contents of This Book\nPart 1 of the book addresses the theoretical and technical foundations of programming, as well as the conceptual basis for financial modelling. It familiarizes the readers with key functional programming principles, alongside introducing important aspects of software engineering relevant to financial modelling.\nParts 2 and 3 bridge the gap between theory and practical applications, underlining the features of Julia that make it a robust tool for real-world financial and actuarial contexts. Through a careful exploration of topics like sensitivity analysis, optimization, stochastic modeling, visualization, and practical financial applications, the book demonstrates how Julia’s high-level, high-performance programming capabilities can enhance accuracy and efficiency in financial modelling. As an up-and-coming language loved for its speed and simplicity, Julia is ripe for wide adoption in the financial sector. The time for this book is ripe, as it will satiate the growing demand for professionals who want to blend programming skills with financial modelling acumen.\nWhile we have chosen to use Julia for the examples in this book, the vast majority of the concepts presented are not Julia-specific. We will attempt to motivate why Julia works so well as a language for financial modeling but like mathematics and applied mathematics, the concepts are portable even if the numbers (language) changes. Readers are encouraged to follow along the examples on their own computer (see instructions for Julia in Chapter 22) and the entire book is available on GitHub at [#TODO: determine book URL].\n\nNotes on formatting\nWhen a concept is defined for the first time, the term will be bold. Code, or references to pieces of code will be formatted in inline code style like 1+1 or in separate code blocks:\n\"This is a code block that doesn't show any results\"\n\n\"This is a code block that does show output\"\n\n\"This is a code block that does show output\"\n\n\nThere will be various callout blocks which indiate tips or warnings. These should be self-evident but we wanted to point to a particular callout which is intended to convey advice that stems from practical modeling experience of the authors:\n\n\n\n\n\n\nFinancial Modeling Pro-tip\n\n\n\nThis box indicates a side note that’s particularly applicable to improving your financial modeling."
  },
  {
    "objectID": "why-program.html#in-this-chapter",
    "href": "why-program.html#in-this-chapter",
    "title": "2  Why Program?",
    "section": "2.1 In this Chapter",
    "text": "2.1 In this Chapter\nWe motivate why a financial professional should adopt programming skills which will improve their own capabilities and enjoyment of the discipline, whilst allowing themselves to better themselves and the industry we work in."
  },
  {
    "objectID": "why-program.html#the-long-view",
    "href": "why-program.html#the-long-view",
    "title": "2  Why Program?",
    "section": "2.2 The Long View",
    "text": "2.2 The Long View\nIt might be odd to say that technology and its use in insurance is on a one-hundred-year cycle, but that seems to be the case.\n130 years ago, actuaries crowded into a room at a meeting of the Actuarial Society of America to watch a demonstration that would revolutionize the industry: Herman Hollerith’s tabulating punch card machine1.\nFor the next half-century, the increasing automation — from tabulating machines to early-adopting mainframes and computers — was a critical competitive differentiator. Companies like Prudential, MetLife, and others partnered with technology companies in the development of hardware and software2.\nThe dramatic embodiment of this information-driven cycle was portrayed in the infamous Billion Dollar Bubble movie, which showcased the power and abstraction of the computer to commit millions of dollars of fraud by creating and maintaining fake insurance policies.\nThe movie also starts to hint at the oscillation away from the technological-competitive focus of insurance companies. I argue that the focus on technology was lost over the last 50 years with the rise of Wall Street finance, investment-oriented life insurance, industry consolidation, and the explosion of financial structuring like derivatives, reserve financing, or other advanced forms of reinsurance.\nValue-add came from the C-Suite, not from the underlying business processes, operations, and analysis. The result is, e.g., ever-more complicated reinsurance treaties layered into mainframes and admin systems older than most of the actuaries interfacing with them.\nThe pace of strategic value-add isn’t slowing, though it must stretch further (in complexity and risk) to find comparable opportunities as the past. Having more agile, data-oriented operations enables companies to be able to react to and implement those opportunities. Technological value-add can improve a company’s bottom line through lower expenses and higher top-line growth, but often with a more favorable risk profile than some of the “strategic” opportunities.\nToday, there is a trend reverting back to technological value-creation and is evident across many traditional sectors. Tesla claims that it’s a technology company; Amazon is the #1 product retailer because of its vehement focus on internal information sharing3; Airlines are so dependent on their systems that the skies become quieter on the rare occasion that their computers give way.\nWhy is it, that companies that are so involved in things (cars, shopping) and physical services (flights) are so much more focused on improving their technological operations than insurance companies whose very focus is ‘information-based’? The market has rewarded those who have prioritized their internal technological solutions.\nCommoditized investing services and low yield environments have reduced insurance companies’ comparative advantage to “manage money”. Yield compression and the explosion of consumer-oriented investment services means a more competitive focus on the ability to manage the entire policy lifecycle efficiently (digitally), perform more real-time analysis of experience and risk management, and handle the growing product and regulatory complexity.\nThese are problems that have technological solutions and are waiting for insurance company adoption.\nCompanies that treat data like coordinates on a grid (spreadsheets) will get left behind. Two main hurdles have prevented technology companies from breaking into insurance:\n\nHigh regulatory barriers to entry, and\nDifficulty in selling complex insurance products without traditional distribution.\n\nOnce those two walls are breached, traditional insurance companies without a strong technology core will struggle to keep up. The key to thriving is not just adding “developers” to an organization; it’s going to be getting domain experts like actuaries to be an integral part of the technology transformation."
  },
  {
    "objectID": "why-program.html#whats-coding-got-to-do-with-this",
    "href": "why-program.html#whats-coding-got-to-do-with-this",
    "title": "2  Why Program?",
    "section": "2.3 What’s coding got to do with this?",
    "text": "2.3 What’s coding got to do with this?\nEverything. Programming is the optimal way to interact between the computer and actuary — and importantly between computer and computer. Programming is the actionable expression of ideas, math, analysis, and information. Think of programming as the 21st-century leap in the actuary’s toolkit, just as spreadsheets were in the preceding 40 years. Versus a spreadsheet-oriented workflow:\n\nMore natural automation of, and between processes\nBetter reproducibility\nScaling to fit any size dataset and workload\nStatistics and machine learning capabilities\nAdvanced visualizations to garner new views into your data\n\nThis list isn’t comprehensive and some benefits are subtle — when you are code-oriented instead of spreadsheet-oriented, you tend to want to structure your data in a portable and shareable way. For example, relying more on data warehouses instead of email attachments. This, in turn, enables data discovery and insights that otherwise wouldn’t be there. Investing in a code-oriented workflow is playing the long-game.\nThe actuary of the future needs to have coding as one of their core skills. Already today, the advances of business processes, insurance products, and financial ingenuity are written with lines of code — not spreadsheets. Not being able to code necessarily means that you are following what others are doing today.\nIt’s commonly accepted now that to gather insights from your data, you need to know how to code. Similar to your data, your business architecture, modeling needs, and product peculiarities are often better suited to customized solutions. Why stop at data science when learning how to solve problems with a computer?"
  },
  {
    "objectID": "why-program.html#the-10x-actuary",
    "href": "why-program.html#the-10x-actuary",
    "title": "2  Why Program?",
    "section": "2.4 The 10x Actuary",
    "text": "2.4 The 10x Actuary\nAs we swing back to a technological focus, we do not leave the finance-driven complexity behind. The increasingly complex business needs will highlight a large productivity difference between an actuary who can code and one who can’t — simply because the former can react, create, synthesize, and model faster than the latter. From the efficiency of transforming administration extracts, summarizing and aggregating valuation output, to analyzing claims data in ways that spreadsheets simply can’t handle, you can become a “10x Actuary”4.\nFlipping switches in a graphical user interface versus being able to build models is the difference between having a surface-level familiarity and having full command over the analysis and the concepts involved — with the flexibility to do what your software can’t.\nYour current software might be able to perform the first layer of analysis but be at a loss when you want to visualize, perform sensitivity analysis, statistics, stochastic analysis, or process automation. Things that, when done programmatically, are often just a few lines of additional code.\nDo I advocate dropping the license for your software vendor? No, not yet anyway. But the ability to supplement and break out of the modeling box has been an increasingly important part of most actuaries’ work.\nAdditionally, code-based solutions can leverage the entire-technology sector’s progress to solve problems that are hard otherwise: scalability, data workflows, integration across functional areas, version control and versioning, model change governance, reproducibility, and more.\n30-40 years ago, there were no vendor-supplied modeling solutions and so you had no choice but to build models internally. This shifted with the advent of vendor-supplied modeling solutions. Today, it’s never been better for companies to leverage open source to support their custom modeling, risk analysis/monitoring, and reporting workflows."
  },
  {
    "objectID": "why-program.html#risk-governance",
    "href": "why-program.html#risk-governance",
    "title": "2  Why Program?",
    "section": "2.5 Risk Governance",
    "text": "2.5 Risk Governance\nCode-based workflows are highly conducive to risk governance frameworks as well. If a modern software project has all of the following benefits, then why not a modern insurance product and associated processes?\n\nAccess control and approval processes\nVersion control, version management, and reproducibility\nContinuous testing and validation of results\nOpen and transparent design\nMinimization of manual overrides, intervention, and opportunity for user error\nAutomated trending analysis, system metrics, and summary statistics\nContinuously updated, integrated, and self-generating documentation\nIntegration with other business processes through a formal boundary (e.g. via an API)\nTools to manage collaboration in parallel and in sequence"
  },
  {
    "objectID": "why-program.html#managing-and-leading-the-transformation",
    "href": "why-program.html#managing-and-leading-the-transformation",
    "title": "2  Why Program?",
    "section": "2.6 Managing and Leading the Transformation",
    "text": "2.6 Managing and Leading the Transformation\nThe ability to understand the concepts, capabilities, challenges, and lingo is not a dichotomy, it’s a spectrum. Most actuaries, even at fairly high levels, are still often involved in analytical work. Still above that, it’s difficult to lead something that you don’t understand.\nConversely, the skill and practice of coding enhances managerial capabilities. When you are really skilled at pulling apart a problem or process into its constituent parts and designing optimal solutions; that’s a core attribute of leadership: having the vision of where the organization should be instead of thinking about where it is now.\nNor is the skillset described here limiting in any other aspect of career development any more than mathematical ability, project collaboration, or financial acumen — just to name a few."
  },
  {
    "objectID": "why-program.html#outlook",
    "href": "why-program.html#outlook",
    "title": "2  Why Program?",
    "section": "2.7 Outlook",
    "text": "2.7 Outlook\nIt will increasingly be essential for companies to modernize to remain competitive. That modernization isn’t built with big black-box software packages; it will be with domain experts who can translate the expertise into new forms of analysis - doing it faster and more robustly than the competition.\nSpaceX doesn’t just hire rocket scientists - they hire rocket scientists who code.\nBe an actuary who codes."
  },
  {
    "objectID": "why-program.html#footnotes",
    "href": "why-program.html#footnotes",
    "title": "2  Why Program?",
    "section": "",
    "text": "Co-evolution of Information Processing Technology and Use: Interaction Between the Life Insurance and Tabulating Industries↩︎\nFrom Tabulators to Early Computers in the U.S. Life Insurance Industry↩︎\nHave you had your Bezos moment? What you can learn from Amazon↩︎\nThe 10x [Rockstar] developer is NOT a myth↩︎"
  },
  {
    "objectID": "why-julia.html#expressiveness-and-syntax",
    "href": "why-julia.html#expressiveness-and-syntax",
    "title": "3  Why use Julia?",
    "section": "3.1 Expressiveness and Syntax",
    "text": "3.1 Expressiveness and Syntax\nExpressiveness is the manner in which and scope of ideas and concepts that can be represented in a programming language. Syntax refers to how the code looks on the screen and its readability.\nIn a language with high expressiveness and pleasant syntax, you:\n\nGo from idea in your head to final product faster.\nEncapsulate concepts naturally and write concise functions.\nCompose functions and data naturally.\nFocus on the end-goal instead of fighting the tools.\n\nExpressiveness can be hard to explain, but perhaps two short examples will illustrate.\n\n3.1.1 Example: Retention Analysis\nThis is a really simple example relating Cessions, Policys, and Lives to do simple retention analysis.\nFirst, let’s define our data:\n\n# Define our data structures\nstruct Life\n  policies\nend\n\nstruct Policy\n  face\n  cessions\n end\n\nstruct Cession\n  ceded\nend\nNow to calculate amounts retained. First, let’s say what retention means for a Policy:\n# define retention\nfunction retained(pol::Policy)\n  pol.face - sum(cession.ceded for cession in pol.cessions)\nend\nAnd then what retention means for a Life:\nfunction retained(l::Life)\n  sum(retained(policy) for policy in life.policies)\nend\nIt’s almost exactly how you’d specify it English. No joins, no boilerplate, no fiddling with complicated syntax. You can express ideas and concepts the way that you think of them, not, for example, as a series of dataframe joins or as row/column coordinates on a spreadsheet.\nWe defined retained and adapted it to mean related, but different things depending on the specific context. That is, we didn’t have to define retained_life(...) and retained_pol(...) because Julia can be dispatch based on what you give it. This is, as some would call it, unreasonably effective.\nLet’s use the above code in practice then.\nThe julia&gt; syntax indicates that we’ve moved into Julia’s interactive mode (REPL mode):\n# create two policies with two and one cessions respectively\njulia&gt; pol_1 = Policy( 1000, [ Cession(100), Cession(500)] )\njulia&gt; pol_2 = Policy( 2500, [ Cession(1000) ] )\n\n# create a life, which has the two policies\njulia&gt; life = Life([pol_1, pol_2])\njulia&gt; retained(pol_1)\n400\njulia&gt; retained(life)\n1900\nAnd for the last trick, something called “broadcasting”, which automatically vectorizes any function you write, no need to write loops or create if statements to handle a single vs repeated case:\njulia&gt; retained.(life.policies) # retained amount for each policy\n[400 ,  1500]\n\n\n3.1.2 Example: Random Sampling\nAs another motivating example showcasing multiple dispatch, here’s random sampling in Julia, R, and Python.\nWe generate 100:\n\nUniform random numbers\nStandard normal random numbers\nBernoulli random number\nRandom samples with a given set\n\n\n\n\nTable 3.1: A comparison of random outcome generation in Julia, R, and Python.\n\n\n\n\n\n\n\nJulia\nR\nPython\n\n\n\n\nusing Distributions\n\nrand(100)\nrand(Normal(), 100)\nrand(Bernoulli(0.5), 100)\nrand([\"Preferred\",\"Standard\"], 100)\nrunif(100)\nrnorm(100)\nrbern(100, 0.5)\nsample(c(\"Preferred\",\"Standard\"),\n100, replace=TRUE)\n\nimport scipy.stats as sps\nimport numpy as np\n\n\nsps.uniform.rvs(size=100)\nsps.norm.rvs(size=100)\nsps.bernoulli.rvs(p=0.5,size=100)\nnp.random.choice([\"Preferred\",\"Standard\"],\nsize=100)\n\n\n\n\n\nBy understanding the different types of things passed to rand(), it maintains the same syntax across a variety of different scenarios. We could define rand(Cession) and have it generate a random Cession like we used above."
  },
  {
    "objectID": "why-julia.html#the-speed",
    "href": "why-julia.html#the-speed",
    "title": "3  Why use Julia?",
    "section": "3.2 The Speed",
    "text": "3.2 The Speed\nAs the journal Nature said, “Come for the Syntax, Stay for the Speed”.\nRecall the Solvency II compliance which ran 1000x faster than the prior vendor solution mentioned earlier: what does it mean to be 1000x faster at something? It’s the difference between something taking 10 seconds instead of 3 hours — or 1 hour instead of 42 days.\nWhat analysis would you like to do if it took less time? A stochastic analysis of life-level claims? Machine learning with your experience data? Daily valuation instead of quarterly?\nSpeaking from experience, speed is not just great for production time improvements. During development, it’s really helpful too. When building something, I can see that I messed something up in a couple of seconds instead of 20 minutes. The build, test, fix, iteration cycle goes faster this way.\nAdmittedly, most workflows don’t see a 1000x speedup, but 10x to 1000x is a very common range of speed differences vs R or Python or MATLAB.\nSometimes you will see less of a speed difference; R and Python have already circumvented this and written much core code in low-level languages. This is an example of what’s called the “two-language” problem where the language productive to write in isn’t very fast. For example, more than half of R packages use C/C++/Fortran and core packages in Python like Pandas, PyTorch, NumPy, SciPy, etc. do this too.\nWithin the bounds of the optimized R/Python libraries, you can leverage this work. Extending it can be difficult: what if you have a custom retention management system running on millions of policies every night?\nJulia packages you are using are almost always written in pure Julia: you can see what’s going on, learn from them, or even contribute a package of your own!"
  },
  {
    "objectID": "why-julia.html#more-of-julias-benefits",
    "href": "why-julia.html#more-of-julias-benefits",
    "title": "3  Why use Julia?",
    "section": "3.3 More of Julia’s benefits",
    "text": "3.3 More of Julia’s benefits\nJulia is easy to write, learn, and be productive in:\n\nIt’s free and open-source\n\nVery permissive licenses, facilitating the use in commercial environments (same with most packages)\n\nLarge and growing set of available packages\nWrite how you like because it’s multi-paradigm: vectorizable (R), object-oriented (Python), functional (Lisp), or detail-oriented (C)\nBuilt-in package manager, documentation, and testing-library\nJupyter Notebook support (it’s in the name! Julia-Python-R)\nMany small, nice things that add up:\n\nUnicode characters like α or β\nNice display of arrays\nSimple anonymous function syntax\nWide range of text editor support\nFirst-class support for missing values across the entire language\nLiterate programming support (like R-Markdown)\n\nBuilt-in Dates package that makes working with dates pleasant\nAbility to directly call and use R and Python code/packages with the PyCall and RCall packages\nError messages are helpful and tell you what line the error came from, not just the type of error\nDebugger functionality so you can step through your code line by line\n\nFor power-users, advanced features are easily accessible: parallel programming, broadcasting, types, interfaces, metaprogramming, and more.\nThese are some of the things that make Julia one of the world’s most loved languages on the StackOverflow Developer Survey.\nFor those who are enterprise-minded: in addition to the liberal licensing mentioned above, there are professional products from organizations like Julia Computing that provide hands-on support, training, IT governance solutions, behind-the-firewall package management, and deployment/scaling assistance."
  },
  {
    "objectID": "why-julia.html#the-tradeoff",
    "href": "why-julia.html#the-tradeoff",
    "title": "3  Why use Julia?",
    "section": "3.4 The Tradeoff",
    "text": "3.4 The Tradeoff\nJulia is fast because it’s compiled, unlike R and Python where (loosely speaking) the computer just reads one line at a time. Julia compiles code “just-in-time”: right before you use a function for the first time, it will take a moment to pre-process the code section for the machine. Subsequent calls don’t need to be re-compiled and are very fast.\nA hypothetical example: running 10,000 stochastic projections where Julia needs to precompile but then runs each 10x faster:\n\nJulia runs in 2 minutes: the first projection takes 1 second to compile and run, but each 9,999 remaining projections only take 10ms.\nPython runs in 17 minutes: 100ms of a second for each computation.\n\nTypically, the compilation is very fast (milliseconds), but in the most complicated cases it can be several seconds. One of these is the “time-to-first-plot” issue because it’s the most common one users encounter: super-flexible plotting libraries have a lot of things to pre-compile. So in the case of plotting, it can take several seconds to display the first plot after starting Julia, but then it’s remarkably quick and easy to create an animation of your model results. The time-to-first plot is a solvable problem that’s receiving a lot of attention from the core developers and will get better with future Julia releases.\nFor users working with a lot of data or complex calculations (like actuaries!), the runtime speedup is worth a few seconds at the start."
  },
  {
    "objectID": "why-julia.html#package-ecosystem",
    "href": "why-julia.html#package-ecosystem",
    "title": "3  Why use Julia?",
    "section": "3.5 Package Ecosystem",
    "text": "3.5 Package Ecosystem\nUsing packages as dependencies in your project is assisted by Julia’ bundled package manager.\nFor each project, you can track the exact set of dependencies and replicate the code/process on another machine or another time. In R or Python, dependency management is notoriously difficult and it’s one of the things that the Julia creators wanted to fix from the start.\nPackages can be one of the thousands of publicly available, or private packages hosted internally behind a firewall.\nAnother powerful aspect of the package ecosystem is that due to the language design, packages can be combined/extended in ways that are difficult for other common languages. This means that Julia packages often interop without any additional coordination.\nFor example, packages that operate on data tables work without issue together in Julia. In R/Python, many features tend to come bundled in a giant singular package like Python’s Pandas which has Input/Output, Date manipulation, plotting, resampling, and more. There’s a new Consortium for Python Data API Standards which seeks to harmonize the different packages in Python to make them more consistent (R’s Tidyverse plays a similar role in coordinating their subset of the package ecosystem).\nIn Julia, packages tend to be more plug-and-play. For example, every time you want to load a CSV you might not want to transform the data into a dataframe (maybe you want a matrix or a plot instead). To load data into a dataframe, in Julia the practice is to use both the CSV and DataFrames packages, which help separate concerns. Some users may prefer the Python/R approach of less modular but more all-inclusive packages."
  },
  {
    "objectID": "why-julia.html#conclusion",
    "href": "why-julia.html#conclusion",
    "title": "3  Why use Julia?",
    "section": "3.6 Conclusion",
    "text": "3.6 Conclusion\nLooking at other great tools like R and Python, it can be difficult to summarize a single reason to motivate a switch to Julia, but hopefully this article piqued an interest to try it for your next project.\nThat said, Julia shouldn’t be the only tool in your tool-kit. SQL will remain an important way to interact with databases. R and Python aren’t going anywhere in the short term and will always offer a different perspective on things!\nIn an earlier article, I talked about becoming a 10x Actuary which meant being proficient in the language of computers so that you could build and implement great things. In a large way, the choice of tools and paradigms shape your focus. Productivity is one aspect, expressiveness is another, speed one more. There are many reasons to think about what tools you use and trying out different ones is probably the best way to find what works best for you.\nIt is said that you cannot fully conceptualize something unless your language has a word for it. Similar to spoken language, you may find that breaking out of spreadsheet coordinates (and even a dataframe-centric view of the world) reveals different questions to ask and enables innovated ways to solve problems. In this way, you reward your intellect while building more meaningful and relevant models and analysis."
  },
  {
    "objectID": "why-julia.html#footnotes",
    "href": "why-julia.html#footnotes",
    "title": "3  Why use Julia?",
    "section": "",
    "text": "Python first appeared in 1990. R is an implementation of S, which was created in 1976, though depending on when you want to place the start of an independent R project varies (1993, 1995, and 2000 are alternate dates). The history of these languages is long and substantial changes have occurred since these dates.↩︎\nAviva Case Study↩︎"
  },
  {
    "objectID": "elements-of-financial-modeling.html#in-this-chapter",
    "href": "elements-of-financial-modeling.html#in-this-chapter",
    "title": "4  Elements of Financial Modeling",
    "section": "4.1 In this Chapter",
    "text": "4.1 In this Chapter\nWe explain what constitutes a financial model and what are common uses of a model. We explain what makes an adept practitioner."
  },
  {
    "objectID": "elements-of-financial-modeling.html#what-is-a-model",
    "href": "elements-of-financial-modeling.html#what-is-a-model",
    "title": "4  Elements of Financial Modeling",
    "section": "4.2 What is a model?",
    "text": "4.2 What is a model?\nA model represents aspects of the world around us distilled down into simpler, more tractable components. It is impossible to fully capture the everything that may affect the objects of our interest.\nFor example, say we want to simulate the returns for the stocks in our retirement portfolio. It would be impossible to try to build a model which would capture all of the individual people working jobs and making decisions, weather events that damage property, political machinations, etc. Instead, we try to capture certain fundamental characteristics. For example, it is common to model equity returns as cumulative pluses and minuses from random movements where those movements have certain theoretical or historical characteristics.\nWhether we are using this model of equity returns to estimate available retirement income or replicate an exotic option price, a key aspect of the model is the assumptions used therein. For the retirement income scenario we might assume a healthy eight percent return on stocks and conclude that such a return will be sufficient to retire at age 53. Alternatively, we may assume that future returns will follow a stochastic path with a certain distribution of volatility and drift. These two assumption sets will produce output - results from our model that must be inpsected, questioned, and understood in the context of the “small world” of the model’s mechanistic workings. Lastly, to be effective practitioners we must be able to contextualize the “small world” results withing the “large world” that exists around us.\nMore on the “small world” vs “large world”: say that our model is one that discounts a fixed set of future cashflows using the US Treasury rate curve. If I run my model using current rates today, and then re-run my model tomorrow with the same future cashlows and the present value of those cashflows has increased by 5% I may ask why the result has changed so much in such a short period of time! In the “small”, mechanistic world of the model I may be able to see that the rates I used to discount the cashflows with have fallen substantially. The “small world” answer is that the inputs have changed which produced a mechanical change in the output. The “big world” answer may be that the Federal Reserve lowered the Federal Funds Rate to prevent the economy from entering a deflationary recession. Of course, we can’t completely explain the relation between our model and the real world (otherwise we could capture that relationship in our model!). An effective practitioner will always try to look up from the immediate work and take stock of how the world at large is or is not relfected in the model."
  },
  {
    "objectID": "elements-of-financial-modeling.html#what-is-a-financial-model",
    "href": "elements-of-financial-modeling.html#what-is-a-financial-model",
    "title": "4  Elements of Financial Modeling",
    "section": "4.3 What is a Financial Model?",
    "text": "4.3 What is a Financial Model?\nFinancial models are those used extensively to ascertain better understanding of complex contracts, perform scenario analysis, and inform market participants’ decisions related to perceived value (and therefore price). It can’t be quantified directly, but it is likely not an exaggeration that many billions of dollars is transacted each day as a result of decisions made from the output of financial models.\nMost financial models can be characterized with a focus on the first or both of:\n\nAttempting to project pattern of cashflows or obligations at future timepoints\nReducing the projected obligations into a current value\n\nExamples of this:\n\nProjecting a retiree’s savings through time (1), and determining how much the should be saving today for their retirement goal (2)\nProjecting the obligation of an exotic option across different potential paths (1), and determining the premium for that option (2)\n\nModels are sometimes taken a step further, such as transforming the underlying economic view into an accounting or regulatory view (such as representing associated debits and credits, capital requirements, or associated intangible, capitalized balances).\nWe should also distinguish a financial model from a purely statistical model, where the often the inputs and output data are known and the intention is to estimate relationships between variables (example: linear regressions). That said, a financial model may have statistical components and many aspects of modeling is shared between the two kinds."
  },
  {
    "objectID": "elements-of-financial-modeling.html#the-process-of-building-a-financial-model",
    "href": "elements-of-financial-modeling.html#the-process-of-building-a-financial-model",
    "title": "4  Elements of Financial Modeling",
    "section": "4.4 The Process of Building a Financial Model",
    "text": "4.4 The Process of Building a Financial Model\n\n\n\n\n\n\nTODO: Describe model building process and make associated diagram\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n    Z[Other Inputs] --&gt; D\n    A[Market Data Inputs] --&gt; D\n    D[Model Assumptions] --&gt; B[Model]\n    B --&gt; C[Model Output]\n    C --&gt;|Relates output back to| A"
  },
  {
    "objectID": "elements-of-financial-modeling.html#predictive-versus-explanatory-models",
    "href": "elements-of-financial-modeling.html#predictive-versus-explanatory-models",
    "title": "4  Elements of Financial Modeling",
    "section": "4.5 Predictive versus Explanatory Models",
    "text": "4.5 Predictive versus Explanatory Models\nGiven a set of inputs, our model will generate an output and we are generally interested in its accuracy. The model need not have a realistic mechanism for how the world works. That is, we may primarily be interested in accurately calculating an output value without the model having any scientific, explanatory power of how different parts of the real-world system interact.\n\n4.5.1 A Historical Example\nConsider the classic underdog story where Copernicus overthrew the status quo when he proposed (correctly) that the earth orbited the sun instead of the other way around1.\nThe existing Ptolemic model used a geocentric view of the solar system in which the planets and sun orbited the Earth in perfect circles with an epicycle used to explain retrograde motion (as see in Figure 4.1). Retrograde motion is the term used to describe the apparent, temporarily reversed motion of a planet as viewed from Earth when the Earth is overtaking the other planet in orbit around the sun. This was accurate enough to match the obersvational data that described the position of the planets in the sky.\n\n\n\nFigure 4.1: In the Ptolemic solar model, the retrograde motion of the planets was explained by adding an epicycle to the circular orbit around the earth.\n\n\nFamously, Copernicus came along and said that the sun, not the Earth, should be at the center (a heliocentric model). Earth revolves around the sun! Today, we know this to be a much better description of reality than one in which the Earth arrogantly sits at the center of the universe. However the model was actually slightly less accurate in predicting the apparent position of the planets (to the limits of observational precision at the time)! Why would this be?\nFirst, the Copernican proposal still used perfectly circular orbits with an epicycle adjustment, which we know today to be inaccurate (in favor of an elliptical orbit consistent with the theory of gravity). Despite being more scientifically correct, it was still not the complete picture.\nSecond, the geocentric model was already very accurate because it was essentially a Taylor-series approximation which described to sufficient observational accuracy the apparent position of the planet relative to the Earth. The heliocentric model was effectively a re-parameterization of the orbital approximation.\nThird, we have considered a limited criteria for which we are evaluating the model for accuracy, namely apparent position of the planets. It’s not until we contemplate other observational data that the Copernican model would demonstrate greater modeling accuracy: apparent brightness of the planets as they undergo retrograde motion and angular relationship of the planets to the sun.\nFor modelers today, this demonstrates a few things to keep in mind:\n\nPredictive models need not have a scientific, causal structure to make accurate predictions.\nIt is difficult to capture the complete scientific inter-relationships of a system and much care and thought needs to be given in what aspects are included in our model.\nWe should look at, or seek out, additional data that is related to our model because we may accurately fit (or overfit) to one outcome while achieving an increasingly poor fit to other related variables.\n\nStriving to better understand the world is a good thing to do but trying to include more components into the model is not always going to help achieve our goals.\n\n\n4.5.2 Examples in the Financial Context\n\n4.5.2.1 Home Prices\nAmerican home prices which have a strong degree of seasonality and have the strongest prices around April of each year. We may find that including a simple oscillating term in our model captures the variability in prices better than if we tried to imperfectly capture the true market dynamics of home sales: supply and demand curves varying by personal (job bonus payment timing, school calendars), local (new homes built, company relocation), and national (monetary policy, tax incentives for home-ownership). In other words, one could likely predict a stable pattern like this with a model that contains a simple sinusoidal periodic component. One could likely spend months trying to build a more scientific model and not achieve as good of fit, even though the latter tries to be more conceptually accurate.\n\n\n\n\nFigure 4.2: House prices peak around April of each year.\n\n\n\n\n\n4.5.2.2 Replicating Portfolio\nAnother example in the financial modeling realm: in attempting to value a portfolio of insurance contracts a replicating portfolio of hypothetical assets will sometimes be constructed2. The point of this is to create a basket of assets that can be more quickly (minutes to hours) valued in response to changing market conditions than it would take to run the actuarial model (hours to days). This is an example where the basket of assets has no ability to explain why the projected cashflows are what they are - but retains strong predictive accuracy."
  },
  {
    "objectID": "elements-of-financial-modeling.html#what-makes-a-good-model",
    "href": "elements-of-financial-modeling.html#what-makes-a-good-model",
    "title": "4  Elements of Financial Modeling",
    "section": "4.6 What makes a good model?",
    "text": "4.6 What makes a good model?\nThe answer is: it depends.\n\n4.6.1 Achieving original purpose\nA model is built for a specific set of reasons and therefore we must evaluate a model in terms of achieving that goal. We should not critique a model if we want to use it outside of what it was inteded to do. This includes: contents of output and required level of accuracy.\nA model may have been created to for scenario analysis to value all assets in a portfolio to within half a percent of a more accurate, but much more computationally expensive model. If we try to add a never-before-seen asset class or use the model to order trades we may be extending the design scope of the original model.\n\n\n4.6.2 Usability\nHow easy is it for someone to use? Does it require pages and pages of documentation, weeks of specialized training and an on-call help desk? All else equal, it is an indicator of how usable the model is by the amount of support and training. However, one may sometimes wish to create a highly capable, complex model which is known to require a high amount of experience and expertise. An analogy here might be the cockpit of a small Cessna aircraft versus a fighter jet: the former is a lot simpler and takes less training to master but is also more limited.\nFigure 4.3 illustrates this concept and shows that if your goal is very high capability that you may need to expect to develop training materials and support the more complex model. On this view, a better model is one that is able to have a shorter amount of time and experience to acheive the same level of capability.\n\n\n\n\nFigure 4.3: Tradeoff between complexity and capability\n\n\n\n\n\n4.6.3 Performance\nFinancial models are generally not used for their awe-inspiring beauty - users are results oriented and the faster a model returns the requested results, the better. Aside from direct computational costs such as server runtime, a shorter model runtime means that one can iterate faster, test new ideas on the fly, and stay focused on the problem at hand.\nMany readers may be familar with the cadence of (1) try running model overnight, (2) see results failed in the morning, (3) spend day developing, (4) repeat step 1. It is preferred if this cycle can be measured in minutes instead of hours or days.\nOf course, requirements must be considered here too: needs for high frequency trading, daily portfolio rebalancing, and quarterly valuations are different when it comes to performance."
  },
  {
    "objectID": "elements-of-financial-modeling.html#what-makes-a-good-modeler",
    "href": "elements-of-financial-modeling.html#what-makes-a-good-modeler",
    "title": "4  Elements of Financial Modeling",
    "section": "4.7 What makes a good modeler?",
    "text": "4.7 What makes a good modeler?\nA model is nothing without it’s operator, and a skilled practitioner is worth their weight in gold. What elements separate a good modeler from a mediocre modeler?\n\n4.7.1 Domain Expertise\n\n\n4.7.2 Model Theory\n\n\n4.7.3 Curiosity\n\n\n4.7.4 Rigor\n\n\n4.7.5 Toolset\n… The skills in this book!"
  },
  {
    "objectID": "elements-of-financial-modeling.html#footnotes",
    "href": "elements-of-financial-modeling.html#footnotes",
    "title": "4  Elements of Financial Modeling",
    "section": "",
    "text": "Prof. Richard Fitzpatrick has excellent coverage of the associated mathematics and implications in “A Modern Almagest”: https://farside.ph.utexas.edu/books/Syntaxis/Almagest/Almagest.html↩︎\nSee, e.g., SOA Investment Symposium March 2010. Replicating Portfolios in the Insurance Industry (Curt Burmeister Mike Dorsel Patricia Matson)↩︎"
  },
  {
    "objectID": "foundations-of-programming.html#in-this-section",
    "href": "foundations-of-programming.html#in-this-section",
    "title": "5  Elements of Programming",
    "section": "5.1 In this section",
    "text": "5.1 In this section\nStart building up computer science concepts by introducing tangible programming essentials. Data types, variables, control flow, functions, and scope are introduced."
  },
  {
    "objectID": "foundations-of-programming.html#computer-science-programming-and-coding",
    "href": "foundations-of-programming.html#computer-science-programming-and-coding",
    "title": "5  Elements of Programming",
    "section": "5.2 Computer Science, Programming, and Coding",
    "text": "5.2 Computer Science, Programming, and Coding\nComputer Science is the study of computing and information. As a science, it is distinct from programming languages which are merely coarse implementations of specific computer science concepts1. Programming (or “coding”) is the art and science of writing code in programming languages to have the computer perform desired tasks. While this may sound mechanistic, programming truly is one of the highest forms of abstract thinking and the design space of potential solutions is so large and potentially complex that much art and experience is needed to create a well-made program.\nThe language of computer science also provides a lexicon so that financial practitioners can discuss model architecture and problem characteristics. Having the language to describe a concept will also help see aspects of the problem in new ways, opening one up to more innovative solutions.\nIn the context of this financial modeling that we do, we can consider a financial model to be a type of computer program. It takes as input abstract information (data), performs calculations (an algorithm), and returns new data as an output. In this context, we generally do not need to consider many things that a software engineer may contemplate such as a graphical user interface, networking, or access restrictions. But there are many similarities: a good financial modeler must understand data types, algorithms, and some hardware details.\nWe will build up the concepts over this and the following chapter:\n\nThis chapter will provide a survey of important concepts in computer science that will prove useful for our financial modeling. First, we will talk about data types, boolean logic, and basic expressions. We’ll build on those to discuss algorithms (functions) which perform useful work and use control flow and recursion.\nThe following chapter will step back and discuss higher level concepts: the “schools of thought” around organizing the relationship between data and functions (functional versus object-oriented programming), design patterns, computational complexity, and compilation.\n\n\n\n\n\n\n\nTip\n\n\n\nThere will be brief references to hardware considerations for completeness, but hardware knowledge is not necessary to understand most programming languages (including Julia). It’s impossible to completely avoid talking about hardware when you care about the performance of your code, so feel free to gloss over the reference to hardware details on the first read and come back later after Chapter 8.\n\n\nIt’s highly recommended that you follow along and have a Julia session open (e.g. a REPL or a notebook) when first going through this chapter. See Chapter 22 if you haven’t gotten that set up yet. Follow along with the examples as we go.\n\n\n\n\n\n\nTip\n\n\n\nYou can get some help in the REPL by typing a ? followed by the symbol you want help with, for example:\n help?&gt; sum\nsearch: sum sum! summary cumsum cumsum! isnumeric MethodSummary VersionNumber issubnormal get_zero_subnormals\n\n  sum(f, itr; [init])\n\n\n  Sum the results of calling function f on each element of itr.\n\n... More text truncated..."
  },
  {
    "objectID": "foundations-of-programming.html#assignment-and-variables",
    "href": "foundations-of-programming.html#assignment-and-variables",
    "title": "5  Elements of Programming",
    "section": "5.3 Assignment and Variables",
    "text": "5.3 Assignment and Variables\nOne of the first things it will be convenient to understand is the concept of variables. In virtually every programming language, we can assign values to make our program more organized and meaningful to the human reader. In the following example, we assign values to intermediate symbols to benefit us humans as we convert (silly!) American distance units:\n\nfeet_per_yard = 3\nyards_per_mile = 1760\n\nfeet = 3000\nmiles = feet / feet_per_yard / yards_per_mile\n\n0.5681818181818182\n\n\nBeyond readability, variables are a form of abstraction which allows us to think beyond specific instances of data and numbers to a more general representation. For example, the last line in the prior code example is a very generic computation of a unit conversion relationship and feet could be any number and the expression remains a valid calculation.\nWe will return to this subject in more detail in (ref-assignment?)."
  },
  {
    "objectID": "foundations-of-programming.html#data-types",
    "href": "foundations-of-programming.html#data-types",
    "title": "5  Elements of Programming",
    "section": "5.4 Data Types",
    "text": "5.4 Data Types\nData types are a way of categorizing information by intrinsic characteristics. We instinctively know that 13.24 is different than \"this set of words\" and types are how we will formalize this distinction. This is a key conceptual point, and mathematically it’s like we have different sets of objects to perform specialized operations on. Beyond this set-like abstraction is implementation details related to computer hardware. You probably know that computers only natively “speak” in binary zeros and ones. Data types are a primary way that a computer can understand if it should interpret 01000010 as B or as 662.\nEach 0 or 1 within a computer is called a bit and eight bits in a row form a byte (such as 01000010 ). This is where we get terms like “gigabytes” or “kilobits per second” as a measure of the quantity or rate of bits something can handle3.\n\n5.4.1 Numbers\nNumbers are usually grouped into two categories: integers and floating-point4 numbers. Integers are like the mathematical set of integers while floating-point is a way of representing decimal numbers. Both have some limitations since computers can only natively represent a finite set of numbers due to the hardware (more on this in Chapter 8). Here are three integers that are input into the REPL (Read-Eval-Print-Loop)5 and the result is printed below the input:\n\n2\n\n2\n\n\n\n423\n\n423\n\n\n\n1929234\n\n1929234\n\n\nAnd three floating-point numbers:\n\n0.2\n\n0.2\n\n\n\n-23.3421\n\n-23.3421\n\n\n\n14e3      # the same as 14,000.0\n\n14000.0\n\n\nOn most systems, 0.2 will be interpreted as a 64-bit floating point type called Float64 in Julia since most architectures these days are 64-bit6, while on a 32-bit system 0.2 would be interpreted as a Float32. Given that there are a finite amount of bits attempting to represent a continuous, infinite set of numbers means that some numbers are not able to be represented with perfect precision. For example, if we ask for 0.2, the closest representations in 64 and 32 bit are:\n\n0.20000000298023223876953125 in 32-bit\n0.200000000000000011102230246251565404236316680908203125 in 64-bit\n\nThis leads to special considerations that computers take when performing calculations on floating point maths, some of which will be covered in more detail in Chapter 8. For now, just note that floating point numbers have limited precision and even if we input 0.2, your computations will use the above decimal representations even if it will print out a number with fewer digits shown:\n\n1x = 0.2\n\n2big(x)\n\n\n1\n\nHere, we assign the value 0.2 to a variable x. More on variables/assignments in Section 5.5.3.\n\n2\n\nbig(x) is a arbitrary precision floating point number and by default prints the full precision that was embedded in our variable x, which was originally Float64.\n\n\n\n\n0.200000000000000011102230246251565404236316680908203125\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote the difference in what printed between the last example and when we input 0.2 earlier in the chapter. The former had the same (not-exactly equal to \\(0.2\\)) value, but it printed an abbreviated set of digits as a nicety for the user, who usally doesn’t want to look at floating point numbers with their full machine precision. The system has the full precision (0.20...3125) but is truncating the ouput.\nIn the last example, we’ve converted the normal Float64 to a BigFloat which will not truncate the output when printing.\n\n\nIntegers are similarly represented as 32 or 64 bits (with Int32 and Int64) and are limited to exact precision:\n\n-32,767 to 32,767 for Int32\n-2,147,483,647 to 2,147,483,647 for Int64\n\nAdditional range in the positive direction if one chooses to use “unsigned”, non-negative numbers (UInt32 and UInt64). Unlike floating point numbers, the integers have a type Int which will use the system bit architecture by default (that is, Int(30) will create a 64 bit integer on 64-bit systems and 32-bit on 32-bit systems)\n\n\n5.4.2 Type Hierarchy\nWe can describe a hierarchy of types. Both Float64 and Int64 are examples of Real numbers (here, Real is an abstract Julia type which corresponds to the mathematical set of real numbers commonly denoted with \\(\\mathbb{R}\\) ). Both Float64 and Int32 are Real numbers, so why not just define all numbers as a Real type? Because for performant calculations, the computer must know in advance how many bits each number is represented with.\nFigure 5.1 shows the type hiearchy for most built-in Julia number types.\n\n\n\n\n\ngraph TD\n    Number --&gt; Real\n    Number --&gt; Complex\n\n    Real --&gt; Integer\n    Real --&gt; AbstractFloat\n    Real --&gt; Rational\n    Real --&gt; Irrational\n\n    Integer --&gt; Signed\n    Integer --&gt; Unsigned\n\n    Signed --&gt; Int8\n    Signed --&gt; Int16\n    Signed --&gt; Int32\n    Signed --&gt; Int64\n    Signed --&gt; Int128\n    Signed --&gt; BigInt\n\n    Unsigned --&gt; UInt8\n    Unsigned --&gt; UInt16\n    Unsigned --&gt; UInt32\n    Unsigned --&gt; UInt64\n    Unsigned --&gt; UInt128\n\n    AbstractFloat --&gt; Float16\n    AbstractFloat --&gt; Float32\n    AbstractFloat --&gt; Float64\n    AbstractFloat --&gt; BigFloat\n\n\nFigure 5.1: Numeric Type Hierarchy in Julia. Leafs of the tree are concrete types.\n\n\n\n\nThe integer and floating point types described in the prior section are known as concrete types because there are no possible sub types (child types). Further, a concrete type can be a bit type if the data type will always have the same number of bits in memory: a Float32 will always be 32 bits in memory, for example. Contrast this with strings (described below) which can contain an arbitrary number of characters.\n\n\n5.4.3 Arrays\n\n\n Julia has very powerful and friendly array types.\nArrays are the most common way to represent a collection of similar data. For example, we can represent a set of integers as follows:\n\n[1, 10, 300]\n\n3-element Vector{Int64}:\n   1\n  10\n 300\n\n\nAnd a floating point array:\n\n[0.2, 1.3, 300.0]\n\n3-element Vector{Float64}:\n   0.2\n   1.3\n 300.0\n\n\nNote the above two arrays are different types of arrays. The first is Vector{Int64} and the second is Vector{Float64}. These are arrays of concrete types and so Julia will know that each element of an array is the same amount of bits which will enable more efficient computations. With the following set of mixed numbers, Julia will promote the integers to floating point since the integers can be accurately represented7 in floating point.\n\n[1, 1.3, 300.0, 21]\n\n4-element Vector{Float64}:\n   1.0\n   1.3\n 300.0\n  21.0\n\n\nHowever, if we explicitly ask Julia to use a Real-typed array, the type is now Vector{Real}. Recall that Real is an abstract type. Having heterogeneous types within the array is conceptually fine, but in practice limits performance. Again, this will be covered in more detail in Chapter 8.\nIn Julia, arrays can be multi-dimensional. Here are are two three-dimensional arrays with length three in each dimension:\n\nrand(3, 3, 3)\n\n3×3×3 Array{Float64, 3}:\n[:, :, 1] =\n 0.520398    0.909055  0.951176\n 0.00230388  0.41053   0.77508\n 0.810103    0.621812  0.533337\n\n[:, :, 2] =\n 0.231323  0.843246  0.137858\n 0.508538  0.313074  0.311081\n 0.995119  0.363183  0.657149\n\n[:, :, 3] =\n 0.741014  0.639419  0.310672\n 0.308226  0.949283  0.896643\n 0.631243  0.463659  0.999873\n\n\n\n[x + y + z for x in 1:3, y in 11:13, z in 21:23]\n\n3×3×3 Array{Int64, 3}:\n[:, :, 1] =\n 33  34  35\n 34  35  36\n 35  36  37\n\n[:, :, 2] =\n 34  35  36\n 35  36  37\n 36  37  38\n\n[:, :, 3] =\n 35  36  37\n 36  37  38\n 37  38  39\n\n\nThe above example demonstrates array comprehension syntax which is a convienient way to create arrays in Julia.\nA two-dimensional array has the rows by semi-colons (;):\n\nx = [1 2 3; 4 5 6]\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn Julia, a Vector{Float64} is simply a one-dimensional array of floating pointsand a Matrix{Float64} is a two-dimentional array. More precisely, they are type aliases of the more generic Array{Float64,1} and Array{Float64,2} names.\n\n\n\n5.4.3.1 Array indexing\nArray elements are accessed with the integer position, starting at 1 for the first element8 9:\n\nv = [10, 20, 30, 40, 50]\nv[2]\n\n20\n\n\nWe can also access a subset of the vector’s contents by passing a range:\n\nv[2:4]\n\n3-element Vector{Int64}:\n 20\n 30\n 40\n\n\nAnd we can generically reference the array’s contents, such as:\n\nv[begin+1:end-1]\n\n3-element Vector{Int64}:\n 20\n 30\n 40\n\n\nWe can assign values into the array as well, as well as combine arrays and push new elements to the end:\n\nv[2] = -1\npush!(v, 5)\nvcat(v, [1, 2, 3])\n\n9-element Vector{Int64}:\n 10\n -1\n 30\n 40\n 50\n  5\n  1\n  2\n  3\n\n\n\n\n\n5.4.4 Characters, Strings, and Symbols\nCharacters are represented in most programming languages as letters within quotation marks. In Julia, individual characters are represented using single quotes:\n\n'a'\n\n'a': ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\n\n\nLetters and other characters present more difficulties than numbers to represent within a computer (think of how many languages and alphabets exist!), and it essentially only works because the world at large has agreed to a given representation. Originally ASCII (American Standard Code for Information Interchange) was used to represent just 95 of the most common English characters (“a” through “z”, zero through nine, etc.). Now, UTF (Unicode Transformation Format) can encode more than a million characters and symbols from many human languages.\nStrings are a collection10 of characters, and can be created in Julia with double quotes:\n\n\"hello world\"\n\n\"hello world\"\n\n\nIt’s easy to ascertain how ‘normal’ characters can be inserted into a string, but what about things like new lines or tabs? They are represented by their own characters but are normally not printed in computer output. However, those otherwise invisible characters do exist. For example, here we will use a string literal (indicated by the \"\"\" ) to tell Julia to interpret the string as given, including the invisible new line created by hitting return on the keyboard between the two words:\n\n\"\"\"\nhello\nworld\n\"\"\"\n\n\"hello\\nworld\\n\"\n\n\nThe output above shows the \\n character contained within the string.\nSymbols are a way of representing an identifier which cannot be seen as a collection of individual characters. :helloworld is distinct from \"helloworld\" - you can kind of think of the former as an un-executed bit of code - if we were to execute it (with eval(:helloworld)), we would get an error UndefVarError: `a` not defined . Symbols can look like strings but do not behave like them. For now, it is best to not worry about symbols but it is an important aspect of Julia which allows the language to represent aspects of itself as data. This allows for powerful self-reference and self-modification of code but this is a more advanced topic generally out of scope of this book.\n\n\n5.4.5 Tuples\nTuples are a set of values that belong together and are denoted by a values inside parenthesis and separated by a comma. An example might be x-y coordinates in 2 dimensional space:\n\nx = 3\ny = 4\np1 = (x, y)\n\n(3, 4)\n\n\nTuple’s values can be accessed like arrays:\n\np1[1]\n\n3\n\n\nTuples fill a middle ground between scalar types and arrays in more ways that one:\n\nTuples have no problem having heterogeneous types in the different slots.\nTuples are immutable, meaning that you cannot overwrite the value in memory (an error will be thrown if we try to do p[1] = 5).\nIt’s generally expected that within an array, you would be able to apply the same operation to all the elements (e.g. square each element) or do something like sum all of the elements together which isn’t generally case for a tuple.\nTuples are generally stack allocated instead of being heap allocated like arrays11, meaning that a lot of times they can be faster than arrays.\n\n\n5.4.5.1 Named Tuples\nNamed tuples provide a way to give each field within the tuple a specific name. For example, our x-y coordinate example above could become:\n\np2 = (x=3, y=4)\n\n(x = 3, y = 4)\n\n\nThe benefit is that we can give more meaning to each field and access the values in a nicer way. Previously, we used location[1] to access the x-value, but with the new definition we can access it by name:\n\np2.x\n\n3\n\n\n\n\n\n5.4.6 Parametric Types\nWe just saw how tuples can contain heterogeneous types of data inside a common container. Let’s look at this a little bit closer by looking at the full type:\n\ntypeof(p1)\n\nTuple{Int64, Int64}\n\n\nlocation is a Tuple{Int64,Int64} type, which means that its first and second elements are both Int64. Contrast this with:\n\ntypeof((\"hello\", 1.0))\n\nTuple{String, Float64}\n\n\nThese tuples are both of the form Tuple{T,U} where T and U are both types. Why does this matter? We and the compiler can distinguish between a Tuple{Int64,Int64} and a Tuple{String,Float64} which allows us to reason about things (“I can add the first element of tuple together only if both are numbers”) and the compiler to optimize (sometimes it can know exactly how many bits in memory a tuple of a certain kind will need and be more efficient about memory use). Further, we will see how this can become a powerful force in writing appropriately abstracted code and more logically organize our entire program when we encounter “multiple dispatch” later on.\n\n\n5.4.7 Creating User Defined Types\nWe’ve talked about some built-in types but so much additional capabilities come from being able to define our own types. For example, taking the x-y-coordinate example from above, we could do the following instead of defining a tuple:\n\nstruct BasicPoint\n    x::Int64\n    y::Int64\nend\n\np3 = BasicPoint(3, 4)\n\nBasicPoint(3, 4)\n\n\nFields are accessed the same way as named tuples:\n\n1p3.x, p3.y\n\n\n1\n\nNote that here, Julia will return a tuple instead of a single value due to the comma separated expressions.\n\n\n\n\n(3, 4)\n\n\nstructs in Julia are immutable like tuples above.\nBut wait, didn’t tuples let us mix types too via parametric types? Yes, and we can do the same with our type!\n\nstruct Point{T}\n    x::T\n    y::T\nend\n\nLine 1 The {T} after the type’s name allows for different Points to be created depending on what the type of the underlying x and y is.\nHere’s two new points which now have different types:\n\np4 = Point(1, 4)\np5 = Point(2.0, 3.0)\n\np4, p5\n\n(Point{Int64}(1, 4), Point{Float64}(2.0, 3.0))\n\n\nNote that the types are not equal because they have different type parameters!\n\ntypeof(p4), typeof(p5), typeof(p4) == typeof(p5)\n\n(Point{Int64}, Point{Float64}, false)\n\n\nBut both are now subtypes of PPoint2D. The expression X isa Y is true when X is a (sub)type of Y:\n\np4 isa Point, p5 isa Point\n\n(true, true)\n\n\nNote though, that the x and y are both of the same type in each PPoint2D that we created. If instead we wanted to allow the coordinates to be of different types, then we could have defined PPoint2D as follows:\nstruct Point{T,U}\n    x::T\n    y::U\nend\n\n\n\n\n\n\nNote\n\n\n\nCan we define the structs above without indicating a (parametric) type? Yes!\nstruct Point\n    x # no type here!\n    y # no type declared here either!\nend\nBut! x and y will both be allowed to be Any, which is the fallback type where Julia says that it doesn’t know any more about the type until runtime (the time at which our program encounters the data when running). This means that the compiler (and us!) can’t reason about or optimize the code as effectively as when the types are explicit or parametric. This is an example of how Julia can provide a nice learning curve - don’t worry about the types until you start to get more sophicistited about the program design or need to extract more performance from the code.\n\n\nThe above structs that we have defined are examples of concrete types types which hold data. Abstract types don’t directly hold data themselves but are used to define a hiearchy of types which we will later exploit (Chapter 6) to implement custom behavior depending on what type our data is.\nHere’s an example of (1) defining a set of related types that sits above our Point2D:\n\nabstract type Coordinate end\nabstract type CartesianCoordinate &lt;: Coordinate end\nabstract type PolarCoordinate &lt;: Coordinate end\n\nstruct Point2D{T} &lt;: CartesianCoordinate\n    x::T\n    y::T\nend\n\nstruct Point3D{T} &lt;: CartesianCoordinate\n    x::T\n    y::T\n    z::T\nend\n\nstruct Polar2D{T} &lt;: PolarCoordinate\n    r::T\n    θ::T\nend\n\n\n\n\n\n\n\nUnicode Characters\n\n\n\nJulia has wonderful Unicode support, meaning that it’s not a problem to include characters like θ. The character can be typed in Julia editors by entering \\theta and then pressing the TAB key on the keyboard.\nUnicode is helpful for following conventions that you may be used to in math. For example, the math formula \\(\\text{circumference}(r) = 2 \\times r \\times \\pi\\) can be written in Julia with circumference(r) = 2 * r * π.\nThe name for the characters follows the same for LaTeX, so you can search the internet for,e.g. “theta LaTeX” to find the appropriate name. Furhter, you can use the REPL help mode to find out how to enter a character if you can copy and paste it from somewhere:\nhelp?&gt; θ\n\"θ\" can be typed by \\theta&lt;tab&gt;\n\n\n\n\n5.4.8 Mutable structs\nIt is possible to define structs where the data can be modified - such a data field is said to be mutable because it can be changed or mutated. Here’s an example of what it would look like if we made Point2D mutable:\nmutable struct Point2D{T}\n    x::T\n    y::T\nend\nYou may find that this more naturally represents what you are trying to do. However, recall that an advantage of an immutable datatype is that costly memory doesn’t necessarily have to be allocated for it. So you may think that you’re being more efficient by re-using the same object… but it may not actually be faster. Again, more will be revealed in Chapter 8.\n\n\n\n\n\n\nFinancial Modeling Pro-tip\n\n\n\nGenerally you should default to using immutable types and consider only moving to mutable types in specific circumstances. You’ll see some examples in the applications later in the book."
  },
  {
    "objectID": "foundations-of-programming.html#expressions-and-control-flow",
    "href": "foundations-of-programming.html#expressions-and-control-flow",
    "title": "5  Elements of Programming",
    "section": "5.5 Expressions and Control Flow",
    "text": "5.5 Expressions and Control Flow\nHaving already seen some more illustrative examples above, we can zoom in onto smaller pieces called expressions which are effectively the basic block of code that gets evaluated. Here is an expression that adds two integers together that evaluate to a new integer (3 in this case):\n\n1 + 2\n\n3\n\n\n\n5.5.1 Compound Expression\nThere’s two kinds of blocks where we can ensure that subexpressions get evaluated in order and return the last expression as the overall return value: begin and let blocks.\n\nc = begin\n    a = 3\n    b = 4\n    a + b\nend\n\na, b, c\n\n(3, 4, 7)\n\n\nThe variables inside the begin block are evaluated in the same scope as c and therefore have the assigned values when we call a and b in the last line. Contrast that with the let block below, where d and e are not available when we try to get the value of f. This is because let creates a new inner scope that’s not available in f’s scope. More on scope later in the chapter.\n\nf = let\n    d = 1\n    e = 2\n    d + e\nend\nf\n\n3\n\n\n\nd\n\nLoadError: UndefVarError: `d` not defined\n\n\n\n\n5.5.2 Conditional Expressions\nConditionals are expressions that evaluate to a boolean true or false. This is the beginning of really being able to assemble complex logic to perform useful work. Here are a handful expressions that would evaluate to true:\n1 &gt; 0\n1 == 1\nFloat64 isa Rational\n(5 &gt; 0) & (-1 &lt; 2) # \"and\" expression\n(5 &gt; 0) | (-1 &gt; 2) # \"or\" expression\n1 != 2\n\n5.5.2.1 Equality\nThe “Ship of Theseus12” problem is an example of how equality can be philosophically complex concept. In computer science we have the advantage that while we may not be able to resolve what’s the “right” type of equality, we can be more precise about it.\nHere is an example for which we can see the difference between two types of equality:\n\nEgal equality is when a program could not distinguish between two objects at all\nEqual equality is when the values of two objects are the same\n\nIf two things are egal, then they are also equal.\nIn the following example, s and t are equal but not egal:\n\ns = [1, 2, 3]\nt = [1, 2, 3]\ns == t, s === t\n\n(true, false)\n\n\nOne way to think about this is that while the values are equal, there is a way that one of the arrays could be made not equal to the other:\n\nt[2] = 5\nt\n\n3-element Vector{Int64}:\n 1\n 5\n 3\n\n\nNow t is no longer equal to s:\n\ns == t\n\nfalse\n\n\nRecall that arrays are able to be modified, but other types like tuples are immutable. Immutable types with the same value are egal because there is no way for us to make them different:\n\n(2, 4) === (2, 4)\n\ntrue\n\n\nUsing this terminology, we could now interpret the “Ship of Theseus” as that his ship is “equal” but not “egal”.\n\n\n\n5.5.3 More on Assignment and Variables\n\n\n5.5.4 Operators\nunary/binary"
  },
  {
    "objectID": "foundations-of-programming.html#functions",
    "href": "foundations-of-programming.html#functions",
    "title": "5  Elements of Programming",
    "section": "5.6 Functions",
    "text": "5.6 Functions\n\n5.6.1 Broadcasting"
  },
  {
    "objectID": "foundations-of-programming.html#scope",
    "href": "foundations-of-programming.html#scope",
    "title": "5  Elements of Programming",
    "section": "5.7 Scope",
    "text": "5.7 Scope\n\n5.7.1 Local Scope\n\n\n5.7.2 Global Scope\n\n\n5.7.3 Modules\n\n\n\n\n\n\nNote\n\n\n\nSummarizing related terminology:\n\nA module is a block of code such as module MySimulation ... end\nA package is a module that has a specific set of files and associated metadata. Essentially, it’s a module with a Project.toml file that has a name and unique identifier listed, and a file in a src/ directory called MySimulation.jl\n\nLibrary is just another name for a package, and the most common context this comes up is when talking about the packages that are bundled with Julia itself called the standard library (stdlib)."
  },
  {
    "objectID": "foundations-of-programming.html#footnotes",
    "href": "foundations-of-programming.html#footnotes",
    "title": "5  Elements of Programming",
    "section": "",
    "text": "Said differently, computer science may contemplate ideas and abstractions more generally than a specific implementation, as in mathematics where a theorem may be proved (\\(a^2 + b^2 = c^2\\)) without resorting to specific numeric examples (\\(3^2 + 4^2 = 5^2\\)).↩︎\nThis binary representations correspond to B and 66 with the ASCII character set and 8-bit integer encodings, discussed later in this chapter.↩︎\nSome distinctions you may encounter: in short-form, “kb” means kilobits while the upper-case “B” in “kB” means kilobytes. Also confusingly, sometimes the “k” can be binary or decimal - because computers speak in binary, a binary “k” means 1024 (equal to 2^10) instead of the usual decimal 1000. In most computer contexts, the binary (multiples of 1024) is more common.↩︎\nThe term floating point refers to the fact that the number’s radix (decimal) point can “float” between the significant digits of the number.↩︎\nThat is, it reads the code input from the user, evaluates what code was given to it, prints the result of the input to the screen, and loops through the process again.↩︎\nThis means that their central processing units (CPUs) use instructions that are 64 bits long.↩︎\nAccurate only to a limited precision, as described in Section 5.4.1.↩︎\nWhether an index starts at 1 or 0 is sometimes debated. Zero-based indexing is natural in the context of low-level programming which deal with bits and positional offsets in computer memory. For higher level programming one-based indexing is more natural: in a set of data stored in an array, it is much more natural to reference the first (through \\(n^{th}\\)) datum instead of the zeroth (through \\((n-1)^{th}\\) datum.↩︎\nArrays in Julia can actually be indexed with an arbitrary starting point: see the package OffsetArrays.jl↩︎\nUnder the hood, strings are essentially a vector of characters but there are complexities with character encoding that don’t allow a lossless conversion to individual characters of uniform bit length. This is for historical compatibility reasons and to avoid making most documents’ file sizes larger than it needs to be.↩︎\nWhat this means will be explained in Chapter 8 .↩︎\nThe Ship of Theseus problem specifically refers to a legendary ancient Greek ship, owned by the hero Theseus. The paradox arises from the scenario where, over time, each wooden part of the ship is replaced with identical materials, leading to the question of whether the fully restored ship is still the same ship as the original. The Ship of Theseus problem is a thought experiment in philosophy that explores the nature of identity and change. It questions whether an object that has had all of its components replaced remains fundamentally the same object.↩︎"
  },
  {
    "objectID": "patterns-abstraction.html#in-this-section",
    "href": "patterns-abstraction.html#in-this-section",
    "title": "6  Patterns of Abstraction",
    "section": "6.1 In this section",
    "text": "6.1 In this section\nWe extend the building blocks from the prior section and talk about how to combine them into more abstract patterns to simplify the design of our programs. Data driven design, object oriented design versus composition, multiple dispatch, and interfaces."
  },
  {
    "objectID": "patterns-abstraction.html#introduction",
    "href": "patterns-abstraction.html#introduction",
    "title": "6  Patterns of Abstraction",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nAbstraction is a selective ignorance—focusing on the aspects of the problem that are relevant, and ignoring the others."
  },
  {
    "objectID": "elements-of-compsci.html#in-this-section",
    "href": "elements-of-compsci.html#in-this-section",
    "title": "7  Elements of Computer Science",
    "section": "7.1 In this section",
    "text": "7.1 In this section\nAdapting computer science concepts to work for financial professionals. Concepts like computability, computational complexity, the language of algorithms and problem solving, looking for and using patterns, and adopting digital-first practices to automate the boring parts of the job."
  },
  {
    "objectID": "elements-of-compsci.html#computer-science-for-financial-professionals",
    "href": "elements-of-compsci.html#computer-science-for-financial-professionals",
    "title": "7  Elements of Computer Science",
    "section": "7.2 Computer Science for Financial Professionals",
    "text": "7.2 Computer Science for Financial Professionals\nComputer science as a term can be a bit misleading because of the overwhelming association with the physical desktop or laptop machines that we call “computers”. The discipline of computer science is much richer than consumer electronics: at it’s core, computer science concerns itself with areas of reserach and answering tough questions:\n\nAlgorithms and Optimization. How can a problem be solved efficiently? How can that problem be solved at all? Given constraints, how can one find an optimal solution?\nInformation Theory. Given limited data, what can be known or inferred from it?\nTheory of Computation. What sorts of questions are even answerable? Is an answer easy to computer or will resolving it require more resources than the entire known universe? Will a computation ever stop calculating?\nData Structures. How to encode, store, and use data? How does that data relate to each other and what are the trade-offs between different representations of that data?\n\nFor a reader in the twenty-first century we hope that’s it’s patently obvious how impactful the applied computer science has been as an end-user of the internet, artificial intelligence, computational photography, safety control systems, etc., etc. have been to our lives. It is a testament to the utlity of being able to harness some of the ideas of this science is. Many of the most impactful advances occur at the boundary between two disciplines. It’s here in this chapter that we desire to bring together the financial discipline together with computer science and to provide the financial practitioner with the language and concepts to leverage some of computer science’s most relevant ideas."
  },
  {
    "objectID": "elements-of-compsci.html#algorithms",
    "href": "elements-of-compsci.html#algorithms",
    "title": "7  Elements of Computer Science",
    "section": "7.3 Algorithms",
    "text": "7.3 Algorithms\n\n7.3.1 Computational Complexity"
  },
  {
    "objectID": "elements-of-compsci.html#data-structures",
    "href": "elements-of-compsci.html#data-structures",
    "title": "7  Elements of Computer Science",
    "section": "7.4 Data Structures",
    "text": "7.4 Data Structures"
  },
  {
    "objectID": "elements-of-compsci.html#information-theory",
    "href": "elements-of-compsci.html#information-theory",
    "title": "7  Elements of Computer Science",
    "section": "7.5 Information Theory",
    "text": "7.5 Information Theory\n\n7.5.1 Signal vs Noise"
  },
  {
    "objectID": "elements-of-compsci.html#formal-verification",
    "href": "elements-of-compsci.html#formal-verification",
    "title": "7  Elements of Computer Science",
    "section": "7.6 Formal Verification",
    "text": "7.6 Formal Verification"
  },
  {
    "objectID": "elements-of-compsci.html#the-discipline-of-software-engineering",
    "href": "elements-of-compsci.html#the-discipline-of-software-engineering",
    "title": "7  Elements of Computer Science",
    "section": "7.7 The Discipline of Software Engineering",
    "text": "7.7 The Discipline of Software Engineering\n\n7.7.1 Patterns"
  },
  {
    "objectID": "hardware.html#in-this-section",
    "href": "hardware.html#in-this-section",
    "title": "8  Hardware and It’s Implications",
    "section": "8.1 In this section",
    "text": "8.1 In this section\nA discussion of why a cursory understanding of modern computing hardware and architecture is important for making the right design decisions within a modeling context. Stack vs heap allocations, pointers, and bit types. A discussion of parallelism and the different kinds of parallelism."
  },
  {
    "objectID": "software.html#in-this-section",
    "href": "software.html#in-this-section",
    "title": "9  Applying Software Engineering Principles",
    "section": "9.1 In this section",
    "text": "9.1 In this section\nWe describe modern software engineering practices such as testing, documentation, and pipelines which can be utlizied by the financial professional to make their own work more robust and automated."
  },
  {
    "objectID": "modeling.html#in-this-chapter",
    "href": "modeling.html#in-this-chapter",
    "title": "10  Modeling",
    "section": "10.1 In This Chapter",
    "text": "10.1 In This Chapter\nWe discuss how to approach a problem and identify the key attributes to include in the model, what are the inherent trade-offs with different approaches, and how to work with data that feeds your model."
  },
  {
    "objectID": "modeling.html#parsimony",
    "href": "modeling.html#parsimony",
    "title": "10  Modeling",
    "section": "10.2 Parsimony",
    "text": "10.2 Parsimony"
  },
  {
    "objectID": "optimization.html#in-this-chapter",
    "href": "optimization.html#in-this-chapter",
    "title": "11  Optimization",
    "section": "11.1 In This Chapter",
    "text": "11.1 In This Chapter\nOptimization as root finding or minimization/maximazation of defined objectives. Differentiable programming and the benefits to optimization problems. Model fitting as an optimization problem."
  },
  {
    "objectID": "sensitivity-analysis.html#in-this-chapter",
    "href": "sensitivity-analysis.html#in-this-chapter",
    "title": "12  Sensitivity Analysis",
    "section": "12.1 In This Chapter",
    "text": "12.1 In This Chapter\nDifferent approaches to understanding the sensitivity of a model to changes in its inputs: derivatives, finite differences, global sensitivity analysis approaches, and statistical approaches."
  },
  {
    "objectID": "stochastic.html#footnotes",
    "href": "stochastic.html#footnotes",
    "title": "13  Stochastic Modeling",
    "section": "",
    "text": "Kalos was a pioneer in Monte Carlo techniques, quoted via https://doi.org/10.1007/978-3-540-74686-7_3↩︎"
  },
  {
    "objectID": "visualization.html#in-this-chapter",
    "href": "visualization.html#in-this-chapter",
    "title": "14  Visiualizations",
    "section": "14.1 In This Chapter",
    "text": "14.1 In This Chapter\nThe evolved brain and pattern recognition, recommended principles for looking at data, and avoiding common mistakes. Exploratory visualization versus visualizations intended for an audience."
  },
  {
    "objectID": "matrices.html#in-this-chapter",
    "href": "matrices.html#in-this-chapter",
    "title": "15  Matrices and Their Uses",
    "section": "15.1 In This Chapter",
    "text": "15.1 In This Chapter\nMatrices and their myriad uses: reframing problems through the eyes of linear algebra, an intuitive refreshing on applicable maths, and recurring patterns of matrix operations in financial modeling."
  },
  {
    "objectID": "data-learning.html#in-this-chapter",
    "href": "data-learning.html#in-this-chapter",
    "title": "16  Learning from Data",
    "section": "16.1 In this chapter",
    "text": "16.1 In this chapter\nUsing data to inform a model: fitting parameters, forecasting, and fundamental limitations on prediction. Also covered are elements of practical review such as static and dynamic validations, and implied rate analysis."
  },
  {
    "objectID": "stochastic-mortality.html#in-this-chapter",
    "href": "stochastic-mortality.html#in-this-chapter",
    "title": "17  Stochastic Mortality Projections",
    "section": "17.1 In This Chapter",
    "text": "17.1 In This Chapter\nA term life insurance policy is used to illustrate: selecting key model features, design tradeoffs between a few different approaches, and a discussion of the performance impacts of the different approaches to parallelism."
  },
  {
    "objectID": "scenario-generation.html#in-this-chapter",
    "href": "scenario-generation.html#in-this-chapter",
    "title": "18  Scenario Generation",
    "section": "18.1 In This Chapter",
    "text": "18.1 In This Chapter\nHow to generate synthetic data for your model using sub-models, with applications to economic scenario generation and portfolio composition."
  },
  {
    "objectID": "similarity-calculation.html#in-this-chapter",
    "href": "similarity-calculation.html#in-this-chapter",
    "title": "19  Similarity Analysis",
    "section": "19.1 In This Chapter",
    "text": "19.1 In This Chapter\nGiven a set of interest, understanding the relative similarity (or not) of features of interest is useful in classification and data compression techniues."
  },
  {
    "objectID": "portfolio-optimization.html#in-this-chapter",
    "href": "portfolio-optimization.html#in-this-chapter",
    "title": "20  Portfolio Optimization",
    "section": "20.1 In This Chapter",
    "text": "20.1 In This Chapter\nOptimization in a portfolio context with examples of asset selection under different constraints and objectives."
  },
  {
    "objectID": "other-techniques.html#in-this-chapter",
    "href": "other-techniques.html#in-this-chapter",
    "title": "21  Other Useful Techniques",
    "section": "21.1 In this chapter",
    "text": "21.1 In this chapter\nOther useful techniques are surveyed, such as: memoization to avoid repeated computations, psuedo-monte carlo, creating a model office, and tips on modeling a complete balance sheet."
  },
  {
    "objectID": "other-techniques.html#taking-things-to-the-extreme",
    "href": "other-techniques.html#taking-things-to-the-extreme",
    "title": "21  Other Useful Techniques",
    "section": "21.2 Taking things to the Extreme",
    "text": "21.2 Taking things to the Extreme\nConsider what happens if something is taken to an extreme. For example, what happens in the model if we input negative rates? Where should negative rates be allowed and can the model handle them?"
  },
  {
    "objectID": "other-techniques.html#range-bounding",
    "href": "other-techniques.html#range-bounding",
    "title": "21  Other Useful Techniques",
    "section": "21.3 Range Bounding",
    "text": "21.3 Range Bounding\nSometimes you just need to know that an outcome is within a certain range - if you can develop a “high” and “low” estimate by making assumptions that you know are outside of feasible ranges, then you can determine whether something is reasonable or within tolerances.\nTo take an example from the pages of interview questions: say you need to determine if a mortgaged property’s value is greater than the amount of the outstanding loan (say $100,000). You don’t have an appraisal, but know that it’s in reasonable condition and that (1) a comparable house with many more issues sold for $100 per square foot. You also don’t know the square footage of the house, but know from the number of rooms and layout that it must be at least 1000 square feet. Therefore you know that the value should at least be greater than:\n\\[\n\\frac{\\$100}{\\text{sq. ft}} \\times 1000 \\text{sq. ft} = \\$100,000\n\\]\nWe’d then conclude that the value of the house very likely exceeds the outstanding balance of the loan and resolves our query without complex modeling or expensive appraisals."
  },
  {
    "objectID": "julia.html#installation",
    "href": "julia.html#installation",
    "title": "22  Set up Julia and the Computing Environment",
    "section": "22.1 Installation",
    "text": "22.1 Installation\nJulia is open source and can be downloaded from JuliaLang.org and is available for all major operating systems. After you download and install, then you have Julia installed and can access the REPL, or Read-Eval-Print-Loop, which can run complete programs or function as powerful day-to-day calculator. However, many people find it more comfortable to work in a text editor or IDE (Integrated Development Environment).\nIf you are looking for managed installations with a curated set of packages for use within an organization, there are ways to self-host package repositories and otherwise administratively manage packages. Julia Computing offers managed support with enterprise solutions, including push-button cloud compute capabilities."
  },
  {
    "objectID": "julia.html#package-management",
    "href": "julia.html#package-management",
    "title": "22  Set up Julia and the Computing Environment",
    "section": "22.2 Package Management",
    "text": "22.2 Package Management\nJulia comes with Pkg, a built-in package manger. With it, you can install packages, pin certain versions, recreate environments with the same set of dependencies, and upgrade/remove/develop packages easily. It’s one of the things that just works and makes Julia stand out versus alternative languages that don’t have a de-facto way of managing or installing packages.\nPackage installation is accomplished interactively in the REPL or executing commands.\n\nIn the REPL, you can change to the Package Management Mode by hitting ] and, e.g., add DataFrames CSV to install the two packages. Hit [backspace] to exit that mode in the REPL.\nThe same operation without changing REPL modes would be: using Pkg; Pkg.add([\"DataFrames\", \"CSV\"])\n\nRelated to packages, are environments which are a self-contained workspaces for your code. This lets you install only packages that are relevant to the current work. It also lets you ‘remember’ the exact set of packages and versions that you used. In fact, you can share the environment with others, and it will be able to recreate the same environment as when you ran the code. This is accomplished via a Project.toml file, which tracks the direct dependencies you’ve added, along with details about your project like its version number. The Manifest.toml tracks the entire dependency tree.\nReproducibility via the environment tools above is a really key aspect that will ensure Julia code is consistent across time and users, which is important for financial controls."
  },
  {
    "objectID": "julia.html#editors",
    "href": "julia.html#editors",
    "title": "22  Set up Julia and the Computing Environment",
    "section": "22.3 Editors",
    "text": "22.3 Editors\nBecause Julia is very extensible and amenable to analysis of its own code, you can typically find plugins for whatever tool you prefer to write code in. A few examples:\n\n22.3.1 Visual Studio Code\nVisual Studio Code is a free editor from Microsoft. There’s a full-featured Julia plugin available, which will help with auto-completion, warnings, and other code hints that you might find in a dedicated editor (e.g. PyCharm or RStudio). Like those tools, you can view plots, search documentation, show datasets, debug, and manage version control.\n\n\n22.3.2 Notebooks\nNotebooks are typically more interactive environments than text editors - you can write code in cells and see the results side-by-side.\nThe most popular notebook tool is Jupyter (“Julia, Python, R”). It is widely used and fits in well with exploratory data analysis or other interactive workflows. It can be installed by adding the IJulia.jl package.\nPluto.jl is a newer tool, which adds reactivity and interactivity. It is also more amenable to version control than Jupyter notebooks because notebooks are saved as plain Julia scripts. Pluto is unique to Julia because of the language’s ability to introspect and analyze dependencies in its own code. Pluto also has built-in package/environment management, meaning that Pluto notebooks contains all the code needed to reproduce results (as long as Julia and Pluto are installed)."
  },
  {
    "objectID": "ecosystem.html",
    "href": "ecosystem.html",
    "title": "23  The Julia Ecosystem Today",
    "section": "",
    "text": "A tour of relevant available packages as of 2023.\nThe Julia ecosystem favors composability and interoperability, enabled by multiple dispatch. In other words, because it’s easy to automatically specialize functionality based on the type of data being used, there’s much less need to bundle a lot of features within a single package.\nAs you’ll see, Julia packages tend to be less vertically integrated because it’s easier to pass data around. Counterexamples of this in Python and R:\n\nNumpy-compatible packages that are designed to work with a subset of numerically fast libraries in Python\nspecial functions in Pandas to read CSV, JSON, database connections, etc.\nThe Tidyverse in R has a tightly coupled set of packages that works well together but has limitations with some other R packages\n\nJulia is not perfect in this regard, but it’s neat to see how frequently things just work. It’s not magic, but because of Julia features outside the scope of this article it’s easy for package developers (and you!) to do this.\nJulia also has language-level support for documentation, so packages can follow a consistent style of help-text and have the docs be auto-generated into web pages available locally or online.\nThe following highlighted packages were chosen for their relevance to typical actuarial work, with a bias towards those used regularly by the authors. This is a small sampling of the over 6000 registered Julia Packages[^2]\n\n23.0.1 Data\nJulia offers a rich data ecosystem with a multitude of available packages. Perhaps at the center of the data ecosystem are CSV.jl and DataFrames.jl. CSV.jl is for reading and writing files text files (namely CSVs) and offers top-class read and write performance. DataFrames.jl is a mature package for working with dataframes, comparable to Pandas or dplyr.\nOther notable packages include ODBC.jl, which lets you connect to any database (given you have the right drivers installed), and Arrow.jl which implements the Apache Arrow standard in Julia.\nWorth mentioning also is Dates, a built-in package making date manipulation straightforward and robust.\nCheck out JuliaData org for more packages and information.\n\n\n23.0.2 Plotting\nPlots.jl is a meta-package providing an interface to consistently work with several plotting backends, depending if you are trying to emphasize interactivity on the web or print-quality output. You can very easily add animations or change almost any feature of a plot.\nStatsPlots.jl extends Plots.jl with a focus on data visualization and compatibility with dataframes.\nMakie.jl supports GPU-accelerated plotting and can create very rich, beautiful visualizations, but it’s main downside is that it has not yet been optimized to minimize the time-to-first-plot.\n\n\n23.0.3 Statistics\nJulia has first-class support for missing values, which follows the rules of three-valued logic so other packages don’t need to do anything special to incorporate missing values.\nStatsBase.jl and Distributions.jl are essentials for a range of statistics functions and probability distributions respectively.\nOthers include:\n\nTuring.jl, a probabilistic programming (Bayesian statistics) library, which is outstanding in its combination of clear model syntax with performance.\nGLM.jl for any type of linear modeling (mimicking R’s glm functionality).\nLsqFit.jl for fitting data to non-linear models.\nMultivariateStats.jl for multivariate statistics, such as PCA.\n\nYou can find more packages and learn about them here.\n\n\n23.0.4 Machine Learning\nFlux, Gen, Knet, and MLJ are all very popular machine learning libraries. There are also packages for PyTorch, Tensorflow, and SciKitML available. One advantage for users is that the Julia packages are written in Julia, so it can be easier to adapt or see what’s going on in the entire stack. In contrast to this design, PyTorch and Tensorflow are built primarily with C++.\nAnother advantage is that the Julia libraries can use automatic differentiation to optimize on a wider range of data and functions than those built into libraries in other languages.\n\n\n23.0.5 Differentiable Programming\nSensitivity testing is very common in actuarial workflows: essentially, it’s understanding the change in one variable in relation to another. In other words, the derivative!\nJulia has unique capabilities where almost across the entire language and ecosystem, you can take the derivative of entire functions or scripts. For example, the following is real Julia code to automatically calculate the sensitivity of the ending account value with respect to the inputs:\njulia&gt; using Zygote\n\njulia&gt; function policy_av(pol)\n    COIs = [0.00319, 0.00345, 0.0038, 0.00419, 0.0047, 0.00532]\n    av = 0.0\n    for (i,coi) in enumerate(COIs)\n        av += av * pol.credit_rate\n        av += pol.annual_premium\n        av -= pol.face * coi\n    end\n    return av                # return the final account value\nend\n\njulia&gt; pol = (annual_premium = 1000, face = 100_000, credit_rate = 0.05);\n\njulia&gt; policy_av(pol)        # the ending account value\n4048.08\n\njulia&gt; policy_av'(pol)       # the derivative of the account value with respect to the inputs\n(annual_premium = 6.802, face = -0.0275, credit_rate = 10972.52)\nWhen executing the code above, Julia isn’t just adding a small amount and calculating the finite difference. Differentiation is applied to entire programs through extensive use of basic derivatives and the chain rule. Automatic differentiation, has uses in optimization, machine learning, sensitivity testing, and risk analysis. You can read more about Julia’s autodiff ecosystem here.\n\n\n23.0.6 Utilities\nThere are also a lot of quality-of-life packages, like Revise.jl which lets you edit code on the fly without needing to re-run entire scripts.\nBenchmarkTools.jl makes it incredibly easy to benchmark your code - simply add @benchmark in front of what you want to test, and you will be presented with detailed statistics. For example:\njulia&gt; using ActuaryUtilities, BenchmarkTools\n\njulia&gt; @benchmark present_value(0.05,[10,10,10])\n\nBenchmarkTools.Trial: 10000 samples with 994 evaluations.\n Range (min … max):  33.492 ns … 829.015 ns  ┊ GC (min … max): 0.00% … 95.40%\n Time  (median):     34.708 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   36.599 ns ±  33.686 ns  ┊ GC (mean ± σ):  4.40% ±  4.55%\n\n  ▁▃▆▆▆██▇▄▃▂         ▁                                        ▂\n  █████████████▆▆▇█▇████▇██▇█▇█▇▇▆▆▅▅▅▅▅▄▅▄▄▅▅▅▅▄▄▁▅▄▄▅▄▄▅▅▆▅▆ █\n  33.5 ns       Histogram: log(frequency) by time      45.6 ns &lt;\n\n Memory estimate: 112 bytes, allocs estimate: 1.\nTest is a built-in package for performing testsets, while Documenter.jl will build high-quality documentation based on your inline documentation.\nClipData.jl lets you copy and paste from spreadsheets to Julia sessions.\n\n\n23.0.7 Other packages\nJulia is a general-purpose language, so you will find packages for web development, graphics, game development, audio production, and much more. You can explore packages (and their dependencies) at https://juliahub.com/.\n\n\n23.0.8 Actuarial packages\nSaving the best for last, the next article in the series will dive deeper into actuarial packages, such as those published by JuliaActuary for easy mortality table manipulation, common actuarial functions, financial math, and experience analysis."
  }
]