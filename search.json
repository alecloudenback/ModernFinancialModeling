[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modern Financial Modeling",
    "section": "",
    "text": "Preface\nThis book enables financial professionals and advanced students to apply the tools and concepts of computational and related sciences to their work.\nThroughout these pages, you’ll see how best to represent ideas so they remain scalable, intelligible to collaborators, and robust against changing conditions. The book covers fundamentals in programming to advanced topics like parallelization and data visualization, always returning to practical applications in insurance, asset modeling, and beyond.\nThe goal is straightforward: to empower financial professionals with the knowledge and confidence to manipulate computational tools in ways that reveal ever richer insights into the systems we seek to understand and influence.\nA PDF of the book is available by clicking here.\n\n\n\n\n\n\nWarning\n\n\n\nThis book is currently being drafted. ANY AND ALL content is subject to change, including the license. To report issues or other feedback, please email Alec at firstnamelastname@gmail.com.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "The approach\nThe world of financial modeling is incredibly complex and variegated. Like many sciences, financial modeling blends practical goals with computational tools to arrive at answers that (we hope) are meaningful in a way that tells us more about the world we live in. What this usually means specifically is that practitioners utilize computers to do the heavy work of processing data or running simulations which reveal insights about the complex systems we seek to represent. In this way, then, financial modelers must also be a craftsman who seeks not only to design new products, but must also think carefully about the tools and the process used therein.\nThis book seeks to aid the practitioner in developing that workmanship: we will develop new ways to look at the process, think about how to most clearly represent ideas, dive into details about computer hardware and bring it back up to the most abstract levels, and develop a vocabulary to more clearly express and communicate these concepts. The book contains a large number of practical examples to demonstrate that the end result is better for the journey we will take.\nThis book addresses programming for the applied financial professional, starting with a fundamental question: “Why is this relevant for financial modeling?”. The answer is simple: financial modeling is complex, data intensive, and often very abstract. Programming is the best tool humans have so far developed for rigorously transforming ideas and data into results. A builder may be the most skilled person in the world with a hammer but another with some basic training in a richer set of tools will build a better house. This book will enhance your toolkit with experience with multiple tools: a specific programming language, yes, but much more than that: a language to talk about solving problems, a deeper understanding of specific problem solving techniques, how to make decisions about what the architecture of a solution looks like, and practical advice from experienced practitioners.\nThe authors of the book are practicing actuaries, but we intend for the content to be applicable to nearly all practitioners in the financial industry. The discussion and examples may have an orientation towards insurance topics, but the concepts and patterns are applicable to a wide variety of related disciplines.\nWe will pull from examples on both sides of the balance sheet: the left (assets) and right (liabilities). We may also take the liberty to, at times, abuse traditional accounting notions: a liability is just an asset with the obligor and obligee switched. When the accounting conventions are important (such as modeling a total balance sheet) we will be mindful in explaining the accounting perspective. In practice, this means that we’ll present examples of assets (fixed income, equity, derivatives) and liabilities (life insurance, annuities, long-term care) and show that similar modeling techniques can be used for both.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#what-you-will-learn",
    "href": "intro.html#what-you-will-learn",
    "title": "Introduction",
    "section": "What you will learn",
    "text": "What you will learn\nIt is our hope that with the help of this book, you will find it more efficient to discuss aspects of modeling with colleagues, borrow problem solving language from computer science, spot recurring structural patterns in problems that arise, and understand how best to make use of the “bicycle for your mind” in the context of financial modeling.\nIt is the experience of the authors that many professionals that do complex modeling as a part of their work have gotten to be very proficient in spite of not having substantive formal training on problem solving, algorithms, or model architecture. This book serves to fill that gap and provide the “missing semester” (and years of practical learning). After reading this book, we hope that you will appreciate the attributes of Microsoft Excel that made it so ubiquitous, but that you prefer to use a programming language for the ability to more naturally express the relevant abstractions which make your models simpler, faster, or more usable by others.\nEven if your role does not entail hands-on coding (e.g., management or low-code tools), the ideas and language should prove useful in guiding the work to a cleaner, more efficient solution.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#the-journey-ahead",
    "href": "intro.html#the-journey-ahead",
    "title": "Introduction",
    "section": "The Journey Ahead",
    "text": "The Journey Ahead\nLearning a new topic, especially one that’s not well-trodden in a given field, can be intimidating. There are many resources available online, this book will recommend some additional resources, and there are community support resources available - check the chat and forums and look for the users talking about the topics that interest you. One of the wonderful things about the technology community is the degree to which content is available online for learning and reference.\nFurther, moving substantial parts of the financial services industry towards a digital-first, modern workflow is a monumental effort and you should seek partners on both the finance and information technology side. In general, good ideas and processes will prevail. The trick to encouraging adoption is finding the right place to plug a new idea or suggestion.\nAdditionally, this book provides the language and technical knowledge to partner with others (such as peers and IT) to make pragmatic decisions about the tradeoffs that will need to be made.\n\nWhat to Expect\nThis book will guide you through:\n\nCore programming concepts applied to finance\nModern software development practices\nComputational approaches to common financial problems\nReal-world examples and applications\n\nThe goal is to build both theoretical understanding and practical skills you can apply immediately in your work.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#prerequisites",
    "href": "intro.html#prerequisites",
    "title": "Introduction",
    "section": "Prerequisites",
    "text": "Prerequisites\nBasic experience with financial modeling is not strictly required, but it will benefit the reader to be familiar so that the examples will not be attempting to teach both financial maths and computer science simultaneously.\nAdvanced financial maths (e.g. stochastic calculus) is not required. Indeed, this book is not oriented to the advanced technicalities of Wall Street “quants” and is instead directed at the multitudes of financial practitioners focused on producing results that are not measured in the microseconds of high-frequency trading.\nPrior programming experience is not required: see 5  Elements of Programming introduces the basic syntax and concepts while 21  Writing Julia Code covers setting up your environment to follow along. For readers with background in programming, we recommend skimming 5  Elements of Programming.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#the-contents-of-this-book",
    "href": "intro.html#the-contents-of-this-book",
    "title": "Introduction",
    "section": "The Contents of This Book",
    "text": "The Contents of This Book\nThe book is organized into eight parts, each addressing key aspects of computational thinking and financial modeling. Part I introduces foundational concepts, explaining why programming matters for financial professionals, and why Julia is particularly well-suited for financial modeling applications.\nParts II, III, and IV establish the theoretical foundations—covering effective financial modeling practices, programming abstractions, and techniques for building performant models. These sections bridge theory with practical implementation, exploring topics like model design, functional programming, data types, and parallelization strategies.\nPart V connects interdisciplinary concepts with practical applications, demonstrating how software engineering practices, computer science principles, statistical methods, and visualization techniques enhance financial modeling.\nPart VI provides detailed guidance on developing in Julia, from writing and troubleshooting code to optimization. This is the section that really leans into Julia-specific ideas and workflows.\nPart VII showcases applied financial modeling techniques through real-world examples, including stochastic mortality projections, scenario generation, sensitivity analysis, and portfolio optimization.\nWhile Julia is used for the examples throughout the book, the concepts presented are largely language-agnostic. The principles of computational thinking and financial modeling remain applicable regardless of implementation language. Readers are encouraged to follow along with the examples on their own computers, with the entire book available at https://ModernFinancialModeling.com.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#a-crash-course",
    "href": "intro.html#a-crash-course",
    "title": "Introduction",
    "section": "A Crash Course",
    "text": "A Crash Course\nThis text is written with the intention to explain and show many core and related concepts that are useful for a practitioner utilizing code-based modeling and workflows. If you are looking for a crash course in getting up to speed in order to contribute to a functional project, here is a “crash course” syllabus for getting the most out of this book:\n\nA suggested short course reading list for chapters from this book.\n\n\n\n\n\n\n\n\nHow\nChapter Number\nChapter Title\nWhy\n\n\n\n\nSkim and re-reference\n5\nElements of Programming\nProgramming syntax and core components provided by most languages\n\n\nRead in entirety\n6\nFunctional Abstractions\nPatterns of functional abstractions that are repeatedly useful in financial modeling\n\n\nRead in entirety\n7\nData and Types\nUtilizing types of data to improve efficiency and architecture of models\n\n\nRead in entirety\n12\nApply Software Engineering Practices\nBest practices for code-based workflows, including version control, testing, documentation, and code distribution.\n\n\nSkim and re-reference\n21-24\nDeveloping in Julia\nWriting, troubleshooting, distributing, and optimizing Julia specific code. Skim lightly if using a language other than Julia; you’ll learn what kinds of tools to look for in other ecosystems.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#notes-on-formatting",
    "href": "intro.html#notes-on-formatting",
    "title": "Introduction",
    "section": "Notes on formatting",
    "text": "Notes on formatting\nWhen a concept is defined for the first time, the term will be bold. Code, or references to pieces of code will be formatted in inline code style like 1+1 or in separate code blocks:\n\"This is a code block that doesn't show any results\"\n\n\"This is a code block that does show output\"\n\n\"This is a code block that does show output\"\n\n\nWhen we show inline commands to be sent to Pkg mode in the REPL (see Environments and Dependencies), such as add DataFrames, we will try to make it clear in the context. If using Pkg mode in standalone codeblocks, it will be presented showing the full prompt, such as:\n(@v1.11) pkg&gt; add DataFrames\nThere will be various callout blocks which indicate tips or warnings. These should be self-evident but we wanted to point to a particular callout which is intended to convey advice that stems from practical modeling experience of the authors:\n\n\n\n\n\n\nTipFinancial Modeling Pro Tip\n\n\n\nThis box indicates a side note that’s particularly applicable to improving your financial modeling.\n\n\n\nColophon\nThe HTML and PDF book were rendered using Quarto and Quarto’s open source dependencies like Pandoc and LaTeX.\nThe HTML version of this book uses Lato for the body font and JuliaMono for the monospace font.\nThe PDF version of this book uses TeX Gyre Pagella for the body font and JuliaMono for the monospace font.\nThe cover was designed by Alec Loudenback using Affinity Designer with the graphic used under permission by GitHub user cormullion.\nThis book was rendered on August 10, 2025. The system used to generate the code and benchmarks was:\n\nversioninfo()\n\nJulia Version 1.11.6\nCommit 9615af0f269 (2025-07-09 12:58 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: macOS (arm64-apple-darwin24.0.0)\n  CPU: 14 × Apple M4 Max\n  WORD_SIZE: 64\n  LLVM: libLLVM-16.0.6 (ORCJIT, apple-m1)\nThreads: 10 default, 0 interactive, 5 GC (on 10 virtual cores)\nEnvironment:\n  JULIA_NUM_THREADS = auto\n  JULIA_PROJECT = @.\n  JULIA_LOAD_PATH = @:@stdlib\n\n\n\n\nLicense and Copyright\nThis work is copyright Alec Loudenback and Yun-Tien Lee. This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License unless otherwise noted. The chapters including and between 21  Writing Julia Code and 24  Optimizing Julia Code are licensed under the CC Attribution-ShareAlike 4.0 International License. This is not a Milliman-sponsored project.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "why-program.html",
    "href": "why-program.html",
    "title": "1  Why Program?",
    "section": "",
    "text": "1.1 Chapter Overview\nWe motivate why a financial professional should adopt programming skills which will improve their own capabilities and enjoyment of the discipline, whilst allowing themselves to better themselves and the industry we work in.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-program.html#introduction",
    "href": "why-program.html#introduction",
    "title": "1  Why Program?",
    "section": "1.2 Introduction",
    "text": "1.2 Introduction\nThe financial sector is undergoing a profound transformation. In an era defined by big data, (pseudo) artificial intelligence, and rapid technological advancement, the traditional boundaries of finance are expanding and blurring. From Wall Street to Main Street, from global investment banks to local credit unions, technology is reshaping how financial services are delivered, how risks are managed, and how decisions are made.\nThis digital revolution is not just changing the tools we use; it’s fundamentally altering the skills required to succeed in finance. In the past, a strong foundation in mathematics, economics, and financial theory was sufficient for most roles in the industry. Today, these skills, while still crucial, are increasingly being augmented (and in some cases) superseded by technological proficiency.\nAt the forefront of this shift is the growing importance of programming skills. In the beginning of the computer era in finance, the differentiating skill was being able to utilize digital computing, data processing, and calculation engines to automate, analyze, and report on the business. These skills required low level programming and the success of many of those early programs is evident in their legacy: many of them are still around in the 2020s!\nAt some point, due to regulatory pressures, attempts at organization efficiencies, or management decision making, the skill of programming became highly specialized and most financial professionals (investment analysts, actuaries, accountants, etc.) became relegated to being “business users”, utilizing either Microsoft Excel or a proprietary third-party software to accomplish their responsibilities. The reasons for this were not totally wrong, even in retrospect.\nAt some point, an increasingly complex stack of software sitting between the developer and the hardware, the proliferation of computer security risks, it makes some sense that many financial developers were pushed out of the programming trade. Instead specialized, separate business and IT units were developed. Of course, this led to many inefficiencies and is now swinging back the other way.\nWhat’s changed that’s enabling financial professionals to re-engage with the powerful tools that programming provides? Some reasons include:\n\nCode management tools. Github and other version control systems provide best-in-class ways of managing codebase changes and collaboration. Tools exist to scan repositories for leaking secrets, security vulnerabilities, and dependency management.\nIncreasingly accessible development. Originally, very few layers of complexity existed between the written code and running it on the mainframe. Over time, drivers, operating systems, networking, dependencies, and compilers made development more complex. Today, languages, libraries, code editors, and deployment tools have smoothed many of these frictions.\nCompetitive Pressures. An increasingly commoditized financial product with evermore competition has led to a need to improve efficiency of manufacturing and selling financial products. Having a business developer is a lot more efficient than a business user who needs to get an IT developer to implement something. Further, pressures from outside the financial sector abound: It’s easier to teach a tech developer enough to be successful in a finance role than it is to teach a finance professional development skills.\nRegulatory and Risk Demands. Pressures that previously motivated the move to proprietary software for modeling included regulatory reporting, internal risk metrics, and management performance evaluation. However, companies are realizing that their unique products, risk frameworks, preferred management measurements, and employee potential means that having a bespoke internal model is seen as a key capability. Many regulatory frameworks also encourage the use of a bespoke model, which is a particularly attractive option especially for those who view the given regulatory framework as inappropriately reflecting their own business and risk profile.\n\nWhether you’re an investment banker modeling complex derivatives, an actuary calculating insurance risks, a financial planner optimizing client portfolios, or a risk manager stress-testing scenarios, the ability to code is becoming as fundamental as the ability to use a spreadsheet was a generation ago. To remain competitive, adaptable, and effective in the evolving landscape of finance, professionals must embrace programming as a core skill.\n\n\n\n\n\n\nNote\n\n\n\nOne subset of business analysts that did not start to migrate away from development as a strategic part of their value were “quants” or quantitative analysts who heavily utilized programming skills to develop unique products, trading strategies, modeling frameworks, and risk engines. This book is not really for that class of people and is instead geared towards the mass of financial professionals who want to get some of the benefits of the tools that the quants have been using for years. Quants may find value here in adapting some of their existing knowledge with the concepts and capabilities that Julia enables.\n\n\nAs we delve into this topic, keep in mind that learning to code is not about replacing traditional financial acumen—it’s about augmenting and enhancing it. It’s about equipping yourself with the tools to tackle the complex, data-driven challenges of modern finance. In short, it’s about future-proofing your career in an industry that is increasingly defined by its ability to innovate and adapt to technological change.\n\n1.2.1 Market Forces\nToday, there is a trend towards technological value-creation and is evident across many traditional sectors. Tesla claims that it’s a technology company; Amazon is the #1 product retailer because of its vehement focus on internal information sharing1; Airlines are so dependent on their systems that the skies become quieter on the rare occasion that their computers give way. Companies that are so involved in things (cars, shopping) and physical services (flights) are so much more focused on improving their technological operations than insurance companies whose very focus is ‘information-based’? The market has rewarded those who have prioritized their internal technological solutions.\nCommoditized investing services and challenging yield environments have reduced companies’ comparative advantage to “manage money”. Spread compression and the explosion of consumer-oriented investment services means a more competitive focus on the ability to manage the entire asset or policy’s lifecycle efficiently (digitally), perform more real-time analysis of experience and risk management, and handle the growing product and regulatory complexity.\nThese are problems that have technological solutions and are waiting for insurance company adoption.\nCompanies that treat data like coordinates on a grid (spreadsheets) will get left behind. Two main hurdles have prevented technology companies from breaking into insurance and traditional finance:\n\nHigh regulatory barriers to entry, and\nDifficulty in selling complex insurance products without traditional distribution.\n\nOnce those two walls are breached, traditional finance companies without a strong technology core will struggle to keep up. The key to thriving is not just adding “developers” to an organization; it’s going to be getting domain experts like financial modelers to be an integral part of the technology transformation.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-program.html#why-programming-matters-now",
    "href": "why-program.html#why-programming-matters-now",
    "title": "1  Why Program?",
    "section": "1.3 Why Programming Matters Now",
    "text": "1.3 Why Programming Matters Now\nProgramming is becoming as fundamental for financial professionals as spreadsheet skills were a generation ago. Here’s why:\n\nEnhanced Analysis Capabilities: Programming allows for more complex analyses, handling of larger datasets, and application of advanced statistical and machine learning techniques.\nAutomation and Efficiency: Repetitive tasks can be automated, freeing up time for more value-added activities.\nCustomization: Bespoke solutions can be developed to address unique business needs, risk frameworks, and regulatory requirements.\nData Handling: As data volumes grow, programming provides tools to efficiently process, analyze, and derive insights from vast amounts of information.\nIntegration: Programming skills enable better integration across different systems and data sources, providing a more holistic view of financial operations.\nCompetitive Edge: In an increasingly technology-driven industry, programming skills can be a significant differentiator.\n\nIt’s now commonly accepted that to gather insights from your data, you need to know how to code. Modeling and valuation needs, too, are often better suited to customized solutions. Let’s not stop at data science when learning how to solve problems with a computer.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-program.html#the-spectrum-of-programming-in-finance",
    "href": "why-program.html#the-spectrum-of-programming-in-finance",
    "title": "1  Why Program?",
    "section": "1.4 The Spectrum of Programming in Finance",
    "text": "1.4 The Spectrum of Programming in Finance\nIt’s important to note that becoming proficient in programming doesn’t mean you need to become a full-time software developer. There’s a spectrum of programming skills that can benefit financial professionals:\n\nBasic Scripting: Automating repetitive tasks in Excel or other tools.\nData Analysis: Using languages like Python or R for statistical analysis and visualization.\nModel Building: Developing financial models or risk assessment tools.\nFull-Scale Application Development: Creating more complex applications for internal use or client-facing solutions.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-program.html#avoiding-red-herrings",
    "href": "why-program.html#avoiding-red-herrings",
    "title": "1  Why Program?",
    "section": "1.5 Avoiding Red Herrings",
    "text": "1.5 Avoiding Red Herrings\nOne tantalizing path to contemplate is to avoid really learning how to code. Between artificial intelligence (AI) solutions being developed and low-code offerings, is there really a need to learn the fundamentals of coding? We argue that there is, for the basic reason that coding is not about mechanically typing out lines in an editor, but both a tool and a craft that is designed to enhance and apply your own creative and logical thinking.\nThe current generation of AI is fundamentally limited. As Yann LeCun, Meta’s Chief AI Scientist, notes, current Large Language Models (LLMs) lack a fundamental grasp of logic and causality. They ‘cannot reason in any reasonable definition of the term and cannot plan… hierarchically,’ making them powerful assistants for syntax and boilerplate, but unreliable architects for the complex, logical structures required in financial modeling (Murphy and Criddle 2024).\nAn important role for AI to play will be to support modelers in boilerplate, syntactical hurdles (“in VBA I would do it like this, but in Julia how do I do X, Y, or Z”), and basic algorithmic support. What is not likely to change in the short term is most of the value that a modeler brings to the table: creative thinking, understanding of company and market dynamics, and capability to understand broader architecture and conceptual aspects of modeling.\nA similarly fraught path is low-code solutions. Low-code solutions are inherently limiting in their capabilities and lock you into a particular vendor’s proprietary ecosystem. If you know enough about what you are trying to do to be able to state it in clearly in plain English, then you are most of the way to being able to program in a full coding solution (AI can actually help bridge this gap here). As soon as you hit a limitation of the system (“I’d like to use XYZ optimization algorithm at each timestep”), you are reliant on the vendor to implement that option in the “low code” solution. Further, you are out-sourcing a lot of the important inner-workings of the model to someone else and not building that expertise yourself of in-house somewhere.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-program.html#the-10x-modeler",
    "href": "why-program.html#the-10x-modeler",
    "title": "1  Why Program?",
    "section": "1.6 The 10x Modeler",
    "text": "1.6 The 10x Modeler\nThe increasingly complex business needs will highlight a large productivity difference between a financial modeler who can code and one who can’t — simply because the former can react, create, synthesize, and model faster than the latter. From the efficiency of transforming administration extracts, summarizing and aggregating valuation output, to analyzing available data in ways that spreadsheets simply can’t handle, you can become a “10x Modeler”2.\n\n\n\n\n\n\nNote\n\n\n\nIn the technology sector, a 10x developer is term for a software engineer who is an order of magnitude more productive, creative, or capable than a typical peer. Here, we extend the notion to developers of financial models.\n\n\nWorking within a vendor’s graphical user interface (GUI) makes you a consumer of their modeling tool. Writing your own model in code makes you an architect of the solution. This is the difference between having a surface-level familiarity and having full command over the analysis and the concepts involved — with the flexibility to do what your software can’t.\nYour current software might be able to perform the first layer of analysis, but be at a loss when you want to take it a step further. Tasks like visualizations, sensitivity analysis, summary statistics, stochastic analysis, or process automation, when done programmatically, are often just a few lines of additional code over and above the primary model.\nShould you drop the license for your software vendor? No, not yet anyway. But the ability to supplement and break out of the modeling box has been an increasingly important part of most professionals’ work and this trend appears to be accelerating.\nAdditionally, code-based solutions can leverage the entire technology sector’s progress to solve problems that are hard otherwise: scalability, data workflows, integration across functional areas, version control and versioning, model change governance, reproducibility, and more.\n30-40 years ago, there were no vendor-supplied modeling solutions and so you had no choice but to build models internally. This shifted with the advent of vendor-supplied modeling solutions. Today, it’s never been better for companies to leverage open and inner source to support their custom modeling, risk analysis/monitoring, and reporting workflows.\n\n\n\n\n\n\nNote\n\n\n\nOpen source refers to software whose source code is freely available for anyone to view, modify, and distribute. It promotes collaboration, transparency, and innovation by allowing developers worldwide to contribute to and improve the codebase. Open source projects often benefit from diverse perspectives and rapid development cycles, resulting in robust and widely-adopted solutions.\nInner source applies open source principles within a single organization. It encourages internal collaboration, code sharing, and transparency across different teams or departments. By adopting inner source practices, companies can reduce duplication of effort, improve code quality, and foster a culture of knowledge sharing. This approach can lead to more efficient development processes and better utilization of internal resources.\n\n\nIt is said that you cannot fully conceptualize something unless your language has a word for it. Similar to spoken language, you may find that breaking out of spreadsheet coordinates (and even a dataframe-centric view of the world) reveals different questions to ask and enables innovative ways to solve problems. In this way, you reward your intellect while building more meaningful and relevant models and analysis.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-program.html#risk-governance",
    "href": "why-program.html#risk-governance",
    "title": "1  Why Program?",
    "section": "1.7 Risk Governance",
    "text": "1.7 Risk Governance\nCode-based workflows are highly conducive to risk governance frameworks as well. If a modern software project has all of the following benefits, then why not a modern insurance product and associated processes?\n\nAccess control and approval processes\nVersion control, version management, and reproducibility\nContinuous testing and validation of results\nOpen and transparent design\nMinimization of manual overrides, intervention, and opportunity for user error\nAutomated trending analysis, system metrics, and summary statistics\nContinuously updated, integrated, and self-generating documentation\nIntegration with other business processes through a formal boundary (e.g. via an API)\nTools to manage collaboration in parallel and in sequence\n\nThese aspects of business processes are what technology companies excel at. There is a litany of highly robust, battle-tested tools used in the information services sectors. This book will introduce much of this to the financial professional (specifically Chapter 12, Chapter 21, and Chapter 23).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-program.html#managing-and-leading-the-transformation",
    "href": "why-program.html#managing-and-leading-the-transformation",
    "title": "1  Why Program?",
    "section": "1.8 Managing and Leading the Transformation",
    "text": "1.8 Managing and Leading the Transformation\nFor managers: the ability to understand the concepts, capabilities, challenges, and lingo is not a dichotomy, it’s a spectrum. Most actuaries, even at fairly high levels, are still often involved in analytical work. Still above that, it’s difficult to lead something that you don’t understand.\nConversely, the skill and practice of coding enhances managerial capabilities. When you are really skilled at pulling apart a problem or process into its constituent parts and designing optimal solutions… that’s a core attribute of leadership as well as the most essential skill in programming. This perspective also allows for a vision of where the organization should be instead of thinking about where it is now.\nThe skillset described herein is as important an aspect of career development as mathematical ability, project collaboration, or financial acumen.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-program.html#outlook",
    "href": "why-program.html#outlook",
    "title": "1  Why Program?",
    "section": "1.9 Outlook",
    "text": "1.9 Outlook\nIt will increasingly be essential for companies to modernize to remain competitive. That modernization isn’t built with big black-box software packages; it will be with domain experts who can translate their expertise into new forms of analysis - doing it faster and more robustly than the competition.\n\n\n\n\nMurphy, Hannah, and Cristina Criddle. 2024. “Meta AI Chief Says Large Language Models Will Not Reach Human Intelligence.” https://www.ft.com/content/23fab126-f1d3-4add-a457-207a25730ad9.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-program.html#footnotes",
    "href": "why-program.html#footnotes",
    "title": "1  Why Program?",
    "section": "",
    "text": "Have you had your Bezos moment? What you can learn from Amazon.↩︎\nThe term ‘10x developer’ originates from the technology sector to describe an engineer considered to be an order of magnitude more productive than their peers. This book adapts the concept to the domain of financial modeling.↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-julia.html",
    "href": "why-julia.html",
    "title": "2  Why use Julia?",
    "section": "",
    "text": "2.1 Chapter Overview\nWe motivate the use of Julia as a preferred language for financial modeling.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "why-julia.html#introduction",
    "href": "why-julia.html#introduction",
    "title": "2  Why use Julia?",
    "section": "2.2 Introduction",
    "text": "2.2 Introduction\nJulia is a relatively new1, productive, and fast programming language. It is evident in its pragmatic, productivity-focused design choices, pleasant syntax, rich ecosystem, thriving communities, and its ability to be both very general purpose and power cutting edge computing.\nWith Julia: math-heavy code looks like math; it’s easy to pick up, and quick to prototype. Packages are well-integrated, with excellent visualization libraries and pragmatic design choices.\nJulia’s popularity continues to grow across many fields and there’s a growing body of online references and tutorials, videos, and print media to learn from.\nLarge financial services organizations have already started realizing gains: BlackRock’s Aladdin portfolio modeling, the Federal Reserve’s economic simulations, and Aviva’s Solvency II-compliant modeling2. The last of these has a great talk on YouTube by Aviva’s Tim Thornham, which showcases an on-the-ground view of what difference the right choice of technology and programming language can make. Moving from their vendor-supplied modeling solution was 1000x faster, took 1/10 the amount of code, and was implemented 10x faster.\nThe language is not just great for data science — but also modeling, ETL, visualizations, package control/version management, machine learning, string manipulation, web-backends, and many other use cases.\nJulia is well suited for financial modeling work: easy to read and write and very performant.\n\n\n\n\n\n\nTip\n\n\n\nThe two language problem is a term describing processes and teams that separate “domain expertise” coding from “production” coding. This isn’t always a “problem”, but the “two language problem” describes the scenario where this arises not out of intention but out of the necessity of dealing with limitations of the programming languages used. The most common combination is when the domain experts utilize Python, while the quants or developers write C++. This arises because the productive, high level language hits a barrier in terms of speed, efficiency, and robustness. Then, as a necessary step to achieve the end goals of the business, the domain experts hand off the logic to be re-implemented into the lower level language. Not only does this effectively limit the architecture, it essentially defines a required staffing model which may introduce a lot of cost and redundancy in expertise. Julia solves this to a large extent, allowing for a high level, productive language to be very fast.\nA similar, related dichotomy is the two culture problem wherein domain experts (e.g. financial analysts) exist in a different sphere from developers. This manifests in many ways, such as restricting the tools that each group is permitted to use (e.g. Excel for domain experts, codebases and Git for developers). This is less of a technical problem and more of a social one. However, Julia is also one of the better languages in this regard, because much of the associated tooling is made as straightforward as possible (e.g. packaging, distribution, workflows, etc.). See Chapter 12 and Chapter 21 through Chapter 24 for more on this.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "why-julia.html#julia-and-this-book",
    "href": "why-julia.html#julia-and-this-book",
    "title": "2  Why use Julia?",
    "section": "2.3 Julia and This Book",
    "text": "2.3 Julia and This Book\nJulia is introduced insofar that a basic understanding is necessary to illustrate certain concepts. Julia is ideal in this context, because it is generally straightforward and concise, allowing the presented idea to have the spotlight (as opposed to language boiler plate or obtuse keywords and variables). The point of structuring the book like this is to allow us to introduce a wide variety of computer science concepts to the financial professional, not to introduce Julia as a programming language (there are many other resources which do that just fine).\nThis chapter seeks to motivate to the skeptical professional why we choose Julia for teaching and for work. Then, in Chapter 5 we provide an introduction to core language concepts and syntax. After this chapter,the content is focused on illustrating a number of key concepts, with Julia taking a secondary role, serving simply as a backdrop. It’s not until Chapter 21 where Julia regains the spotlight and we discuss particulars which generally matter only to those heavily using Julia more vigorously.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "why-julia.html#expressiveness-and-syntax",
    "href": "why-julia.html#expressiveness-and-syntax",
    "title": "2  Why use Julia?",
    "section": "2.4 Expressiveness and Syntax",
    "text": "2.4 Expressiveness and Syntax\nExpressiveness is the manner in which and scope of ideas and concepts that can be represented in a programming language. Syntax refers to how the code looks on the screen and its readability.\nIn a language with high expressiveness and pleasant syntax, you:\n\nGo from idea in your head to final product faster.\nEncapsulate concepts naturally and write concise functions.\nCompose functions and data naturally.\nFocus on the end-goal instead of fighting the tools.\n\nExpressiveness can be hard to explain, but perhaps two short examples will illustrate.\n\n2.4.1 Example 1: Retention Analysis\nThis is a really simple example relating Cessions, Policys, and Lives to do simple retention analysis. Retention is a measure of how much risk an insurance company holds on a policy after its own reinsurance risk transfer (ceded amount of coverage are called “cessions”).\nFirst, let’s define our data:\n\n\n# Define our data structures\nstruct Life\n    policies\nend\n\nstruct Policy\n    face\n    cessions\nend\n\nstruct Cession\n    ceded\nend\n\nNow to calculate amounts retained. First, let’s say what retention means for a Policy:\n\n# define retention\nfunction retained(pol::Policy)\n    pol.face - sum(cession.ceded for cession in pol.cessions)\nend\n\nretained (generic function with 1 method)\n\n\nAnd then what retention means for a Life:\n\nfunction retained(l::Life)\n    sum(retained(policy) for policy in life.policies)\nend\n\nretained (generic function with 2 methods)\n\n\nIt’s almost exactly how you’d specify it English. No joins, no boilerplate, no fiddling with complicated syntax. You can express ideas and concepts the way that you think of them, not, for example, as a series of dataframe joins or as row/column coordinates on a spreadsheet.\nWe defined retained and adapted it to mean related, but different things depending on the specific context. That is, we didn’t have to define retained_life(...) and retained_pol(...) because Julia can dispatch based on what you give it (this is a more powerful, generalized version of method dispatch commonly used in object-oriented programming, see Chapter 7 for more).\nLet’s use the above code in practice then.\n\n# create two policies with two and one cessions respectively\npol_1 = Policy(1000, [Cession(100), Cession(500)])\npol_2 = Policy(2500, [Cession(1000)])\n\n# create a life, which has the two policies\nlife = Life([pol_1, pol_2])\n\nLife(Policy[Policy(1000, Cession[Cession(100), Cession(500)]), Policy(2500, Cession[Cession(1000)])])\n\n\n\nretained(pol_1)\n\n400\n\n\n\nretained(life)\n\n1900\n\n\nAnd for the last trick, something called “broadcasting”, which automatically vectorizes any function you write, no need to write loops or create if statements to handle a single vs repeated case:\n\nretained.(life.policies) # retained amount for each policy\n\n2-element Vector{Int64}:\n  400\n 1500\n\n\n\n\n2.4.2 Example 2: Random Sampling\nAs another motivating example showcasing multiple dispatch, here’s random sampling in Julia, R, and Python.\nWe generate 100:\n\nUniform random numbers\nStandard normal random numbers\nBernoulli random number\nRandom samples with a given set\n\n\n\n\n\nTable 2.1: A comparison of random outcome generation in Julia, R, and Python.\n\n\n\n\n\n\n\n\n\n\nJulia\nR\nPython\n\n\n\n\nusing Distributions\n\nrand(100)\nrand(Normal(), 100)\nrand(Bernoulli(0.5), 100)\nrand([\"Preferred\",\"Standard\"], 100)\nrunif(100)\nrnorm(100)\nrbern(100, 0.5)\nsample(c(\"Preferred\",\"Standard\"),\n100, replace=TRUE)\nimport scipy.stats as sps\nimport numpy as np\n\n\nsps.uniform.rvs(size=100)\nsps.norm.rvs(size=100)\nsps.bernoulli.rvs(p=0.5,size=100)\nnp.random.choice([\"Preferred\",\"Standard\"],\nsize=100)\n\n\n\n\n\n\n\nBy understanding the different types of things passed to rand(), it maintains the same syntax across a variety of different scenarios. We could define rand(Cession) and have it generate a random Cession like we used above.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "why-julia.html#the-speed",
    "href": "why-julia.html#the-speed",
    "title": "2  Why use Julia?",
    "section": "2.5 The Speed",
    "text": "2.5 The Speed\nAs stated in the journal Nature, “Come for the Syntax, Stay for the Speed”.\nEarlier we described Aviva’s Solvency II compliance modeling, which ran 1000x faster than the prior vendor solution mentioned earlier: what does it mean to be 1000x faster at something? It’s the difference between something taking 10 seconds instead of 3 hours — or 1 hour instead of 42 days.\nWith this difference in speed, you would be able to complete existing processes much faster, or extend the analysis further. This speed could allow you to do new things: a stochastic analysis of life-level claims, machine learning with your experience data, or perform much more frequent valuation.\nHere’s a real example, comparing the runtime to calculate the price of a vanilla European call option using the Black-Scholes-Merton formula, as well as the associated code for each. Here’s the mathematical formula we are using:\n\\[\n\\begin{aligned}\n\\text{Call}(S_t, t) &= N(d_1)S_t - N(d_2)Ke^{-r(T - t)} \\\\\nd_1 &= \\frac{1}{\\sigma\\sqrt{T - t}}\\left[\\ln\\left(\\frac{S_t}{K}\\right) + \\left(r + \\frac{\\sigma^2}{2}\\right)(T - t)\\right] \\\\\nd_2 &= d_1 - \\sigma\\sqrt{T - t}\n\\end{aligned}\n\\]\nusing Distributions\n\nfunction d1(S,K,τ,r,σ)\n    (log(S/K) + (r + σ^2/2) * τ) / (σ * √(τ))\nend\n\nfunction d2(S,K,τ,r,σ)\n    d1(S,K,τ,r,σ) - σ * √(τ)\nend\n\nfunction Call(S,K,τ,r,σ)\n    N(x) = cdf(Normal(),x)\n    d₁ = d1(S,K,τ,r,σ)\n    d₂ = d2(S,K,τ,r,σ)\n    return N(d₁)*S - N(d₂) * K * exp(-r*τ)\nend\n\nS,K,τ,σ,r = 300, 250, 1, 0.15, 0.03\n\nCall(S,K,τ,r,σ) # 58.81976813699322\nfrom scipy import stats\nimport math\n\ndef d1(S,K,τ,r,σ):\n    return (math.log(S/K) + (r + σ**2/2) * τ) / (σ * math.sqrt(τ))\n\ndef d2(S,K,τ,r,σ):\n    return d1(S,K,τ,r,σ) - σ * math.sqrt(τ)\n\ndef Call(S,K,τ,r,σ):\n    N = lambda x: stats.norm().cdf(x)\n    d_1 = d1(S,K,τ,r,σ)\n    d_2 = d2(S,K,τ,r,σ)\n    return N(d_1)*S - N(d_2) * K * math.exp(-r*τ)\n\nS = 300\nK = 250\nτ = 1\nσ = 0.15\nr = 0.03\n\nCall(S,K,τ,r,σ) # 58.81976813699322\nd1&lt;- function(S,K,t,r,sig) {\n  ans &lt;- (log(S/K) + (r + sig^2/2)*t) / (sig*sqrt(t))\n  return(ans)\n} \n\nd2 &lt;- function(S,K,t,r,sig) {\n  return(d1(S,K,t,r,sig) - sig*sqrt(t))\n}\n\nCall &lt;- function(S,K,t,r,sig) {\n  d_1 &lt;- d1(S,K,t,r,sig)\n  d_2 &lt;- d2(S,K,t,r,sig)\n  return(S*pnorm(d_1) - K*exp(-r*t)*pnorm(d_2))\n}\nS &lt;- 300\nK &lt;- 250\nt &lt;- 1\nr &lt;- 0.03\nsig &lt;- 0.15\n\nCall(S,K,t,r,sig) # 58.81977\nWe find in Table 2.2 that despite the syntactic similarity, Julia is much faster than the other two.\n\n\n\nTable 2.2: Julia is nearly 20,000 times faster than Python, and two orders of magnitude faster than R.\n\n\n\n\n\n\n\n\n\n\n\nLanguage\nMedian (nanoseconds)\nMean (nanoseconds)\nRelative Mean\n\n\n\n\nPython\nnot calculated by benchmarking library\n817000.0\n19926.0\n\n\nR\n3649.0\n3855.2\n92.7\n\n\nJulia\n41.0\n41.6\n1.0\n\n\n\n\n\n\n\n2.5.1 Development Speed\nSpeed is not just great for improvement in production processes. During development, it’s really helpful too. When building something, the faster feedback loop allows for more productive development. The build, test, fix, iteration cycle goes faster this way.\nAdmittedly, most workflows don’t see a 1000x speedup, but 10x to 1000x is a very common range of speed differences vs R or Python or MATLAB.\n\n\n\n\n\n\nNote\n\n\n\nSometimes you will see less of a speed difference; R and Python have already circumvented this and written much core code in low-level languages. This is an example of what’s called the “two-language” problem where the language productive to write in isn’t very fast. For example, more than half of R packages use C/C++/Fortran and core packages in Python like Pandas, PyTorch, NumPy, SciPy, etc. do this too.\nWithin the bounds of the optimized R/Python libraries, you can leverage this work. Extending it can be difficult: what if you have a custom retention management system running on millions of policies every night? In technical terms, libraries like NumPy are not able to handle custom data types, and instead limit use to pre-built types within the library. In contrast, all types in Julia are effectively equal, even ones that you might create yourself.\nJulia packages you are using are almost always written in pure Julia: you can see what’s going on, learn from them, or even contribute a package of your own!",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "why-julia.html#more-of-julias-benefits",
    "href": "why-julia.html#more-of-julias-benefits",
    "title": "2  Why use Julia?",
    "section": "2.6 More of Julia’s benefits",
    "text": "2.6 More of Julia’s benefits\nJulia is easy to write, learn, and be productive in:\n\nIt’s free and open-source\n\nVery permissive licenses, facilitating the use in commercial environments (same with most packages)\n\nLarge and growing set of available packages\nWrite how you like because it’s multi-paradigm: vector-izable (R), object-oriented (Python), functional (Lisp), or detail-oriented (C)\nBuilt-in package manager, documentation, and testing-library\nJupyter Notebook support (it’s in the name! Julia-Python-R)\nMany small, nice things that add up:\n\nUnicode characters like α or β\nNice display of arrays\nSimple anonymous function syntax\nWide range of text editor support\nFirst-class support for missing values across the entire language\nLiterate programming support (like R-Markdown)\n\nBuilt-in Dates package that makes working with dates pleasant\nAbility to directly call and use R and Python code/packages with the PythonCall.jl and RCall packages\nError messages are helpful and tell you what line the error came from, not just the type of error\nDebugger functionality so you can step through your code line by line\n\nFor power-users, advanced features are easily accessible: parallel programming, broadcasting, types, interfaces, metaprogramming, and more.\nThese are some of the things that make Julia one of the world’s most loved languages on the StackOverflow Developer Survey.\nIn addition to the liberal licensing mentioned above, there are professional products from organizations like JuliaHub that provide hands-on support, training, IT governance solutions, behind-the-firewall package management, and deployment/scaling assistance.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "why-julia.html#tradeoffs-when-using-julia",
    "href": "why-julia.html#tradeoffs-when-using-julia",
    "title": "2  Why use Julia?",
    "section": "2.7 Tradeoffs when Using Julia",
    "text": "2.7 Tradeoffs when Using Julia\n\n2.7.1 Just-In-Time Compilation\nJulia is fast because it’s compiled, unlike R and Python where (loosely speaking) the computer just reads one line at a time. Julia compiles code “just-in-time” (JIT): right before you use a function for the first time, it will take a moment to pre-process the code section for the machine3. Subsequent calls don’t need to be re-compiled and are very fast.\nA hypothetical example: running 10,000 stochastic projections where Julia needs to pre-compile but then runs each 10x faster:\n\nJulia runs in 2 minutes: the first projection takes 1 second to compile and run, but each 9,999 remaining projections only take 10ms.\nPython runs in 17 minutes: 100ms of a second for each computation.\n\nTypically, the compilation is very fast (milliseconds), but in the most complicated cases it can be several seconds. One of these is the “time-to-first-plot” issue because it’s the most common one users encounter: super-flexible plotting libraries have a lot of things to pre-compile. So in the case of plotting, it can take several seconds to display the first plot after starting Julia, but then it’s remarkably quick and easy to create an animation of your model results. The time-to-first plot is a solvable problem that’s receiving a lot of attention from the core developers and will get better with future Julia releases.\nFor users working with a lot of data or complex calculations (like actuaries!), the runtime speedup is worth a few seconds at the start.\n\n\n2.7.2 Static Binaries\nStatic binaries are self contained executable programs which can run very specific bits of code. Simple programs which can compile down to small (in terms of size on disk) binaries which accomplish just their pre-programmed tasks. Another use case for this is to create shared libraries which could be called from other languages (this can already be done, but again requires bundling the runtime).\nJulia’s dynamic nature means that it needs to include the supporting infrastructure in order to run general code, similar to how running Python code needs to be bundled with the Python runtime.\nDevelopment is happening at the language level which would allow Julia to be compiled to a smaller, more static set of features for use in environments which are memory constrained and can’t bundle a supporting runtime.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "why-julia.html#package-ecosystem",
    "href": "why-julia.html#package-ecosystem",
    "title": "2  Why use Julia?",
    "section": "2.8 Package Ecosystem",
    "text": "2.8 Package Ecosystem\nUsing packages as dependencies in your project is assisted by Julia’ bundled package manager.\nFor each project, you can track the exact set of dependencies and replicate the code/process on another machine or another time. In R or Python, dependency management is notoriously difficult and it’s one of the things that the Julia creators wanted to fix from the start.\nThere are thousands of publicly available packages already published. It’s also straightforward to share privately, such as proprietary packages hosted internally behind a firewall.\nAnother powerful aspect of the package ecosystem is that due to the language design, packages can be combined/extended in ways that are difficult for other common languages. This means that Julia packages often interoperable without any additional coordination.\nFor example, packages that operate on data tables work without issue together in Julia. In R/Python, many features tend to come bundled in a giant singular package like Python’s Pandas which has Input/Output, Date manipulation, plotting, resampling, and more. There’s a new Consortium for Python Data API Standards which seeks to harmonize the different packages in Python to make them more consistent (R’s Tidyverse plays a similar role in coordinating their subset of the package ecosystem).\nIn Julia, packages tend to be more plug-and-play. For example, every time you want to load a CSV you might not want to transform the data into a dataframe (maybe you want a matrix or a plot instead). To load data into a dataframe, in Julia the practice is to use both the CSV and DataFrames packages, which help separate concerns. Some users may prefer the Python/R approach of less modular but more all-inclusive packages.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "why-julia.html#tools-in-your-toolbox",
    "href": "why-julia.html#tools-in-your-toolbox",
    "title": "2  Why use Julia?",
    "section": "2.9 Tools in Your Toolbox",
    "text": "2.9 Tools in Your Toolbox\nLooking at other great tools like R and Python, it can be difficult to summarize a single reason to motivate a switch to Julia, but hopefully we have sufficiently piqued your interest and we can turn to introducing important concepts.\nThat said, Julia shouldn’t be the only tool in your toolkit. SQL will remain an important way to interact with databases. R and Python aren’t going anywhere in the short term and will always offer a different perspective on things!\nBeing a productive financial profession means being proficient in the language of computers so that you could build and implement great things. In a large way, the choice of tools and paradigms shape your focus. Productivity is one aspect, expressiveness is another, speed one more. There are many reasons to think about what tools you use and trying out different ones is probably the best way to find what works best for you.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "why-julia.html#footnotes",
    "href": "why-julia.html#footnotes",
    "title": "2  Why use Julia?",
    "section": "",
    "text": "Python first appeared in 1990. R is an implementation of S, which was created in 1976, though depending on when you want to place the start of an independent R project varies (1993, 1995, and 2000 are alternate dates). The history of these languages is long and substantial changes have occurred since these dates.↩︎\nAviva Case Study↩︎\nJulia can also precompile code ahead of time to avoid the latency involved with running a function for the first time each time a Julia session is started.↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "foundations-financial.html",
    "href": "foundations-financial.html",
    "title": "Foundations: Effective Financial Modeling",
    "section": "",
    "text": "“The map is not the territory.” – Alfred Korzybski (1933)\n\nWelcome to the heart of our exploration of financial modeling. Over the next two chapters, we’ll lay out not only the conceptual foundations that shape how and why we model financial phenomena, but also the practical approaches and data management techniques that make these models work in real-world settings. This chapter is not the technical foundations of financial modeling - instead we focus on the “how” and “why” of modeling in a professional setting.\nFirst, we’ll focus on why financial models matter and what sets them apart from other forms of modeling. This discussion moves from the philosophical–how simplified representations of reality can still provide powerful insights–right down to the practical concerns of “small world” assumptions and “large world” complexities. You’ll discover how models can exist purely for predictive accuracy, even when they don’t fully capture the underlying causal mechanisms, and why this can be both a strength and a danger. Along the way, we’ll unpack the attributes of effective modelers and explore how curiosity and rigor create more resilient analytical tools.\nThen, armed with this solid conceptual footing, we’ll turn to the practicalities: selecting the right modeling approach, handling messy data, and navigating trade-offs such as complexity vs. interpretability. You’ll learn about building processes that prepare, transform, and feed data into models in ways that ensure results remain transparent and reliable. We’ll also explore techniques for balancing simulation, machine learning, and statistical models based on your project’s unique requirements.\nThink of these chapters as two sides of the same coin: if the first chapter sets the stage by clarifying ‘why’ and ‘what’ we need to model, the second chapter equips you with ‘how’. By the end, you’ll have an integrated perspective: broad conceptual principles for designing a meaningful model and the hands-on skills to bring that model to life.",
    "crumbs": [
      "Foundations: Effective Financial Modeling"
    ]
  },
  {
    "objectID": "elements-of-financial-modeling.html",
    "href": "elements-of-financial-modeling.html",
    "title": "3  Elements of Financial Modeling",
    "section": "",
    "text": "3.1 Chapter Overview\nWe explain what constitutes a financial model and what are common uses of a model. We explore the key attributes of models, discuss different modeling approaches and their trade-offs, and some philosophy around what models are and how they capture important dynamics about the world.",
    "crumbs": [
      "Foundations: Effective Financial Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Elements of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "elements-of-financial-modeling.html#what-is-a-model",
    "href": "elements-of-financial-modeling.html#what-is-a-model",
    "title": "3  Elements of Financial Modeling",
    "section": "3.2 What is a Model?",
    "text": "3.2 What is a Model?\nA model is a simplified representation of reality designed to help us understand, analyze, and make predictions about complex systems. In finance, models distill the intricate web of market behaviors, economic factors, and financial instruments into tractable mathematical and computational components. We may build models for a variety of reasons, as listed in Table 3.1.\n\n\n\nTable 3.1: The REDCAPE model use framework, from “The Model Thinker” by Scott Page.\n\n\n\n\n\n\n\n\n\nUse\nDescription\n\n\n\n\nReason\nTo identify conditions and deduce logical implications.\n\n\nExplain\nTo provide (testable) explanations for empirical phenomena.\n\n\nDesign\nTo choose features of institutions, policies, and rules.\n\n\nCommunicate\nTo relate knowledge and understandings.\n\n\nAct\nTo guide policy choices and strategic actions.\n\n\nPredict\nTo make numerical and categorical predictions of future and unknown phenomena.\n\n\nExplore\nTo investigate possibilities and hypotheticals.\n\n\n\n\n\n\nFor example, say we want to simulate the returns for the stocks in our retirement portfolio. It would be impossible to try to build a model which would capture all of the individual people working jobs and making decisions, weather events that damage property, political machinations, etc. Instead, we try to capture certain fundamental characteristics. For example, it is common to model equity returns as cumulative pluses and minuses from random movements where those movements have certain theoretical or historical characteristics.\nWhether we are using this model of equity returns to estimate available retirement income or replicate an exotic option price, a key aspect of the model is the assumptions used therein. For the retirement income scenario we might assume a healthy 8% return on stocks and conclude that such a return will be sufficient to retire at age 53. Alternatively, we may assume that future returns will follow a stochastic path with a certain distribution of volatility and drift. These two assumption sets will produce output - results from our model that must be inspected, questioned, and understood in the context of the “small world” of the model’s mechanistic workings. Lastly, to be effective practitioners we must be able to contextualize the “small world” results within the “large world” that exists around us.\n\n3.2.1 “Small World” vs “Large World”\nThe distinction between the modeled “small world” and real life “large world” modeling is fundamental to understanding model limitations:\n\nSmall World: The constrained, well-defined environment within your model where all rules and relationships are explicitly specified.\nLarge World: The complex, real-world environment where your model must operate, including factors not captured in your assumptions.\n\nSay that our model is one that discounts a fixed set of future cashflows using the US Treasury rate curve. If I run my model using current rates today, and then re-run my model tomorrow with the same future cashflows and the present value of those cashflows has increased by 5%, I may ask: “why has the result changed so much in such a short period of time?”\nIn the “small”, mechanistic world of the model I may be able to see that the discount rates have fallen substantially. The small world answer is that the inputs have changed which produced a mechanical change in the output. The large world answer may be that the Federal Reserve lowered the Federal Funds Rate to prevent the economy from entering a deflationary recession.\nOf course, we can’t completely explain the relation between our model and the real world (otherwise we could capture that relationship in our model!). An effective practitioner will always try to look up from the immediate work and take stock of how the world at large is or is not reflected in the model.\n\n\n\n\n\n\nTipFinancial Modeling Pro Tip\n\n\n\nWhen a model output swings materially between runs, first check the Small World mechanics (e.g., input rates changed) before hypothesizing Large World causes (e.g., policy shifts). Then reconcile both views to avoid over- or under-reacting.",
    "crumbs": [
      "Foundations: Effective Financial Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Elements of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "elements-of-financial-modeling.html#what-is-a-financial-model",
    "href": "elements-of-financial-modeling.html#what-is-a-financial-model",
    "title": "3  Elements of Financial Modeling",
    "section": "3.3 What is a Financial Model?",
    "text": "3.3 What is a Financial Model?\nFinancial models are those used extensively to ascertain better understanding of complex contracts, perform scenario analysis, and inform market participants’ decisions related to perceived value (and therefore price). It can’t be quantified directly, but it is likely not an exaggeration that many billions of dollars are transacted each day as a result of decisions made from the output of financial models.\nMost financial models can be characterized with a focus on the first or both of:\n\nAttempting to project patterns of cashflows or obligations at future timepoints\nReducing the projected obligations into a current value\n\nExamples of this:\n\nProjecting a retiree’s savings through time (1), and determining how much they should be saving today for their retirement goal (2)\nProjecting the obligation of an exotic option across different potential paths (1), and determining the premium for that option (2)\nDetermining a range of outcomes (1) for risk modeling purposes, and translating those outcomes into a risk or capitalization level (2)\n\nModels are sometimes taken a step further, such as transforming the underlying economic view into an accounting or regulatory view (such as representing associated debits and credits, capital requirements, or associated intangible, capitalized balances). In other words, many models go further than simply modeling anticipated cashflows by building representations of entities (companies or portfolios) which are associated with the cashflows.\n\n3.3.1 Difference from Data Science\nWe should also distinguish a financial model from a purely statistical model, where often the inputs and output data are known and the intention is to estimate relationships between variables (example: linear regressions). That said, a financial model may have statistical components and many aspects of modeling are shared between the two kinds.\nIn this book, we focus on the practice of financial modeling in the ways that it is distinct from statistics and data science. To delineate the practices:\n\nStatistics, or Statistical Modeling is the practice of applying procedures and probabilistic reasoning to data in order to determine relationships between things or to predict outcomes.\nData Science includes statistical modeling but also incorporates the art and science of good data hygiene, data pipelines and pre-processing, and more programming than a pure statistician usually uses.\n\nFinancial Modeling is similar in the goal of modeling complex relationships or making predictions (a modeled price of an asset is simply a prediction of what its value is), however, it differs from data science in a few ways:\n\nFinancial modeling often encodes theory-driven structures and constraints, then calibrates parameters to data, rather than inferring all relationships purely from data.\nFinancial modeling generally contains more unique ‘objects’ than a statistical model. The latter may have derived numerical relationships between data, however a financial model would have things like the concept of a company or portfolio, or sets of individually identifiable assets, or distinct actors in a system.\nFinancial modeling often involves a lot more simulation and hypothecating, while data science is focused on drawing conclusions from what data has already been observed.\n\nNonetheless, there is substantial overlap in practice. For example, the assumption in a financial model (volatility, economic conditions, etc.) may be derived statistically from observed data. Given the overlap in topics, statistical content is sometimes covered in this book (including a from-the-ground-up view of modern Bayesian approaches in Chapter 14).",
    "crumbs": [
      "Foundations: Effective Financial Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Elements of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "elements-of-financial-modeling.html#key-considerations-for-a-model",
    "href": "elements-of-financial-modeling.html#key-considerations-for-a-model",
    "title": "3  Elements of Financial Modeling",
    "section": "3.4 Key Considerations for a Model",
    "text": "3.4 Key Considerations for a Model\nWhen creating a model, whether a data model, a conceptual model, or any other type, certain key considerations are generally important to include to ensure it is effective and useful. Some essential considerations include:\n\n\n\n\n\n\n\nConsideration\nDescription\n\n\n\n\nObjective\nClearly define what the model aims to achieve.\n\n\nBoundaries\nSpecify the limits and constraints of the model to avoid scope creep.\n\n\nVariables\nIdentify and define all variables involved in the model.\n\n\nParameters\nInclude constants or coefficients that influence the variables.\n\n\nDependencies\nDescribe how variables interact with each other.\n\n\nRelationships\nDetail the connections between different components of the model.\n\n\nInputs\nSpecify the data or resources required for the model to function.\n\n\nOutputs\nDefine what results or predictions the model produces.\n\n\nUnderlying Assumptions\nDocument any assumptions made during the model’s development to clarify its limitations and validity.\n\n\nValidation Criteria\nOutline how the model’s accuracy and reliability are tested.\n\n\nPerformance Metrics\nDefine the metrics used to evaluate the model’s performance.\n\n\nScalability\nEnsure the model can handle increased data or complexity if needed.\n\n\nAdaptability\nAllow for adjustments or updates as new information or requirements arise.\n\n\nDocumentation\nProvide comprehensive documentation explaining how the model works, including algorithms, data sources, and methods.\n\n\nTransparency\nMake the model’s workings understandable to stakeholders or users.\n\n\nUser Interface\nDesign an intuitive interface if the model is interactive.\n\n\nEase of Use\nEnsure that the model is user-friendly and accessible to its intended audience.\n\n\nEthics\nAddress any ethical concerns related to the model’s application or impact.\n\n\nRegulations\nEnsure compliance with relevant laws and regulations.\n\n\n\nIncluding these attributes helps create a robust, reliable, and practical model that effectively serves its intended purpose.",
    "crumbs": [
      "Foundations: Effective Financial Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Elements of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "elements-of-financial-modeling.html#sec-predictive-vs-explanatory",
    "href": "elements-of-financial-modeling.html#sec-predictive-vs-explanatory",
    "title": "3  Elements of Financial Modeling",
    "section": "3.5 Predictive versus Explanatory Models",
    "text": "3.5 Predictive versus Explanatory Models\nGiven a set of inputs, our model will generate an output and we are generally interested in its accuracy. The model need not have a realistic mechanism for how the world works. That is, we may primarily be interested in accurately calculating an output value without the model having any scientific, explanatory power of how different parts of the real-world system interact.\n\n3.5.1 A Historical Example\nA historical parallel can be found in the shift from the Ptolemaic to the Copernican model of the solar system. While the Ptolemaic model was predictively accurate for its time, the Copernican model offered a more fundamentally correct explanation of the underlying mechanics1.\nThe existing Ptolemaic model used a geocentric view of the solar system in which the planets and sun orbited the Earth in perfect circles with an epicycle used to explain retrograde motion (as seen in Figure 3.1). Retrograde motion is the term used to describe the apparent, temporarily reversed motion of a planet as viewed from Earth when the Earth is overtaking the other planet in orbit around the sun. Despite a fundamentally flawed underlying theory, the geocentric model was accurate enough to match the observational data for the position of the planets in the sky.\n\n\n\n\n\n\nFigure 3.1: In the Ptolemaic solar model, the retrograde motion of the planets was explained by adding an epicycle to the circular orbit around the earth.\n\n\n\nFamously, Copernicus came along and said that the sun, not the Earth, should be at the center (a heliocentric model). Earth revolves around the sun! Today, we know this to be a much better description of reality than one in which the Earth arrogantly sits at the center of the universe. However the model was actually slightly less accurate in predicting the apparent position of the planets (to the limits of observational precision at the time)! Why would this be?\nFirst, the Copernican proposal still used perfectly circular orbits with an epicycle adjustment, which we know today to be inaccurate (versus an elliptical orbit consistent with the theory of gravity). Despite being more scientifically correct, it was still not the complete picture.\nSecond, the geocentric model was already very accurate because it was essentially a Taylor series approximation which described, to sufficient observational accuracy, the apparent position of the planet relative to the Earth. The heliocentric model was effectively a re-parameterization of the orbital approximation.\nThird, we have considered a limited criteria for which we are evaluating the model for accuracy, namely apparent position of the planets. It’s not until we contemplate other observational data that the Copernican model would demonstrate greater modeling accuracy: apparent brightness of the planets as they undergo retrograde motion and angular relationship of the planets to the sun.\nFor modelers today, this demonstrates a few things to keep in mind:\n\nPredictive models need not have a scientific, causal structure to make accurate predictions.\nIt is difficult to capture the complete scientific inter-relationships of a system and much care and thought needs to be given in what aspects are included in our model.\nWe should look at, or seek out, additional data that is related to our model because we may accurately overfit to one outcome while achieving an increasingly poor fit to other related variables.\n\nStriving to better understand the world is a good thing to do but trying to include more components into the model is not always going to advance a given goal.\n\n\n3.5.2 Examples in the Financial Context\n\n3.5.2.1 Home Prices\nAmerican home prices exhibit strong seasonality and peak around April of each year. We may find that including a simple oscillating term in our model captures the variability in prices better than if we tried to imperfectly capture the true market dynamics of home sales: supply and demand curves varying by personal (job bonus payment timing, school calendars), local (new homes built, company relocation), and national (monetary policy, tax incentives for home-ownership) factors. In other words, one could likely predict a stable pattern like this with a model that contains a simple sinusoidal periodic component. One could likely spend months trying to build a more scientific model and not achieve as good of fit, even though the latter tries to be more conceptually accurate.\n\n\n\n\n\n\nFigure 3.2: House prices (seasonally unadjusted) often peak around April each year. A simple seasonal component can capture much of this variation without modeling full market microstructure, for example \\(p_t = \\alpha + \\beta \\sin(2\\pi t/12 + \\phi) + \\varepsilon_t\\), where t is in months.\n\n\n\n\n\n3.5.2.2 Replicating Portfolio\nAnother example in the financial modeling realm: in attempting to value a portfolio of insurance contracts a replicating portfolio of hypothetical assets will sometimes be constructed2. The point of this is to create a basket of assets that can be more quickly (minutes to hours) valued in response to changing market conditions than it would take to run the actuarial model (hours to days). This is an example where the basket of assets has no ability to explain why the projected cashflows are what they are - but retains strong predictive accuracy.\n\n\n\n3.5.3 Predictive vs Explanatory Models in Practice\nSection 32.5 describes practical considerations for evaluating models in both predictive and explanatory contexts.",
    "crumbs": [
      "Foundations: Effective Financial Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Elements of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "elements-of-financial-modeling.html#types-of-models",
    "href": "elements-of-financial-modeling.html#types-of-models",
    "title": "3  Elements of Financial Modeling",
    "section": "3.6 Types of Models",
    "text": "3.6 Types of Models\nDifferent modeling approaches come with their own sets of trade-offs. Common modeling approaches, and the inherent trade-offs, may include:\n\n\n\n\n\n\n\n\n\n\nModel Type\nDescription\nExamples\nTrade-offs\nAssumptions\n\n\n\n\nStatistical Models\nUse mathematical relationships to describe data.\nLinear regression, logistic regression\nSimplicity vs. Accuracy: Often simpler and more interpretable but may not capture complex relationships.\nTypically rely on assumptions like linearity and normality that may not always hold true.\n\n\nMachine Learning Models\nLearn patterns from data using algorithms.\nDecision trees, random forests, neural networks\nComplexity vs. Interpretability: Capture complex patterns but are often less interpretable.\nOverfitting: Risk of overfitting, requiring careful validation and tuning. Data Requirements: Require large datasets.\nPerformance can degrade with limited or noisy data.\n\n\nProbabilistic Models\nUse probability distributions to model uncertainty and relationships.\nBayesian networks, probabilistic graphical models\nFlexibility vs. Computational Complexity: Handle uncertainty and complex relationships but require sophisticated computations.\nMay require assumptions about the nature of probability distributions and dependencies.\n\n\nSimulation Models\nUse computational models to simulate complex systems or scenarios.\nMonte Carlo simulations, agent-based models\nAccuracy vs. Computational Expense: Can be computationally expensive and time-consuming.\nDetail vs. Generalization: High-fidelity simulations may be overkill for simpler problems.\nMay require simplifications or assumptions for computational feasibility.\n\n\nTheoretical Models\nBased on theoretical principles to explain phenomena.\nEconomic models, physical models\nPrecision vs. Practicality: Provide foundational understanding but may rely on idealizations.\nApplicability: Highly accurate in specific contexts but less so in broader situations.\nOften rely on idealizations or simplifications.\n\n\nEmpirical Models\nUse historical data to predict future outcomes.\nTime series forecasting, econometric models\nData Dependence vs. Predictive Power: Rely heavily on historical data and may not perform well if patterns change.\nContext Sensitivity: Accurate for specific data but may not generalize well.\nPerformance is heavily dependent on the quality and relevance of historical data.\n\n\nHybrid Models\nCombine different modeling approaches to leverage their strengths.\nCombining statistical and machine learning approaches\nComplexity vs. Versatility: Aim to leverage strengths of different approaches but can be complex to design.\nIntegration Challenges: May present challenges in integration and consistency.\nMay inherit assumptions from combined models.\n\n\n\nSummary of Common Trade-offs:\n\nComplexity vs. Simplicity: More complex models can capture more nuanced details but are harder to understand and manage.\nAccuracy vs. Interpretability: High-accuracy models may be less interpretable, making it harder to understand their decision-making process.\nData Requirements: Some models require large amounts of data or very specific types of data, which can be a limitation in practice.\nComputational Resources: More sophisticated models or simulations can require significant computational power, which may not always be feasible.\n\nUnderstanding these trade-offs helps in selecting the most appropriate modeling approach based on the specific needs of the problem at hand and the resources available.",
    "crumbs": [
      "Foundations: Effective Financial Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Elements of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "elements-of-financial-modeling.html#footnotes",
    "href": "elements-of-financial-modeling.html#footnotes",
    "title": "3  Elements of Financial Modeling",
    "section": "",
    "text": "Prof. Richard Fitzpatrick has excellent coverage of the associated mathematics and implications in “A Modern Almagest”: https://farside.ph.utexas.edu/books/Syntaxis/Almagest/Almagest.html↩︎\nSee, e.g., SOA Investment Symposium March 2010. Replicating Portfolios in the Insurance Industry (Curt Burmeister Mike Dorsel Patricia Matson)↩︎",
    "crumbs": [
      "Foundations: Effective Financial Modeling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Elements of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "financial-modeling-practice.html",
    "href": "financial-modeling-practice.html",
    "title": "4  The Practice of Financial Modeling",
    "section": "",
    "text": "4.1 Chapter Overview\nHaving covered what models are and what they accomplish, we turn to the craft of modeling: what distinguishes a good model from a bad model; likewise what are attributes of an astute practitioner. Lastly, we cover some more “nuts and bolts” topics such as data handling and good governance practices.",
    "crumbs": [
      "Foundations: Effective Financial Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Practice of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "financial-modeling-practice.html#what-makes-a-good-model",
    "href": "financial-modeling-practice.html#what-makes-a-good-model",
    "title": "4  The Practice of Financial Modeling",
    "section": "4.2 What makes a good model?",
    "text": "4.2 What makes a good model?\nThe answer is: it depends.\n\n4.2.1 Achieving original purpose\nA model is built for a specific set of reasons and therefore we must evaluate a model in terms of achieving that goal. We should not critique a model if we want to use it outside of what it was intended to do. This includes: contents of the output and required level of accuracy.\nA model may have been created for scenario analysis to value all assets in a portfolio to within half a percent of a more accurate, but much more computationally expensive model. If we try to add a never-before-seen asset class or use the model to order trades, then we may be extending beyond the design scope of the original model and lose predictive accuracy.\n\n\n4.2.2 Usability\nHow easy is it for someone to use? Does it require pages and pages of documentation, weeks of specialized training and an on-call help desk? All else equal, the heavier the support and training required, the lower the model’s usability. However, one may sometimes wish to create a highly capable, complex model which is known to require a high amount of experience and expertise. An analogy here might be the cockpit of a small Cessna aircraft versus a fighter jet: the former is a lot simpler and takes less training to master but is also more limited.\nFigure 4.1 illustrates this concept and shows that if your goal is very high capability that you may need to expect to develop training materials and support the more complex model. On this view, a better model is one that is able to have a shorter amount of time and experience to achieve the same level of capability.\n\n\n\n\n\n\n\nFigure 4.1: Tradeoff between complexity and capability\n\n\n\n\n\n\n4.2.3 Performance\nFinancial models are generally not used for their awe-inspiring beauty - users are results oriented and the faster a model returns the requested results, the better. Aside from direct computational costs such as server runtime, a shorter model runtime means that one can iterate faster, test new ideas on the fly, and stay focused on the problem at hand.\nMany readers may be familiar with the cadence of (1) try running model overnight, (2) see results failed in the morning, (3) spend day developing, (4) repeat step 1. It is preferred if this cycle can be measured in minutes instead of hours or days.\nOf course, requirements must be considered here too: needs for high frequency trading, daily portfolio rebalancing, and quarterly financial reporting models have different requirements when it comes to performance.\n\n\n4.2.4 Separation of Model Logic and Data\nWhen data is intertwined with business logic it can be difficult to understand, maintain, or adapt a model. Spreadsheets are a common example of where data exists commingled with business logic. An alternative which separates data sources from the computations provides for better model service in the future.\n\n\n4.2.5 Organization of Model Components and Architecture\nIf model components or data inputs are spread out in a disorganized way, it can lead to usability and maintenance issues. As an example, oftentimes it’s incredibly difficult to ascertain a model’s operation if inputs are spread out across locations on many spreadsheet tabs. Or if related calculations are performed in multiple locations, or if it’s not clear where the line is drawn between calculations performed in the worksheets or in macros.\nIf logical components or related data are broken out into discrete parts of a model, it becomes easier to reason about model behavior or make modifications. Compartmentalization is an important principle which allows a larger model to remain comprised of simpler components where whole model is greater than the sum of the pieces.\n\n\n4.2.6 Abstraction of Modeled Systems\nAt different times we are interested in different ladder of abstraction: sometimes we are interested in the small details, but other times we are interested in understanding the behavior of systems at a higher level.\nSay we are an insurance company with a portfolio of fixed income assets supporting long term insurance liabilities. We might delineate different levels of abstraction like so:\n\n\n\n\n\nThink about moving up and down a ladder of abstraction when analyzing a problem.\n\n\n\n\n\nTable 4.1: An example of the different levels of abstraction when thinking about modeling an insurance company’s assets and liabilities.\n\n\n\n\n\n\nItem\n\n\n\n\nMore Abstract\nSensitivity of an entire company’s solvency position\n\n\n\nSensitivity of a portfolio of assets\n\n\n\nBehavior over time of an individual contract\n\n\nMore granular\nMechanics of an individual bond or insurance policy\n\n\n\n\n\n\nAt different times, we are often interested in different aspects of a problem. In general, you start to be able to obtain more insights and a greater understanding of the system when you move up the ladder of abstraction.\nIn fact, a lot of designing a model is essentially trying to figure out where to put the right abstractions. What is the right level of detail to model this in and what is the right level of detail to expose to other systems?\nLet us also distinguish between vertical abstraction, as described above, and horizontal abstraction which will refer to encapsulating different properties, or mechanics of components of the model at the same level of vertical abstraction. For example, both asset and liability mechanics sit at the most granular level in Table 4.1, But it may make sense in our model to separate the data and behavior from each other. If we were to do that, that would be an example of creating horizontal abstraction in service of our overall modeling goals.\nThis book will introduce powerful, programmatic ways to handle this through things like packages, modules, namespaces, and functions.",
    "crumbs": [
      "Foundations: Effective Financial Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Practice of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "financial-modeling-practice.html#what-makes-a-good-modeler",
    "href": "financial-modeling-practice.html#what-makes-a-good-modeler",
    "title": "4  The Practice of Financial Modeling",
    "section": "4.3 What makes a good modeler?",
    "text": "4.3 What makes a good modeler?\nA model is nothing without its operator, and a skilled practitioner is worth their weight in gold. What elements separate a good modeler from a mediocre modeler?\n\n4.3.1 Domain Expertise\nAn expert who knows enough about all of the domains that are applicable is crucial. Imagine if someone said let’s emulate an architect by having a construction worker and an artist work together. It’s all too common for business to attempt to pair a business expert with an information technologist in the same way.\nUnfortunately, this means that there’s generally no easy way out of learning enough about finance, actuarial science, computers, and/or programming in order to be an effective modeler.\nAlso, a word of warning for the financial analysts out there: the computer scientists may find it easier to learn applied financial modeling than the other way around since the tools, techniques, and language of problem solving is already a more general and flexible skill-set. There are more technologists starting banks than there are financiers starting technology companies.\n\n\n4.3.2 Model Theory\nIf it is granted that financial modeling must involve, as the essential part, a building up of modeler’s knowledge, the next issue is to characterize that knowledge more explicitly. The modeler’s knowledge should be regarded as a theory, in the sense of Ryle’s1 “Concept of the Mind.” Very briefly: a person who has or possesses a theory in this sense knows how to do certain things and in addition can support the actual doing with explanations, justifications, and answers to queries, about the model and its results2.\nA financial model is rarely left in a final state. Regulatory changes, additional mechanics, sensitivity testing, market dynamics, new products, and new systems to interact with force a model to undergo change and development through its entire life. And like a living thing, it must have nurturing caregivers. This metaphor sounds extended, but Naur’s point is that unless the model also lives in the heads of its developers then it cannot successfully be maintained through time:\n\n“The conclusion seems inescapable that at least with certain kinds of large programs, the continued adaptation, modification, and correction of errors in them, is essentially dependent on a certain kind of knowledge possessed by a group of programmers who are closely and continuously connected with them.” - Peter Naur, Programming as Theory Building, page 395.\n\nAssume that we need to adapt the model to fit a new product. One possessing a high degree of model theory includes:\n\nthe ability to describe the trade-offs between alternate approaches that would accomplish the desired change\nrelate the proposed change to the design of the current system and any challenges that will arise as a result of prior design decisions\nprovide a quantitative estimation for the impact the change will have: runtime, risk metrics, valuation changes, etc.\nAnalogize system behavior to themselves and to others\nDescribe key limitations that the model has and where it is most divorced from the reality it seeks to represent.\n\nAbstractions and analogies of the system are a critical aspect of model theory, as the human mind cannot retain perfectly precise detail about how the system works in each sub-component. The ability to, at some times, collapse and compartmentalize parts of the model to limit the mental overload while at others recall important implementation details requires training - and is enhanced by learning concepts like those which will be covered in this book.\nAn example of how the right abstractions (and language describing those abstractions) can be helpful in simplifying the mental load:\nInstead of:\nThe valuation process starts by reading an extract into three tabs of the spreadsheet. A macro loops through the list of policies on the first tab and in column C it gives the name of the applicable statutory valuation ruleset. The ruleset is defined as the combination of (1) the logic in the macro in the “Valuation” VBA module with, (2) the underlying rate tables from the tabs named XXX to ZZZ, along with (3) the additional policy level detail on the second tab. The valuation projection is then run with the current policy values taken from the third tab of the spreadsheet and the resulting reserve (equal to the actuarial present value of claims) is saved and recorded in column J of the first tab. Finally, a pivot table is used to sum up the reserves by different groups.\nWe could instead design the process so that the following could be said instead:\nPolicy extracts are parsed into a Policy datatype which contains a subtype ValuationKind indicating the applicable statutory ruleset to apply. From there, we map the valuation function over the set of Policys and perform an additive reduce to determine the total reserve.\nThere are terminologies and concepts in the second example which we will develop over the course of this section of the book - we don’t want to dwell on the details right now. However, we do want to emphasize that the process itself being able to condensed down to descriptions that are much more meaningful to the understanding of the model is a key differentiator for a code-based model instead of spreadsheets. It is not exaggerating that we could develop a handful of compartmentalized logics such that our primary valuation process described above could look like this in real code:\npolicies = parse(Policy,CSV.File(\"extract.csv\")) \nreserve = mapreduce(value,+,policies)\nWe’ve abstracted the mechanistic workings of the model into concise and meaningful symbols that not only perform the desired calculations but also make it obvious to an informed but unfamiliar reader what it’s doing.\nparse , mapreduce, + , value , Policy are all imbued with meaning - the first three would be understood by any computer scientist by the nature of their training (and is training that this book covers). The last two are unique to our model and have “real world” meaning that our domain expert modeler would understand which analogizes very directly to the way we would suggest implementing the details of value or Policy. The benefit of this, again, is to provide tools and concepts which let us more easily develop model theory.\n\n\n4.3.3 Curiosity\nBy cultivating curiosity, modelers can drive innovation, uncover new insights, and continuously improve their models and understanding of financial systems.\nNo model, no matter how sophisticated, ever delivers a “final” answer. If anything, a good financial model sparks as many new questions as it answers. This is where the best modelers distinguish themselves: they nurture a healthy skepticism with surface-level explanations.\nTake, for instance, the gnawing feeling you get when a model’s output seems “off” but you can’t quite put your finger on why. The untrained eye might chalk it up to randomness or let it slide, but genuine curiosity won’t settle for a hand-wavy excuse. The itch to resolve every weird edge case or apparent contradiction, to ask “what if?” and “why not?” is the spark that propels a practitioner beyond rote calculation into discovery.\nIn practice, carrying curiosity into modeling means:\n\nTaking the time to poke holes in every story your model tells. If two approaches give wildly different answers for the same scenario, don’t sweep that under the rug. Dig until you’ve either found the bug or learned a new subtlety.\nGoing down rabbit holes. Sometimes the best model improvements stem from following up on the “trivial” anomaly hiding in the numerical ‘blip’ every 12 months. Ask yourself: is there a structural reason, a missing piece of data, or an assumption that needs to be made explicit?\nPursuing the “why” behind the numbers. Instead of blindly running scenarios, become obsessed with the model’s behavior. If changing an input slightly has an outsized effect elsewhere, dig into the feedback mechanism that causes it.\nChallenging your own assumptions. No matter how seasoned you are, ask foundational questions and reconsider the “obvious.” There are no dumb questions! You’d be surprised how many “everybody knows that” ideas are actually half-remembered lore.\nLearning from surprises. Whenever the model spits out something bizarre, treat it like an opportunity rather than a headache. Sometimes the oddball result teaches you more about the system than any routine validation could.\nTrying new techniques and tools to keep your intellectual toolbox diverse. Look for the overlap between different things, such as recognizing the similarities between different areas of practice, even if it’s not your ‘specialty’.\n\nThe best modelers I’ve worked with aren’t necessarily the flashiest coders or the most fluent in finance. They’re just relentless in their quest to not leave loose ends untied.\n\n\n4.3.4 Rigor\nIf curiosity is the fuel, rigor is the steering wheel. All of that wandering through the thickets of “why?” needs a reliable process to keep from becoming noise or hand-waving. Rigor is what separates “I think it works” from “Here’s why it works, and here are its limits.”\nWhen developing a model it’s important to ensure that assumptions and parameters are very clear, the methodology is in line with established theory, and appropriate thought has been given to how the model will be used. Additionally one should be mindful of standards of practice. For example, professional actuarial societies have a long list of Actuarial Standards of Practice (“ASOPs”), some of which apply to modeling and the use of data that models ultimately rely on. Regardless of the applicable standards, many of them share these aspects of the best modelers:\n\nDocument your thinking as you go. Write it out, whether it’s in a code comment, a README, or your own notebook. If you can’t explain your logic and your parameters, you probably don’t understand them as well as you think.\nDemand evidence for your choices—don’t just trust your gut or yesterday’s industry standard. Check your results against reality, not just an assumed “right answer.” This means obsessive test cases, sensitivity checks, and “could we break this?” scenarios.\nHold the model to a higher standard than tradition requires. Don’t just meet regulator norms if you can do better—set your own red-lines for quality, accuracy, and reproducibility.\nDon’t hide the warts. Make uncertainty visible, not hypothetical. Annotate what’s based on thin data versus what’s on solid ground. Rigor means being honest about what you don’t know—or what the model simply can’t say.\nLean on first principles. Oftentimes there will be a ‘simpler’ way to model something, but making explicit all components of an interaction can be illuminating. For example, if you have a complex, multi-leg transaction that ‘works like exotic option ABC’, don’t always rely on that heuristic. Alternatively, model out each leg of the transaction for clarity and confirmation of your understanding.\n\nA bad model can be worse than no model at all. Through rigorous efforts, a minimum standard of quality can be obtained.\n\n\n4.3.5 Clarity\n“Clarity” means never losing sight of the fact your model is only as useful as it is understandable.\n\nPrecise language: Use well-defined terms and avoid ambiguity in communications. If a term has overloaded meanings (“reserve,” “duration,” “return”), either define it up front or pick a less ambiguous word.\nSpell out your assumptions, not just your output. Make the philosophy and the scaffolding explicit—what did you leave in, what did you leave out, and why?\nVisual communication: Utilize diagrams and visualizations to explain complex concepts. A simple stylized sketch rarely hurts and often helps.\nAudience-appropriate communication: Adjust your explanations depending on whether you’re talking to other developers, business stakeholders, or end users.\nRegular review: Periodically update documentation to ensure ongoing clarity and accuracy. If you woke up with amnesia, would the next steps seem obvious?\n\nClarity is about making your future self—and your colleagues—thankful, not furious, that you were ever given keyboard access.\n\n\n4.3.6 Humility\nThe world is complicated in ways we can sometimes describe and never fully anticipate. A humble modeler tries to understand what the model can and cannot claim. And in good faith will try to share the limitations the model has with stakeholders, such as saying something like “we have a lot of data for low-rate environments but rapidly rising environments haven’t been observed in the dataset”.\nIrreducible & reducible (epistemic) uncertainty are critical concepts for a modeler to understand and communicate:\n\nIrreducible uncertainty: Also known as aleatoric uncertainty, this refers to the inherent randomness in a system that cannot be reduced by gathering more information.\n\nExamples include: future market fluctuations, individual policyholder behavior, or natural disasters.\n\nReducible (epistemic) uncertainty: This type of uncertainty stems from a lack of knowledge and can potentially be reduced through further study or data collection.\n\nExamples include: parameter estimation errors, model specification errors, or data quality issues.\n\n\nTable 4.2 describes this in more detail. It’s not always necessary to describe each of these types of uncertainty for every model but knowing your enemy is the first step in fighting it.\nA humble modeler acknowledges these uncertainties and communicates them clearly to stakeholders. This avoids overconfidence in model predictions and keeps one open to new information and alternative perspectives. By maintaining a humble attitude, modelers can build trust with stakeholders and make more informed decisions based on model outputs.\n\n\n\n\nTable 4.2: In attempting to model an uncertain world, we can be even more granular and specific in discussing sources of that uncertainty. This table summarizes commonly noted kinds of uncertainty that arise, and whether we can reduce the uncertainty by doing better (more data, better data, better models, etc.) or not.\n\n\n\n\n\n\n\n\n\n\n\nType of Uncertainty\nKey Characteristics\nReducibility\nExample\n\n\n\n\nAleatory (Process) Uncertainty\n- Inherent randomness (aka “irreducible uncertainty”)\n- Cannot be eliminated, even with perfect knowledge\nIrreducible\nRolling dice or coin flips; outcome is inherently uncertain despite full knowledge of initial state\n\n\nEpistemic (Parameter) Uncertainty\n- Due to limited data/knowledge (aka “reducible uncertainty”)\n- Imperfect information or model parameters\nReducible\n(more data / improved modeling)\nUncertainty in a model’s parameters (e.g., climate sensitivity) that can be refined with more research\n\n\nModel Structure Uncertainty\n- Uncertainty about the correct model or framework\n- Often considered a special subset of epistemic uncertainty\nPartially reducible\n(better theory/model selection)\nLinear vs. nonlinear models in complex systems; risk of omitting key variables or mis-specified dynamics\n\n\nDeep (Knightian) Uncertainty\n- “Unknown unknowns”\n- Probability distributions themselves are not well-defined or are fundamentally unquantifiable\nNot quantifiable\n(cannot assign probabilities)\nImpact of radically new technology on society\n\n\nMeasurement Uncertainty\n- Errors in data collection or instrument readings\n- Systematic biases or random errors in measurement\nPartially reducible\n(improved measurement methods)\nInstrument precision limits in experiments; calibration errors in sensor data\n\n\nOperational Uncertainty\n- Uncertainty in implementation/execution\n- Human error, mechanical failure, or miscommunication in processes\nPartially reducible\n(better training/processes)\nSurgical errors, system failures, or incorrect handling of a financial trade order\n\n\n\n\n\n\n\n\n\n4.3.7 Architecture\nAny sufficiently complex project benefits from architectural thinking. Think of your model like a house: if you don’t plan the plumbing, you’ll have a mess down the line. Data should be separate from the logic and the model itself should not embed substantial datum. Instead, dynamically load data from appropriate data stores and leave the “model” as the implementation of datatypes and algorithms.\n\nModular design: Break complex models into reusable, independent components.\nSeparation of concerns: Keep data, logic, and presentation layers distinct for better maintainability.\nScalability: Design models to handle increasing data volumes and complexity.\nMaintainability: Use version control, stable interfaces, clear documentation, and automated tests.\nPerformance optimization: Use efficient data structures and algorithms to enhance model speed.\nSecurity: Ensure proper data protection and regulatory compliance.\n\nDon’t underestimate the value of a well-organized model: it’s how you scale from small prototypes to systems you can trust in production.\n\n\n4.3.8 Planning\nWhen tackling a large problem, it helps to have a well-structured planning process. Specific to building a financial model, one should take steps that include:\n\nClear objectives: Understand the purpose of the model and what questions it needs to answer.\nScope definition: Determine the boundaries of the model, including what to include and what to exclude.\nData assessment: Identify required data sources, assess data quality, and plan for data preparation.\nMethodology selection: Choose appropriate modeling techniques based on the problem and available data.\nResource allocation: Estimate time and resources needed for model development, testing, and implementation.\nStakeholder engagement: Identify key stakeholders and plan for their involvement throughout the modeling process.\nRisk assessment: Anticipate potential challenges and develop mitigation strategies.\nTimeline development: Create a realistic timeline with key milestones and deliverables.\nDocumentation strategy: Plan for comprehensive documentation of assumptions, methodologies, and limitations.\nValidation and testing approach: Outline strategies for model validation and testing to ensure reliability.\nImplementation and maintenance plan: Who owns the model once initial objectives are met?\n\nTime invested at the planning stage often pays dividends through shorter model build times, fewer errors, and clarity from stakeholders at the start of the project. Additionally, it’s often easier to make changes to a well-planned project halfway through since the necessary accommodations are more clearly defined.\n\n\n4.3.9 Essential Tools and Skills\nAn experienced professional is aware of a number of approaches that can be used in solving a problem. From heuristics that are able to be calculated on a napkin to complex economic models, the ability to draw on a wide tool set allows a practitioner to find the right solution for a given problem. It is the intention of this book to enumerate a number of additional approaches that may prove useful in practice. This includes both soft and hard skills, such as those in Table 4.3\n\n\n\nTable 4.3: A variety of skills have their place in the proficient financial modeler’s toolbelt.\n\n\n\n\n\n\n\n\n\nCategory\nExamples\n\n\n\n\nDiverse Modeling Techniques\n\nStatistical methods (e.g. regression, time series analysis, machine learning)\nOptimization techniques (e.g. linear, non-linear, black-box)\nSimulation methods (e.g. Monte-Carlo, agent-based, seriatim)\n\n\n\nSoftware Proficiency\n\nProgramming languages\nDatabase and data handling\nProprietary tools (e.g. Bloomberg)\n\n\n\nFinancial Theory\n\nAsset pricing\nPortfolio theory\nRisk Management frameworks\n\n\n\nQuantitative techniques\n\nNumerical methods and algorithms\nBayesian inference\nStochastic calculus\n\n\n\nSoft Skills\n\nVerbal and written communication\nStakeholder engagement\nProject Management",
    "crumbs": [
      "Foundations: Effective Financial Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Practice of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "financial-modeling-practice.html#feeding-the-model",
    "href": "financial-modeling-practice.html#feeding-the-model",
    "title": "4  The Practice of Financial Modeling",
    "section": "4.4 Feeding The Model",
    "text": "4.4 Feeding The Model\nIf the soul of the model is its logic, then the lifeblood is its data. In practice, a model’s fate is often sealed not in the sophistication of its algorithms, but in the quality of the data it consumes. Even the most elegant model is helpless in the face of stale, sloppy, or misunderstood inputs.\n\n4.4.1 “Garbage In, Garbage Out”\nEvery experienced modeler has a story where a subtle data quirk led to a dramatic miscalculation—a column header shifted by one, a stale price feed, or a single outlier that quietly cascaded into a million-dollar mistake. The lesson: treat the data with every bit as much skepticism (and care) as you give the model itself.\n\n\n\n\n\n\nNoteExample: The JPMorgan ‘London Whale’\n\n\n\nIn 2012, JPMorgan Chase suffered over $6 billion in losses, partly due to errors in a Value-at-Risk (VaR) model. The model relied on data being manually copied and pasted into spreadsheets, a process that introduced errors. Furthermore, a key metric was calculated by taking the sum of two numbers instead of their average. This seemingly small data handling error magnified the model’s inaccuracy, demonstrating that even the most sophisticated institutions are vulnerable to the ‘Garbage In, Garbage Out’ principle.\n\n\n\n\n4.4.2 A Modeler’s Data Instincts\nRather than thinking of data handling as a rigid checklist, approach it as a series of habits and questions:\n\nKnow Your Sources. Where did this data come from? Who collected it, and how? Is it raw, or has someone already “cleaned” it in ways you need to understand (or undo)? Data provenance is not a formality—it’s the first step in understanding what can go wrong.\nTrust, But Verify. Never take a dataset at face value, even if it comes from a trusted system. Run summary statistics. Plot the distributions. Check for the bizarre and the mundane: are dates reasonable, units consistent, and identifiers unique?\nExpect Messiness. Real-world data is rarely pristine. Missing values, odd encodings, duplicated rows, and outliers are the norm, not the exception. The best modelers are part detective, part janitor: they track down wonky values, document their triage decisions, and know when to escalate a data quality concern upstream.\nFeature Engineering Is Judgment, Not Magic. Choosing which fields to keep, combine, or discard is where domain expertise shines. Sometimes a new ratio or flag column, born from your unique understanding of the business, makes all the difference. Beware of “kitchen sink” modeling—too many features can obscure, rather than reveal, the truth.\nBe Wary of Temporal Traps. Mixing data from different time periods, or accidentally leaking future information into a model (a classic error), can invalidate results without any warning sign. When in doubt, plot your data against time and look for jumps, gaps, or trends that defy explanation.\nKeep Data and Logic Separate. As harped on earlier: don’t hard-code data into the model. Keep sources external, interfaces clean, and ingest paths well documented. If someone wants to rerun last year’s scenario, they shouldn’t have to guess which tab or variable held the original rates.\n\n\n\n4.4.3 Data Is Never “Done”\nData handling is not a one-time hurdle to clear. Markets move, data feeds change, formats drift. Build routines to check for “data drift” and have a plan for periodic validations and refreshes.\nA few practical tips:\n\nMaintain a simple data log or data dictionary—even if informal—so others can trace what each field means and where it came from.\nAutomate the boring parts: validation scripts, input checks, and sanity tests pay off a hundredfold.\nVersion your datasets, just as you do your code. Nothing is more frustrating than trying to reproduce a result only to discover “the input file changed.” See Section 12.5.3\n\nData is unruly, idiosyncratic, and absolutely central to every model’s fate. Treat it as a first-class concern, not an afterthought, and your models will be far sturdier for it. As a methodical guide, Table 4.4 lists key steps to follow when bringing data into the model.\n\n\n\nTable 4.4: Typical Steps in the Data-to-Model Process.\n\n\n\n\n\n\n\n\n\n\nStep\nKey Actions\nPurpose / Notes\n\n\n\n\nData Collection\n\nIdentify sources\nAcquire data (e.g., APIs, databases, scraping)\n\nEnsures data is relevant, reliable, and timely\n\n\nData Exploration & Understanding\n\nSummary statistics\nVisualization\nData profiling\n\nUncovers initial insights, errors, distributions, and relationships\n\n\nData Cleaning\n\nHandle missing values\nDetect/treat outliers\nData transformation/formatting\n\nImproves data quality, reduces noise and bias\n\n\nData Preprocessing\n\nScale/normalize features\nEncode categorical variables\nAugment data (if needed) with other datasets\n\nPrepare data so it fits the format and requirements of the model\n\n\nFeature Engineering\n\nSelect important features\nCreate new features (e.g., ratios, aggregates)\n\nEnhance or create new variables that improve model performance\n\n\nData Splitting\n\nDivide into training, testing, (validation) sets\nApply cross-validation or static/dynamic validations\n\nPrevents overfitting and enables robust performance assessment\n\n\nData Storage & Management\n\nStore in databases/data lakes\nMaintain version control\n\nSupports reproducibility, scalability, and reliable access\n\n\nEthical Considerations\n\nEvaluate bias and fairness\nEnsure privacy and regulatory compliance\n\nAvoids perpetuating bias and protects sensitive information\n\n\nContinuous Monitoring & Updating\n\nMonitor model/data performance\nDetect data drift\nRetrain/update as needed\n\nMaintains accuracy and relevance as data and conditions change",
    "crumbs": [
      "Foundations: Effective Financial Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Practice of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "financial-modeling-practice.html#model-management",
    "href": "financial-modeling-practice.html#model-management",
    "title": "4  The Practice of Financial Modeling",
    "section": "4.5 Model Management",
    "text": "4.5 Model Management\n\n4.5.1 Risk Governance\nAn effective risk governance framework for financial modeling begins with clearly stating why such oversight is necessary—namely, to prevent costly missteps in managing complex portfolios or complying with regulations. Organizations often adopt a written policy delineating responsibilities across different levels: management or board-level committees set high-level objectives, while operational teams handle day-to-day processes.\nAt the heart of this framework lies a structured model inventory, a catalog of all models in use that details each model’s purpose, assumptions, and present status (for example, whether it is in a prototype phase or fully deployed in production). This inventory helps institutions understand their cumulative exposure to errors or assumptions gone awry.\nIn practice, many firms adopt tiered risk classifications to decide how much scrutiny a model warrants. Classification schemes may range from “low impact” for small-scale financial calculators to “mission-critical” for enterprise valuation engines. Validation and testing approaches vary according to a model’s assigned tier.\nHighly critical models undergo more extensive backtesting, benchmarking, or sensitivity analyses, with results escalated to senior management. Risk governance also encompasses ongoing monitoring and scheduled reports about model health. By publicizing validation findings and model performance metrics, the organization fosters a culture where potential failures are escalated early and openly, rather than hidden away until a crisis emerges.\n\n\n4.5.2 Change Management\nNo model remains static for long; assumptions evolve, new asset classes appear, and software libraries update. For this reason, a firm’s change management process should standardize how modifications are proposed, evaluated, and documented, ensuring continuity of both the model’s logic and the data that feeds it.\nA central repository or version control system is essential: whenever the model or its associated data structures shift, the changes and their justifications must be recorded. This makes it easier to track lineage and revert to a prior version if an update proves problematic in a live environment. Later in this book, we will introduce modern version control systems and workflows that are facilitated by the code-based models that we develop.\nEqually important is assessing the ripple effects of each change. Simplifying a routine or adjusting a discount rate assumption may be minor in isolation but can have broad implications when integrated across multiple components. Projects often require up-front impact assessments to determine which historical results need recalculating and whether stakeholder training or documentation updates are needed. One strategy, that of package and model version numbering schemes, will be described in Chapter 23.\nCommunication around changes should be systematic, distributing concise notes on new features, potential risks, and recommended usage practices to both internal users and (where relevant) regulators. Well-handled change management fosters stability and trust, enabling prompt innovation without sacrificing the reliability of the overall modeling ecosystem.\n\n\n4.5.3 Data Controls\nSound data controls are paramount in financial modeling because flawed or unverified inputs quickly undermine even the sturdiest model architecture. Organizations typically define data quality standards that address accuracy, completeness, and timeliness. These standards help detect common pitfalls, such as inconsistent formatting, delayed updates, or incorrect data mappings. Complementing formal policies, automated checks are often placed at ingestion points to spot irregularities—anything from out-of-range values that might indicate data corruption, to suspicious spikes hinting at a data input error.\nSecurity and access protocols add another layer of protection. Role-based permission schemes or strong authentication measures minimize the risk of data tampering, accidental deletions, or unauthorized viewings of confidential information.\nAlthough data versioning may sound like a software concept, it applies equally to financial datasets. Keeping a record of each dataset’s evolution allows managers and auditors to pinpoint when and how anomalies first appeared. Where legislation like GDPR or industry-specific regulations come into play, data controls must also reflect broader requirements about personal information, consent, and retention periods. Coordinating these efforts under a unified data governance approach ensures that model outputs stand on a solid factual foundation.\n\n\n4.5.4 Peer and Technical Review\nEven the most experienced modelers benefit from additional eyes on their work. Peer review, whether informal or systematically mandated, identifies blind spots in assumptions, conceptual design, or coding. Though some organizations require independent reviewers who have not contributed to the original model, smaller teams may rely on a rotating schedule of internal experts sharing responsibility for checks. The key is cultivating a culture where open dialogue about potential faults is not only accepted but encouraged.\nTechnical review goes one step further, focusing on deeper verification of the computations themselves. Complex spreadsheets, code modules, or integrated software platforms may require structured walk-throughs in which reviewers verify arithmetic, confirm the alignment of calculation steps with business logic, or run test scenarios to ensure the model behaves as intended. This process should generate formal documentation capturing who performed the review, what methods they used, and which issues surfaced. Likewise, conceptual soundness—how well the model aligns with economic theory or domain-specific knowledge—merits discussion in a thorough review. If challenges are identified, revisions loop back into the change management system, promoting iterative refinements. By conducting peer and technical reviews in earnest, organizations reinforce consistent quality and reduce the likelihood of undetected errors slipping into production.",
    "crumbs": [
      "Foundations: Effective Financial Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Practice of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "financial-modeling-practice.html#conclusion",
    "href": "financial-modeling-practice.html#conclusion",
    "title": "4  The Practice of Financial Modeling",
    "section": "4.6 Conclusion",
    "text": "4.6 Conclusion\nThe art and science of financial modeling require a unique blend of skills, knowledge, and personal qualities. A proficient modeler combines domain expertise, theoretical understanding, and practical skills with a curious and rigorous mindset. They leverage a diverse toolset, employ sound architectural principles, and communicate with clarity. The ability to navigate the complexities of financial systems while maintaining humility in the face of irreducible uncertainties is paramount.\nAs the financial world continues to evolve, so too must the modeler’s approach. By cultivating these attributes and continuously refining their craft, financial modelers can create more robust, insightful, and valuable models that drive informed decision-making in an increasingly complex economic landscape. The journey of a financial modeler is one of perpetual learning and adaptation, where each challenge presents an opportunity for growth and innovation.",
    "crumbs": [
      "Foundations: Effective Financial Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Practice of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "financial-modeling-practice.html#footnotes",
    "href": "financial-modeling-practice.html#footnotes",
    "title": "4  The Practice of Financial Modeling",
    "section": "",
    "text": "Ryle, G. The Concept of Mind. Harmondsworth, England, Penguin, 1963, first published 1949. Applying “Theory Building”↩︎\nThe idea of “model theory” is adapted from Peter Naur’s 1985 essay, “Programming as Theory Building”. Indeed, this whole paragraph is only a slightly modified version of Naur’s description of theory in the programming context.↩︎",
    "crumbs": [
      "Foundations: Effective Financial Modeling",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The Practice of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "conceptual-foundations.html",
    "href": "conceptual-foundations.html",
    "title": "Foundations: Programming and Abstractions",
    "section": "",
    "text": "“Out of intense complexities intense simplicities emerge.” — Winston Churchill (1923)\n\nOver the next several chapters, we build essential concepts that enable sophisticated financial models while maintaining clarity and control. We begin with core programming building blocks—the vocabulary and grammar of communicating with computers. From there, we explore how to decompose complex problems through abstraction: functions and methods, structs and parametric types, and multiple dispatch as the organizing principle.\nAbstraction is selective attention—focusing on what matters for a given purpose while hiding irrelevant detail. Just as financial statements aggregate transaction‑level noise to reveal performance and risk, thoughtful abstraction lets us manage complexity by working at the right level of detail: a pricing function that hides payoff minutiae, a portfolio type that encapsulates assets and weights, or a generic risk routine that dispatches on instrument type.\nThe goal is not to make you a computer scientist, but rather to equip you with the mental models and practical techniques needed to effectively leverage programming in your financial work. By understanding these foundations, you’ll be better equipped to design clean, maintainable models that can evolve with your needs.\nThink of this section as building your modeling toolkit, one concept at a time. We’ll introduce ideas progressively, with plenty of concrete examples to ground the theory in practical application. The concepts build on each other, so take time to ensure your understanding before moving forward. Let’s begin with the fundamental elements of programming.",
    "crumbs": [
      "Foundations: Programming and Abstractions"
    ]
  },
  {
    "objectID": "foundations-of-programming.html",
    "href": "foundations-of-programming.html",
    "title": "5  Elements of Programming",
    "section": "",
    "text": "5.1 Chapter Overview\nStart building up computer science concepts by introducing tangible programming essentials. Data types, variables, control flow, functions, and scope are introduced.",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "foundations-of-programming.html#chapter-overview",
    "href": "foundations-of-programming.html#chapter-overview",
    "title": "5  Elements of Programming",
    "section": "",
    "text": "TipOn Your First Read-through\n\n\n\nThis chapter is intended to be an introductory reference for most of the basic building blocks for which we will build abstractions on top of in chapters that follow. We want this chapter to essentially be an easy and mildly opinionated stepping-stone on your journey.\nAt some point, you will likely find yourself seeking more precise or thorough documentation and will begin directly searching or reading the documentation of a language or library itself. However, it may be intimidating or frustrating reading reference documentation due to the density and terminology - let this chapter (and book, writ large) be a bridge for you.\nIf reading this book in a linear fashion and new to programming, we suggest skipping the following sections and returning when encountering the concept or term later in the book:\n\nSection 5.4.4 through Section 5.4.9 which covers advanced and custom data types\nAfter Section 5.5.3 which deals with advanced function usage and program organization via scope\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThis introductory chapter is intended to provide a survey of the important concepts and building blocks, not to be a complete reference. For full details on available functions, more complete definitions, and a more complete tour of all language features, see the Manual at docs.julialang.org.",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "foundations-of-programming.html#computer-science-programming-and-coding",
    "href": "foundations-of-programming.html#computer-science-programming-and-coding",
    "title": "5  Elements of Programming",
    "section": "5.2 Computer Science, Programming, and Coding",
    "text": "5.2 Computer Science, Programming, and Coding\nComputer Science is the study of computing and information. As a science, it is distinct from programming languages which are merely coarse implementations of specific computer science concepts1.\nProgramming (or “coding”) is the art and science of writing code in programming languages to have the computer perform desired tasks. While this may sound mechanistic, programming truly is one of the highest forms of abstract thinking. The design space of potential solutions is so large and potentially complex that much art and experience is needed to create a well-made program.\nThe language of computer science also provides a lexicon so that financial practitioners can discuss model architecture and characteristics of problems with precision and clarity. Simply having additional terminology and language to describe a concept illuminates aspects of the problem in new ways, opening one’s self up to more innovative solutions.\nIn this light, the financial modeling that we do can be considered a type of computer program. It takes as input abstract information (data), performs calculations (an algorithm), and returns new data as an output. We generally do not need to consider many things that a software engineer may contemplate such as a graphical user interface, networking, or access restrictions. However, the programming fundamentals are there: a good financial modeler must understand data types, algorithms, and some hardware details.\nWe will build up the concepts over this and the following chapter:\n\nThis chapter will provide a survey of important concepts in computer science that will prove useful for our financial modeling. First, we will talk about data types, boolean logic, and basic expressions. We’ll build on those to discuss algorithms (functions) which perform useful work and use control flow and recursion.\nIn the following chapters about abstraction, we will step back and discuss higher level concepts: the “schools of thought” around organizing the relationship between data and functions (functional versus object-oriented programming), design patterns, computational complexity, and compilation.\n\n\n\n\n\n\n\nTip\n\n\n\nThere will be brief references to hardware considerations for completeness, but hardware knowledge is not necessary to understand most programming languages (including Julia). It’s impossible to completely avoid talking about hardware when you care about the performance of your code, so feel free to gloss over the reference to hardware details on the first read and come back later after Chapter 9.\n\n\nIt’s highly recommended that you follow along and have a Julia session open (e.g. a REPL or a notebook) when first going through this chapter. See the first part of Chapter 21 if you haven’t gotten that set up yet. Follow along with the examples as we go.\n\n\n\n\n\n\nTip\n\n\n\nYou can get some help in the REPL by typing a ? followed by the symbol you want help with, for example:\n help?&gt; sum\nsearch: sum sum! summary cumsum cumsum! ...\n\n  sum(f, itr; [init])\n\n\n  Sum the results of calling function f on each element of itr.\n\n... More text truncated...",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "foundations-of-programming.html#expressions-and-control-flow",
    "href": "foundations-of-programming.html#expressions-and-control-flow",
    "title": "5  Elements of Programming",
    "section": "5.3 Expressions and Control Flow",
    "text": "5.3 Expressions and Control Flow\n\n5.3.1 Naming Values with Variables\nOne of the first things it will be convenient to understand is the concept of variables. In virtually every programming language, we can assign values to make our program more organized and meaningful to the human reader. In the following example, we assign values to intermediate symbols to benefit us humans as we convert (silly!) American distance units:\n\nfeet_per_yard = 3\nyards_per_mile = 1760\n\nfeet = 3000\nmiles = feet / feet_per_yard / yards_per_mile\n\n0.5681818181818182\n\n\nThe above is technically the same thing as just computing 3000 / 3 / 1760, however we’ve given the elements names meaningful to the human user.\nBeyond readability, variables are a form of abstraction which allows us to think beyond specific instances of data and numbers to a more general representation. For example, the last line in the prior code example is a very generic computation of a unit conversion relationship and feet could be any number and the expression remains a valid calculation.\n\n\n\n\n\n\nTip\n\n\n\nWe will dive a little bit deeper into variables and assignment in Section 5.3.4, distinguishing between assignment and references.\n\n\n\n\n5.3.2 Expressions\nWithin the code examples above, we can zoom in onto small pieces of code called expressions. Expressions are effectively the basic block of code that gets evaluated to produce a value. Here is an expression that adds two integers together that evaluate to a new integer (3 in this case):\n\n1 + 2\n\n3\n\n\nA bigger program is built up of many of these smaller bits of code.\n\n5.3.2.1 Compound Expression\nThere’s two kinds of blocks where we can ensure that sub-expressions get evaluated in order and return the last expression as the overall return value: begin and let blocks.\n\nc = begin\n    a = 3\n    b = 4\n    a + b\nend\n\na, b, c\n\n(3, 4, 7)\n\n\nAlternatively, you can chain together ;s to create a compound expression:\n\nz = (x = 1; y = 2; x + y)\n\n3\n\n\nCompound expressions allow you to group multiple operations together while still having the entire block evaluate to a single value, typically the last expression. This makes it easy to use complex logic anywhere a value is needed, like in function arguments or assignments.\nlet blocks define variables within its scope and cannot be accessed outside that scope. More on scope in Section 5.6.\n\nc = let\n    g = 3\n    h = 4\n    g + h\nend\n\n@show c\n@show g\n\nc = 7\n\n\n\nUndefVarError: `g` not defined in `Main.Notebook`\nSuggestion: check for spelling errors or missing imports.\nStacktrace:\n [1] macro expansion\n   @ show.jl:1232 [inlined]\n [2] top-level scope\n   @ ~/prog/julia-fin-book/foundations-of-programming.qmd:134\n\n\n\n\n\n5.3.2.2 Conditional Expressions\nConditionals are expressions that evaluate to a boolean true or false. This is the beginning of really being able to assemble complex logic to perform useful work. Here are a handful expressions that would evaluate to true:\n\n1 &gt; 0\n1 == 1 # check for equality\n1.0 isa Float64\n(5 &gt; 0) & (-1 &lt; 2) # \"and\" expression\n(5 &gt; 0) | (-1 &gt; 2) # \"or\" expression\n1 != 2\n\ntrue\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn Julia, the booleans have an integer equality: true is equal to 1 (true == 1) and false is equal to 0 (false == 0).\nHowever, true != 5. Only 1 is equal to true (in some languages, any non-zero number is “truthy”).\n\n\nConditionals can be used to assemble different logical paths for the program to follow and the general pattern is an if block:\nif condition\n    # do one thing\nelseif condition\n    # do something else\nelse\n    # do something if none of the \n    # other conditions are met\nend\nA complete example:\n\nfunction buy_or_sell(my_value, market_price)\n    if my_value &gt; market_price\n        \"buy more\"\n    elseif my_value &lt; market_price\n        \"sell\"\n    else\n        \"hold\"\n    end\nend\n\nbuy_or_sell(10, 15), buy_or_sell(15, 10), buy_or_sell(10, 10)\n\n(\"sell\", \"buy more\", \"hold\")\n\n\n\n\n\n\n\n\nNoteAdvanced: Short Circuiting Operator\n\n\n\nJulia has short-circuiting boolean operators that avoid evaluating the right-hand side when it isn’t needed: - a && b evaluates b only if a is true - a || b evaluates b only if a is false\n\nfunction bigger_than_and_print(a, b)\n    println(\"comparing $a and $b\")\n    a &gt; b\nend\n\n(5 &gt; 0) && bigger_than_and_print(5, 7)  # prints, returns false\n(5 &lt; 0) && bigger_than_and_print(5, 7)  # skips RHS, returns false\n\n(5 &gt; 0) || bigger_than_and_print(5, 7)  # skips RHS, returns true\n(5 &lt; 0) || bigger_than_and_print(5, 7)  # prints, returns false\n\ncomparing 5 and 7\ncomparing 5 and 7\n\n\nfalse\n\n\nBy contrast, & and | are non–short-circuit (bitwise/logical) and always evaluate both sides:\n\n(5 &gt; 0) & bigger_than_and_print(5, 7)   # prints, returns false\n(5 &lt; 0) | bigger_than_and_print(5, 7)   # prints, returns false\n\ncomparing 5 and 7\ncomparing 5 and 7\n\n\nfalse\n\n\nThis matters for performance and safety:\n\nfalse && error(\"boom\")  # no error (RHS skipped)\nfalse &  error(\"boom\")  # ERROR: boom\n\n\nboom\nStacktrace:\n [1] error(s::String)\n   @ Base ./error.jl:35\n [2] top-level scope\n   @ ~/prog/julia-fin-book/foundations-of-programming.qmd:218\n\n\n\nTips: - Use && and || for control flow on Bool values. - With arrays, use broadcasting: .& and .| (not &&/||). - With missing data, &&/|| require Bool and will error; & and | propagate missing.\n\n\n\n\n\n5.3.3 Equality\nThe “Ship of Theseus problem”2 is an example of how equality can be philosophically complex concept. In computer science we have the advantage that while we may not be able to resolve what’s the “right” type of equality, we can be more precise about it.\nHere is an example for which we can see the difference between two types of equality:\n\nEgal equality is when a program could not distinguish between two objects at all\nEqual equality is when the values of two objects are the same\n\nIf two things are egal, then they are also equal.\nIn the following example, s and t are equal but not egal:\n\ns = [1, 2, 3]\nt = [1, 2, 3]\ns == t, s === t\n\n(true, false)\n\n\nOne way to think about this is that while the values are equal, there is a way that one of the arrays could be made not equal to the other:\n\nt[2] = 5\nt\n\n3-element Vector{Int64}:\n 1\n 5\n 3\n\n\nNow t is no longer equal to s:\n\ns == t\n\nfalse\n\n\nThe reason this happens is that arrays are containers that can have their contents modified. Even though they originally had the same values, s and t are different containers, and it just so happened that the values they contained started out the same.\nSome data can’t be modified, including some kinds of collections. Immutable types like the following tuple, with the same stored values, are egal because there is no way for us to make them different:\n\n(2, 4) === (2, 4)\n\ntrue\n\n\nUsing this terminology, we could now interpret the “Ship of Theseus” as that his ship is “equal” but not “egal”.\n\n\n5.3.4 Assignment, References, and Mutability\nWhen we say x = 2 we are assigning the integer value of 2 to the variable x. This is an expression that lets us bind a something to a variable so that it can be referenced more concisely or in different parts of our code. When we re-assign the variable we are not mutating the value: x = 3 does not change the 2.\nWhen we have a mutable object (e.g. an Array or a mutable struct), we can mutate the value inside the referenced container. For example:\n\n1x = [1, 2, 3]\n2x[1] = 5\nx\n\n\n1\n\nx refers to the array which currently contains the elements 1, 2, and 3.\n\n2\n\nWe re-assign the first element of the array to be the value 5 instead of 1\n\n\n\n\n3-element Vector{Int64}:\n 5\n 2\n 3\n\n\nIn the above example, x has not been reassigned. It is possible for two variables to refer to the same object:\n\nx = [1, 2, 3]\n1y = x\nx[1] = 6\ny\n\n\n1\n\ny refers to the same underlying array as x\n\n\n\n\n3-element Vector{Int64}:\n 6\n 2\n 3\n\n\nNote that variables can be re-assigned unless they are marked as const:\nconst TAU =  π * 2\n\n1\n\nCapitalizing constant variables is a convention in Julia.\n\n\nIf we tried to re-assign TAU, we would get an error.\n\n\n\n\n\n\nWarning\n\n\n\nNote that if we declare a const variable that refers to a mutable container like an array, the container can still be mutated. It’s the reference to the container that remains constant, not necessarily the elements within the container.\nFor example, while MY_ARRAY will point always to the same array, the array itself can get mutated\n\nconst MY_ARRAY = [1,2]\nMY_ARRAY[1] = 99\nMY_ARRAY\n\n2-element Vector{Int64}:\n 99\n  2\n\n\n\n\n\n\n5.3.5 Loops\nLoops are ways for the program to move through a program and repeat expressions while we want it to. There are two primary loops: for and while.\nfor loops are loops that iterate over a defined range or set of values. Let’s assume that we have the array v = [6,7,8]. Here are multiple examples of using a for loop in order to print each value to output (println):\nv = [6,7,8]\n# use fixed indices\nfor i in 1:3\n    println(v[i])\nend\n# use indices the of the array\nfor i in eachindex(v)\n    println(v[i])\nend\n# use the elements of the array\nfor x in v\n    println(x)\nend\n# use the elements of the array\nfor x ∈ v          # ∈ is typed \\in&lt;tab&gt;\n    println(x)\nend\nwhile loops will run repeatedly until an expression is false. Here’s some examples of printing each value of v again:\n# index the array\ni = 1\nwhile i &lt;= length(v) \n    println(v[i])\n1    global i += 1\nend\n\n1\n\nglobal is used to increment i by 1. i is defined outside the scope of the while loop (see Section 5.6).\n\n\n# index the array\ni = 1\nwhile true\n    println(v[i])\n    if i &gt;= length(v)\n1        break\n    end\n    global i += 1 \nend\n\n1\n\nbreak is used to terminate the loop manually, since the condition that follows the while will never be false.\n\n\n\n\n5.3.6 Performance of Loops\nLoops are highly performant in Julia and often the fastest way to accomplish things. This approach contrasts with advice often given to Python or R users, where vectorized operations are heavily favored over loops for performance. In Julia, ⁠for loops are highly performant and often the most efficient and readable way to implement an algorithm.",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "foundations-of-programming.html#sec-data-types",
    "href": "foundations-of-programming.html#sec-data-types",
    "title": "5  Elements of Programming",
    "section": "5.4 Data Types",
    "text": "5.4 Data Types\nData types are a way of categorizing information by intrinsic characteristics. We instinctively know that 13.24 is different than \"this set of words\" and types are how we will formalize this distinction. This is a key conceptual point, and mathematically it’s like we have different sets of objects to perform specialized operations on. Beyond this set-like abstraction is implementation details related to computer hardware. You probably know that computers only natively “speak” in binary zeros and ones. Data types are a primary way that a computer can understand if it should interpret 01000010 as B or as 663.\nEach 0 or 1 within a computer is called a bit and eight bits in a row form a byte (such as 01000010). This is where we get terms like “gigabytes” or “kilobits per second” as a measure of the quantity or rate of bits something can handle4.\n\n5.4.1 Numbers\nNumbers are usually grouped into two categories: integers and floating-point5 numbers. Integers are like the mathematical set of integers while floating-point is a way of representing decimal numbers. Both have some limitations since computers can only natively represent a finite set of numbers due to the hardware (more on this in Chapter 9). Here are three integers that are input into the REPL (Read-Eval-Print-Loop)6 and the result is printed below the input:\n\n2\n\n2\n\n\n\n423\n\n423\n\n\n\n1929234\n\n1929234\n\n\nAnd three floating-point numbers:\n\n0.2\n\n0.2\n\n\n\n-23.3421\n\n-23.3421\n\n\n\n14e3      # the same as 14,000.0\n\n14000.0\n\n\nOn most systems, 0.2 will be interpreted as a 64-bit floating point type called Float64 in Julia since most architectures these days are 64-bit7, while on a 32-bit system 0.2 would be interpreted as a Float32. Given that there are a finite amount of bits attempting to represent a continuous, infinite set of numbers means that some numbers are not able to be represented with perfect precision. For example, if we ask for 0.2, the closest representations in 64 and 32 bit are:\n\n0.20000000298023223876953125 in 32-bit\n0.200000000000000011102230246251565404236316680908203125 in 64-bit\n\nThis leads to special considerations that computers take when performing calculations on floating point maths, some of which will be covered in more detail in Chapter 9. For now, just note that floating point numbers have limited precision and even if we input 0.2, your computations will use the above decimal representations even if it will print out a number with fewer digits shown:\n\n1x = 0.2\n\n2big(x)\n\n\n1\n\nHere, we assign the value 0.2 to a variable x. More on variables/assignments in Section 5.3.4.\n\n2\n\nbig(x) is a arbitrary precision floating point number and by default prints the full precision that was embedded in our variable x, which was originally Float64.\n\n\n\n\n0.200000000000000011102230246251565404236316680908203125\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote the difference in what printed between the last example and when we input 0.2 earlier in the chapter. The former had the same (not-exactly equal to \\(0.2\\)) value, but it printed an abbreviated set of digits as a nicety for the user, who usually doesn’t want to look at floating point numbers with their full machine precision. The system has the full precision (0.20...3125) but is truncating the output.\nIn the last example, we’ve converted the normal Float64 to a BigFloat which will not truncate the output when printing.\n\n\nIntegers are similarly represented as 32 or 64 bits (with Int32 and Int64) and are limited to exact precision:\n\nInt32 range: −2,147,483,648 to 2,147,483,647\nInt64 range: −9,223,372,036,854,775,808 to 9,223,372,036,854,775,807\n\nAdditional range in the positive direction if one chooses to use “unsigned”, non-negative numbers (UInt32 and UInt64). Unlike floating point numbers, the integers have a type Int which will use the system bit architecture by default (that is, Int(30) will create a 64 bit integer on 64-bit systems and 32-bit on 32-bit systems).\n\n\n\n\n\n\nTipFloating Point and Excel\n\n\n\nExcel’s numeric storage and routine is complex and not quite the same as most programming languages, which follow the Institute of Electrical and Electronics Engineer’s standards (such as the IEEE 754 standard for double precision floating point numbers). Excel uses IEEE for the computations but results (and therefore the cells that comprise many calculations interim values) are stored with 15 significant digits of information. In some ways this is the worst of both worlds: having the sometimes unusual (but well-defined) behavior of floating point arithmetic and having additional modifications to various steps of a calculation. In general, you can assume that the programming language result (following the IEEE 754 standard) is a better result because there are aspects to the IEEE 754 defines techniques to minimize issues that arise in floating point math. Some of the issues (round-off or truncation) can be amplified instead of minimized with Excel.\nIn practice, this means that it can be difficult to exactly replicate a calculation in Excel in a programming language and vice-versa. It’s best to try to validate a programming model versus Excel model using very small unit calculations (e.g. a single step or iteration of a routine) instead of an all-in result. You may need to define some tolerance threshold for comparison of a value that is the result of a long chain of calculation.\n\n\n\n\n\n\n\n\nTipCurrencies and Decimals\n\n\n\nDue to the inherent imprecision with floating point numbers, they should not be used in storing financial transaction records! The trade-offs inherent floating point math described above to not lend itself to accurate record-keeping. For example, in looking at summing up two products sold for $19.99 and $84.99, since floating point operations like 19.99 + 84.99 do not precisely equal 105.98.\n\nBigFloat(19.99 + 84.99)\n\n104.979999999999989768184605054557323455810546875\n\n\nSee how the prior is slightly lower than 105.98 — if we were adding US Dollars here, our system would be off by fractions of a cent. Do that for millions of transactions in a day and you have a problem!\nGenerally, when doing modeling or even creating a valuation model, it’s okay to use floating point math. As an example, if you are trying to determine the value of an exotic option, your model is likely just fine outputting a value like 101.987087 . If you go and sell this option, you’ll have to settle for either 101.98 or 101.99 when booking it. In most contexts this imprecision is likely okay!\nIf you are implementing a transaction or trading system, ensure proper treatment of the types representing your monetary numbers. A full treatment is beyond the scope of this book, but for a good introduction to the subject, see https://cs-syd.eu/posts/2022-08-22-how-to-deal-with-money-in-software.\n\n\n\n\n5.4.2 Type Hierarchy\nWe can describe a hierarchy of types. Both Float64 and Int64 are examples of Real numbers (here, Real is an abstract Julia type which corresponds to the mathematical set of real numbers commonly denoted with \\(\\mathbb{R}\\) ). Both Float64 and Int32 are Real numbers, so why not just define all numbers as a Real type? Because for performant calculations, the computer must know in advance how many bits each number is represented with.\n?fig-julia-numeric-types shows the type hierarchy for most built-in Julia number types.\nTODO: Once Quarto Issue #10961 is resolved, render the mermaid diagram.\n%%| label: fig-julia-numeric-types\n%%| fig-cap: \"Numeric Type Hierarchy in Julia. Leafs of the tree are concrete types.\"\n%%| fig-width: 6.5\ngraph TD\n    Number --&gt; Real\n    Number --&gt; Complex\n\n    Real --&gt; Integer\n    Real --&gt; AbstractFloat\n    Real --&gt; Rational\n    Real --&gt; Irrational\n\n    Integer --&gt; Signed\n    Integer --&gt; Unsigned\n\n    Signed --&gt; Int8\n    Signed --&gt; Int16\n    Signed --&gt; Int32\n    Signed --&gt; Int64\n    Signed --&gt; Int128\n    Signed --&gt; BigInt\n\n    Unsigned --&gt; UInt8\n    Unsigned --&gt; UInt16\n    Unsigned --&gt; UInt32\n    Unsigned --&gt; UInt64\n    Unsigned --&gt; UInt128\n\n    AbstractFloat --&gt; Float16\n    AbstractFloat --&gt; Float32\n    AbstractFloat --&gt; Float64\n    AbstractFloat --&gt; BigFloat\nThe integer and floating point types described in the prior section are known as concrete types because there are no possible sub types (child types). Further, a concrete type can be a bit type if the data type will always have the same number of bits in memory: a Float32 will always be 32 bits in memory, for example. Contrast this with strings (described below) which can contain an arbitrary number of characters.\n\n\n5.4.3 Collections\nCollections are types that are really useful for storing data which contains many elements. This section describes some of the most common and useful types of containers.\n\n5.4.3.1 Arrays\nArrays are the most common way to represent a collection of similar data. For example, we can represent a set of integers as follows:\n\n[1, 10, 300]\n\n3-element Vector{Int64}:\n   1\n  10\n 300\n\n\nAnd a floating point array:\n\n[0.2, 1.3, 300.0]\n\n3-element Vector{Float64}:\n   0.2\n   1.3\n 300.0\n\n\nNote the above two arrays are different types of arrays. The first is Vector{Int64} and the second is Vector{Float64}. These are arrays of concrete types and so Julia will know that each element of an array is the same amount of bits which will enable more efficient computations. With the following set of mixed numbers, Julia will promote the integers to floating point since the integers can be accurately represented8 in floating point.\n\n[1, 1.3, 300.0, 21]\n\n4-element Vector{Float64}:\n   1.0\n   1.3\n 300.0\n  21.0\n\n\nHowever, if we explicitly ask Julia to use a Real-typed array, the type is now Vector{Real}. Recall that Real is an abstract type. Having heterogeneous types within the array is conceptually fine, but in practice limits performance. Again, this will be covered in more detail in Chapter 9.\nIn Julia, arrays can be multi-dimensional. Here are two three-dimensional arrays with length three in each dimension:\n\nrand(3, 3, 3)\n\n3×3×3 Array{Float64, 3}:\n[:, :, 1] =\n 0.0322753   0.014331  0.241287\n 0.560409    0.851011  0.377413\n 0.00361425  0.531222  0.735742\n\n[:, :, 2] =\n 0.100149   0.41092   0.411634\n 0.968557   0.128415  0.3576\n 0.0511605  0.824797  0.638683\n\n[:, :, 3] =\n 0.588012  0.0206799  0.74092\n 0.195709  0.531179   0.577103\n 0.721473  0.765971   0.504337\n\n\n\n[x + y + z for x in 1:3, y in 11:13, z in 21:23]\n\n3×3×3 Array{Int64, 3}:\n[:, :, 1] =\n 33  34  35\n 34  35  36\n 35  36  37\n\n[:, :, 2] =\n 34  35  36\n 35  36  37\n 36  37  38\n\n[:, :, 3] =\n 35  36  37\n 36  37  38\n 37  38  39\n\n\nThe above example demonstrates array comprehension syntax which is a convenient way to create arrays in Julia.\nA two-dimensional array has the rows separated by semi-colons (;):\n\nx = [1 2 3; 4 5 6]\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn Julia, a Vector{Float64} is simply a one-dimensional array of floating points and a Matrix{Float64} is a two-dimensional array. More precisely, they are type aliases of the more generic Array{Float64,1} and Array{Float64,2} names. Arrays with three or more dimensions don’t have a type alias pre-defined.\n\n\n\n\n5.4.3.2 Array indexing\nArray elements are accessed with the integer position, starting at 1 for the first element9 10:\n\nv = [10, 20, 30, 40, 50]\nv[2]\n\n20\n\n\nWe can also access a subset of the vector’s contents by passing a range:\n\nv[2:4]\n\n3-element Vector{Int64}:\n 20\n 30\n 40\n\n\nAnd we can generically reference the array’s contents, such as:\n\nv[begin+1:end-1]\n\n3-element Vector{Int64}:\n 20\n 30\n 40\n\n\nWe can assign values into the array as well, as well as combine arrays and push new elements to the end:\n\nv[2] = -1\npush!(v, 5)\nvcat(v, [1, 2, 3])\n\n9-element Vector{Int64}:\n 10\n -1\n 30\n 40\n 50\n  5\n  1\n  2\n  3\n\n\n\n\n5.4.3.3 Array Alignment\nWhen you have an MxN matrix (M rows, N columns), a choice must be made as to which elements are next to each other in memory. Typical math convention and fundamental computer linear algebra libraries (dating back decades!) are column major and Julia follows that legacy. Column major means that elements going down the rows of a column are stored next to each other in memory. This is important to know so that (1) you remember that vectors are treated like a column vector when working with arrays (that is: a N element 1D vector is like a Nx1 matrix), and (2) when iterating through an array, it will be faster for the computer to access elements next to each other column-wise. A 10x10 matrix is actually stored in memory as 100 elements coming in order, one after another in single file.\nThis 3x3 matrix is stored with the elements of columns next to each other, which we can see with vec:\n\nmat = [1 2 3; 4 5 6; 7 8 9]\n\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\n\n\nvec(mat) # column major order\n\n9-element Vector{Int64}:\n 1\n 4\n 7\n 2\n 5\n 8\n 3\n 6\n 9\n\n\n\n\n5.4.3.4 Ranges\nA range is a representation of a range of numbers. We actually used them above to index into arrays. They are expressed as start:stop\nWe don’t have to actually store all of these numbers on the computer somewhere as in an Array. Instead, this is an object that represents the ordered set of numbers. So for example, we can sum up 1 through the number of atoms on Earth almost instantaneously:\n\nsum(1:big(100_000_000_000_000_000_000_000_000_000_000_000_000_000_000_000_000))\n\n5000000000000000000000000000000000000000000000000050000000000000000000000000000000000000000000000000\n\n\nThis is possible due to two things:\n\nnot needing to actually store that many numbers in memory, and\nJulia being smart enough to apply the triangular number formula11 when sum is given a consecutive range.\n\nThere are more general ways to construct ranges:\nStep by another number instead of the default 1:\n\n1:2:7\n\n1:2:7\n\n\nSpecify the number of values within the range, inclusive of the first number12:\n\n# range(start, stop, length)\nrange(0, 10, 21)\n\n0.0:0.5:10.0\n\n\n\n\n5.4.3.5 Characters, Strings, and Symbols\nCharacters are represented in most programming languages as letters within quotation marks. In Julia, individual characters are represented using single quotes:\n\n'a'\n\n'a': ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\n\n\nLetters and other characters present more difficulties than numbers to represent within a computer (think of how many languages and alphabets exist!), and it essentially only works because the world at large has agreed to a given representation. Originally ASCII (American Standard Code for Information Interchange) was used to represent just 95 of the most common English characters (“a” through “z”, zero through nine, etc.). Now, UTF (Unicode Transformation Format) can encode more than a million characters and symbols from many human languages.\nStrings are a collection13 of characters, and can be created in Julia with double quotes:\n\n\"hello world\"\n\n\"hello world\"\n\n\nIt’s easy to ascertain how ‘normal’ characters can be inserted into a string, but what about things like new lines or tabs? They are represented by their own characters but are normally not printed in computer output. However, those otherwise invisible characters do exist. For example, here we will use a string literal (indicated by the \"\"\" ) to tell Julia to interpret the string as given, including the invisible new line created by hitting return on the keyboard between the two words:\n\n\"\"\"\nhello\nworld\n\"\"\"\n\n\"hello\\nworld\\n\"\n\n\nThe output above shows the \\n character contained within the string.\nSymbols are a way of representing an identifier which cannot be seen as a collection of individual characters. :helloworld is distinct from \"helloworld\" - you can kind of think of the former as an un-executed bit of code - if we were to execute it (with eval(:helloworld)), we would get an error UndefVarError: `helloworld` not defined . Symbols can look like strings but do not behave like them. For now, it is best to not worry about symbols but it is an important aspect of Julia which allows the language to represent aspects of itself as data. This allows for powerful self-reference and self-modification of code but this is a more advanced topic generally out of scope of this book.\n\n\n5.4.3.6 Tuples\nTuples are a set of values that belong together and are denoted by a values inside parenthesis and separated by a comma. An example might be x-y coordinates in 2 dimensional space:\n\nx = 3\ny = 4\np1 = (x, y)\n\n(3, 4)\n\n\nA tuple’s values can be accessed like arrays:\n\np1[1]\n\n3\n\n\nTuples fill a middle ground between scalar types and arrays in more ways that one:\n\nTuples have no problem having heterogeneous types in the different slots.\nTuples are immutable, meaning that you cannot overwrite the value in memory (an error will be thrown if we try to do p[1] = 5).\nIt’s generally expected that within an array, you would be able to apply the same operation to all the elements (e.g. square each element) or do something like sum all of the elements together which isn’t generally case for a tuple.\nTuples are generally stack allocated instead of being heap allocated like arrays14, meaning that a lot of times they can be faster than arrays.\n\n\n5.4.3.6.1 Named Tuples\nNamed tuples provide a way to give each field within the tuple a specific name. For example, our x-y coordinate example above could become:\n\np2 = (x=3, y=4)\n\n(x = 3, y = 4)\n\n\nThe benefit is that we can give more meaning to each field and access the values in a nicer way. Previously, we used location[1] to access the x-value, but with the new definition we can access it by name:\n\np2.x\n\n3\n\n\n\n\n\n5.4.3.7 Dictionaries\nDictionaries are a container which relates a key to an associated value. Kind of like how arrays relate an index to a value, but the difference is that a dictionary is (1) un-ordered and (2) the key doesn’t have to be an integer.\nHere’s an example which relates a name to an age:\n\nd = Dict(\n    \"Joelle\" =&gt; 10,\n    \"Monica\" =&gt; 84,\n    \"Zaylee\" =&gt; 39,\n)\n\nDict{String, Int64} with 3 entries:\n  \"Monica\" =&gt; 84\n  \"Zaylee\" =&gt; 39\n  \"Joelle\" =&gt; 10\n\n\nThen we can look up an age given a name:\n\nd[\"Zaylee\"]\n\n39\n\n\nDictionaries are super flexible data structures and can be used in many situations.\n\n\n\n5.4.4 Parametric Types\nWe just saw how tuples can contain heterogeneous types of data inside a common container. Parametric Types are a way of allowing types themselves to be variable, with a wrapper type containing a to-be-specified inner-type.\nLet’s look at this a little bit closer by looking at the full type:\n\ntypeof(p1)\n\nTuple{Int64, Int64}\n\n\np1 is a Tuple{Int64,Int64} type, which means that its first and second elements are both Int64. Contrast this with:\n\ntypeof((\"hello\", 1.0))\n\nTuple{String, Float64}\n\n\nThese tuples are both of the form Tuple{T,U} where T and U are both types. Why does this matter? We and the compiler can distinguish between a Tuple{Int64,Int64} and a Tuple{String,Float64} which allows us to reason about things (“I can add the first element of tuple together only if both are numbers”) and the compiler to optimize (sometimes it can know exactly how many bits in memory a tuple of a certain kind will need and be more efficient about memory use). Further, we will see how this can become a powerful force in writing appropriately abstracted code and more logically organize our entire program when we encounter “multiple dispatch” later on.\nThis is a very powerful technique - we’ve already seen the flexibility of having an Array type which can contain arbitrary inner types and dimensions. The full type signature for an Array looks like Array{InnerType,NumDimensions}.\n\nlet\n    x = [1 2\n        3 4]\n    typeof(x)\nend\n\n\nMatrix{Int64} (alias for Array{Int64, 2})\n\n\n\n\n\n5.4.5 Types for things not there\nnothing represents that there’s nothing to be returned - for example if there’s no solution to an optimization problem or if a function just doesn’t have any value to return (such as in the case with input/output like println).\nmissing is to represent something should be there but it’s not, as is all too common in real-world data. Julia natively supports missing and three-value logic, which an an extension of the two-value boolean (true/false) logic, to handle missing logical values:\n\n\n\nTable 5.1: Three value logic with true, missing, and false.\n\n\n\n\n\n\n\n(a) NOT logic\n\n\n\n\n\nNOT (!)\nValue\n\n\n\n\ntrue\nfalse\n\n\nmissing\nmissing\n\n\nfalse\ntrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) AND logic\n\n\n\n\n\nAND (&)\ntrue\nmissing\nfalse\n\n\n\n\ntrue\ntrue\nmissing\nfalse\n\n\nmissing\nmissing\nmissing\nfalse\n\n\nfalse\nfalse\nfalse\nfalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) OR Logic\n\n\n\n\n\nOR (|)\ntrue\nmissing\nfalse\n\n\n\n\ntrue\ntrue\ntrue\ntrue\n\n\nmissing\ntrue\nmissing\nmissing\n\n\nfalse\ntrue\nmissing\nfalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nMissing and Nothing are the types while missing and nothing are the values here15. This is analogous to Float64 being a type and 2.0 being a value.\n\n\n\n\n5.4.6 Union Types\nWhen two types may arise in a context, union types are a way to represent that. For example, if we have a data feed and we know that it will produce either a Float64 or a Missing type then we can say that the value for this is Union{Float64,Missing}. This is much better for the compiler (and our performance!) than saying that the type of this is Any.\n\n\n5.4.7 Creating User Defined Types\nWe’ve talked about some built-in types but so much additional capabilities come from being able to define our own types. For example, taking the x-y-coordinate example from above, we could do the following instead of defining a tuple:\n\nstruct BasicPoint\n    x::Int64\n    y::Int64\nend\n\np3 = BasicPoint(3, 4)\n\nBasicPoint(3, 4)\n\n\nBasicPoint is a composite type because it is composed of elements of other types. Fields are accessed the same way as named tuples:\n\n1p3.x, p3.y\n\n\n1\n\nNote that here, Julia will return a tuple instead of a single value due to the comma separated expressions.\n\n\n\n\n(3, 4)\n\n\nstructs in Julia are immutable like tuples above.\nBut wait, didn’t tuples let us mix types too via parametric types? Yes, and we can do the same with our type!\n\nstruct Point{T}\n    x::T\n    y::T\nend\n\nLine 1 The {T} after the type’s name allows for different Points to be created depending on what the type of the underlying x and y is.\nHere’s two new points which now have different types:\n\np4 = Point(1, 4)\np5 = Point(2.0, 3.0)\n\np4, p5\n\n(Point{Int64}(1, 4), Point{Float64}(2.0, 3.0))\n\n\nNote that the types are not equal because they have different type parameters!\n\ntypeof(p4), typeof(p5), typeof(p4) == typeof(p5)\n\n(Point{Int64}, Point{Float64}, false)\n\n\nBoth p4 and p5 are instances of different concrete types Point{Int} and Point{Float64}. The expression X isa Y is true when X is a (sub)type of Y:\n\np4 isa Point, p5 isa Point\n\n(true, true)\n\n\nNote though, that the x and y are both of the same type in each PPoint2D that we created. If instead we wanted to allow the coordinates to be of different types, then we could have defined PPoint2D as follows:\nstruct Point{T,U}\n    x::T\n    y::U\nend\n\n\n\n\n\n\nNote\n\n\n\nCan we define the structs above without indicating a (parametric) type? Yes!\n\nstruct PointUntyped\n    x # no type here!\n    y # no type declared here either!\nend\n\nBut! x and y will both be allowed to be Any, which is the fallback type where Julia says that it doesn’t know any more about the type until runtime (the time at which our program encounters the data when running). Observe that the type of x and y here is Any:\n\nfieldtypes(PointUntyped)\n\n(Any, Any)\n\n\nThis means that the compiler (and us!) can’t reason about or optimize the code as effectively as when the types are explicit or parametric. This is an example of how Julia can provide a nice learning curve - don’t worry about the types until you start to get more sophisticated about the program design or need to extract more performance from the code.\n\n\nThe above structs that we have defined are examples of concrete types types which hold data. Abstract types don’t directly hold data themselves but are used to define a hierarchy of types which we will later exploit (Chapter 8) to implement custom behavior depending on what type our data is.\nHere’s an example of (1) defining a set of related types that sits above our Point2D:\n\nabstract type Coordinate end\nabstract type CartesianCoordinate &lt;: Coordinate end\nabstract type PolarCoordinate &lt;: Coordinate end\n\nstruct Point2D{T} &lt;: CartesianCoordinate\n    x::T\n    y::T\nend\n\nstruct Point3D{T} &lt;: CartesianCoordinate\n    x::T\n    y::T\n    z::T\nend\n\nstruct Polar2D{T} &lt;: PolarCoordinate\n    r::T\n    θ::T\nend\n\n\n\n\n\n\n\nTipUnicode Characters\n\n\n\nJulia has wonderful Unicode support, meaning that it’s not a problem to include characters like θ. The character can be typed in Julia editors by entering \\theta and then pressing the TAB key on the keyboard.\nUnicode is helpful for following conventions that you may be used to in math. For example, the math formula \\(\\text{circumference}(r) = 2 \\times r \\times \\pi\\) can be written in Julia with circumference(r) = 2 * r * π.\nThe name for the characters follows the same for LaTeX, so you can search the internet to find the appropriate name: for example search “theta LaTeX”. Further, you can use the REPL help mode to find out how to enter a character if you can copy and paste it from somewhere:\nhelp?&gt; θ\n\"θ\" can be typed by \\theta&lt;tab&gt;\n\n\nTo constrain the types that could be used within our coordinates above, such as if we wanted the fields to all be Real-valued, we could modify the struct definitions with the &lt;:Real annotation:\nstruct Point2D{T&lt;:Real} &lt;: CartesianCoordinate\n    # ...\nend\n\nstruct Point3D{T&lt;:Real} &lt;: CartesianCoordinate\n    # ...\nend\n\nstruct Polar2D{T&lt;:Real} &lt;: PolarCoordinate\n    # ...\nend\n\n\n5.4.8 Mutable structs\nIt is possible to define structs where the data can be modified - such a data field is said to be mutable because it can be changed or mutated. Here’s an example of what it would look like if we made Point2D mutable:\nmutable struct Point2D{T}\n    x::T\n    y::T\nend\nYou may find that this more naturally represents what you are trying to do. However, recall that an advantage of an immutable datatype is that costly memory doesn’t necessarily have to be allocated for it. So you may think that you’re being more efficient by re-using the same object… but it may not actually be faster. Again, more will be revealed in Chapter 9.\n\n\n\n\n\n\nTipFinancial Modeling Pro Tip\n\n\n\nFor financial models, it is best practice to default to immutable ⁠structs. Immutability prevents accidental modification of data, making your model’s state easier to reason about and debug. This is especially critical in complex models with many interacting components. Use ⁠mutable struct only when you have a specific reason to modify data in-place.\n\n\n\n\n5.4.9 Constructors\nConstructors are functions that return a data type (functions will be covered more generally later in the chapter). When we declare a struct, an implicit function is defined that takes a tuple of arguments and returns the data type that was declared. In the following example, after we define MyType the struct, Julia creates a function (also called MyType) which takes two arguments and will return the datatype MyType:\n\nstruct MyDate\n    year::Int\n    month::Int\n    day::Int\nend\n\nmethods(MyDate)\n\n# 2 methods for type constructor: Main.Notebook.MyDate(year::Int64, month::Int64, day::Int64) in Main.Notebook at /Users/alecloudenback/prog/julia-fin-book/foundations-of-programming.qmd:1002  Main.Notebook.MyDate(year, month, day) in Main.Notebook at /Users/alecloudenback/prog/julia-fin-book/foundations-of-programming.qmd:1002 \n\n\nImplicit constructors are nice in that you don’t have to define a default method and the language does it for you. Sometimes there’s reasons to want to control how an object is created, either for convenience or to enforce certain restrictions.\nWe can use an inner constructor (i.e. inside the struct block) to enforce restrictions:\nstruct MyDate\n    year::Int\n    month::Int\n    day::Int\n\n    function MyDate(y,m,d)\n        if ~(m in 1:12)\n            error(\"month is not between 1 and 12\")\n        elseif ~(d in 1:31)\n            error(\"day is not between 1 and 31\")\n        else\n            return new(y,m,d)\n        end\n\n    end\n                \nend\nAnd outer constructors are simply functions defined that have the same name as the data type , but are not defined inside the struct block. Extending the MyDate example, say we want to provide a default constructor for if no day is given such that the date returns the 1st of the month:\nfunction MyDate(y,m)\n    return MyDate(y,m,1)\nend",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "foundations-of-programming.html#functions",
    "href": "foundations-of-programming.html#functions",
    "title": "5  Elements of Programming",
    "section": "5.5 Functions",
    "text": "5.5 Functions\nFunctions are a set of expressions that take inputs and return specified outputs.\n\n5.5.1 Special Operators\nOperators are the glue of expressions which combine values. We’ve already seen quite a few, but let’s develop a little bit of terminology for these functions.\nUnary operators are operators which only take a single argument. Examples include the ! which negates a boolean value or - which negates a number:\n\n!true, -5\n\n(false, -5)\n\n\nBinary operators take two arguments and are some of the most common functions we encounter, such as + or - or &gt;:\n\n1 + 2, 1 - 2, 1 &gt; 2\n\n(3, -1, false)\n\n\nThe above unary and binary operators are special kinds of functions which don’t require the use of parenthesis. However, they can be written with parentheses for greater clarity:\n\n!(true), -(5), +(1, 2), -(1, 2)\n\n(false, -5, 3, -1)\n\n\nIn Julia, we distinguish between functions which define behavior that maps a set of inputs to outputs. But a single function can adapt its behavior to the arguments themselves. We have just seen the function - be used in two different ways: negation and subtraction depending on whether it had one or two arguments given to it. In this way there is a conceptual hierarchy of functions that complements the hierarchy we have discussed in relation to types:\n\n- is the overall function\n-(x) is a unary function which negates its values, -(x,y) subtracts y from x\nSpecific methods are then created for each combination of concrete types: -(x::Float64) is a different method than -(x::Int)\n\nMethods are specific compiled versions of the function for specific types. This is important because at a hardware level, operations for different types (e.g. integers versus floating point) differ considerably. By optimizing for the specific types Julia is able to achieve nearly ideal performance without the same sacrifices of other dynamic languages. We will develop more with respect to methods when we talk about dispatch in Chapter 8.\nFor example, factorial would be referred to as the function, while specific implementations are called methods. We can see all of the methods for any function with the methods function, like the following for factorial which has implementations taking into account the specialized needs for different types of arguments:\n\nmethods(factorial)\n\n# 7 methods for generic function factorial from \u001b[90mBase\u001b[39m: factorial(n::UInt128) in Base at combinatorics.jl:26  factorial(n::Int128) in Base at combinatorics.jl:25  factorial(x::BigFloat) in Base.MPFR at mpfr.jl:769  factorial(n::BigInt) in Base.GMP at gmp.jl:703  factorial(n::Union{Int16, Int32, Int8, UInt16, UInt32, UInt8}) in Base at combinatorics.jl:33  factorial(n::Union{Int64, UInt64}) in Base at combinatorics.jl:27  factorial(n::Integer) in Base at intfuncs.jl:1137 \n\n\n\n\n5.5.2 Defining Functions\nFunctions more generally are defined like so:\nfunction functionname(arguments)\n    # ... code that does things\nend\nHere’s an example which returns the difference between the highest and lowest values in a collection:\nfunction value_range(collection)\n\n    hi = maximum(collection)\n    lo = minimum(collection)\n1    return hi - lo\nend\n\n1\n\nreturn is optional but recommended to convey to readers of the program where you expect your function to terminate and return a value.\n\n\n\n\n5.5.3 Defining Methods on Types\nHere’s another example of a function which calculates the distance between a point and the origin:\n\n1function distance(point)\n2    return sqrt(point.x^2 + point.y^2)\nend\n\n\n1\n\nA function block is declared with the name distance which takes a single argument called point\n\n2\n\nWe compute the distance formula for a point with x and y coordinates. The return value make explicit what value the function will output.\n\n\n\n\ndistance (generic function with 1 method)\n\n\n\n\n\n\n\n\nNote\n\n\n\nAn alternate, simpler function syntax for distance would be:\ndistance(point) = sqrt(point.x^2 + point.y^2)\n\n\nHowever, we might at this point note a flaw in our function’s definition if we think about the various Coordinates we defined earlier: our definition would currently only work for Point2D. For example, if we try a Point3D we will get the wrong answer:\n\ndistance(Point3D(1, 1, 1,))\n\n1.4142135623730951\n\n\nThe above value should be \\(\\sqrt(3)\\), or approximately \\(1.73205\\).\nWhat we need to do is define a refined distance for each type, which we’ll call dist to distinguish from the earlier definition.\n\n\"\"\"\n    dist(point)\n\nThe euclidean distance of a point from the origin.\n\"\"\"\ndist(p::Point2D) = sqrt(p.x^2 + p.y^2)\ndist(p::Point3D) = sqrt(p.x^2 + p.y^2 + p.z^2)\ndist(p::Polar2D) = p.r\n\ndist (generic function with 3 methods)\n\n\nNow our result will be correct:\n\ndist(Point3D(1, 1, 1,))\n\n1.7320508075688772\n\n\nThis is referred to dispatching on the argument types. Julia will look up to find the most specific method of a function for the given argument types, and falling back to a generic implementation if one is defined.\nIn Chapter 8 we will see how dispatch (single and multiple) can provide very nice abstractions to simplify the design of a model.\n\n\n\n\n\n\nNoteDocstrings (Documentation Strings)\n\n\n\nNotice the strings preceding the definition of dist. In Julia, putting a string (\"...\") or string literal (\"\"\"...\"\"\") right above the definition will allow Julia to recognize the string as documentation and provided it to the user in help mode (Section 21.4.1) and/or have a documentation tool create a webpage or PDF documentation resource.\n\n\n\n\n\n\n\n\nCautionDefining Methods for Parametric Types\n\n\n\nWe learned that Float64 &lt;: Real in the type hierarchy. However, note that Tuple{Float64} is not a sub-type of Tuple{Real}. This is called being invariant in type theory… but for our purposes this just practically means that when we define a method we need to specify that we want it to apply to all subtypes.\nFor example, myfunction(x::Tuple{Real}) would not be called if x was a Tuple{Float64} because it’s not a sub-type of Tuple{Real}. To act the way we want, would define the method with the signature of myfunction(Tuple{&lt;:Real}) or myfunction{Tuple{T}} where {T&lt;:Real}.\n\n\n\n\n5.5.4 Keyword Arguments\nKeyword arguments are arguments that are passed to a function but do not use position to pass data to functions but instead used named arguments. In the following example, filepath is a positional argument while the two arguments after the semicolon (;) are keyword arguments.\nfunction read_data(filepath; normalizenames, hasheaderrow)\n    # ... function would be defined here\nend\nThe function would need to be called and have the two keyword arguments specified:\nread_data(\"results.csv\"; normalizenames=true, hasheaderrow=false)\n\n\n5.5.5 Default Arguments\nWe are able to define default arguments for both positional and keyword arguments via an assignment expression in the function signature. For example, we can make it so that the user need not specify all the options for each call. Modifying the prior example so that typical CSVs work with less customization from the user:\nfunction read_data(filepath;\n    normalizenames = true,\n    hasheader = false\n)\nThis is a simplified example, but if you look at the documentation for most data import packages you’ll see a lot of functionality defined via keyword arguments which have sensible defaults so that most of the the time you need not worry about modifying them.\n\n\n5.5.6 Anonymous Functions\nAnonymous functions are functions that have no name and are used in contexts where the name does not matter. The syntax is x -&gt; ...expression with x.... As an example, say that we want to create a vector from another where each element is squared. map applies a function to each member of a given collection:\n\nv = [4, 1, 5]\n1map(x -&gt; x^2, v)\n\n\n1\n\nThe x -&gt; x^2 is the anonymous function in this example.\n\n\n\n\n3-element Vector{Int64}:\n 16\n  1\n 25\n\n\nThey are often used when constructing something from another value, or defining a function within optimization or solving routines.\n\n\n5.5.7 First Class Nature\nFunctions in many languages including Julia are first class which means that functions can be assigned and moved around like data variables.\nIn this example, we have a general approach to calculate the error of a modeled result compared to a known truth. In this context, there are different ways to measure error of the modeled result and we can simplify the implementation of loss by keeping the different kinds of error defined separately. Then, we can assign a function to a variable and use it as an argument to another function:\n\nfunction square_error(guess, correct)\n    (correct - guess)^2\nend\n\nfunction abs_error(guess, correct)\n    abs(correct - guess)\nend\n\n# obs meaning \"observations\"\nfunction loss(modeled_obs,\n    actual_obs,\n1    loss_function\n)\n    sum(\n        loss_function.(modeled_obs, actual_obs)\n    )\nend\n\n2let\n3    a = loss([1, 5, 11], [1, 4, 9], square_error)\n    b = loss([1, 5, 11], [1, 4, 9], abs_error)\n    a, b\nend\n\n\n1\n\nloss_function is a variable that will refer to a function instead of data.\n\n2\n\nUsing a let block here is good practice to not have temporary variables a and b scattered around our workspace.\n\n3\n\nUsing a function as an argument to another function is an example of functions being treated as “first class”.\n\n\n\n\n(5, 3)\n\n\n\n\n5.5.8 Broadcasting\nLooking at the prior definition of dist, what if we wanted to compute the squared distance from the origin for a set of points? If those points are stored in an array, we can broadcast functions to all members of a collection at the same time. This is accomplished using the dot-syntax as follows:\n\npoints = [Point2D(1, 2), Point2D(3, 4), Point2D(6, 7)]\ndist.(points) .^ 2\n\n3-element Vector{Float64}:\n  5.000000000000001\n 25.0\n 85.0\n\n\nLet’s unpack that a bit more:\n\nThe . in dist.(points) tells Julia to apply the function dist to each element in points.\nThe . in .^ tells Julia to square each values as well\n\nWhy broadcasting is useful:\n\nWithout needing any redefinition of functions we were able to transform the function dist and exponentiation (^) to work on a collection of data. This means that we can keep our code simpler and easier to reason about (operating on individual things is easier than adding logic to handle collections of things).\nWhen multiple broadcasted operations are joined together, Julia can fuse the operations so that each operation is performed at the same time instead of each step sequentially. That is, if the operation were not fused, the computer would first calculate dist for each point, and then apply the square on the collection of distances. When it’s fused, the operations can happen at the same time without creating an interim set of values.\n\n\n\n\n\n\n\nNote\n\n\n\nFor readers coming from numpy-flavored Python or R, broadcasting is a way that can feel familiar to the array-oriented behavior of those two languages. Once you feel comfortable with Julia in general, you may find yourself relaxing and relying less on array-oriented design and instead picking whichever iteration paradigm feels most natural for the problem at hand: loops or broadcasting over arrays.\n\n\n\n5.5.8.1 Broadcasting Rules\nWhat happens if one of the collections is not the same size as the others? When broadcasting, singleton dimensions (i.e. the 1 in 1xN, “1-by-N”, dimensions) will be expanded automatically when it makes sense. For example, if you have a single element and a one dimensional array, the single element will be expanded in the function call without using any additional memory (if that dimension matches one of the dimensions of the other array).\nThe rules with an MxN and a PxQ array:\n\neither (M and P) or (N and Q) need to be the same, and\none of the non-matching dimensions needs to be 1\n\nSome examples might clarify. This 1x1 element is being combined with a 4x1, so there is a compatible dimension (N and Q match, M is 1):\n\n2 .^ [0, 1, 2, 3]\n\n4-element Vector{Int64}:\n 1\n 2\n 4\n 8\n\n\nHere, this 1x3 works with the 2x3 (N and Q match, M is 1)\n\n[1 2 3] .+ [1 2 3; 4 5 6]\n\n2×3 Matrix{Int64}:\n 2  4  6\n 5  7  9\n\n\nThis 3x1 isn’t compatible with this 2x3 array (neither M and P nor N and Q match)\n#| error: true\n[1, 2, 3] .+ [1 2 3; 4 5 6]\nThis 2x4 isn’t compatible with the 2x3 (M and P match, but N nor Q is 1):\n#| error: true\n[1 2; 3 4] .+ [1 2 3; 4 5 6]\n\n\n5.5.8.2 Not Broadcasting\nWhat if you do not want the array to be used element-wise when broadcasting? Then you can wrap the array in a Ref, which is used in broadcasting to make the array be treated like a scalar. In the example below, in(needle,haystack) searches a collection (haystack) for an item (needle) and returns true or false if the item is in the collection:\n\nin(4, [1 2 3; 4 5 6])\n\ntrue\n\n\nWhat if we had an array of things (“needles”) that we wanted to search for? By default, broadcasting would effectively split the array up into collections of individual elements to search:\n\nin.([1, 9], [1 2 3; 4 5 6])\n\n2×3 BitMatrix:\n 1  0  0\n 0  0  0\n\n\nEffectively, the result above is the result of this broadcasted result:\nin(1, [1,2,3]) # the first row of the above result\nin(9, [4,5,6])\nIf we were expecting Julia to return [true,false] (that the first needle is in the haystack but the second needle is not), then we need to tell Julia not to broadcast along the second array with Ref:\n\nin.([1, 9], Ref([1 2 3; 4 5 6]))\n\n2-element BitVector:\n 1\n 0\n\n\n\n\n\n5.5.9 Passing by Sharing\nWe often want to share data between scopes, such as between modules or by passing something into a function’s scope. Arguments to a function in Julia are passed-by-sharing which means that an outside variable can be mutated from within a function. We can modify the array in the outer scope (scope discussed later in this chapter) from within the function. In this example, we modify the array that is assigned to v by doubling each element:\n\nv = [1, 2, 3]\n\nfunction double!(v)\n    for i in eachindex(v)\n        v[i] = 2 * v[i]\n    end\nend\n\ndouble!(v)\n\nv\n\n3-element Vector{Int64}:\n 2\n 4\n 6\n\n\n\n\n\n\n\n\nTip\n\n\n\nConvention in Julia is that a function that modifies its arguments has a ! in its name and we follow this convention in double! above. Another example would be the built-in function sort! which will sort an array in-place without allocating a new array to store the sorted values.\n\n\nWe won’t discuss all potential ways that programming languages can behave in this regard, but an alternative that one may have seen before (e.g. in Matlab) is pass-by-value where a modification to an argument only modifies the value within the scope. Here’s how to replicate that in Julia by copying the value before handing it to a function. This time, v is not modified because we only passed a copy of the array and not the array itself:\n\nv = [1, 2, 3]\ndouble!(copy(v))\nv\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\n\n5.5.10 The Function Type\nIn Julia, every function is an object with its own unique type. Function is the abstract supertype of all functions. You can see this by inspecting the type of a function:\n\ntypeof(+)\n\ntypeof(+) (singleton type of function +, subtype of Function)\n\n\nThe output, typeof(+), indicates that the function + has its own special type. This specific type is a subtype of the abstract Function type:\n\ntypeof(+) &lt;: Function\n\ntrue\n\n\nThis is true for any function, including ones you define:\n\nfunction my_func(x)\n    x + 1\nend\n\ntypeof(my_func) &lt;: Function\n\ntrue",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "foundations-of-programming.html#sec-scope",
    "href": "foundations-of-programming.html#sec-scope",
    "title": "5  Elements of Programming",
    "section": "5.6 Scope",
    "text": "5.6 Scope\nIn projects of even modest complexity, it can be challenging to come up with unique identifiers for different functions or variables. Scope refers to the bounds for which an identifier is available. We will often talk about the local scope that’s inside some expression that creates a narrowly-defined scope (such as a function or let or module block) or the global scope which is the top level scope that contains everything else inside of it. Here are a few examples to demonstrate scope.\n\n1i = 1\n2let\n3    j = 3\n    i + j\nend\n\n\n1\n\ni is defined in the global scope and would be available to other inner scopes.\n\n2\n\nThe let ... end block creates a local scope which inherits the defined global scope definitions.\n\n3\n\nj is only defined in the local scope created by the let block.\n\n\n\n\n4\n\n\nIn fact, if we try to use j outside of the scope defined above we will get an error:\n\nj\n\nUndefVarError: UndefVarError(:j, Main.Notebook)\nUndefVarError: `j` not defined in `Main.Notebook`\nSuggestion: check for spelling errors or missing imports.\n\n\n\n\n\n\n\n\nTip\n\n\n\nlet blocks are a great way to organize your code in bite-sized chunks or to be able to re-use common variable names without worrying about conflict. Here’s an example of using let blocks to:\n\nPerform intermediate calculations without fear of returning a partially modified variable\nRe-use common variable names\n\n\nbonds = let\n    df = CSV.read(\"bonds.csv\", DataFrame)\n    df.issuer = lookup_issuer(df.CUSIP)\n    df\nend\n\nmortgages = let\n    df = CSV.read(\"mortgages.csv\", DataFrame)\n    df.issuer = lookup_issuer(df.CUSIP)\n    df\nend\nIf we were running this interactively (e.g. step-by step in VS Code, the REPL, or notebooks) then these two code blocks will run completely and will run separately. The short, descriptive name df is reused, but there’s no chance of conflict. We also can’t easily run the block of code (let ... end) and get a partially evaluated result (e.g. getting the dataframe before it has been appropriately modified to add the issuer column).\n\n\nHere is an example with functions:\n\nx = 2\nbase = 10\n1foo() = base^x\n2foo(x) = base^x\n3foo(x, base) = base^x\n\nfoo(), foo(4), foo(4, 4)\n\n\n1\n\nBoth base and x are inherited from the global scope.\n\n2\n\nx is based on the local scope from the function’s arguments and base is inherited from the global scope.\n\n3\n\nBoth base and x are defined in the local scope via the function’s arguments.\n\n\n\n\n(100, 10000, 256)\n\n\nIn Julia, it’s always best to explicitly pass arguments to functions rather than relying on them coming from an inherited scope. This is more straight-forward and easier to reason about and it also allows Julia to optimize the function to run faster because all relevant variables coming from outside the function are defined at the function’s entry point (the arguments).\n\n5.6.1 Modules and Namespaces\nModules are ways to encapsulate related functionality together. Another benefit is that the variables inside the module don’t “pollute” the namespace of your current scope. Here’s an example:\n\n1module Shape\n\nstruct Triangle{T}\n    base::T\n    height::T\nend\n\n2function area(t::Triangle)\n    return t.base * t.height / 2\nend\nend\n\n3t = Shape.Triangle(4, 2)\n4area = Shape.area(t)\n\n\n1\n\nmodule defines an encapsulated block of code which is anchored to the namespace Shape\n\n2\n\nHere, area a function defined within the Shape module.\n\n3\n\nOutside of Shape module, we can access the definitions inside via the Module.identifier syntax.\n\n4\n\nHere, area is a variable in our global scope that does not conflict with the area defined within the Shape module. If Shape.area were not within a module then when we said area = ... we would have reassigned area to no longer refer to the function and instead would refer to the area of our triangle.\n\n\n\n\n4.0\n\n\n\n\n\n\n\n\nNote\n\n\n\nSummarizing related terminology:\n\nA module is a block of code such as module MySimulation ... end\nA package is a module that has a specific set of files and associated metadata. Essentially, it’s a module with a Project.toml file that has a name and unique identifier listed, and a file in a src/ directory called MySimulation.jl\n\nLibrary is just another name for a package, and the most common context this comes up is when talking about the packages that are bundled with Julia itself called the standard library (stdlib).",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "foundations-of-programming.html#footnotes",
    "href": "foundations-of-programming.html#footnotes",
    "title": "5  Elements of Programming",
    "section": "",
    "text": "Said differently, computer science may contemplate ideas and abstractions more generally than a specific implementation, as in mathematics where a theorem may be proved (\\(a^2 + b^2 = c^2\\)) without resorting to specific numeric examples (\\(3^2 + 4^2 = 5^2\\)).↩︎\nThe Ship of Theseus problem specifically refers to a legendary ancient Greek ship, owned by the hero Theseus. The paradox arises from the scenario where, over time, each wooden part of the ship is replaced with identical materials, leading to the question of whether the fully restored ship is still the same ship as the original. The Ship of Theseus problem is a thought experiment in philosophy that explores the nature of identity and change. It questions whether an object that has had all of its components replaced remains fundamentally the same object.↩︎\nThis binary representations correspond to B and 66 with the ASCII character set and 8-bit integer encodings respectively, discussed later in this chapter.↩︎\nSome distinctions you may encounter: in short-form, “kb” means kilobits while the upper-case “B” in “kB” means kilobytes. Also confusingly, sometimes the “k” can be binary or decimal - because computers speak in binary, a binary “k” means 1024 (equal to 2^10) instead of the usual decimal 1000. In most computer contexts, the binary (multiples of 1024) is more common.↩︎\nThe term floating point refers to the fact that the number’s radix (decimal) point can “float” between the significant digits of the number.↩︎\nThat is, it reads the code input from the user, evaluates what code was given to it, prints the result of the input to the screen, and loops through the process again.↩︎\nThis means that their central processing units (CPUs) use instructions that are 64 bits long.↩︎\nAccurate only to a limited precision, as described in Section 5.4.1.↩︎\nWhether an index starts at 1 or 0 is sometimes debated. Zero-based indexing is natural in the context of low-level programming which deal with bits and positional offsets in computer memory. For higher level programming one-based indexing is more natural: in a set of data stored in an array, it is much more natural to reference the first (through \\(n^{th}\\)) datum instead of the zeroth (through \\((n-1)^{th}\\) datum.↩︎\nArrays in Julia can actually be indexed with an arbitrary starting point: see the package OffsetArrays.jl↩︎\nThe triangular numbers (sum of integers from \\(1\\) to \\(n\\)) are:\\[\nT_n = \\sum_{k=1}^n k = 1 + 2 + \\cdots + n\n    = \\frac{n^2 + n}{2} = \\frac{n(n+1)}{2}\n    = \\binom{n+1}{2}\n\\]↩︎\nIf the step keyword argument is defined: whether the last number is in the resulting range depends on if the step evenly divides the end of the range. In the example 1:2:7, the step is 2.↩︎\nUnder the hood, strings are essentially a vector of characters but there are complexities with character encoding that don’t allow a lossless conversion to individual characters of uniform bit length. This is for historical compatibility reasons and to avoid making most documents’ file sizes larger than it needs to be.↩︎\nWhat this means will be explained in Chapter 9 .↩︎\nMissing and Nothing are instances of singleton type, which means that there is only a single value that either type can take on.↩︎",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "first-abstractions.html",
    "href": "first-abstractions.html",
    "title": "6  Functional Abstractions",
    "section": "",
    "text": "6.1 Chapter Overview\nDemonstrate different approaches to a problem which gradually introduce more re-usable or general techniques. These techniques will allow for constructing sophisticated models while maintaining consistency and simplicity. Imperative programming, functional programming, and recursion.",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functional Abstractions</span>"
    ]
  },
  {
    "objectID": "first-abstractions.html#introduction",
    "href": "first-abstractions.html#introduction",
    "title": "6  Functional Abstractions",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nThis chapter will center around a simple task: calculate the present value of a portfolio of a single fixed, risk-free, coupon-paying bond under two different interest rate environments. The focus will be on describing different approaches to this problem, not complexity to the problem (e.g. no getting into credit spreads, settlement timing, etc.).\nMathematically, the problem is to determine the \\(\\text{Present Value}\\), where:\n\\[\n\\text{Present Value} = \\sum{\\text{Cashflow}_t \\times \\text{Discount Factor}_t}\n\\]\nWhere\n\\[\n\\text{Discount Factor}_t = \\prod^t{\\frac{1}{1+\\text{Discount Rate}_i}}\n\\]\n\ncf_bond = [10, 10, 10, 10, 110];\n1rate = [0.05, 0.06, 0.05, 0.04, 0.05];\n\n\n1\n\nThe rates are the one year forward rates for time 0, 1, 2, etc.\n\n\n\n\nWe will focus on this first discount vector, and introduce more scenarios later in the chapter.\nWe will repeatedly solve the same problem before extending it to more examples. It may feel repetitive, but the goal is to highlight variations in approach rather than the problem itself.",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functional Abstractions</span>"
    ]
  },
  {
    "objectID": "first-abstractions.html#imperative-style",
    "href": "first-abstractions.html#imperative-style",
    "title": "6  Functional Abstractions",
    "section": "6.3 Imperative Style",
    "text": "6.3 Imperative Style\nOne of the most familiar styles of programming is called imperative (or procedural), where we provide explicit, step-by-step instructions to the computer. The programmer defines the data involved and how that data moves through the program one step at a time. It commonly uses loops to perform tasks repeatedly or across a set of data. The program’s state (assignment and logic of the program’s variables) is defined and managed by the programmer explicitly.\nHere’s an imperative style of calculating the present value of the bond.\n\nlet\n1    pv = 0.0\n    discount = 1.0\n\n2    for i in 1:length(cf_bond)\n        discount = discount / (1 + rate[i])\n3        pv = pv + discount * cf_bond[i]\n    end\n    pv\nend\n\n\n1\n\nDeclare variables to keep track of the discount rate and running total for the present value pv\n\n2\n\nLoop over the length of the cashflow vector.\n\n3\n\nAt each step of the loop, look up (via index i) update the discount factor to account for the prevailing rate and add the discounted cashflow to the running total present value.\n\n\n\n\n121.48888490821489\n\n\nThis style is simple, digestible, and clear. If we were performing the calculation by hand, it would likely follow a pattern very similar to this. Look up the first cashflow and discount rate, compute a discount factor, and subtotal the value. Repeat for the next set of values.\n\n6.3.1 Iterators\nNote that in the prior code example we defined an index variable i and had to manually define the range over which it would operate (1 through the length of the bond’s cashflow vector). A couple of reasons this could be sub-optimal:\n\nJulia arrays are 1-indexed by default. Some custom arrays (e.g., via OffsetArrays.jl) can use different starting indices: using eachindex avoids hard-coding index ranges and works for arrays with arbitrary axes.\nWe manually perform the lookup of the values within each iteration.\n\nWe can solve the first one (partially) by letting Julia return an iterable set of values corresponding to the indices of the cf_bond vector. This is an example of an iterator which is an object upon which we can repeatedly ask for the next value until it tells us to stop.\nBy using eachindex we can get the indices of the vector since Julia already knows what they are:\n\neachindex(cf_bond) # an efficient index iterator\n\nBase.OneTo(5)\n\n\n\n\n\n\n\n\nNoteLazy Programming\n\n\n\nThe result, Base.OneTo(5), returns an efficient iterator over valid indices (e.g., Base.OneTo(5) for a Vector). Iterators represent sequences without allocating per-element containers. You can traverse them directly, or materialize them if needed with collect.\nAn analogy is that we can write the “set of all numbers from 1 to 100” without writing out each of the 100 numbers, but we are referring to the same thing.\nAn example of operating on a lazy iterator, is that we could find the largest index:\n\nmaximum(eachindex(cf_bond)) # operates without materializing all the indices\n\n5\n\n\nThe point is if we have an object that represents a set, we need not actually enumerate each element of the set to interact with it.\nWe can fully instantiate an iterator with collect\n\ncollect(eachindex(cf_bond))  # materialize indices as a Vector\n\n5-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n\n\nLaziness is generally a good thing in programming because sometimes it can be computationally or memory expensive to fully instantiate the collection of interest (this will be discussed further in Chapter 9).\n\n\nAnd when used in context:\n\nlet\n    pv = 0.0\n    discount = 1.0\n\n    for i in eachindex(cf_bond)\n        discount = discount / (1 + rate[i])\n        pv = pv + discount * cf_bond[i]\n    end\n    pv\nend\n\n121.48888490821489\n\n\nHere Julia gave us the index associated with the bond cashflows, but we are still looking up the values (why not just ask for the values instead of their index?) as well as assuming that the indices are the same for the discount rates.\nWe can get the value and the associated index with enumerate:\n\ncollect(enumerate(cf_bond))\n\n5-element Vector{Tuple{Int64, Int64}}:\n (1, 10)\n (2, 10)\n (3, 10)\n (4, 10)\n (5, 110)\n\n\nThis would allow us to skip the step of needing to look up the bond’s cashflows. However, we can go even further by just asking for value associated with both collections. With zip (named because it’s sort of like zipping up two collections together), we get an iterator that provides the values of the underlying collections:\n\ncollect(zip(cf_bond, rate))\n\n5-element Vector{Tuple{Int64, Float64}}:\n (10, 0.05)\n (10, 0.06)\n (10, 0.05)\n (10, 0.04)\n (110, 0.05)\n\n\nThis provides the simplest implementation of the imperative approaches:\n\nlet\n    pv = 0.0\n    discount = 1.0\n\n    for (cf, r) in zip(cf_bond, rate)\n        discount = discount / (1 + r)\n        pv = pv + discount * cf\n    end\n    pv\nend\n\n121.48888490821489\n\n\nThe primary downsides to iterative approaches to algorithms are:\n\nNeeding to keep track of state is fine in simple cases, but can quickly become difficult to reason about and error prone as the number and complexity of variables grows.\nProgram flow is explicitly stated, leaving fewer places that the compiler can automatically optimize or parallelize.\n\n\n\n\n\n\n\nTip\n\n\n\nIn the imperative style, we mentioned needing to explicitly handle program state. In general, it’s advisable to minimize as many temporary state variables as possible - more mutability tends to produce more complex, difficult to maintain code. An example of maintaining state in the examples above is keeping track of the current index as well as interim pv and discount variables.\nAvoiding modifying values is often avoidable by restructuring the logic, using functional techniques, or finding the right abstractions. However, sometimes for performance reasons, clarity, or expediency you may find modifying state to be the preferred option and that’s okay.",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functional Abstractions</span>"
    ]
  },
  {
    "objectID": "first-abstractions.html#functional-techniques-and-terminology",
    "href": "first-abstractions.html#functional-techniques-and-terminology",
    "title": "6  Functional Abstractions",
    "section": "6.4 Functional Techniques and Terminology",
    "text": "6.4 Functional Techniques and Terminology\nFunctional programming is a paradigm which attempts to minimize state via composing functions together.\nTable 6.1 introduces a set of core functional methods to familiarize yourself with. Note that anonymous functions (Section 5.5.6) are used frequently to define intermediary steps.\n\n\n\n\nTable 6.1: Important Functional Methods.\n\n\n\n\n\n\n\n\n\n\nFunction\nDescription\nExample\n\n\n\n\nmap(f,v)\nApply function f to each element of the collection v.\nmap(\n    x-&gt;x^2,\n    [1,3,5]\n) # [1,9,25]\n\n\nreduce(op,v)\nApply binary op to pairs of values, reducing the dimension of the collection v.\n\nHas a couple of important, optional keyword arguments to note (which also apply to other variants of reduce below):\n\ninit defines the identity element (e.g. the initial value of + and * is 0 and 1 respectively)\ndims defines which dimension to reduce across (if the dimension of v is more than one).\n\nreduce(\n    *,\n    [1,3,5]\n) # 15\n\n\nmapreduce(f,op,v)\nMaps f over collection v and returns a reduced result using op.\nmapreduce(\n    x-&gt;x^2,\n    *,\n    [1,3,5]\n) # 225\n\n\nfoldl(op,v)foldr(op,v)\nLike reduce, but applies op from left to right (foldl) or right to left (foldr). Also has mapfoldl and mapfoldr versions.\nfoldl(\n    *,\n    [1,3,5]\n) # 15\n\n\naccumulate(op,v)\nApply op along v , creating a vector with the cumulative result.\naccumulate(\n    +,\n    [1,3,5]\n) # [1, 4, 9]\n\n\nfilter(f,v)\nApply f along v and return a copy of v with elements where f is true\nfilter(\n    &gt;=(3),\n    [1,3,5]\n) # [3, 5]\n\n\n\n\n\n\n\nThis paradigm is very powerful in a few ways:\n\nIt provides a language for talking about what a computation is doing. Instead of “looping over a collection called portfolio and calling a value function” we can more concisely refer to this as mapreduce(value,portfolio).\nOften you are forced to think about the design of the program more deeply, recognizing the core calculations and data used within the model.\nThe compiler is free to apply more optimizations. For example, with reduce, the compiler could optimize the calculation since the operation is assumed to be associative.\nThe lack of mutable state.\n\nLet’s build a version of the present value calculation using the functional building blocks described above. We will work up to it by discussing the core functional programming building blocks, culminating in combining mapreduce and accumulate to do the bond valuation.\n\n6.4.1 map\nmap is so named for the mathematical concept of mapping an input to an output. Here, it’s effectively the same thing. We take a collection and use the given function to calculate an output. The size of the output equals the size of the input.\nFirst, we will use map to compute the one-period discount factors:\n\nmap(x -&gt; 1 / (1 + x), rate)\n\n5-element Vector{Float64}:\n 0.9523809523809523\n 0.9433962264150942\n 0.9523809523809523\n 0.9615384615384615\n 0.9523809523809523\n\n\nmap transforms the rate collection by applying the anonymous function x -&gt; 1 / (1 + x), which is the single period discount factor. This operation is conveyed visually in Figure 6.1.\n\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\nInput Array (rate)\n\n\ncluster_1\n\nMap Function\n\n\ncluster_2\n\nOutput Array\n\n\n\nrate1\n\n0.05\n\n\n\nmap1\n\n1 / (1+x)\n\n\n\nrate1-&gt;map1\n\n\n\n\n\nrate2\n\n0.06\n\n\n\nmap2\n\n1 / (1+x)\n\n\n\nrate2-&gt;map2\n\n\n\n\n\nrate3\n\n0.05\n\n\n\nmap3\n\n1 / (1+x)\n\n\n\nrate3-&gt;map3\n\n\n\n\n\nrate4\n\n0.04\n\n\n\nmap4\n\n1 / (1+x)\n\n\n\nrate4-&gt;map4\n\n\n\n\n\nrate5\n\n0.05\n\n\n\nmap5\n\n1 / (1+x)\n\n\n\nrate5-&gt;map5\n\n\n\n\n\noutput1\n\n0.9524\n\n\n\nmap1-&gt;output1\n\n\n\n\n\noutput2\n\n0.9434\n\n\n\nmap2-&gt;output2\n\n\n\n\n\noutput3\n\n0.9524\n\n\n\nmap3-&gt;output3\n\n\n\n\n\noutput4\n\n0.9615\n\n\n\nmap4-&gt;output4\n\n\n\n\n\noutput5\n\n0.9524\n\n\n\nmap5-&gt;output5\n\n\n\n\n\n\n\n\nFigure 6.1: A diagram showing that map creates a new collection mirroring the old one, after applying the given function to each element in the original collection.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nmap is an absolute workhorse of a function and the authors recommend using it liberally within your code. We find ourselves using map frequently, usually avoiding defining an explicit loop (unless we are modifying some existing collection).\nmap would likely be a better tool for a loop like this:\noutput = []\nfor x in collection\n    result = # ... do stuff ...\n    push!(output,result)\nend\noutput\nInstead, map simplifies this to:\nmap(collection) do x\n    # ... do stuff\nend\nNot only does this have the advantage of being clearer, more concise, and less work, it also lets Julia infer the output type of your computation so you don’t have to worry about the type of output.\n\n\n\n\n6.4.2 accumulate\naccumulate takes an operation and a collection and returns a collection where each element is the cumulative result of applying the operation from the first element to the current one. For example, to calculate the cumulative product of the one-period discount factors:\n\nlet\n    rates =\n        accumulate(*, map(x -&gt; 1 / (1 + x), rate))\nend\n\n5-element Vector{Float64}:\n 0.9523809523809523\n 0.898472596585804\n 0.8556881872245752\n 0.822777103100553\n 0.7835972410481457\n\n\nThis results in a vector of the cumulative discount factors for each point in time corresponding to the given cashflows.\n\n\n\n\n\n\n\n\nG\n\n\ncluster_3\n\n\n\ncluster_2\n\nOutput Array\n\n\ncluster_0\n\nInput Array (discount factors)\n\n\ncluster_1\n\nAccumulate Function (*)\n\n\n\ndf1\n\n0.9524\n\n\n\nacc1\n\ndf1 * init\n\n\n\ndf1-&gt;acc1\n\n\n\n\n\ndf2\n\n0.9434\n\n\n\nacc2\n\ndf2 * output_1\n\n\n\ndf2-&gt;acc2\n\n\n\n\n\ndf3\n\n0.9524\n\n\n\nacc3\n\ndf3 * output_2\n\n\n\ndf3-&gt;acc3\n\n\n\n\n\ndf4\n\n0.9615\n\n\n\nacc4\n\ndf4 * output_3\n\n\n\ndf4-&gt;acc4\n\n\n\n\n\ndf5\n\n0.9524\n\n\n\nacc5\n\ndf5 * output_4\n\n\n\ndf5-&gt;acc5\n\n\n\n\n\noutput1\n\n0.9524\n\n\n\nacc1-&gt;output1\n\n\n\n\n\noutput2\n\n0.8985\n\n\n\nacc2-&gt;output2\n\n\n\n\n\noutput3\n\n0.8561\n\n\n\nacc3-&gt;output3\n\n\n\n\n\noutput4\n\n0.8230\n\n\n\nacc4-&gt;output4\n\n\n\n\n\noutput5\n\n0.7831\n\n\n\nacc5-&gt;output5\n\n\n\n\n\noutput0\n\ninit=1.0\n\n\n\noutput0-&gt;acc1\n\n\n\n\n\noutput1-&gt;acc2\n\n\n\n\n\noutput2-&gt;acc3\n\n\n\n\n\noutput3-&gt;acc4\n\n\n\n\n\noutput4-&gt;acc5\n\n\n\n\n\n\n\n\nFigure 6.2: A diagram showing that accumulate creates a new collection where each element is the cumulative result of applying the given operation to all previous elements.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor accumulate and reduce, an important, optional value is the init (an optional keyword argument), which is the initial value to start the accumulation or reduction. For common operations this identity element is already predefined. For example, for + the identity is 0 while for * it is 1. The identity element \\(e\\) is the one where for a given binary operation \\(\\bigodot\\), that \\(x \\bigodot e = x\\).\nAnother example is string concatenation. In Julia, two strings are concatenated with * (like in mathematics, \\(a * b\\) is also written as \\(ab\\)). The identity element for strings where the binary operation \\(\\bigodot = *\\) is \"\". For example:\n\naccumulate(*, [\"a\", \"b\", \"c\"], init=\"\")\n\n3-element Vector{String}:\n \"a\"\n \"ab\"\n \"abc\"\n\n\nThis is a taste of a branch of mathematics known as Category Theory, a very rich subject but largely beyond the immediate scope of this book. The category theoretical term for sets of things that work with the binary operator and identity elements as described above is a monoid. There will not be a quiz on this trivia.\n\n\n\n\n6.4.3 reduce\nreduce takes an operation and a collection and applies the operation repeatedly to pairs of elements until there is only a single value left.\nFor example, we start with the calculation of the vector of discounted cashflows\n\ndfs = accumulate(*, map(x -&gt; 1 / (1 + x), rate))\ndiscounted_cfs = map(*, cf_bond, dfs)\n\n5-element Vector{Float64}:\n  9.523809523809524\n  8.98472596585804\n  8.556881872245752\n  8.22777103100553\n 86.19569651529602\n\n\nThen we can sum them with reduce:\n\nreduce(+, discounted_cfs)\n\n121.48888490821487\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\nInput Array (discounted cashflows)\n\n\ncluster_1\n\nReduce Function (+)\n\n\ncluster_2\n\nOutput Value\n\n\n\ncf1\n\n9.52\n\n\n\nred0\n\ninit=0.0 + cf1\n\n\n\ncf1-&gt;red0\n\n\n\n\n\ncf2\n\n8.98\n\n\n\nred1\n\nred0 + cf2\n\n\n\ncf2-&gt;red1\n\n\n\n\n\ncf3\n\n8.56\n\n\n\nred2\n\nred1 + cf3\n\n\n\ncf3-&gt;red2\n\n\n\n\n\ncf4\n\n8.23\n\n\n\nred3\n\nred2 + cf4\n\n\n\ncf4-&gt;red3\n\n\n\n\n\ncf5\n\n86.20\n\n\n\nred4\n\nred3 + cf5\n\n\n\ncf5-&gt;red4\n\n\n\n\n\nred0-&gt;red1\n\n\n\n\n\nred1-&gt;red2\n\n\n\n\n\nred2-&gt;red3\n\n\n\n\n\nred3-&gt;red4\n\n\n\n\n\noutput\n\n121.49\n\n\n\nred4-&gt;output\n\n\n\n\n\n\n\n\nFigure 6.3: A diagram showing how reduce applies the given operation to pairs of elements, ultimately reducing the collection to a single value.\n\n\n\n\n\n\n\n6.4.4 mapreduce\nWe can combine map, accumulate and reduce to concisely calculate the present value in a functional style. This calculates the discount factors, applies them to the cashflows with map, and sums the result with a reduction:\n\n1dfs = accumulate(*, map(x -&gt; 1 / (1 + x), rate))\n2mapreduce(*, +, cf_bond, dfs)\n\n\n1\n\nMultiplicatively accumulate a discount factor derived from the rate vector.\n\n2\n\nMultiply the discount factor and bond cashflows (map the multiplication), then sum the result (additive reduce).\n\n\n\n\n121.48888490821487\n\n\nContrast this example with the earlier imperative styles:\n\nThis functional approach is more concise.\nThe functions used are more descriptive and obvious (once familiar with them, of course!).\nThere is no state that the user/programmer keeps track of.\nThe compiler is able to potentially optimize the code, as it can deduce that certain operations are associative.\n\nThis completes the example of using a functional approach to determine the present value of bond cashflows.\n\n\n6.4.5 filter\nFor completeness, we will also cover filter even though it’s not necessary for the bond cashflow example.\nfilter does what you might think - filter a collection based on some criterion that can be determined as true or false.\nFor example filtering out even numbers using the isodd function:\n\nfilter(isodd, 1:6)\n\n3-element Vector{Int64}:\n 1\n 3\n 5\n\n\nOr filtering out things that don’t match a criteria:\n\nfilter(x -&gt; ~(x == 5), 1:6)\n\n5-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 6\n\n\nWhile we didn’t need filter to calculate a bond’s present value in the example above, one can imagine how you may want to filter dates that a bond might pay a cashflow, say last day of a quarter:\n\nusing Dates\nlet d = Date(2024, 01, 01)\n    filter(d -&gt; lastdayofquarter(d) == d, d:Day(1):lastdayofyear(d))\nend\n\n4-element Vector{Date}:\n 2024-03-31\n 2024-06-30\n 2024-09-30\n 2024-12-31\n\n\n\n\n6.4.6 More Tips on Functional Styles\n\n6.4.6.1 do Syntax for Function Arguments\nIn more complex situations such as with multiple collections or multi-line logic, there is a clearer syntax that is often used. do is a reserved keyword in Julia that creates an anonymous function and passes its arguments to a function like map. For example, this (terrible) code which decides if a number is prime. The anonymous function requires a begin block since the logic of the function is extended into multiple lines.\n\nmap(x -&gt; begin\n        if x == 1\n            \"prime\"\n        elseif x == 2\n            \"not prime\"\n        elseif x == 3\n            \"prime\"\n        elseif x &gt; 4\n            \"probably not prime\"\n        end\n    end,\n    [1, 2, 3, 10]\n)\n\n4-element Vector{String}:\n \"prime\"\n \"not prime\"\n \"prime\"\n \"probably not prime\"\n\n\nThis can be written more cleanly with the do syntax:\n\nmap([1, 2, 3, 10]) do x\n    if x == 1\n        \"prime\"\n    elseif x == 2\n        \"not prime\"\n    elseif x == 3\n        \"prime\"\n    elseif x &gt; 4\n        \"probably not prime\"\n    end\nend\n\n4-element Vector{String}:\n \"prime\"\n \"not prime\"\n \"prime\"\n \"probably not prime\"\n\n\n\n\n6.4.6.2 Multiple Collections\nmap and the other functional operators discussed in this section can take multiple arguments. This is convenient if you have multiple arguments to a function:\n\ndiscounts = [0.9, 0.81, 0.73]\ncashflows = [10, 10, 10]\n\nmap((d, c) -&gt; d * c, discounts, cashflows)\n\n3-element Vector{Float64}:\n 9.0\n 8.100000000000001\n 7.3\n\n\nOr an example with the do syntax:\n\nmap(discounts, cashflows) do d, c\n    d * c\nend\n\n3-element Vector{Float64}:\n 9.0\n 8.100000000000001\n 7.3\n\n\n\n\n6.4.6.3 Using More Functions\nAt the risk of sounding obvious, an easy way to make the program more “functional” is to simply use more functions. Do this one thing and it will improve the model’s organization, maintainability, and reduce bugs!\nTake the example from earlier:\npv = 0.0\ndiscount = 1.0\n\nfor (r,cf) in zip(cf_bond, rate)\n    discount = discount / (1 + r)\n    pv = pv + discount * cf\nend\npv\nWe can easily turn this code into a function so that it can operate on data beyond the single pair of cf_bond and rate previously defined:\nfunction pv(rates,cashflows)\n    pv = 0.0\n    discount = 1.0\n\n1    for (r,cf) in zip(rates, cashflows)\n        discount = discount / (1 + r)\n        pv = pv + discount * cf\n    end\n    pv\nend\n\n1\n\nHere, cf_bond and rate would refer to whatever was passed as arguments to the function instead of any globally defined values.\n\n\nNow we could use this definition of pv on other instances of rates and cashflows.\n\n\n6.4.6.4 Mixing Functional And Imperative Styles\nOne of the best things about Julia is how natural it can be to mix different styles. Sometimes the best approach is a mix of both. That’s one of the benefits of Julia: use the style that’s most natural to the problem.\n\n\n\n\n\n\nNoteFlexibility and the Lisp Curse\n\n\n\nLisp (“list processing”) is another, much older language than Julia (created in the 1950s!). One of its claims to fame is how flexible and powerful the tools are within the language to build upon. There’s a couple aspects of this curse that we wish to describe because we can learn from it while Julia is still a relatively young language.\nPart of the “curse” is that: because there’s so much freedom in what can be expressed in the language, there’s not an obvious “best” way of doing things. This can lead to decision paralysis where you are trying to over-analyze what’s the best way to write part of your code. Our advice: don’t worry about it! A working implementation of something is better than an over-optimized idea.\nThe other part of the “curse” is that because it is relatively easy to implement so many things from the building blocks that Julia provides and compose them together to do what you want. This has a downside because the general approach to packages is smaller, standalone pieces that you compose as needed. For example, consider Python’s Pandas library, upon which Python’s data science community was built. It came bundled with a CSV reader, Excel reader, Database reader, DataFrame type, visualization library, and statistical functions. In Julia, each of those are separate packages that specialize for the respective topics. This is advantageous in that they can progress independently from one another, you don’t have to include functionality that you don’t need, and you can mix and match libraries depending on your preference.",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functional Abstractions</span>"
    ]
  },
  {
    "objectID": "first-abstractions.html#sec-array-oriented-styles",
    "href": "first-abstractions.html#sec-array-oriented-styles",
    "title": "6  Functional Abstractions",
    "section": "6.5 Array-Oriented Styles",
    "text": "6.5 Array-Oriented Styles\nAnother paradigm is array-oriented, which is a style that relies heavily on putting similar data into arrays and operating on the entire array at the same time (as opposed to going element-by-element).\nArray-oriented programming is one that is practiced in two main contexts:\n\nGPU programming\nPython numerical computing\n\nThe former because GPUs want large blocks of similar data to operate in parallel. The latter is because native Python is too slow for many modeling problems so libraries like NumPy, SciPy,and tensor libraries utilize C++ (or similar) libraries for users to call out to.\nArray-oriented programming is not always natural for financial and actuarial applications. Differences in behavior or timing of underling cashflows can make a set of otherwise similar products difficult to capture in nicely gridded arrays. Nonetheless, certain applications (scenario generation, some valuation routines) fit very naturally into this paradigm. Furthermore, for those that work well it’s often a great way to extract additional performance due to the parallelization offered via CPU or GPU array programming.\nTable 6.2 shows the bond present value example in this style.\n\n\n\nTable 6.2: The two code examples demonstrate the same logic using Julia and Numpy (Python’s most popular array package). Julia’s broadcasting facilitates an array-oriented style, similar to the approach that would be used with Python’s NumPy.\n\n\n\n\n\n\n\n\n\nJulia\nPython (NumPy)\n\n\n\n\ncf_bond = [10, 10, 10, 10, 110]\nrate = [0.05, 0.06, 0.05, 0.04, 0.05]\n\ndiscount_factors = cumprod(1 ./ (1 .+ rate))\nresult = sum(cf_bond .* discount_factors)\nimport numpy as np\n\ncf_bond = np.array([10, 10, 10, 10, 110])\nrate = np.array([0.05, 0.06, 0.05, 0.04, 0.05])\n\ndiscount_factors = np.cumprod(1 / (1 + rate))\nresult = np.sum(cf_bond * discount_factors)\n\n\n\n\n\n\nThe downsides to this style are:\n\nSometimes it is unnatural because of non-uniformity of the data we are working with. For example if the length of the cashflows were shorter than the discount rates, we would have to perform intermediate steps to shorten or lengthen arrays in order to get them to be the same size.\nA good bit of runtime performance is lost because the computer needs to allocate and fill many intermediate arrays (note how in Table 6.2, the discount_factors needs to instantiate an entirely new vector even though it’s only temporarily used). See more on allocations in Chapter 9.",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functional Abstractions</span>"
    ]
  },
  {
    "objectID": "first-abstractions.html#recursion",
    "href": "first-abstractions.html#recursion",
    "title": "6  Functional Abstractions",
    "section": "6.6 Recursion",
    "text": "6.6 Recursion\nA recursive function which is a pattern where current steps are defined in a way that depends on previous steps. Typically, an explicit starting condition is also required to be specified.\nThe Fibonacci sequence is a classic example of a recursive algorithm, with the starting conditions of \\(n\\) specified for the first two steps:\n\\[F(n) = \\begin{cases}\n0, & \\text{if } n = 0\\\\\n1, & \\text{if } n = 1\\\\\nF(n-1) + F(n-2), & \\text{if } n &gt; 1\n\\end{cases}\\]\nIn code, this translates into a function definition that refers to itself:\nfunction fibonacci(n)\n    if n == 0\n        return 0\n    elseif n == 1\n        return 1\n    else\n        return fibonacci(n-1) + fibonacci(n-2)\n    end\nend\nHow could a recursive pattern be defined for valuing our bond? A possible pattern is defining the present value to be the discounted value of:\n\nthe current period’s cashflow, plus\nthe accumulated cashflows up to that point in time\n\nHere’s how that might be defined:\n\nfunction pv_recursive(rates,cashflows,accumulated_value=0.0,discount_factor=1.0)\n1  if isempty(cashflows)\n    return accumulated_value\n  else\n2    discount_factor = discount_factor / (1+first(rates))\n3    av = first(cashflows) * discount_factor + accumulated_value\n    remaining_rates = rates[begin+1:end]\n    remaining_cfs = cashflows[begin+1:end]\n4    return pv_recursive(remaining_rates,remaining_cfs, av,discount_factor)\n  end\nend\n\n\n1\n\nAdd a terminating condition, that if we have no more cashflows then return the accumulated value.\n\n2\n\nDecrement the discount factor as we step forward in time.\n\n3\n\nTake the prior accumulated value and add the first value in the given cashflows.\n\n4\n\nPass the remaining subset of the cashflow vector, the running total, and the current discount factor to the next call of the recursive function.\n\n\n\n\npv_recursive (generic function with 3 methods)\n\n\nAnd an example of its use:\n\npv_recursive(rate,cf_bond)\n\n121.48888490821489\n\n\nThe recursive pattern often works very nicely for simpler examples. However, more complex logic and conditionals can make this approach unwieldy. Nonetheless, attempting to distill the desired functionality into a single function can be a beneficial thought exercise.",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functional Abstractions</span>"
    ]
  },
  {
    "objectID": "type-abstractions.html",
    "href": "type-abstractions.html",
    "title": "7  Data and Types",
    "section": "",
    "text": "7.1 Chapter Overview\nThe powerful benefits that using assigning types to data has within the model’s system, some examples of utilizing types to simplify a programs logic, and comparing aspects of different type related program organization (such as object oriented design versus composition).",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data and Types</span>"
    ]
  },
  {
    "objectID": "type-abstractions.html#using-types-to-value-a-portfolio",
    "href": "type-abstractions.html#using-types-to-value-a-portfolio",
    "title": "7  Data and Types",
    "section": "7.2 Using Types to Value a Portfolio",
    "text": "7.2 Using Types to Value a Portfolio\nWe will assemble the tools and terminology to value a portfolio of assets by leverage types (@sec-data-types). Using the constructs introduced in the prior chapter, we can describe the portfolio valuation as additively reducing the mapped value of assets in the portfolio. If value is our valuation function), we are trying to do the following:\nmapreduce(value,+,portfolio)\nThe challenge is how do design an all-purpose value function? In portfolio, the assets may be heterogeneous, so we will need to define what the valuation semantics are for the different kinds of assets. To get to our end goal, we will need to:\n\nDefine the different kinds of assets within our portfolio\nHow the assets are to be valued.\n\nWe will accomplish this by utilizing data types.",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data and Types</span>"
    ]
  },
  {
    "objectID": "type-abstractions.html#benefits-of-using-types",
    "href": "type-abstractions.html#benefits-of-using-types",
    "title": "7  Data and Types",
    "section": "7.3 Benefits of Using Types",
    "text": "7.3 Benefits of Using Types\nAs a preview of why we want to utilize types in our program, there are a number of benefits:\n\nSeparate concerns. For example, deciding how to value an option need not know how we value a bond. The code and associated logic is kept distinct which is easier to reason about and to test.\nRe-use code. When a set of types within a hierarchy all share the same logic, then we can define the method at the highest relevant level and avoid writing the method for each possible type. In our simple example we won’t get as much benefit here since the hierarchy is simple and the set of types small.\nExtensibility through dispatch. By defining types for our assets, we can use multiple dispatch to define specialized behavior for each type. This allows us to write generic code that works with any asset type, and the Julia compiler will automatically select the appropriate method based on the type of the asset at runtime. This is a powerful feature that enables extensibility and modularity in our code.\nImprove readability and clarity. By defining types for our assets, we make our code more expressive and self-documenting. The types provide a clear indication of what kind of data we are working with, making it easier for other developers (or ourselves in the future) to understand and maintain the codebase.\nEnable type safety. By specifying the expected types for function arguments and return values, we can catch type-related errors at compile time rather than at runtime. This helps prevent bugs and makes our code more robust.\n\nWith these benefits in mind, let’s start by defining the types for our assets. We’ll create an abstract type called Asset that will serve as the parent type for all our asset types. If you haven’t read it already, Section 5.4.7 is a good reference for details on types at the language level (this section is focused on organization and building up the abstracted valuation process).",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data and Types</span>"
    ]
  },
  {
    "objectID": "type-abstractions.html#defining-types-for-portfolio-valuation",
    "href": "type-abstractions.html#defining-types-for-portfolio-valuation",
    "title": "7  Data and Types",
    "section": "7.4 Defining Types for Portfolio Valuation",
    "text": "7.4 Defining Types for Portfolio Valuation\nWe will define five types of assets in this simplified universe:\n\nCash\nRisk Free Bonds (coupon and zero-coupon varieties)\n\nTo do the valuation of these, we need some economic parameters as well: risk free rates for discounting.\nHere’s the outline of what follows to get an understanding of types, type hierarchy, and multiple dispatch.\n\nDefine the Cash and Bond types.\nDefine the most basic economic parameter set.\nDefine the value functions for Cash and Bonds.\n\n\n## Data type definitions\n1abstract type AbstractAsset end\n\n3struct Cash &lt;: AbstractAsset\n    balance::Float64\nend\n\n2abstract type AbstractBond &lt;: AbstractAsset end\n\nstruct CouponBond &lt;: AbstractBond\n    par::Float64\n    coupon::Float64\n    tenor::Int\nend\n\nstruct ZeroCouponBond &lt;: AbstractBond\n    par::Float64\n    tenor::Int\nend\n\n\n1\n\nGeneral convention is to name abstract types beginning with Abstract...\n\n2\n\nThere can exist an abstract type which is a subtype of another abstract type.\n\n3\n\nWe define concrete data types (structs) with the fields necessary for valuing those assets.\n\n\n\n\nNow to define the economic parameters:\n\nstruct EconomicAssumptions{T}\n  riskfree::T\nend\n\nThis is a parametric type because later on we will vary what objects we use for riskfree. For now, we will use simple scalar values, like in this potential scenario:\n\necon_baseline = EconomicAssumptions(0.05)\n\nEconomicAssumptions{Float64}(0.05)\n\n\nNow on to defining the valuation for Cash and AbstractBonds. Cash is always equal to it’s balance:\n\n\nvalue(asset::Cash, ea::EconomicAssumptions) = asset.balance\n\nvalue (generic function with 1 method)\n\n\nRisk free bonds are the discounted present value of the riskless cashflows. We first define a method that generically operates on any fixed bond, all that’s left to do is for different types of bonds to define how much cashflow occurs at the given point in time by defining cashflow for the associated type.\n\n2function value(asset::AbstractBond, r::Float64)\n    discount_factor = 1.0\n    value = 0.0\n    for t in 1:asset.tenor\n1        discount_factor /= (1 + r)\n        value += discount_factor * cashflow(asset, t)\n    end\n    return value\nend\n\nfunction cashflow(bond::CouponBond, time)\n    if time == bond.tenor\n        (1 + bond.coupon) * bond.par\n    else\n        bond.coupon * bond.par\n    end\nend\n\n3function value(bond::ZeroCouponBond, r::Float64)\n    return bond.par / (1 + r)^bond.tenor\nend\n\n\n1\n\nx /= y, x += y, etc. are shorthand ways to write x = x / y or x = x + y\n\n2\n\nvalue is defined for AbstractBonds in general…\n\n3\n\n… and then more specifically for ZeroCouponBonds. This will be explained when discussing “dispatch” below.\n\n\n\n\nvalue (generic function with 3 methods)\n\n\n\n7.4.1 Dispatch\nWhen a function is called, the computer has to decide which method to use. In the example above, when we want to value a ZeroCouponBond, does the value(asset::AbstractBond, r) or value(bond::ZeroCouponBond, r) version get used?\nDispatch is the process of determining the right method to use and the rule is that the most specific defined method gets used. In this case, that means that even though our ZeroCouponBond is an AbstractBond, the routine that will used is the most specific value(bond::ZeroCouponBond, r).\nAlready, this is a powerful tool to simplify our code. Imagine the alternative of a long chain of conditional statements trying to find the right logic to use:\n# don't do this!\nfunction value(asset,r)\n    if asset.type == \"ZeroCouponBond\"\n        # special code for Zero coupon bonds\n        # ...\n    elseif asset.type == \"ParBond\"\n        # special code for Par bonds\n        # ...\n    elseif asset.type == \"AmortizingBond\"\n        # special code for Amortizing Bonds\n        # ...\n    else\n        # here define the generic AbstractBond logic\n    end\nend\nWith dispatch, the compiler does this lookup for us, and more efficiently than enumerating a list of possible codepaths.\nIn contrast with the prior code example, we didn’t have a long chain of if statements, and instead are letting the types themselves dictate which functions are relevant and will be called. We provided a generic value function for any AbstractBond which loops through time, and a specialized one for ZeroCouponBond.\nWe could have simply used the generic AbstractBond method for the ZeroCouponBond as well. To do so, we would only need to define its cashflow method:\n# An alternative, but less efficient, implementation\nfunction cashflow(bond::ZeroCouponBond, time)\n    # A zero-coupon bond only pays its par value at the very end\n    if time == bond.tenor\n        return bond.par\n    else\n        return 0.0\n    end\nend\nWith this method, the generic value function would have worked correctly, looping from t=1 to t=tenor and finding only a single non-zero cashflow to discount.\nHowever, this is inefficient. We know there is a more direct, closed-form formula to value a zero-coupon bond: \\(PV = \\frac{\\text{Par}}{(1+r)^\\text{tenor}}\\). There’s no need to loop through intermediate years where the cashflow is zero.\nThis is the power of dispatch. By defining a more specific method, value(bond::ZeroCouponBond, r::Float64), we are telling Julia: “When you have a ZeroCouponBond, use this highly efficient, direct formula. For any other kind of AbstractBond, you can fall back on the generic looping version.” Dispatch ensures that the most specific, and in this case most performant, implementation is automatically chosen. This allows you to build a system that is both general and extensible, while also being highly optimized for the most common and simple cases.\n\n7.4.1.1 Integrating Economic Assumptions\nDespite the definitions above, the following will error because we haven’t defined a method for value which takes as it’s second argument a type of EconomicAssumptions:\n#| error: true\nvalue(ZeroCouponBond(100.0,5),econ_baseline)\nLet’s fix that by defining a method which takes the economic assumption type and just relays the relevant risk free rate to the value methods already defined (which take an AbstractBond and a scalar r).\n\nvalue(bond::AbstractBond,econ::EconomicAssumptions) = value(bond,econ.riskfree)\n\nvalue (generic function with 4 methods)\n\n\nNow this following works:\n\nvalue(ZeroCouponBond(100.0, 5), econ_baseline)\n\n78.35261664684589\n\n\nHere’s an example of how this would be used:\n\nportfolio = [\n    Cash(50.0),\n    CouponBond(100.0, 0.05, 5),\n    ZeroCouponBond(100.0, 5),\n]\n\nmap(asset-&gt; value(asset,econ_baseline), portfolio)\n\n3-element Vector{Float64}:\n 50.0\n 99.99999999999999\n 78.35261664684589\n\n\nThis is very close to the goal that we set out at the end of the section. We can complete it by reducing over the collection to sum up the value:\n\nmapreduce(asset -&gt; value(asset,econ_baseline), +, portfolio)\n\n228.3526166468459\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis code:\nmapreduce(asset-&gt; value(asset,econ_baseline), +, portfolio)\nis more verbose than what we set out do at the start (mapreduce(value,+,portfolio)) due to the two-argument value function requiring a second argument for the economic variables. This works well! However, there is a way to define it which avoids the anonymous function, which in some cases will end up needing to be compiled more frequently than you want it to. Sometime we want a lightweight, okay-to-compile-on-the-fly function. Other times, we know it’s something that will be passed around in compute-intensive parts of the code. A technique in this situation is to define an object which “locks in” one of the arguments but behaves like the anonymous version. There is a pair of types in the Base module, Fix1 and Fix2, which represent partially-applied versions of the two-argument function f, with the first or second argument fixed to the value “x”.\nThis is, Base.Fix1(f, x) behaves like y-&gt;f(x, y) and Base.Fix2(f, x) behaves like y-&gt;f(y, x).\nIn the context of our valuation model, this would look like:\n\nval = Base.Fix2(value,econ_baseline)\nmapreduce(val,+,portfolio)\n\n228.3526166468459\n\n\n\n\n\n\n7.4.1.2 Multiple Dispatch\nA more general concept is that of multiple dispatch, where the types of all arguments are used to determine which method to use. This is a very general paradigm, and in many ways is more extensible than traditional object oriented approaches, (more on that in Section 7.5). What if instead of a scalar interest rate value we wanted to instead pass an object that represented a term structure of interest rates?\nExtending the example, we can use a time-varying risk free rate instead of a constant. For fun, let’s say that the risk free rate has a sinusoidal pattern:\n\necon_sin = EconomicAssumptions(t -&gt; 0.05 + sin(t) / 100)\n\nEconomicAssumptions{var\"#5#6\"}(var\"#5#6\"())\n\n\nNow value will not work, because we’ve only defined how value works on bonds if the given rate is a Float64 type:\n\nvalue(ZeroCouponBond(100.0, 5), econ_sin)\n\n\nMethodError: no method matching value(::ZeroCouponBond, ::var\"#5#6\")\nThe function `value` exists, but no method is defined for this combination of argument types.\n\nClosest candidates are:\n  value(::ZeroCouponBond, ::Float64)\n   @ Main.Notebook ~/prog/julia-fin-book/type-abstractions.qmd:127\n  value(::AbstractBond, ::EconomicAssumptions)\n   @ Main.Notebook ~/prog/julia-fin-book/type-abstractions.qmd:202\n  value(::AbstractBond, ::Float64)\n   @ Main.Notebook ~/prog/julia-fin-book/type-abstractions.qmd:109\n  ...\n\nStacktrace:\n [1] value(bond::ZeroCouponBond, econ::EconomicAssumptions{var\"#5#6\"})\n   @ Main.Notebook ~/prog/julia-fin-book/type-abstractions.qmd:202\n [2] top-level scope\n   @ ~/prog/julia-fin-book/type-abstractions.qmd:262\n\n\n\nWe can extend our methods to account for this:\n\n1function value(bond::ZeroCouponBond, r::T) where {T&lt;:Function}\n2    return bond.par / (1 + r(bond.tenor))^bond.tenor\nend\n\n\n1\n\nThe r::T ... where {T&lt;:Function} says use this method if r is any concrete subtype of the (abstract) Function type.\n\n2\n\nr is a function, where we call the time to get the zero coupon bond (a.k.a. spot) rate for the given timepoint.\n\n\n\n\nvalue (generic function with 5 methods)\n\n\nNow it works:\n\nvalue(ZeroCouponBond(100.0, 5), econ_sin)\n\n82.03058910862806\n\n\nThe important thing to note here is that the compiler is using the most specific method of the function (value(bond::ZeroCouponBond, r::T) where {T&lt;:Function}). Both the types of the arguments are influencing the decision of which method to use. We could go on to define the appropriate method for CouponBond to complete the example.",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data and Types</span>"
    ]
  },
  {
    "objectID": "type-abstractions.html#sec-object-oriented",
    "href": "type-abstractions.html#sec-object-oriented",
    "title": "7  Data and Types",
    "section": "7.5 Object-Oriented Design",
    "text": "7.5 Object-Oriented Design\nObject oriented (OO) type systems use the analogy that various parts of the system are their own objects which encapsulate both data and behavior. Object oriented design is often one of the first computer programming abstractions introduced because it very relatable1, however there are a number of its flaws in over-relying on OO patterns. Julia does not natively have traditional OO classes and types, but much of OO design can be emulated in Julia except for data inheritance.\n\n\n\n\n\n\nNote\n\n\n\nFor readers without background in OO programming, the main features of OO languages are:\n\nHierarchical type structures, which include concrete and abstract (often called classes instead of types).\nSub-classes inherit both behavior and data (in Julia, subtypes only inherit behavior, not data).\nFunctions that depend on the type of the object need to be ascribed to a single class and then can dispatch more specifically on the given argument’s type.\n\n\n\nWe bring up object oriented design for comparison’s sake, but think that ultimately choosing a data driven or functional design is better for financial modeling. Of course, many robust, well-used financial models have been built this way but in our experience the abstractions become unnatural. Additionally, maintenance unwieldy beyond simple examples. We’ll now discuss some of the aspects of OO design and why the overuse of OO is not preferred.\n\n7.5.1 Assigning Behavior\nNeeding to assign methods to a single class can lead to awkward design limitations - when multiple objects are involved in a computation, why dictate that only one of them “controls” the logic?\nThe value function is a good example of this. If we had to assign value to one of the objects involved, should it be the economic parameter object or the asset objects? The choice is not obvious at all. Isn’t it the market (economic parameters) that determines the value? But then if value were to be a method wholly owned by the economic parameters, how could it possible define in advance the valuation semantics of all types of assets? What if one wanted to extend the valuation to a new asset class? Downstream users or developers would need to modify the economic types to handle new assets they wanted to value. However, because the economic types were owned by an upstream package, they can’t be extended this way.\nThis is an issue with traditional OO designs and that resolves itself so elegantly with multiple dispatch.\n\n7.5.1.1 Example: The Expression Problem\nA fundamental limitation of OOP is what’s called the Expression Problem. The challenge (or problem) is that with OOP languages it is difficult to extend both datatypes and behavior. In the example that follows, we define types of insurance products with associated methods.\nHere’s the setup: we are modeling insurance contracts and someone has provided a nice library which we will call Insurance.jl and pyInsurance for a Julia and Python package. The package defines datatypes for Term and Whole Life insurance products, as well as a lot of utilities related to calculating premiums and reserves (i.e. performing valuations). Defining the functionality is straightforward enough in both languages/approaches:\n\n\nInsurance.jl\nA hypothetical package available for your use.\nabstract type InsuranceProduct end\n\nstruct TermLife &lt;: InsuranceProduct\n    term::Int\n    face_amount::Float64\n    age::Int\nend\n\nstruct WholeLife &lt;: InsuranceProduct\n    face_amount::Float64\n    age::Int\nend\n\n# Calculate premium\npremium(p::TermLife) = ...\npremium(p::WholeLife) = ...\n\n# Calculate reserves\nreserve(p::TermLife, t::Int) = ...\nreserve(p::WholeLife, t::Int) = ...\n\n\n\n\n\npyInsurance\nA hypothetical package available for your use.\nfrom abc import ABC, abstractmethod\nfrom dataclasses import dataclass\n\nclass InsuranceProduct(ABC):\n    @abstractmethod\n    def calculate_premium(self) -&gt; float:\n        pass\n    \n    @abstractmethod\n    def calculate_reserve(self, t: int) -&gt; float:\n        pass\n\n@dataclass\nclass TermLife(InsuranceProduct):\n    term: int\n    face_amount: float\n    age: int\n\n    # Term-specific calculations\n    def calculate_premium(self) -&gt; float:\n        return ...\n    \n    def calculate_reserve(self, t: int) -&gt; float:\n        return ...\n\n@dataclass\nclass WholeLife(InsuranceProduct):\n    face_amount: float\n    age: int\n\n    # WholeLife-specific calculations\n    def calculate_premium(self) -&gt; float:\n        return ...\n    \n    def calculate_reserve(self, t: int) -&gt; float:\n        return ...\n\n\nNow, say that we want to utilize this package and extend the behavior. Specifically, we want to add a Deferred Annuity type and add functionality (for all products) related to determining a cash surrender value.\nWe run into limitations with Python version. We can extend a new representation (dataclass), but adding new functionality (e.g. cash_value) requires modifying other classes which you may not own and for which the method not apply.\n\n\nJulia (Multiple Dispatch)\nstruct DeferredAnnuity &lt;: InsuranceProduct\n    premium::Float64\n    deferral_period::Int\n    age::Int\nend\n\n# Implement existing methods\npremium(p::DeferredAnnuity) = ...\nreserve(p::DeferredAnnuity, t::Int) = ...\n\n# Adding new function\ncash_value(p::WholeLife, t::Int) = ...\ncash_value(p::DeferredAnnuity, t::Int) = ...\n\n\n\nPython (Object-Oriented)\n@dataclass\nclass DeferredAnnuity(InsuranceProduct):\n    premium: float\n    deferral_period: int\n    age: int\n    \n    def calculate_premium(self) -&gt; float:\n        # Annuity-specific premium calc\n        return ...\n    \n    def calculate_reserve(self, t: int) -&gt; float:\n        # Annuity-specific reserve calc\n        return ...\n\n\nThere are workarounds to handle this, which include:\n\n\n\n\n\n\n\nWorkaround to OOP Expression Problem\nConcerns with Workaround\n\n\n\n\nMonkey-Patching\nYou can dynamically inject the method into the library’s class definition at runtime.\n# WARNING: This is generally considered bad practice\nfrom ultimate_insurance_models import WholeLife\n\ndef _calculate_cash_value_for_wholelife(self, t: int) -&gt; float:\n  return ...\n\n# At the start of your app, \"patch\" the library's class\nWholeLife.calculate_cash_value = _calculate_cash_value_for_wholelife\n\nwl_policy = WholeLife(face_amount=500000, age=40)\nwl_policy.calculate_cash_value(t=10)\n\nOverwriting Conflicts: If two different parts of a program try to patch the same method, the last one to run will overwrite the others. This can disable expected functionality in a way that is difficult to predict.\nHarder to Debug: Patching makes the code’s runtime behavior different from its source code. This complicates debugging because the written code no longer represents what is actually happening.\nUpgrade Instability: A patch may break when the underlying code it modifies is updated. This creates a maintenance burden, as patches need to be re-validated and potentially re-written with each library upgrade.\n\n\n\nSubclassing\nYou can inherit from the parent class to create your own custom version.\nfrom ultimate_insurance_models import WholeLife\n\nclass MyExtendedWholeLife(WholeLife):\n  def calculate_cash_value(self, t: int) -&gt; float:\n    # Your brilliant logic\n    return self.face_amount * 0.1 * t\n\n# You have to make sure you ONLY create your extended version\nmy_policy = MyExtendedWholeLife(face_amount=500000, age=40)\n\nIncomplete Coverage: The new functionality only exists on your subclass. Any code that creates an instance of the original parent class will produce an object that lacks your new methods.\nBreaks Polymorphism: It forces you to check an object’s specific type before using the new functionality (e.g., using isinstance). This defeats the purpose of having a common interface and makes the code more complex and less robust.\nDoesn’t Affect Object Creation: Functions within the original library will continue to create and return instances of the original parent class. You cannot alter this behavior, meaning objects created by the library will not have your added methods.\n\n\n\n\nThe object oriented paradigm does not allow for extension of both representation (data types) and behavior (methods).\nIn Julia, functions are defined separately from data types. This allows you to add new functions to existing types—even those from external libraries—without altering their original code. Your new functionality works on all instances of the original type, avoiding both the conflicts of patching and the type-checking required by subclassing. whereas a more general, data-oriented approach does facilitate this using Julia’s multiple dispatch.\n\n\n\n7.5.2 Inheritance\nAs seen in the prior example, most OO implementations this hierarchy comes with inheriting both data and behavior. This is different from Julia where subtypes inherit behavior but not data from the parent type.\nInheriting the data tends to introduce a tight coupling between the parent and the child classes in OO systems. This tight coupling can lead to several issues, particularly as systems grow in complexity. For example, changes in the parent class can inadvertently affect the behavior of all its child classes, which can be problematic if these changes are not carefully managed. This is often referred to as the “fragile base class problem,” where base classes are delicate and changes to them can have widespread, unintended consequences.\nAnother issue with inheritance in OO design is the temptation to use it for code reuse, which can lead to inappropriate hierarchies. Developers might create deep inheritance structures just to reuse code, leading to a scenario where classes are not logically related but are forced into a hierarchy. This can make the system harder to understand and maintain.\n\n7.5.2.1 Composition over Inheritance\nTo mitigate some of the problems associated with inheritance, there’s a growing preference for composition. Composition involves creating objects that contain instances of other objects to achieve complex behaviors. This approach is more flexible than inheritance as it allows for the creation of more modular and reusable code. There is a general preference for “composition over inheritance” among professional developers these days.\nIn composition, objects are constructed from other objects, and behaviors are delegated to these contained objects. This approach allows for greater flexibility, as it’s easier to change the behavior of a system by replacing parts of it without affecting the entire hierarchy, as is often the case with inheritance.\nComposition looks like this:\nstruct CUSIP\n    code::string\nend\n\nstruct FixedBond\n    coupon::Float64\n    tenor::Float64\nend\n\nstruct FloatingBond\n    spread::Float64\n    tenor::Float64\nend\n\nstruct MunicipalBond\n    cusip::CUSIP\n    fi::FixedBond\nend\n\nstruct Swap\n    float_leg::FloatingBond\n    fixed_leg::FixedBond\nend\n\nstruct ListedOption\n    cusip::CUSIP\n    #... other data fields\nend\n\nstruct UnlistedBond\n    fi::FixedIncome\nend\n\n\n\n# define behavior which relies on delegation to components \nlast_transaction(c::CUSIP) = # ...perform lookup of data\nlast_transaction(asset) = last_transaction(asset.cusip)\n\nduration(f::FixedIncome) = # ... calculate duration\nduration(asset) = duration(asset.fi)\nIn the above example, there are number of asset classes that have CUSIP related attributes (i.e. the 9 character code) and behavior (e.g. being able to look up transaction data). Other assets have fixed income attributes (e.g. calculating a duration). There’s no clear hierarchy here.\nComposition lets us bundle the data and behavior together without needing complex chains of inheritance.\n\n\n\n\n\n\nNote\n\n\n\nA CUSIP (Committee on Uniform Security Identification Procedures) number, is a unique nine-character alphanumeric code assigned to securities, such as stocks and bonds, in the United States and Canada. This code is used to facilitate the clearing and settlement process of securities and to uniquely identify them in transactions and records.",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data and Types</span>"
    ]
  },
  {
    "objectID": "type-abstractions.html#data-oriented-design",
    "href": "type-abstractions.html#data-oriented-design",
    "title": "7  Data and Types",
    "section": "7.6 Data-Oriented Design",
    "text": "7.6 Data-Oriented Design\nData-Oriented Programming (DOP), especially in a computational field like financial modeling, is an approach that prioritizes the data itself—its structure, its layout in memory, and how it’s processed in bulk. This stands in contrast to Object-Oriented Programming, which prioritizes encapsulating data within objects and interacting with that data through the object’s methods.\nDOP separates the data from the behavior:\n\nData is transparent and inert. We define structures (like the Cash and CouponBond structs in our example) that are simple, transparent containers for information. Their job is to hold data, not to have complex internal logic.\nBehavior is handled by functions. Logic is implemented in generic functions (like our value function) that operate on this data.\n\nThe portfolio valuation model we have built in this chapter is an example of data-oriented design. We created a collection of data—the portfolio array. We then used Julia’s functions (mapreduce, and our own value function) to transform that data into a final result. We can easily add a completely new operation, say calculate_duration(asset, econ_assumptions), without ever modifying the original struct definitions for our assets.\nThis approach is pertinent for financial modeling for several key reasons:\n\nFlexibility: Financial models require many different views of the same data. Today we need to value a portfolio. Tomorrow, we might need to calculate its credit risk, liquidity risk, or run a stress test. With a data-oriented approach, each new requirement is simply a new set of functions that we write to operate on the same underlying data structures. In a strict OO world, we might be forced to add a .calculate_credit_risk() method to every single asset class, which can become unwieldy.\nPerformance: Financial computations often involve applying a single operation to millions or billions of items (e.g., valuing every asset in a large portfolio, running a Monte Carlo simulation with millions of paths). DOP allows the data to be laid out in memory in a way that is highly efficient for modern CPUs or GPUs to process (e.g., in contiguous arrays). By processing data in bulk, we leverage how computer hardware is designed to work, leading to significant performance gains over designs that require calling methods on individual objects one by one.\nSimplicity and Scalability: As a system grows, data-oriented designs can be easier to reason about. The “state” of the system is just the data itself. The logic is contained in pure functions that transform data. This avoids the complex webs of object relationships, inheritance hierarchies, and hidden state that can make large OO systems difficult to maintain and debug.\n\nWhile object-oriented design patterns can be useful, for the performance-critical and mathematically intensive world of financial modeling, a data-oriented approach often proves to be a more natural, scalable, and efficient choice. It aligns perfectly with the core task: the transformation of data (market and instrument parameters) into insight (value, risk, etc.).",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data and Types</span>"
    ]
  },
  {
    "objectID": "type-abstractions.html#footnotes",
    "href": "type-abstractions.html#footnotes",
    "title": "7  Data and Types",
    "section": "",
    "text": "“Many people who have no idea how a computer works find the idea of object-oriented programming quite natural. In contrast, many people who have experience with computers initially think there is something strange about object oriented systems.” - David Robson, “Object Oriented Software Systems” in Byte Magazine (1981).↩︎",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data and Types</span>"
    ]
  },
  {
    "objectID": "patterns-abstraction.html",
    "href": "patterns-abstraction.html",
    "title": "8  Higher Levels of Abstraction",
    "section": "",
    "text": "8.1 Chapter Overview\nWhy abstraction matters as a technique in its own right, with a focus on code organization, interfaces, and reusable design patterns.",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Higher Levels of Abstraction</span>"
    ]
  },
  {
    "objectID": "patterns-abstraction.html#introduction",
    "href": "patterns-abstraction.html#introduction",
    "title": "8  Higher Levels of Abstraction",
    "section": "8.2 Introduction",
    "text": "8.2 Introduction\nIn programming and modeling, as in mathematics, abstraction permits the definition of interchangeable components and patterns that can be reused. Abstraction is a selective ignorance—focusing on the aspects of the problem that are relevant, and ignoring the others. The last two chapters described what we might call “micro-level” abstractions: specific functions and types.\nIn this chapter, we zoom out and examine some principles that guide good model development, manifesting in architectural concerns such as how different parts of the code are organized, what parts of the program are considered ‘public’ versus ‘private’, and patterns themselves.\nSection Chapter 5 described a number of tools that we can utilize as interfaces within our model. We use these tools that are provided by our programming language in service of the conceptual abstraction described above.\n\nFunctions let us implement behavior, where we need trouble ourselves with the low level details.\nData types provide a hierarchical structure to provide meaning to things, and to group those things together into more meaningful structures.\nModules allow us to group related data and functions into cohesive namespaces that can be shared across different parts of a model.",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Higher Levels of Abstraction</span>"
    ]
  },
  {
    "objectID": "patterns-abstraction.html#principles-for-abstraction",
    "href": "patterns-abstraction.html#principles-for-abstraction",
    "title": "8  Higher Levels of Abstraction",
    "section": "8.3 Principles for Abstraction",
    "text": "8.3 Principles for Abstraction\nHere is a list of some principles that arise when developing a particular abstraction. Not all abstractions serve all of these purposes but generally fit one or more of them.\n\n\n\n\nTable 8.1: Finding abstractions generally means finding patterns that fit into one of these principles.\n\n\n\n\n\n\n\n\n\n\n\nPrinciple\nWhat\nWhy\nExample\n\n\n\n\nSeparation of Concerns\nDivide the system into distinct parts, each addressing a separate concern.\nPromote modularity and reduce coupling between components.\nSeparating data retrieval, data processing, and output generation steps in a process.\n\n\nEncapsulation\nHide the internal details of a component and expose only a clean, well-defined set of functionality (interface).\nDon’t let other parts of the program modify internal data and make the system easier to understand and maintain.\nDefining a type or module with well defined behavior and responsibility.\n\n\nComposability\nDesign simple components that can be combined to create more complex behaviors, rather than a single component that attempts to handle all behavior.\nPromote reuse and allow for the components to be combined creatively.\nSeparate details about economic conditions into different types than contracts/instruments.\n\n\nGeneralization\nIdentify common patterns and create generic components that can be specialized as needed. Often this means identifying common behaviors that arise repeatedly in a model.\nAvoid duplication and make the system more expressive and extensible.\nDefining a generic Instrument type that can be specialized for different asset classes.\n\n\n\n\n\n\n\nThese principles provide guidance for creating abstractions that are modular, reusable, and maintainable. By following these principles, developers can create financial models that are easier to understand, extend, and adapt to changing requirements.\n\n8.3.1 Pragmatic Considerations for Model Design\n\n8.3.1.1 Behavior-Oriented\nThis strategy groups together components within a model that behave similarly. So, in our example of bonds and interest-rate swaps fundamentally, they share many characteristics and are used in very similar ways within a model. Therefore, it might make sense to group them together when developing a model.\n\n\n8.3.1.2 Domain Expertise\nIt may be that components of the model require sufficient expertise that different persons or groups are involved in the development. This may warrant separating a model’s design so that different groups can focus on narrower aspects, regardless of similarities among components. For example, at a higher vertical level of obstruction, financial derivatives may fall under similar grouping, but sufficient differences exist for equity credit or foreign exchange derivatives that the model should separate those three asset classes for development purposes.\n\n\n8.3.1.3 Composability versus All-in-One\nFor some model design goals, it may be warranted to attempt to bundle together more functionality instead of allowing users to compose a functionality that comes from different packages. For example, perhaps aa certain visualization of model results is particularly useful, not easy to create from scratch, and widely desired to see the model output visualized that way. Instead of relying on the user to install a separate visualization package and develop the visualization themselves, it could make sense to bundle visualization functionality with a model that is otherwise unconcerned with graphical capabilities.\nIn general, though it is preferred to try to loosely couple systems, you can pick and choose which components you use and that those components work well together.",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Higher Levels of Abstraction</span>"
    ]
  },
  {
    "objectID": "patterns-abstraction.html#interfaces",
    "href": "patterns-abstraction.html#interfaces",
    "title": "8  Higher Levels of Abstraction",
    "section": "8.4 Interfaces",
    "text": "8.4 Interfaces\nInterfaces are the boundary between different encapsulated abstractions. The user-facing interface is the set of functionality and details that the user of the package or model must consider, which is separate from the intermediate variables, logic, and complexity that may be contained within.\n\n\n\n\n\n\nNoteExample of an interface\n\n\n\nWhen looking up a ticker for a market quote, one need not be mindful of the underlying realtime databases, networking, rendering text to the screen, memory management, etc. The interface is “put in symbol, get out number”. By design, there are multiple layers of interfaces and abstractions used under the hood, but the financial modeler need only be actively concerned about the points that he or she comes in contact with, not the entire chain of complexity.\n\n\nFor a financial model, there might be interfaces for bonds or for interest rate swaps, and separate interfaces for calculating risk metrics or visualizing results. Good design separates visualization concerns from contract mechanics and other domain logic. Open-source ecosystems (for visualization, data frames, file I/O, statistics, etc.) make these boundaries easier to see; the hard part is drawing similar boundaries inside your financial model.\nHowever, it’s often difficult to find where to draw lines within financial models. For example, should bonds and interest-rate swaps be in separate packages? Or both part of a broader fixed income package? This is where much of the art and domain expertise of the financial professional comes to bear in modeling. There would be no way for a pure software engineer to think about the right design for the system without understanding how underlying components share, similarities or differences and how those components interact.\n\n8.4.1 Defining Good Interfaces\nA well-designed interface should follow these principles:\n\nBe minimal and focused. The interface should provide only the essential functionality needed, without unnecessary clutter. This makes the interface easier to understand and and facilitates building the necessary complexity through digestible, composable components.\nBe consistent and intuitive. The interface should use consistent naming conventions, parameter orders, and behaviors. It should match the user’s mental model and expectations.\nHide implementation details. The interface should abstract away the internal complexity and expose only what the user needs to know. This separation of concerns allows the implementation to change without affecting users of the interface.\nBe documented and contractual. The interface should clearly specify what inputs it expects and what outputs or behaviors it provides. It forms a contract between the implementation and the users.\nBe testable. A good interface allows the functionality to be easily tested through the public interface, without needing to access internal details.\n\n\n\n8.4.2 Interfaces: A Financial Modeling Case Study\nAs a case study, we’ll look at the FinanceModels.jl and related packages to discuss some of the background and design choices that went into the functionality. This suite was written by one of the authors and is publicly available as set of installable Julia packages.\n\n8.4.2.1 Background\nIn actuarial work, it is common to need to work with interest rate and bond yield curves to determine current forward rates, estimates of the shape of future yield curves, or discount a series of cashflows to determine a present value. Determining things like “given a par yield curve, what’s the implied discount factor for a cashflow at time 10” or “what is the 10 year BBB public corporate rate implied by the current curve in five years’ time” is cumbersome at best in a spreadsheet.\nFor example, to determine the answer to the first one (“a discount factor for time 10”) actually requires quite a bit of detail and assumption to derive:\n\nReference market data and a specification for how that market data should be interpreted. For example, if given the rate 0.05 for time 10, is it quoted as a continuously compounded rate or as an annual effective rate? Is that a par rate, a zero-coupon bond (spot) rate, or a one-year-forward rate from time 10?\nSmoothing, interpolation, or extrapolation for noisy or sparse data. Should the rates be bootstrapped or fit to a parametrically specified curve?\n\nThis is the type of complexity that we wish to save the user from needing to keep front of mind when the primary goal is, e.g., valuation of a stream of riskless life insurance payments, which might look like this:\n  risk_free_rates = [0.05, 0.055, 0.06]      # example par yields\n  tenors = [1.0, 5.0, 10.0]                  # years\n  yield_curve = Yields.Par(risk_free_rates, tenors)\n\n  cashflow_vector = [1_000_000.0, 3_000_000.0, 1_000.0]\n  present_value(yield_curve, cashflow_vector)\nThis is very clear from the variable and function names what the purpose and steps in the analysis are. Imagine starting with rates and cashflows in a spreadsheet, needing to perform the bootstrapping, interpolation, and discounting before getting to the simple present value sought in the analysis. What can be, with the right abstractions, distilled into five lines of code would take hundreds of cells in a spreadsheet. Providing abstractions like this at the hand of financial modelers is a productivity multiplier.\n\n\n8.4.2.2 Initial Versions\nThere were two main abstractions to talk about from early versions of the packages.\n\n8.4.2.2.1 Rates\nUtilizing the benefit of the type system, it was decided that it would be most useful to represent rates not as simple floating point numbers (e.g. 0.05) but instead with dedicated types to distinguish between rate conventions. The abstract type CompoundingFrequency had two subtypes: Continuous and Periodic so that a 5% rate compounded continuously versus an effective per period rate would be distinguished via Continuous(0.05) versus Periodic(0.05,1). The two could be converted between by extending the built-in Base.convert function.\nThis was useful because once rates were converted into Rates within the ecosystem, that data contained within itself characteristics that could distinguish how downstream functionality should treat the rates.\n\n\n8.4.2.2.2 Yield Curves\nAt first, only bootstrapping was supported as a method to construct curve objects. This required that there was only one rate given per time period (no noisy data) and only supported linear, quadratic, and cubic splines.\nFurther, there was a specific constructor for different common types of instruments. From the old documentation:\n\n\nYields.Zero(rates,maturities) using a vector of zero, or spot, rates\nYields.Forward(rates,maturities) using a vector of one-period\nYields.Constant(rate) takes a single constant rate for all times\nYields.Par(rates,maturities) takes a series of yields for securities priced at par. Assumes that maturities &lt;= 1 year do not pay coupons and that after one year, pays coupons with frequency equal to the CompoundingFrequency of the corresponding rate.\nYields.CMT(rates,maturities) takes the most commonly presented rate data (e.g. Treasury.gov) and bootstraps the curve given the combination of bills and bonds.\nYields.OIS(rates,maturities) takes the most commonly presented rate data for overnight swaps and bootstraps the curve.\n\n\nThis covered a lot of lightweight use-cases, but made a lot of implicit assumptions about how the given rates should be interpreted.\n\n\n\n8.4.2.3 The Birth of FinanceModels\nThere were a multiple of insights that led to a more flexible interface in more recent versions.\n\n\n\nA conceptual sketch of FinanceModels.jl components.\n\n\nFirst, realizing that yield curves were just a particular kind of model - one that used interest rates to discount cashflows. But you can have different kinds of models - such as Black-Scholes option valuation or a Monte Carlo valuation approach. Likewise, the cashflows need not simply be a vector of floating point values, and instead it could be the representation of a generic financial contract. As long as the model knew how to value it, an appropriate present value could be derived.\nWhere previously it was:\npresent_value(yield_curve,cashflow_vector)\nNow, it was\npresent_value(model,contract)\nSecond, that a model was simply some generic box that had been “fit” to previously observed prices for similar types of contracts we would be trying to value in the model. The combination of a contract and a price constituted a “quote” and with multiple quotes a model could be fit using various algorithms.\nWith these changes, the package that was originally called Yields.jl was renamed to FinanceModels.jl. The updated code from the earlier example now would be implemented like this:\nusing FinanceModels\nrisk_free_rates = [0.05, 0.055, 0.06]\ntenors = [1.0, 5.0, 10.0]\nquotes = ParYield.(risk_free_rates, tenors)\n\nmodel = fit(Spline.Cubic(), quotes, Fit.Bootstrap())\n\ncashflow_vector = [1_000_000.0, 3_000_000.0, 1_000.0]\npresent_value(model, cashflow_vector)\nIt’s slightly more verbose, but notice how much more powerful and extensible fit(Spline.Cubic(), quotes, Fit.Bootstrap() is than Yields.Par(risk_free_rates,tenors) . The end result is the same, but now the same package and interface can clearly interchange other options, such as a NelsonSiegelSvennson curve instead of a spline. And the quotes could be a combination of observed bonds of different technical parameters (though still sharing characteristics which make it relevant for the model being constructed).\nThe same pattern also applies for option valuation, such as this example of vanilla euro options with an assumed constant volatility assumption:\n\n1a = Option.EuroCall(CommonEquity(), 1.0, 1.0)\nb = Option.EuroCall(CommonEquity(), 1.0, 2.0)\n\n2qs = [\n    Quote(0.0541,   a),\n    Quote(0.072636, b),\n]\n\n3model = Equity.BlackScholesMerton(0.01, 0.02, Volatility.Constant())\n\n4m = fit(model, qs)\n\n5present_value(m,qs[1].instrument)\n\n1\n\nThe arguments to EuroCall are the underlying asset type, strike, and maturity time.\n\n2\n\nA vector of observed option prices.\n\n3\n\nA BSM model with a given risk free rate, dividend yield, and a to-be-fit constant volatility component.\n\n4\n\nFits the model and derives an estimated volatility close to 0.15 for the 1-year call (given the quoted price).\n\n5\n\nValues the contract; in this simple, noiseless setup we recover the original price of approximately 0.0541.\n\n\nWith a consistent interface able to handle a wide variety of situations, the modeler is free to expand the model in new directions of analysis with the built in functionality allowing him or her to compose pieces together that was not possible with the less abstracted design. For example, the equity option example had no parallel when all of the available constructors were Yields.Zero or Yields.Par and would have required a completely from-scratch implementation with newly defined functions.\nFurther, and critically, the new design allows modelers to create their own models or contracts1 and extend the existing methods rather than needing to create their own: the function signature fit(model,quotes) handles a very wide variety of cases, as does present_value(model,contract).",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Higher Levels of Abstraction</span>"
    ]
  },
  {
    "objectID": "patterns-abstraction.html#macros-homoiconicity",
    "href": "patterns-abstraction.html#macros-homoiconicity",
    "title": "8  Higher Levels of Abstraction",
    "section": "8.5 Macros & Homoiconicity",
    "text": "8.5 Macros & Homoiconicity\nWe’ve talked about transforming data and restructuring logic in order to make the model more effective. We can go still deeper!(Or is it higher level?) We can actually abstract the process of writing code itself! This subject is a bit advanced, so we are simply going to introduce it because you will likely find many convenient instances of it as a user even if you never find a need to implement this yourself.\nHomoiconicity refers to the property of a programming language where the language’s code can be represented and manipulated as a data structure in the language itself. Think of a recipe. You can follow the recipe’s instructions (the code) to bake a cake. But you could also treat the recipe itself as data: you could write a program to scan thousands of recipes, find every instance of ‘sugar,’ and reduce the quantity by 25%. This is the essence of homoiconicity: the code (the recipe) can also be treated as data to be manipulated.\nIn a homoiconic language like Julia, the code is data and can be treated as such. This enables powerful metaprogramming (i.e. code that writes other code) capabilities, where code can be generate or transform code during macro expansion (before runtime), producing expressions that are then compiled.\nMacros are a metaprogramming feature that leverage homoiconicity in Julia. They allow the programmer to write code that generates or manipulates other code at compile-time. Macros take code as input, transform it based on certain rules or patterns, and return the modified code which then gets compiled.\nFor example, a built-in macro is @time which will measure the elapsed runtime for a piece of code2.\n@time exp(rand())\nWill effectively expand to:\nt0 = time_ns()\nvalue = exp(rand())\nt1 = time_ns()\nprintln(\"elapsed time: \", (t1-t0)/1e9, \" seconds\")\nvalue\nHere it is when we run it:\n\n@time exp(rand())\n\n  0.000004 seconds\n\n\n1.4120026466601727\n\n\n\n8.5.1 Metaprogramming in Financial Modeling\nIn the context of financial modeling, macros can be used to simplify repetitive or complex code patterns, enforce certain conventions or constraints, or generate code based on data or configuration.\nHere are a few potential use cases of macros in financial modeling. Again, these are more advanced use-cases but knowing that these paths exist may benefit your work in the future.\n\nDefining custom DSLs (Domain-Specific Languages): Macros can be used to create expressive and concise DSLs tailored to financial modeling. For example, a macro could allow defining financial contracts using a syntax closer to the domain language, which then gets expanded into the underlying implementation code.\nAutomating boilerplate code: Macros can help reduce code duplication by generating common patterns or boilerplate code. This can include generating accessor functions3, constructors, or serialization logic based on type definitions.\nEnforcing conventions and constraints: Macros can be used to enforce coding conventions, such as naming rules or type checks, by automatically transforming code that doesn’t adhere to the conventions. They can also be used to add runtime assertions or checks based on certain conditions.\nOptimizing performance: Macros can be used to perform code optimizations at compile-time. For example, a macro could unroll loops, inline functions, or specialize generic code based on specific types or parameters, resulting in more efficient runtime code.\nGenerating code from data: Macros can be used to generate code based on external data or configuration files. For example, a macro could read a specification file and generate the corresponding financial contract types and functions.",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Higher Levels of Abstraction</span>"
    ]
  },
  {
    "objectID": "patterns-abstraction.html#footnotes",
    "href": "patterns-abstraction.html#footnotes",
    "title": "8  Higher Levels of Abstraction",
    "section": "",
    "text": "And projections, which is handled by defining a ProjectionKind , such as a cashflow or accounting basis. This topic is covered in more detail in the FinanceModels.jl documentation.↩︎\n@time is a simple, built-in function. For true benchmarking purposes, see Section 24.4.↩︎\nAccessor functions are useful when working with nested data structures. For example, if you have a struct within a struct and want to conveniently access an inner struct’s field.↩︎",
    "crumbs": [
      "Foundations: Programming and Abstractions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Higher Levels of Abstraction</span>"
    ]
  },
  {
    "objectID": "conceptual-foundations2.html",
    "href": "conceptual-foundations2.html",
    "title": "Foundations: Building Performant Models",
    "section": "",
    "text": "“Premature optimization is the root of all evil (or at least most of it) in programming.” - Donald Knuth\n\nAfter establishing foundational programming concepts, we turn our attention to performance - a critical consideration for real-world financial models. While modern computers are remarkably powerful, thoughtlessly constructed models can still grind to a halt when faced with large portfolios or complex analyses. This section explores how to harness computational resources effectively, starting from the hardware fundamentals that both constrain and enable our work.\nUnderstanding performance requires looking beneath the abstractions we’ve built. Just as a financial modeler benefits from understanding the mechanics of markets and instruments rather than treating them as black boxes, knowledge of computational infrastructure allows us to make informed decisions about model architecture and implementation. We’ll examine how hardware characteristics influence algorithm design, memory usage patterns, and execution speed.\nWe’ll introduce when optimization does matter, and equally important when it doesn’t. The goal isn’t to optimize prematurely or pursue performance at all costs. Rather, we aim to build models that scale gracefully as demands grow, whether through larger datasets, more sophisticated analyses, or tighter time constraints. We’ll progress from single-threaded optimization techniques to parallel processing approaches, always with an eye toward practical application in financial contexts.\nBy the end of this section, you’ll have the knowledge needed to diagnose performance bottlenecks and implement appropriate solutions, ensuring your models remain responsive and reliable as they evolve. Let’s begin by examining the hardware foundation upon which all our computational work rests.",
    "crumbs": [
      "Foundations: Building Performant Models"
    ]
  },
  {
    "objectID": "hardware.html",
    "href": "hardware.html",
    "title": "9  Hardware and Its Implications",
    "section": "",
    "text": "9.1 Chapter Overview\nIn this chapter, we’ll explore why a basic understanding of computing hardware is essential for optimizing financial models and working efficiently with data. Understanding how data is stored and processed can help you make better design decisions, improve performance, and avoid common pitfalls. We’ll cover topics like memory architecture, data storage, and the impact of hardware on computational speed.",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware and Its Implications</span>"
    ]
  },
  {
    "objectID": "hardware.html#introduction",
    "href": "hardware.html#introduction",
    "title": "9  Hardware and Its Implications",
    "section": "9.2 Introduction",
    "text": "9.2 Introduction\nThe quote that opens the chapter is a silly way of describing that most modern computers are made with silicon, a common mineral found in rocks. However, we will not concern ourselves with the raw materials of computers and instead will focus on the key architectural aspect.\nA computer handles data at rest (in memory) or data being acted upon (processed). This chapter will try to explain both of those processes in a way that reveals key reasons why different approaches to programming can yield different results in terms of processing speed, memory usage, and compiled outputs.",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware and Its Implications</span>"
    ]
  },
  {
    "objectID": "hardware.html#memory-and-moving-data-around",
    "href": "hardware.html#memory-and-moving-data-around",
    "title": "9  Hardware and Its Implications",
    "section": "9.3 Memory and Moving Data Around",
    "text": "9.3 Memory and Moving Data Around\nThe core of modeling on computers is to perform computations on data, but unfortunately the speed at which data can be accessed has grown much slower than the rate the actual computations can be performed. Further, the size of the available persistent data storage (HDDs/SSDs) has ballooned, exacerbating the problem: the overall throughput of memory is the typical workflow constraint. To try to address the bottleneck (memory throughput), solutions have been developed to create a pipeline to efficiently shuttle data to and from the processor and the persistent storage. This memory and processing architecture applies at both the single computer level as well as extending to workflows between different data stores and computers.\nWe will focus primarily on the architecture of a single computer, as even laptop computers today contain enough power for most modeling tasks, if the computer is used effectively. Further, learning how to optimize a program for a single computer/processor is almost always a precursor step to effective parallelization as well.\n\n9.3.1 Memory Types and Location\nMemory has an inverse relationship between size and proximity to the central processing unit (CPU). The closer the data is to the processor units, the smaller the storage and the less likely the data will persist at that location for very long.\n\n\n\n\n\n\n\n\n\nKind\nTypical capacity\nTypical latency\nPersistence\n\n\n\n\nSSD or HDD\n1–10+ TB\n100,000–10,000,000 ns (read/seek dependent)\nPersistent\n\n\nRAM (DRAM)\n8–256+ GB\n60–120 ns\nVolatile (power-dependent)\n\n\nCPU Cache – L3\n4–128 MB\n10–40 ns\nVolatile\n\n\nCPU Cache – L2\n256 KB–8 MB\n3–10 ns\nVolatile\n\n\nCPU Cache – L1\n32–128 KB\n0.5–1.5 ns\nVolatile\n\n\nCPU Registers\na few hundred bytes\nsingle-cycle access\nVolatile\n\n\n\nAfter requesting data from a persistent location like a Solid State Drive (SSD), the memory is read into Random Access Memory (RAM). The advantage of RAM over a persistent location is speed - typically that memory can be accessed and modified many times faster than the persistent data location. The tradeoff is that RAM is not persistent: when the computer is powered down, the RAM loses the information stored within.\nWhen data is needed by the CPU, data is read from RAM into a small hierarchy of caches before being accessed by the CPU. The CPU caches are small and very fast, and they move data in fixed-size ‘cache lines’ (typically 64 bytes). The caches are also physically co-located with the CPU for efficiency. Data is organized and funneled through the caches as an intermediary between the CPU and RAM and is fed from Level 3 (L3) cache in steps down to L1 cache as the data gets closer to the processor.\nAt the very top of the memory hierarchy are CPU registers—tiny storage locations on the chip that the processor can read and write in a single cycle. This is where the CPU actually performs calculations. The CPU’s execution units operate on values held in registers. Most arithmetic, logical, and comparison instructions take their operands from registers and write results back to registers; separate load/store instructions move data between memory and registers.\n\n\n\n\n\n\nNote\n\n\n\nMemory units at a glance:\n\n1 bit is a single 0 or 1; 8 bits = 1 byte.\nBinary multiples (common for RAM/caches and many OS readouts):\n1 KiB = 1,024 bytes; 1 MiB = 1,024 KiB; 1 GiB = 1,024 MiB.\nDecimal multiples (common for storage devices and network speeds):\n1 kB = 1,000 bytes; 1 MB = 1,000,000 bytes; 1 Gb = 1,000,000,000 bits.\nKB vs Kb: uppercase B = bytes; lowercase b = bits. 1 KB is eight times larger than 1 Kb.\n\nRule of thumb: treat memory sizes (RAM, caches) as base‑2, and disk/network specs as base‑10. The “i” in KiB/MiB/GiB explicitly means base‑2 \\((2^{10} = 1{,}024)\\).\n\n\n\n\n9.3.2 Stack vs Heap\nSitting within the RAM region of memory are two sections called Stack and Heap. These are places where variables created from our program’s code can be stored. In both cases, the program will request memory space but they have some differences to be aware of.\n\nThe stack stores small, fixed-size (known bit length), data and program components. The stack is a last-in-first-out queue of data that is able to be written to and read from very quickly. The stack is CPU cache friendly.\nThe heap is a region which can be dynamically sized and has random read/write (you need not access the data in a particular order). The heap is much slower but more flexible. The heap is not very CPU cache friendly.\n\n\n9.3.2.1 Garbage Collector\nThe garbage collector is a program that gets run to free up previously requested/allocated memory. It accomplishes this by keeping track of references to data in memory by section of your code. If a section of code is no longer reachable (e.g. inside a function that will never get called again, or a loop that ran earlier in the program but is now complete), then, periodically, the garbage collector will pause execution of the primary program in order to “sweep” the memory. This step marks the space as able to be reused by your program or the operating system.\n\n\n\n\n\n\nTip\n\n\n\nJulia uses a generational, stop-the-world garbage collector to reclaim heap memory that is no longer reachable by your program. Periodically, execution pauses briefly while the GC identifies unreachable objects and frees them. In performance-sensitive code, reducing heap allocations (e.g., by reusing buffers, working in-place, and writing type-stable code) minimizes GC pressure and improves throughput.",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware and Its Implications</span>"
    ]
  },
  {
    "objectID": "hardware.html#sec-hardware-processor",
    "href": "hardware.html#sec-hardware-processor",
    "title": "9  Hardware and Its Implications",
    "section": "9.4 Processor",
    "text": "9.4 Processor\nThe processor reads cache lines (typically 64 bytes) from caches and moves the needed values into registers and then executes instructions. An example would be to take the bytes from register 10 and add the bytes from register 11 to them. This is really all a processor does at the lowest level: combining bits of data using logical circuits.\nLogical circuits (transistors) are an arrangement of wires that output a new electrical signal that varies depending on the input. From a collection of smaller building block gates (e.g. AND, OR, NOR, XOR) more complex operations can be built up1, into operations like addition, multiplication, division, etc. Electric impulses move the state of the program forward once time per CPU cycle (controlled by a master “clock” ticking billions of times per second). CPU cycle speed is what’s quoted for chip performance, e.g. when a CPU is advertised as 3.0 GHz (or 3 billion cycles per second).\nThe programmer (or compiler, if we are working in a higher level language like Julia) tells the CPU which instruction to run. The set of instructions that are valid for a given processor are called the Instruction Set Architecture, or ISA. In computer Assembly language (roughly one-level above directly manipulating the bits), the instructions are given names like ADD, SUB, MUL, DIV, and MOV. These instructions mirror the raw instruction that is part of the ISA.\nNot all instructions are created equal, however. Some instructions take many CPU cycles to complete. For example, floating point DIV (division) takes 10-20 CPU cycles while ADD only takes a single CPU cycle.\nSome architecture examples that may be familiar:\n\nIntel x86-64 (a.k.a. AMD64) are common computer processors that use registers that are 64 bits wide (the prior generation was 32 bits wide) and use the x86 instruction set developed by Intel.\nARM chips, including the Apple M-Series processors are characterized by the use of the ARM instruction set and recent processors of this kind are also 64 bit.\n\nThe ARM architecture is known as a reduced instruction set chip (RISC), which means that it has fewer available instructions compared to, e.g. the x86 architecture. The benefit of the reduced instruction set is that it is generally much more power efficient, but comes at the cost of sacrificing specialized instructions such as string manipulation, or in lower-end chips, even the division operation (which have to be implemented via software routines instead of CPU operations). However, for specialized workloads, the availability of a key instruction can make a program run on the CPU 10-100x faster at times. An example of this is that at the time of writing, AVX512 processors are becoming available (see Chapter 11 for a discussion of vectorization) which can benefit some workloads greatly.\n\n\n\n\n\n\nTip\n\n\n\nTrying to optimize your program via selecting specialized chips should be one of the last ways that you seek to optimize runtime, as generally a similar order of magnitude speedup can be achieved through more efficient algorithm design or general parallelization techniques. Developing programs in this way makes the performance portable, able to be used on other systems and not just special architectures.\n\n\nWhen writing in Julia, you need not be concerned with the low-level instructions as the compiler will optimize the execution for you. However, should it be useful, it is easy to inspect the compiled code. For example, if we create a function to add three numbers, we can see that the ADD instruction is called twice: first adding the first and second arguments, and then adding the third argument to that intermediate sum.\n\nmyadd(x, y, z) = x + y + z\n@code_native myadd(1, 2, 3)\n\n\n   .section    __TEXT,__text,regular,pure_instructions\n    .build_version macos, 15, 0\n    .globl  _julia_myadd_6893               ; -- Begin function julia_myadd_6893\n    .p2align    2\n_julia_myadd_6893:                      ; @julia_myadd_6893\n; Function Signature: myadd(Int64, Int64, Int64)\n; ┌ @ /Users/alecloudenback/prog/julia-fin-book/hardware.qmd:112 within `myadd`\n; %bb.0:                                ; %top\n; │ @ /Users/alecloudenback/prog/julia-fin-book/hardware.qmd within `myadd`\n    ;DEBUG_VALUE: myadd:x &lt;- $x0\n    ;DEBUG_VALUE: myadd:x &lt;- $x0\n    ;DEBUG_VALUE: myadd:y &lt;- $x1\n    ;DEBUG_VALUE: myadd:y &lt;- $x1\n    ;DEBUG_VALUE: myadd:z &lt;- $x2\n    ;DEBUG_VALUE: myadd:z &lt;- $x2\n; │ @ /Users/alecloudenback/prog/julia-fin-book/hardware.qmd:112 within `myadd`\n; │┌ @ operators.jl:596 within `+` @ int.jl:87\n    add  x8, x1, x0\n    add  x0, x8, x2\n    ret\n; └└\n                                        ; -- End function\n.subsections_via_symbols\n\n\n\n\nCompilers are complex, hyper-optimized programs which turn your source code into the raw bits executed by the computer. Key steps in the process of converting Julia code you write all the way to binary machine instructions include the items in Table 9.1. Note the Julia @code_... macros allow the programmer to inspect the intermediate representations.\n\n\n\n\nTable 9.1: Part the key to Julia’s speed is to be able to compile down to a different, specialized version of the machine code depending on the types given to a function. As described in the table above, the instructions for adding floating point together or integer numbers together are different. Julia code can reflect that distinction by compiling a different method for each combination of input types.\n\n\n\n\n\n\n\n\n\n\nStep\nDescription\nExample\n\n\n\n\nJulia Source Code\nThe level written by the programmer in a high level language.\nmyadd(x,y,z) = x + y + z\n\n\nLowered Abstract Syntax Tree (AST)\nAn intermediate representation of the code after the first stage of compilation, where the high-level syntax is simplified into a more structured form that’s easier for the compiler to work with.\njulia&gt; @code_lowered myadd(1,2,3)\nCodeInfo(\n1 ─ %1 = x + y + z\n└──      return %1\n)\n\n\nLLVM\nLow-Level Virtual Machine language, which is a massively popular compiler used by languages like Julia and Rust. The core logic are the three lines with add, add, and ret.\nNote that the the add instruction is add i64 which means an addition operation of 64 bit integers.\njulia&gt; @code_llvm myadd(1,2,3)\n;  @ REPL[7]:1 within `myadd`\ndefine i64 @julia_myadd_2022(i64 signext %0, i64 signext %1, i64 signext %2) #0 {\ntop:\n; ┌ @ operators.jl:587 within `+` @ int.jl:87\n   %3 = add i64 %1, %0\n   %4 = add i64 %3, %2\n   ret i64 %4\n; └\n}\n\n\nNative\nThe final machine code output, specific to the target CPU architecture. This is at the same level as Assembly language. The core logic are the three lines beginning with add, add, and ret.\nIf we used floating point addition instead, the CPU instruction would be fadd instead of add.\njulia&gt; @code_native myadd(1,2,3)\n        .section        __TEXT,__text,regular,pure_instructions\n        .build_version macos, 14, 0\n        .globl  _julia_myadd_1851               ; -- Begin function julia_myadd_1851\n        .p2align        2\n_julia_myadd_1851:                      ; @julia_myadd_1851\n; ┌ @ REPL[7]:1 within `myadd`\n; %bb.0:                                ; %top\n; │┌ @ operators.jl:587 within `+` @ int.jl:87\n        add     x8, x1, x0\n        add     x0, x8, x2\n        ret\n; └└\n\n\n\n\n\n\n\n\n9.4.0.1 Increasing Complexity in Search of Performance\nTransistors are the building-block that creates the CPU and enables the physical process which governs the computations. For a very long time, the major source of improved computer performance was simply to make smaller transistors, allowing more of them to be packed together to create computer chips. This worked for many years and the propensity was for the transistor count to double about every two years. In this way, software performance improvements came as side effect of the phenomenal scaling in hardware capability. However, raw single core performance and clock frequency (CPU cycle speed) dramatically flattened out starting a bit before the year 2010. This was due to the fact that transistor density has been starting to be limited by:\n\nPure physical constraints (transistors can be measured in width of atoms) where we have limited ability to manufacture something so small.\nThermodynamics, where heat can’t be removed from the CPU core fast enough to avoid damaging the core and therefore operations per second are capped.\n\nTo obtain increasing performance, two main strategies have been employed in lieu of throwing more transistors into a single core:\n\nParallelism: utilize multiple, separate cores and operate in an increasingly parallel way.\nMicroarchitectural techniques: clever tricks to predict, schedule, and optimize the computations to make better use of the memory pipeline and otherwise idle CPU cycles.\n\nWe will cover techniques to utilize concurrent/parallel processing in Chapter 11. As for the second technique, it is capable of very impressive accelerations (on the order 2x to 100x faster than a naive implementation. However, it has sometimes caused issues. There have been some famous security vulnerabilities such as Spectre and Meltdown, which exploited speculative execution – a technique used to optimize CPU performance which will execute code before being explicitly asked to because the scheduler anticipates the next steps (with very good, but imperfect accuracy).",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware and Its Implications</span>"
    ]
  },
  {
    "objectID": "hardware.html#logistics-warehouse-analogy",
    "href": "hardware.html#logistics-warehouse-analogy",
    "title": "9  Hardware and Its Implications",
    "section": "9.5 Logistics Warehouse Analogy",
    "text": "9.5 Logistics Warehouse Analogy\nThe problem is analogous to a logistics warehouse (persistent data) which needs to package up orders (processor instructions). There’s a conveyor belt of items being constantly routed to the packaging station. In order to keep the packing station working at full capacity, the intermediate systems (RAM & CPU caches) are funneling items they think will be needed to the packager (data that’s expected to be used in the processor). Most of the time, the necessary item (data) is optimally brought to the packaging station (process), or a nearby holding spot (CPU cache).\nThis system has grown very efficient, but sometimes the predictions miss or a never-before-ordered item needs to be picked from the far side of the warehouse and this causes significant delays to the system. Sometimes a package will start to be assembled before the packager has even gotten to that order (branch prediction) which can make the system faster most of the time, but if the predicted package isn’t actually what the customer ordered, then the work is lost and has to be redone (branch mis-predict).\nThere are a lot more optimizations along the way:\n\nSince the items are already mostly arranged so that related items are next to each other, the conveyor belt will bring nearby items at the same time it brings the requested item (memory blocks).\nIf an item usually ordered after another one is, the conveyor system will start to bring that second item as soon as the first one is ordered (prefetching).\nDifferent types of packaging stations might be used for specialized items (e.g. vector processing or cryptography instructions in the CPU).\n\n\n\n\n\n\n\nTipFinancial Modeling Pro Tip\n\n\n\nJulia arrays are column-major. Access memory contiguously to exploit caches:\nX = randn(10_000, 100)  # rows × cols\n# Fast: process columns (contiguous memory)\nsums = map(sum, eachcol(X))\n\n# Slower: process rows (strided access)\nsums_rows = map(sum, eachrow(X))\nIn portfolio/risk code, organize factor loadings or scenario matrices so your hot loops iterate along columns where possible. This often yields 2–10× speedups purely from better cache locality.",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware and Its Implications</span>"
    ]
  },
  {
    "objectID": "hardware.html#speed-of-computer-actions",
    "href": "hardware.html#speed-of-computer-actions",
    "title": "9  Hardware and Its Implications",
    "section": "9.6 Speed of Computer Actions",
    "text": "9.6 Speed of Computer Actions\nIn a financial model, even small delays (such as main memory references vs. L1 cache access) can accumulate quickly in high-frequency trading or risk calculation routines. Understanding these timings can guide decisions on structuring data access patterns and deciding what data to cache or load in memory for optimal performance.\nRepresentative time is given in Table 9.2 for a variety of common actions performed on a computer. It’s clear that having memory access from a local source is better for computer performance!\n\n\n\nTable 9.2: How long different actions take on a computer. As an interesting yardstick, the distance a beam of light travels is also given to provide a sense of scale (this comparison originally comes from Admiral Grace Hopper). Source for the timings comes from: https://cs.brown.edu/courses/csci0300/2022/assign/labs/lab4.html. Also note that timings vary substantially by hardware and workload; treat these as order-of-magnitude references.\n\n\n\n\n\n\n\n\n\n\nOperation\nTime (ns)\nDistance light traveled\n\n\n\n\nSingle CPU cycle (e.g., one add on a register)\n0.3\n9 centimeters\n\n\nL1 cache reference\n1\n30 centimeters\n\n\nBranch mispredict\n5\n150 centimeters\n\n\nL2 cache reference\n5\n150 centimeters\n\n\nMain memory reference\n100\n30 meters\n\n\nRead 1 MB sequentially from RAM\n250,000\n75 km (≈2 marathons)\n\n\nRound trip within a datacenter\n500,000\n150 km (thickness of Earth’s atmosphere)\n\n\nRead 1 MB sequentially from SSD\n1,000,000\n300 km (Washington, D.C. to New York City)\n\n\nHard disk seek\n10,000,000\n3,000 km (width of the continental U.S.)\n\n\nSend packet CA→Netherlands→CA\n150,000,000\n45,000 km (Earth’s circumference)\n\n\n\n\n\n\nAs introduced in Section 9.4, calculations on the CPU vary widely. Take, for example, Table 9.3 shows runtime varying quite a bit for common mathematical operations. Despite mathematically all essentially being elementary operations, the compute intensity varies widely:\n\n\n\nTable 9.3: Different operations utilize different levels of required compute. On different hardware, the result may be different depending on the fundamental computational complexity of an operation and what hardware instructions are built into the CPU.\n\n\n\n\n\n\n\n\n\n\nMathematical Operation\nJulia Function\nRuntime in nanoseconds (Float64 arguments)\n\n\n\n\n\\(a+b\\)\n+\n1.7\n\n\n\\(a-b\\)\n-\n1.7\n\n\n\\(a/b\\)\n/\n1.7\n\n\n\\(a \\times b\\)\n*\n1.7\n\n\n\\(a^b\\)\n^\n10.8\n\n\n\\(exp(a)\\)\nexp\n3.9\n\n\n\\(log_{10}(a)\\)\nlog\n5.4\n\n\n\\(log(a)\\)\nln\n3.7\n\n\n\\(abs(a)\\)\nabs\n1.7\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOne way to speed up financial models is to utilize techniques to avoid using the ^ operation, as it’s one of the most expensive basic operations, but unfortunately is also one of the most common when dealing with rates and compounding.\nSome strategies:\n\nInstead of using interest rates which require exponentiation, utilize continuously compounded rates and use the exp and log functions instead.\nFor small r, use expm1/log1p to improve accuracy:\n\nexpm1(x) computes \\(\\exp(x) - 1\\) accurately for small \\(x\\).\nlog1p(x) computes \\(\\log(1 + x)\\) accurately for small \\(x\\).\n\nSay you are running a monthly model with inputs that are annual rates. Before running through the ‘hot loop’, pre-compute the annual rates into a vector of monthly rates to avoid re-computing this transformation at the cell/seriatim level.",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware and Its Implications</span>"
    ]
  },
  {
    "objectID": "hardware.html#footnotes",
    "href": "hardware.html#footnotes",
    "title": "9  Hardware and Its Implications",
    "section": "",
    "text": "In fact, only two logical gates are needed to reproduce all boolean logical gates: NAND (Not AND) and NOR gates can be composed to create AND, OR, NOR, etc. gates.↩︎",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware and Its Implications</span>"
    ]
  },
  {
    "objectID": "performance-single.html",
    "href": "performance-single.html",
    "title": "10  Writing Performant Single-Threaded Code",
    "section": "",
    "text": "10.1 Chapter Overview\nUnderstanding single-threaded performance, strategies for efficient sequential code, recognizing when parallelization is needed, practical comparisons of single-threaded and parallel approaches, optimizing code in Julia.",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Writing Performant Single-Threaded Code</span>"
    ]
  },
  {
    "objectID": "performance-single.html#introduction",
    "href": "performance-single.html#introduction",
    "title": "10  Writing Performant Single-Threaded Code",
    "section": "10.2 Introduction",
    "text": "10.2 Introduction\nOn today’s hardware, the highest-throughput workloads often run on GPUs via massive parallelism. However, writing correct, performant parallel code relies on understanding efficient sequential patterns first. Secondly, many problems are not “massively parallelizable” and a sequential model architecture is required. For these reasons, it’s critical to understand sequential patterns before moving onto parallel code.\nFor those coming from fundamentally slower languages (such as R or Python), the common advice to speed things up is often to try to parallelize code or use array-based techniques. With many high level languages the only way to achieve reasonable runtime is to utilize parallelism. In contrast, fast languages like Julia can output surprisingly quick single threaded programs.\nFurther, it may be that a simpler, easier to maintain sequential model is preferable to a more complex parallel version if maximum performance is not required. Like the quote that opened the chapter, you may prefer a simpler sequential version of a model to a more complex parallel one.\n\n\n\n\n\n\nTip\n\n\n\nDeveloper time (your time) is often more expensive than runtime, so be prepared to accept a good-enough but sub-optimal code instead of spending a lot of time optimizing every last nanosecond out of it!\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis section of the book continues to focus on concepts that are more general than just Julia. We will elaborate on Julia-specific techniques in the section starting with Chapter 21.",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Writing Performant Single-Threaded Code</span>"
    ]
  },
  {
    "objectID": "performance-single.html#patterns-of-performant-sequential-code",
    "href": "performance-single.html#patterns-of-performant-sequential-code",
    "title": "10  Writing Performant Single-Threaded Code",
    "section": "10.3 Patterns of Performant Sequential Code",
    "text": "10.3 Patterns of Performant Sequential Code\nSequential code performance depends on five key patterns:\n\nEfficient memory usage\nOptimized memory access\nAppropriate data types\nType stability\nBranch prediction optimization\n\nUnderstanding these patterns helps you write faster code by leveraging CPU architecture and Julia’s compiler optimizations. We’ll look at each of these in turn.\n\n10.3.1 Minimize Memory Allocations\nAllocating memory onto the Heap takes a lot more time than (1) not using intermediate memory storage at all, or (2) utilizing the Stack. Each allocation requires time for memory management and requires the garbage collector, which can significantly impact performance, especially in tight loops or frequently called functions.\n\n\n\n\n\n\nNote\n\n\n\nTight loops or hot loops are the performance critical section of the code that are performed many times during a computation. They are often the “inner-most” loop of a nested loop algorithm.\n\n\nA general rule of thumb is that dynamically sizable or mutable objects (arrays, mutable structs) will be heap allocated while small fixed size objects can be stack allocated. For mutable objects, a common technique is to pre-allocate an array and then re-use that array for subsequent calculations. In the following example, note how we pre-allocate the output vector instead of creating vectors for each bond and then summing the vectors together at the end:\n\nend_time = 10\ncashflow_output = zeros(end_time)\n\npar_bonds = map(1:1000) do i\n    (tenor=rand((3, 5, 10)), rate=rand() / 10)\nend\n\n# sum up all of the bond cashflows into cashflow_output\nfor asset in par_bonds\n    for t in 1:end_time\n        if t == asset.tenor\n            cashflow_output[t] += 1 + asset.rate\n        else\n            cashflow_output[t] += asset.rate\n        end\n    end\nend\ncashflow_output\n\n10-element Vector{Float64}:\n  48.67424713112049\n  48.67424713112049\n 353.674247131121\n  48.67424713112049\n 376.674247131121\n  48.67424713112049\n  48.67424713112049\n  48.67424713112049\n  48.67424713112049\n 415.67424713112104\n\n\nJulia’s @allocated macro will display the number of bytes allocated by an expression, helping you identify and eliminate unnecessary allocations.\n\nrandom_sum_alloc() = sum([rand() for _ in 1:10])      # allocates\nrandom_sum_noalloc() = sum(rand() for _ in 1:10)      # generator, no array\n\n@allocated random_sum_alloc()\n@allocated random_sum_noalloc()\n\n0\n\n\n\n\n10.3.2 Optimize Memory Access Patterns\nOptimizing memory access patterns is essential for leveraging the CPU’s cache hierarchy effectively. Modern CPUs have multiple levels of cache (L1, L2, L3), each with different sizes and access speeds. By structuring your code to access memory in a cache-friendly manner, you can significantly reduce memory latency and improve overall performance.\nWhat is cache-friendly memory access? Essentially it boils down to spatial and temporal locality.\n\n10.3.2.1 Spatial Locality\nSpatial locality refers to accessing data that is physically near each other in memory (e..g contiguous blocks of data in an array).\nFor example, it’s better to access data in a linear order rather than random order. For example, if we sum up the elements of an array in order it will be significantly faster than if we do it randomly:\n\nusing BenchmarkTools, Random\n\n# Create a large array of structs to emphasize memory access patterns\nstruct DataPoint\n    value::Float64\n    # Add padding to make each element 64 bytes (a typical cache line size)\n    padding::NTuple{7,Float64}\nend\n\nfunction create_large_array(n)\n    [DataPoint(rand(), tuple(rand(7)...)) for _ in 1:n]\nend\n\n# Create a large array\nconst N = 1_000_000\nlarge_array = create_large_array(N)\n\n# Function for sequential access\nfunction sequential_sum(arr)\n    total = 0.0\n    for i in eachindex(arr)\n        total += arr[i].value\n    end\n    total\nend\n\n# Function for random access\nfunction random_sum(arr, indices)\n    total = 0.0\n    for i in indices\n        total += arr[i].value\n    end\n    total\nend\n\n# Create shuffled indices\nshuffled_indices = shuffle(1:N)\n\n# Benchmark\nprintln(\"Sequential access:\")\n@btime sequential_sum($large_array)\n\nprintln(\"\\nRandom access:\")\n@btime random_sum($large_array, $shuffled_indices)\n\nSequential access:\n  506.000 μs (0 allocations: 0 bytes)\n\nRandom access:\n  2.216 ms (0 allocations: 0 bytes)\n\n\n500305.8776795043\n\n\nWhen the data is accessed in a linear order, it means that the computer can load chunks of data into the cache and it can operate on that cached data for several cycles before new data needs to be loaded into the cache. In contrast, when accessing the data randomly, then the cache frequently needs to be populated with a different set of bits from a completely different part of our array.\n\n10.3.2.1.1 Column vs Row Major Order\nAll multi-dimensional arrays in computer memory are actually stored linearly. When storing the multi-dimensional array, an architectural decision needs to be made at the language-level and Julia is column-major, similar to many performance-oriented languages and libraries (e.g. LAPACK, Fortran, Matlab). Values are stored going down the columns instead of across the rows.\nFor example, this 2D array would be stored as [1,2,3,...] in memory, which is made clear via vec (which turns a multi-dimensional array into a 1D vector):\n\nlet\n    array = [\n        1 4 7\n        2 5 8\n        3 6 9\n    ]\n\n    vec(array)\nend\n\n9-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n\n\nWhen working with arrays, prefer accessing elements in column-major order (the default in Julia) to maximize spatial locality. This allows the CPU to prefetch data more effectively.\nYou can see how summing up values across the first (column) dimension is much faster than summing across rows:\n\n@btime sum(arr, dims=1) setup = arr = rand(1000, 1000)\n@btime sum(arr, dims=2) setup = arr = rand(1000, 1000)\n\n  71.750 μs (3 allocations: 8.08 KiB)\n  110.167 μs (3 allocations: 8.08 KiB)\n\n\n1000×1 Matrix{Float64}:\n 498.12851699816514\n 492.5651939267358\n 508.5408080490933\n 506.0569801580026\n 497.0551658879414\n 511.1538546805099\n 511.95700394443384\n 506.9804994904254\n 496.5712980007822\n 494.886213620193\n   ⋮\n 511.1641210165381\n 505.1811074929135\n 511.10349582227116\n 495.71144899491867\n 491.07713303426834\n 513.0959685894588\n 509.02602096475346\n 504.13104363084847\n 501.9999794584621\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn contrast to the prior example, a row-major memory layout would have the associated data stored in memory as:\n[1,4,7,2,5,8,3,6,9]\nThe choice between row and column major reflects the historical development of scientific computing and mathematical conventions. Column-major originated in Fortran and LAPACK, cornerstones of high-performance computing and linear algebra. This aligns with mathematical notation where vectors are typically represented as matrix columns.\nRow major languages include:\n\nC/C++\nPython (NumPy arrays)\nC#\nJava\nJavaScript\nRust\n\nColumn major languages:\n\nJulia\nMATLAB/Octave\nR\nFortran\nLAPACK/BLAS libraries\n\n\n\n\n\n\n10.3.2.2 Temporal Locality\nHardware prefetchers (predictors) and the cache hierarchy exploit temporal locality by keeping recently used data ‘hot’ and proactively fetching nearby data based on recent access patterns. This is an example of keeping “hot” data more readily accessible to the CPU than “cold” data.\nWhen working sets exceed available RAM, the operating system may page memory to a “swap file” on disk. Recently accessed pages tend to remain in RAM, while ‘colder’ pages are more likely to be swapped out.\n\n\n\n10.3.3 Use Efficient Data Types\nThe right data type can lead to more compact memory representations, better cache utilization, and more efficient CPU instructions. This is another case of where having a smaller memory footprint allows for higher utilization of the CPU since computers tend to be memory-constrained in speed.\nOn some CPUs, you may find performance by using the smallest data type that can accurately represent your data. For example, prefer Int32 over Int64 if your values will never exceed 32-bit integer range. For floating-point numbers, use Float32 instead of Float64 if the reduced precision is acceptable for your calculations. These smaller types not only save memory but also allow for more efficient vectorized operations (see Chapter 11) on modern CPUs.\n\n\n\n\n\n\nWarning\n\n\n\nWhen choosing data types, consider that on modern 64-bit architectures, using smaller integer or floating-point types doesn’t always improve performance. In fact, many operations are optimized for 64-bit (or even larger SIMD) registers, and using smaller data types may introduce unnecessary overhead due to conversions or misalignment.\nAs a rough guideline, if your data naturally fits within 64 bits (e.g. Float64 or Int64), starting there is often the best balance of performance and simplicity. You can experiment with smaller types if you know your values never exceed certain ranges and memory footprint is critical. However, always benchmark to confirm any gains—simply using a smaller type does not guarantee improved throughput on modern CPUs.\n\n\nFor collections, choose appropriate container types based on your use case. Arrays are efficient for calculations that loop through all or most elements, while Dictionaries are better for sparse look-ups or outside of the “hot loop” portion of a computation.\nConsider using small, statically sized collections when the data is suited for it. Small, fixed-size arrays, (such as StaticArrays.jl in Julia) can be allocated on the stack and lead to better performance in certain scenarios than dynamically sizable arrays. The trade-off is that the static arrays require more up-front compile time and after a certain point (length in the 50-100 element range) it usually isn’t worth trying to use them.\n\n\n10.3.4 Avoid Type Instabilities\nType instabilities occur when the compiler cannot infer a single concrete type for a variable or function return value. These instabilities can significantly hinder Julia’s ability to generate optimized machine code, leading to performance degradation. When the compiler is not able to infer the types at compile-time (compile time dispatch), then while the program is running a lookup needs to be performed to find the most appropriate functions for the given type (runtime dispatch). When the types are known at compile-time, Julia is able to create efficient machine code that will point directly to the desired function instead of needing to perform that lookup.\nTo avoid type instabilities, ensure that functions have inferrable, concrete types across all code paths. For example:\n\n\nfunction unstable_example(array)\n1    x = []\n    for y in array\n        push!(x, y)\n    end\n    sum(x)\nend\n\nfunction stable_example(array)\n2    x = eltype(array)[]\n    for y in array\n        push!(x, y)\n    end\n    sum(x)\nend\n\ndata = rand(1000)\n@btime unstable_example(data)\n@btime stable_example(data)\n\n\n1\n\nWithout a type given, [] will create a Vector{Any} which can contain elements of Any type which is flexible but requires runtime dispatch to determine correct behavior.\n\n2\n\neltype(array) returns the type contained within the array, which for data is Float64. Thus x is created as a Vector{Float64} which is more efficient code.\n\n\n\n\n  10.333 μs (2007 allocations: 53.73 KiB)\n  795.316 ns (9 allocations: 22.52 KiB)\n\n\n498.904974167776\n\n\nThe unstable_example function illustrates a common anti-pattern wherein an Any typed array is created and then elements are added to it. Because any type can be added to an Any array (we happen to just add floats to it) then Julia’s not sure what types to expect inside the container and therefore has to determine it at runtime.\nNote that having heterogeneous types as above is not the same thing as type instability, which is when Julia cannot determine in advance what the data types will be. In the example above, the return type is not unstable: the compiler recognizes that the single parametric type Union{Float64,Int64} will be returned., even though two different types can be returned the When the types cannot be determined by the compiler, it leads to runtime dispatch.\n\n\n\n\n\n\nTip\n\n\n\nSee Section 24.6.1 and Section 24.6.2 for tools in Julia to troubleshoot type instabilities.\n\n\n\n\n10.3.5 Optimize for Branch Prediction\nModern CPUs use branch prediction to speculatively execute instructions before knowing the outcome of conditional statements. This was described in the prior chapter in the logistics warehouse analogy as trying to predict where a package will be going before you’ve inspected the label. CPUs execute one branch of the instructions without seeing the data (either true data, or data in the form of CPU instructions) because the CPU has higher speed/capacity than the memory throughput allows. This is another example of a technique developed for performance in a memory-throughput constrained world.\nOptimizing your code for branch prediction can significantly improve performance, especially in tight loops or frequently executed code paths.\nTo optimize for branch prediction:\n\nStructure your code to make branching patterns more predictable. For instance, in if-else statements, put the more likely condition first. This allows the CPU to more accurately predict the branch outcome.\nUse loop unrolling to reduce the number of branches. This technique involves manually repeating loop body code to reduce the number of loop iterations and associated branch instructions. See Section 11.4 for more on what this means.\nConsider using Julia’s @inbounds macro to eliminate bounds checking in array operations when you’re certain the accesses are safe. This reduces the number of conditional checks the CPU needs to perform.\nFor performance-critical sections with unpredictable branches, consider using branch-free algorithms or bitwise operations instead of conditional statements. This can help avoid the penalties associated with branch mispredictions.\nIn some cases, it may be beneficial to replace branches with arithmetic operations (e.g., using the ternary operator or multiplication by boolean values) to allow for better vectorization and reduce the impact of branch mispredictions. An example of this would be using a statement like y += max(x,0) instead of if x &gt; 0; y += x; end.\n\nHere’s an example demonstrating the impact of branch prediction:\n\nfunction process_data(data, threshold)\n    sum = 0.0\n    for x in data\n        if x &gt; threshold\n            sum += log(x)\n        else\n            sum += x\n        end\n    end\n    sum\nend\n\n# Random data = unpredictable branches\nrand_data = rand(1_000_000)\n\n# Sorted data = predictable branches\nsorted_data = sort(rand(1_000_000))\n\n@btime process_data($rand_data, 0.5)\n@btime process_data($sorted_data, 0.5);\n\n  4.941 ms (0 allocations: 0 bytes)\n  1.590 ms (0 allocations: 0 bytes)\n\n\nIn this example, having sorted data means that the CPU will predict which branch of the if statement is likely to be utilized. By then speculatively executing the code that it thinks will be used, the overall program time is faster when processing sorted_data.\nRemember that optimizing for branch prediction often involves trade-offs. The benefits can vary depending on the specific hardware and the nature of your data. If performance critical, profile your code to ensure that your optimizations are actually improving performance in your specific use case. Over-optimizing on one set of hardware (e.g. local computer) may not translate the same on another set of hardware (e.g. server deployment).\n\n\n10.3.6 Further Reading\n\nWhat scientists must know about hardware to write fast code\nOptimizing Serial Code, ScIML Book",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Writing Performant Single-Threaded Code</span>"
    ]
  },
  {
    "objectID": "parallelization.html",
    "href": "parallelization.html",
    "title": "11  Parallelization",
    "section": "",
    "text": "11.1 Chapter Overview\nFundamentals of parallel workloads, different mechanisms to distribute work: vectorization, multi-threading, GPU, and multi-device workflows. Different programming models: map-reduce, arrays, and tasks.",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Parallelization</span>"
    ]
  },
  {
    "objectID": "parallelization.html#amdahls-law-and-the-limits-of-parallel-computing",
    "href": "parallelization.html#amdahls-law-and-the-limits-of-parallel-computing",
    "title": "11  Parallelization",
    "section": "11.2 Amdahl’s Law and the Limits of Parallel Computing",
    "text": "11.2 Amdahl’s Law and the Limits of Parallel Computing\nAn important ground-truth in computing is that there is an upper limit to how fast a workload can be sped up through distributing the workload among multiple processor units. For example, if there is a modeling workload wherein 90% of the work is independent (say policy or asset level calculations) and the remaining 10% of the workload is an aggregate (say company or portfolio level), then the theoretical maximum speedup of the process is 10x faster (1 / 90% parallelizable load). This is captured in a law known as Amdahl’s Law and it reflects the theoretical maximum speedup a workload could see. In practice, the speedup is worse than this due to overhead of moving data around, scheduling the tasks, and aggregating results. This is why in many cases a good effort in sequential workloads (see Chapter 10) is often a more fruitful effort than trying to parallelize some workloads.\nThat said, there are still many modeling use-cases for parallelization. Modern investment and insurance portfolios can easily contain 100’s of thousands or millions of seriatim holdings. In many cases, these can be evaluated independently, though on the oftentimes there is interaction with the total portfolio (contract dividends, non-guaranteed elements, profit sharing, etc.). Further, even if the holdings are not parallelizable across the holdings dimension, we are often interested in independent evaluations across economic scenarios which is amenable to parallelization. \\[\nS(n) = \\frac{1}{(1-p) + \\frac{p}{n}}\n\\]\nWhere:\n\n\\(S(n)\\) is the theoretical speedup of the execution of the whole task\n\\(n\\) is the number of processors\n\\(p\\) is the proportion of the execution time that benefits from improved resources\n\nWe can visualize this for different combinations of \\(p\\) and \\(n\\) in Figure 11.1.\n\nusing CairoMakie\n\nfunction amdahl_speedup(p, n)\n    return 1 / ((1 - p) + p / n)\nend\n\nfunction main()\n    fig = Figure(size=(800, 600))\n    ax = Axis(fig[1, 1],\n        title=\"Amdahl's Law\",\n        xlabel=L\"Number of processors ($n$)\",\n        ylabel=\"Speedup\",\n        xscale=log2,\n        xticks=2 .^ (0:16),\n        xtickformat=x -&gt; \"2^\" .* string.(Int.(log.(2, x))),\n        yticks=0:2:20\n    )\n\n    n = 2 .^ (0:16)\n    parallel_portions = [0.5, 0.75, 0.9, 0.95]\n    linestyles = [:solid, :dash, :dashdot, :solid]\n\n    for (i, p) in enumerate(parallel_portions)\n        speedup = [amdahl_speedup(p, ni) for ni in n]\n        lines!(ax, n, speedup, label=\"$(Int(p*100))%\", linestyle=linestyles[i])\n    end\n\n    xlims!(ax, 1, 2^16)\n    ylims!(ax, 0, 20)\n\n    axislegend(ax, L\"Parallel portion ($p$)\", position=:lt)\n    fig\nend\n\nmain()\n\n\n[ Info: Precompiling CairoMakie [13f3f980-e62b-5c42-98c6-ff1f3baf88f0] (cache misses: wrong dep version loaded (14), dep missing source (2), mismatched flags (2))\n\n\n\n\n\n\n\n\n\nFigure 11.1: Theoretical upper bound for speedup of a workload given the parallelizable portion \\(p\\) and number of processors \\(n\\).\n\n\n\n\nA real-world analogy: imagine building a house. You can hire many workers to frame the walls, install plumbing, and run electrical wires simultaneously (the parallel part). However, all work must stop to wait for the single foundation to be poured and cured (the serial part). No matter how many workers you add, the total project time can never be faster than the time it takes to cure the foundation. This fundamental bottleneck is what Amdahl’s Law describes.\nWith this understanding, we will be able to set expectations and analyze the benefit of parallelization.",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Parallelization</span>"
    ]
  },
  {
    "objectID": "parallelization.html#types-of-parallelism",
    "href": "parallelization.html#types-of-parallelism",
    "title": "11  Parallelization",
    "section": "11.3 Types of Parallelism",
    "text": "11.3 Types of Parallelism\nParallel processing comes in different flavors and is related to the details of hardware as discussed in Chapter 9. We will necessarily extend the discussion of hardware here, as parallelization is (mostly) inextricably tied to hardware details (we will revisit this in Section 11.8).\n\nMajor types of computational parallelism highlighting their key characteristics, advantages, and potential drawbacks.\n\n\n\n\n\n\n\n\nType\nDescription\nStrengths\nWeaknesses\n\n\n\n\nVectorization (SIMD)\nPerforms same operation on multiple data points simultaneously\nEfficient for data-parallel tasks, uses specialized CPU instructions\nLimited to certain types of operations, data must be contiguous\n\n\nMulti-Threading\nExecutes multiple threads concurrently on a single CPU\nGood for task parallelism, utilizes multi-core processors effectively\nOverhead from thread management, potential race conditions\n\n\nGPU\nUses graphics processing units (GPUs) for parallel computations\nExcellent for massively parallel tasks, high throughput\nSpecialized programming required, data transfer overhead\n\n\nMulti-Device / Distributed\nSpreads computation across multiple machines or devices\nScales to very large problems, can use heterogeneous hardware\nComplex to implement and manage, network latency issues",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Parallelization</span>"
    ]
  },
  {
    "objectID": "parallelization.html#sec-vectorization",
    "href": "parallelization.html#sec-vectorization",
    "title": "11  Parallelization",
    "section": "11.4 Vectorization",
    "text": "11.4 Vectorization\nVectorization in the context of parallel processing refers to special circuits within the CPU wherein the CPU will load multiple data units (e.g. 4 or 8 floating point numbers) in a contiguous block and perform the same instruction on them at the same time. This is also known as SIMD, or Single-Instruction Multiple Data. Think of it like a for loop where instead of each iteration doing one operation, instead it does four or eight at a time.\nThe requirements for SIMD-able code are that:\n\nThe intended section for SIMD is inside the inner-most loop.\nThere are no branches (if-statements) inside the loop body.\n\n::callout-tip Note that indexing an array is actually a branch in the code, as two cases could arise: the index is either inbounds or out-of-bounds. To avoid this, either use for x in collection, for i in eachindex(collection) or for i in 1:n; @inbounds collection[i] though the last of these is discouraged in favor of the prior, safer options. :::\n\nusing BenchmarkTools\n\nfunction prevent_simd(arr)\n    sum = 0\n    for x in arr\n        if x &gt; 0\n            sum += x\n        end\n    end\n    return sum\nend\n\nfunction allow_simd(arr)\n    sum = 0\n    for x in arr\n        sum += max(x, 0)\n    end\n    return sum\nend\n\nlet\n    x = rand(10000)\n\n    @btime prevent_simd($x)\n    @btime allow_simd($x)\nend\n\n  39.791 μs (0 allocations: 0 bytes)\n  4.815 μs (0 allocations: 0 bytes)\n\n\n4983.565210388819\n\n\nIn testing the above code, the allow_simd version should be several times faster than the prevent_simd example. The reason is that prevent_simd has a branch (if x &gt; 0) where the behavior of the code may change depending on the value in arr. Conversely, the behavior of allow_simd is always the same in each iteration, no matter the value of x. This allows the compiler to generate vectorized code automatically.\n\n\n\n\n\n\nTip\n\n\n\nNote that Julia’s compiler is able to identify vectorizable code in many cases, though some cases may benefit from a more manual hint to the compiler through macro annotations (see ?@simd for details). See Section 24.9.4 for more.\n\n\nOther types of parallelism that we will discuss in this chapter have some risk of errors or data corruption if not used correctly. SIMD isn’t prone to issues like this because if the code is not SIMD-able then the compiler will not auto-vectorize the code block.\n\n11.4.1 Hardware\nVectorization is hardware dependent. If the CPU does not support vectorization you will not see speedups from it. Many consumer and professional chips have AVX2 (Advanced Vector Extensions, with the 2 signifying second-generation 256 bit width, allowing four simultaneous 64-bit operations). The next generation is AVX512, having twice the SIMD capacity as AVX2. However, as of 2025 most consumer chips do not yet have that and commercial chips may not actually be faster than the AVX2 due to thermal restrictions (SIMD uses more power and generates more heat).",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Parallelization</span>"
    ]
  },
  {
    "objectID": "parallelization.html#sec-multithreading",
    "href": "parallelization.html#sec-multithreading",
    "title": "11  Parallelization",
    "section": "11.5 Multi-Threading",
    "text": "11.5 Multi-Threading\nThis subsection starts by introducing tasks — lightweight units of computation. Next, we’ll see how tasks can communicate using channels, and then how multiple tasks (and channels) can be leveraged to achieve true parallelism through multi-threading. Think of it as layers building on one another: tasks define work units, channels allow them to share data, and multi-threading enables tasks to run simultaneously on multiple CPU cores. Exact details regarding tasks, channels, and multi-threading vary by language but the general ideas remain the same.\n\n11.5.1 Tasks\nTo understand multi-threading examples, we first need to discuss Tasks, which are chunks of computation that get performed together, but after which the computer is free to switch to a new task. Technically, there are some instructions within a task that will let the computer pause and come back to that task later (such as sleep).\nTasks do not, by themselves, allow for multiple computations to be performed in parallel. For example, one task might be loading a data file from persistent storage into RAM. After that task is complete, the computer continues on with another task in the queue (rendering a web page, playing a song, etc.). In this way even a single processor computer could be “doing multiple things at once” (or “multi-tasking”) even though nothing is running in parallel. The scheduling of the tasks is handled automatically by the program’s compiler or the operating system.\nHere’s an example of a couple of tasks where we write to an array. Despite being called last, the second task should actually write to the array before the first task. This is because we asked the first task to sleep (pause, while allowing the computer to yield to other tasks in the queue)1.\n\nlet\n   shared_array = zeros(5)\n\n   task1 = @task begin\n        sleep(1)\n       shared_array[1] = 1\n\n       println(\"Task 1: \", shared_array)\n   end\n\n   task2 = @task begin\n       shared_array[2] = 2\n       println(\"Task 2: \", shared_array)\n   end\n\n   schedule(task1)\n   schedule(task2)\n   wait(task1)\n   wait(task2)\n\n   println(\"Main: \", shared_array)\nend\n\nTask 2: [0.0, 2.0, 0.0, 0.0, 0.0]\nTask 1: [1.0, 2.0, 0.0, 0.0, 0.0]\nMain: [1.0, 2.0, 0.0, 0.0, 0.0]\n\n\n\n11.5.1.1 Channels\nChannels are a way to communicate data in a managed way between tasks. You specify a type of data that the buffer (a chunk of assigned memory) will contain and how many elements it can hold. It then stores items (via put!) in a first-in-first-out (FIFO) queue, which can be popped off the queue (via take!) by other tasks.\nHere’s an example of a system which generates trades in the financial markets at random time intervals, and a monitoring tasks takes the results and tabulates running statistics:\n\nlet\n\n    # simulate random trades and the associated profits \n    function trade_producer(channel,i)\n1            sleep(rand())\n2            profit = randn()\n            put!(channel, profit)\n            println(\"Producer: Trade Result #$i $(round(profit, digits=3))\")\n    end\n\n    # intake trades via the communication channel\n    function portfolio_monitor(channel,n)\n        sum = 0.0\n3        for _ in 1:n\n            profit = take!(channel)\n            sum += profit\n            println(\"Monitor: Received $(round(profit, digits=3)), Cumulative profit: $(round(sum, digits=3))\")\n        end\n    end\n\n4    channel = Channel{Float64}(32)\n    \n    # Start producer and consumer tasks\n5    @sync begin\n6        for i in 1:5; @async trade_producer(channel,i); end\n        @async portfolio_monitor(channel,5)\n    end\n    \n    \n    # Close the channel and wait for tasks to finish\n    close(channel)\nend\n\n\n1\n\nRandom sleep between 0 and 1 seconds to simulate real trading activity and latency.\n\n2\n\nGenerate a random number from standard normal distribution to simulate profit or loss from a trade.\n\n3\n\nIn this teaching example, we’ve limited the system to produce just five “trades”. In practice, this could be kept running indefinitely via, e.g., while true.\n\n4\n\nCreate a channel with a buffer size of 32 floats (in this limited example, we could have gotten away with just 5 since that’s how many the demonstration produces). In practice, you want this to be long enough that the consumer of the channel never gets so far behind that the channel fills up. The channel is created outside of the @sync block so that channel is in scope when we close it.\n\n5\n\n@sync waits (like wait(task)) for all of the scheduled tasks within the block to complete before proceeding with the rest of the program.\n\n6\n\n@async does the combination of creating a task via @task and schedule-ing in one, simpler call.\n\n\n\n\nProducer: Trade Result #4 1.267\nMonitor: Received 1.267, Cumulative profit: 1.267\nProducer: Trade Result #2 2.488\nMonitor: Received 2.488, Cumulative profit: 3.755\nProducer: Trade Result #5 1.347\nMonitor: Received 1.347, Cumulative profit: 5.102\nProducer: Trade Result #3 0.12\nMonitor: Received 0.12, Cumulative profit: 5.222\nProducer: Trade Result #1 0.393\nMonitor: Received 0.393, Cumulative profit: 5.616\n\n\nThis is really useful for handling events that are “external” to our program. If we were just doing a modeling exercise using static data, then we could control the order of processing and not need to worry about monitoring a volatile source of data. Nonetheless, tasks can still be useful in some cases even if a model is not using “live” data: for example if one of the steps in a model is to load a very large dataset, it may be possible to perform some computations while chunked task requests are queued to load more data from the disk.\nA garbage collector will usually clean up unused channels that are still open. However, it’s a good practice to explicitly close them to ensure proper resource management, clear signaling of completion, and to avoid potential blocking or termination issues in your programs.\n\n\n\n\n\n\nCaution\n\n\n\nIf the task never finishes properly inside the @sync, then your program may get stuck in an infinite loop and hang. Such as if one of the tasks never has a termination condition such as an upper bound on a loop, or a clear way to break out of a while true loop. While not different than a normal loop, such issues become less obvious underneath the layer of task abstractions.\n\n\nThe key takeaway for tasks is that it’s a way to chunk work into bundles that can be run in a concurrent fashion, even if nothing is technically being processed in parallel. The multi-threading and parallel programming paradigms sections build off of tasks so an understanding of tasks is helpful. However, some of the higher level libraries hide the task-based building blocks from you as the user/developer and so an intricate understanding of tasks is not required to be successful in parallelizing your Julia code.\n\n\n\n11.5.2 Multi-Threading Overview\nWhen a program starts on your computer, a process is created which is where the operating system allocates some overhead items (keeping track of the the code and memory allocations and layout) and block of memory in RAM that can be utilized by that process. Different processes do not have access to each other’s allocated memory.\n\n\n\n\n\n\nNote\n\n\n\nReaders may be familiar with starting Excel in different processes. When workbooks are opened within the same process (e.g. when creating a new workbook from Excel’s File menu), the workbooks may seamlessly talk to each other (copy and paste from one to another). However, when Microsoft Excel is opened in different processes, then the workbooks in each respective process do not share memory and cannot create links or use full copy/paste functionality between them (this is what happens when you hold the control button and open Excel multiple times).\n\n\nWithin each process, a main thread is created. That thread is where the running of the code occurs. For the level of the discussion here, you can mainly think of a process as a container with shared memory for threads, which do the real work (as illustrated in Figure 11.2). Besides the main thread, other threads can be created within the process and access the same shared memory.\n\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\nOperating System\n\n\ncluster_1\n\nProcess ABC\n\n\ncluster_2\n\nprocess XYZ\n\n\n\nA\n\nthread 1\n\n\n\nB\n\nthread 1\n\n\n\nC\n\nthread 2\n\n\n\n\n\n\nFigure 11.2: When a program starts, the operating system creates a process for which multiple threads (a main thread plus optional additional threads) share memory.\n\n\n\n\n\nThe advantage of threads is that within a single physical processor, there may be multiple cores. Those cores can access the shared process memory and run tasks from different threads simultaneously. This is a technique that takes advantage of modern processor architecture wherein several (sometimes as many as 32 or more) cores exist on the same chip.\n\n\n\n\n\n\nNote\n\n\n\nTechnically, there are different flavors of threading. While not critical for the understanding and modeling-focused discussion here, here is a bit more detail on different thread types for completeness:\n\nMulti-Tasking. Recall that tasks are chunks of computation that get performed together, but after which the computer is free to switch to a new task. For example, one task might be loading a data file from persistent storage into RAM. After that task is complete, the computer continues on with another task in the queue (rendering a web page, playing a song, etc.). In this way even with a single processor and core, a computer could be “doing multiple things at once” (or “multi-tasking”) even though nothing is running in parallel.\nOperating System Threads or just Threads are managed (as the name implies) at the operating system level. The benefit to this is that operating system level threads have more power: the operating system can pause or limit throughput on running programs if the operating system needs the resources for something it deems higher priority. It’s technically possible to use this power to force a higher priority for your own code, but Julia and many other languages do not offer creating of these types of threads in favor of the next type of threads. Operating system threads have a higher amount overhead (time and memory) involved in creating and destroying the threads.\nGreen threads, cooperative threads, fibers, or user-threads are the type of threads that Julia provides. They are managed at the process (Julia) level and don’t have as much overhead in their creation as operating system threads. Also in Julia, a thread is implemented via Tasks\n\nParallelism in modern computing comes in many flavors, occurs at many different levels (hardware, OS, software, network), and has many different implementations of similar concepts. The terminology of threading in practice and online documentation is prone to confusing even seasoned developers. If you are having a discussion or asking a question, feel free to take the time to ask for clarification on the terminology being used at a given point in time.\n\n\n\n11.5.2.1 Multi-Threading Pitfalls\nDifferent threads being able to access the same memory is a double-edged sword. It is useful because we do not need to create multiple copies of the data in RAM or in the cache2 and can improve the overall throughput of our usually memory-bandwidth-limited machines. The downside is that if we are mutating the shared data for which our program relies upon, then our program may produce unintended results if the modification occurs carelessly. There are a couple of related issues to be aware of:\n\n11.5.2.1.1 Race Conditions\nThe first issue is known as a race condition, which occurs when a block of memory has been read from or written to in an unintended order. For example, if we have two threads which are accumulating a sub-total, each process may read the running sub-total before the other thread has finished its update.\nIn the following example, we use the Threads.@threads to tell Julia to automatically distribute the work across threads.\n\nfunction sum_bad(n)\n    subtotal = 0\n    Threads.@threads for i in 1:n\n        subtotal += i\n    end\n    subtotal\nend\n\nsum_bad(100_000)\n\n673251753\n\n\nThe result will be less than the expected sum (5000050000) due to a race condition. Here’s what happens:\n\nMultiple threads read the current value of subtotal simultaneously.\nEach thread adds its own, local value to that reading.\nOnly one thread writes its result back to subtotal first.\nA different thread then overwrites subtotal with its calculation based on the out-dated starting point for subtotal.\n\nThis means some thread contributions are lost when they overwrite each other’s results. The threads may not see each other’s updates, leading to missing values in the final sum.\n\n\n\n11.5.2.2 Avoiding Multi-threading Pitfalls\nWe will cover several ways to manage multi-threading race conditions, but it is the recommendation of the authors to primarily utilize higher level library code, which will be demonstrated after covering some of the more basic, manual techniques.\n\n11.5.2.2.1 Chunking up work into single-threaded work\nFirst, let’s level-set with a single-threaded result:\n\nfunction sum_single(a)\n    s = 0\n    for i in a\n        s += i\n    end\n    s\nend\n@btime sum_single(1:100_000)\n\n  1.500 ns (0 allocations: 0 bytes)\n\n\n5000050000\n\n\nNote that in the single-threaded case, Julia is able to identify this common pattern and use a shortcut, calculating the sum of the integers \\(1\\) through \\(n\\) as \\(\\frac{n(n+1)}{2}\\) through a compiler optimization and essentially avoid the loop entirely.\nWe can implement a correct threaded version by splitting the work into different threads, each of which is independent. Then, we can aggregate the results of each of the chunks.\n\nfunction sum_chunker(a)\n\n1    chunks = Iterators.partition(1:a, a ÷ Threads.nthreads())\n   \n2    tasks = map(chunks) do chunk\n       Threads.@spawn sum_single(chunk)\n    end\n   \n    chunk_sums = fetch.(tasks)\n   \n    return sum_single(chunk_sums)\n\nend\n\n@btime sum_chunker(100_000)\n\n\n1\n\nCreate chunks of the integer range from 1 to a. Iterators.partition(1:a, a ÷ Threads.nthreads()) splits the range 1:a into contiguous subranges (chunks), each of size a ÷ Threads.nthreads(). For example, if a = 100_000 and nthreads() = 4, you’ll get four chunks of size 25_000.\n\n2\n\nCreate a set of tasks (futures) using Threads.@spawn that call sum_single(chunk) on each of the chunks. This initiates parallel computation.\n\n\n\n\n  4.334 μs (72 allocations: 5.72 KiB)\n\n\n5000050000\n\n\n\n\n11.5.2.2.2 Using Locks\nLocks prevent memory from being accessed from more than one thread at a time.\n\nfunction sum_with_lock(n)\n1    subtotal = 0\n    \n2    lock = ReentrantLock()\n\n3    Threads.@threads for i in 1:n\n4        Base.@lock lock begin\n            subtotal += i\n        end\n    end\n\n    subtotal\nend\n@btime sum_with_lock(100_000)\n\n\n1\n\nInitialize a running total to zero.\n\n2\n\nCreate a reentrant lock to ensure only one thread updates subtotal at a time.\n\n3\n\nParallelize the loop over 1 to n using Threads.@threads.\n\n4\n\nAcquire the lock before updating the shared variable subtotal. This ensures that only one thread updates subtotal at a time, preventing race conditions. The lock is automatically released at the end of the block.\n\n\n\n\n  16.377 ms (199514 allocations: 3.05 MiB)\n\n\n5000050000\n\n\n\n\n11.5.2.2.3 Using Atomics\nAtomics are certain primitive values with a reduced set of operations for which Julia and the compiler can automatically create thread-safe code. This is often significantly faster than the context-switching overhead needed with locking and unlocking memory for threaded tasks. Compared with locks, atomics are simpler to implement and easier to reason about. The downside is that atomics are limited to the available primitive atomics types and methods.\n\nfunction sum_with_atomic(n)\n1    subtotal = Threads.Atomic{Int}(0)\n    Threads.@threads for i in 1:n \n2        Threads.atomic_add!(subtotal, i)\n    end\n    subtotal[]\nend\n\n@btime sum_with_atomic(100_000)\n\n\n1\n\nInitialize an atomic integer (Threads.Atomic{Int}) to store the subtotal. An atomic variable ensures that increments are performed atomically, preventing race conditions without needing explicit locks.\n\n2\n\nAtomically add i to subtotal. The Threads.atomic_add! function ensures that the addition and update of subtotal happens as one atomic step, preventing multiple threads from interfering with each other’s updates.\n\n\n\n\n  641.250 μs (53 allocations: 5.31 KiB)\n\n\n5000050000",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Parallelization</span>"
    ]
  },
  {
    "objectID": "parallelization.html#sec-GPUs",
    "href": "parallelization.html#sec-GPUs",
    "title": "11  Parallelization",
    "section": "11.6 GPU and TPUs",
    "text": "11.6 GPU and TPUs\n\n11.6.1 Hardware\nGraphics Processing Units (GPUs) and Tensor Processing Units (TPUs) are hardware accelerators for massively parallel computations. A TPU is very similar to a GPU but have special ability to handle data types and instructions that are more specialized for linear algebra operations; going forward we will simply refer to these types of accelerators as GPUs.\nGPUs have similar components as the CPU as discussed in Chapter 9. They have RAM, caches for the cores, and cores that run the coded instructions on the data. The differences from a CPU are primarily:\n\nA GPU typically has thousands of cores while a CPU generally has single or double digit cores.\n\nThe cores typically operate at a slower clock speed than CPUs, relying on the sheer number of cores to perform computations faster.\n\nThe GPU cores essentially have to be running the same set of instructions on all of the data, not unlike vectorization (Section 11.4).\n\nGPU code is not suited for code with branching conditions (e.g. if statements) and so is more limited in the kinds of computations it can handle compared to the CPU.\n\nThe RAM is typically much more constrained, typically less than a quarter of what primary RAM might be.\n\nAs a result, GPUs may need strategies to move chunks of data to and from the GPU memory for moderately large datasets. Further, it’s actually fairly common to use a lower-precision datatype (e.g. Float16 or Float32) to improve overall program throughput at the cost of some precision.\n\nThe caches are similar in concept to CPU, but unlike most CPU caches, there is relative locality to data (wherein core #1 will have much quicker access to a different subset of data than, say, core #1024).\nA GPU is usually a secondary device of sorts: it physically and in device architecture is separate from the CPU. The CPU remains in charge of overall computer execution.\n\nThe implication of this (as with any movement of memory) is that there is overhead to moving data to and from the GPU. Your calculations will need to be in the single milliseconds range of time in order to start to see benefit from utilizing a GPU.\nTo some extent, separable CPU, RAM, and GPU is changing with some of the latest computer hardware. For example, the M-series of Apple processors have the CPU, GPU, and RAM in a single tightly integrated package for efficiency and computational power.\n\n\n\n11.6.1.1 Notable Vendors and Libraries\nLike the difference between x86 and ARM architectures, GPU also have specific architectures which vary by the vendor. To make full use of the hardware, the vendors need to (1) provide device drivers which allow the CPU to talk to the GPU, and (2) provide the libraries (lower level application programming interfaces, or APIs) which allow developers to utilize different hardware features without needing to write machine code.\nAs of the mid 2020s, here are the most important GPU vendors and the associated programming library for utilizing their specific hardware:\n\nImportant GPU and TPU vendors and the associated library/interface.\n\n\n\n\n\n\n\nVendor\nHardware\nAPI Library/Package\n\n\n\n\nNVIDIA\nGeforce, GTX/RTX, various Data Center focused hardware\nCUDA\n\n\nAMD\nRadeon, various Data Center focused hardware\nROCm\n\n\nIntel\nCore, Xeon, Arc processors\nOneAPI\n\n\nApple\nM Series processors\nMetal\n\n\nGoogle\nTensor processors\nXLA (via TensorFlow or JAX)\n\n\n\n\n\n\n11.6.2 Utilizing a GPU\nWith some of the key conceptual differences between CPUs and GPUs explained, let’s explore how to incorporate these powerful hardware accelerators. We will use Julia libraries to illustrate GPU programming, though the concepts are generally applicable to other high-level languages that offer GPU interface libraries.\n\n11.6.2.1 Julia GPU Libraries\nThere’s essentially two types of GPU programming we will discuss here:\n\nArray-based programming, where arrays are stored and operated on directly on the GPU memory. This approach abstracts away the low-level details, allowing you to work with familiar array operations that are automatically executed on the GPU.\nCustom kernels, which are specialized functions that define exactly how each GPU thread should process data in parallel. A kernel explicitly specifies the computation that each GPU thread will perform on its portion of the data.\n\n\n\n\n\n\n\nNote\n\n\n\nKernels in this context are specialized functions that run directly on the GPU. Rather than relying solely on high-level array operations, kernels explicitly define the sequence of low-level, parallel instructions executed across many GPU threads. In other words, a kernel directly expresses the computation you want to perform on the data, enabling fine-grained control over GPU execution.\n\n\nA third approach would be to implement GPU code in a low-level vendor toolkit (such as C++ and associate CUDA libraries), but this approach will not be illustrated here.\nJulia has wonderful support for several of the primary vendors (at the time of writing, CUDA, Metal, OneAPI, and ROCm) via the JuliaGPU organization. Installation of the required dependencies is also very straightforward and the interfaces at the array and generated kernel levels are very similar. The differences are obvious at the lower level vendor-API wrappers (which is the lower-level technique that will not be covered here).\nThe benefit of the consistency of the higher level libraries we will use here is that examples written for one of the types of accelerators will be largely directly translatable to another. This is especially true for array programming, though less so for the kernel style as architecture-specific considerations often creep in3.\n\n\n\n\n\n\nNote\n\n\n\nThis book will be rendered on a Mac and therefore the examples will use Metal in order to run computational cells, however we’ll show a CUDA translation for some of the examples in order to show the straight-forward nature of translating higher level GPU code in Julia is.\n\n\n\n\n\nGPU API\nGPU Array Type\nKernel Macro\n\n\n\n\nCUDA\nCuArray\n@cuda\n\n\nMetal\nMtlArray\n@metal\n\n\noneAPI\noneArray\n@oneAPI\n\n\nROCm\nROCArray\n@roc\n\n\n\n\n\n11.6.2.2 Array Programming on the GPU\nFirst described in Section 6.5, array programming eschews writing loops and and instead favors initializing blocks of heap-allocated memory and filling it with data to be operated on at a single point in time. While this is often not the most efficient way to utilize CPUs, it’s essentially the required style of code to utilize GPUs.\nFor the example below, we will calculate the present value of a series of cashflows across a number of different scenarios. An explanation of the code is given below the example.\n\nusing Metal\n\n1function calculate_present_values!(present_values,cashflows, discount_matrices)\n    # Perform element-wise multiplication and sum along the time dimension\n2    present_values .= sum(cashflows .* discount_matrices, dims=1)\nend\n\n# Example usage using 100 time periods, 100k scenarios\nnum_scenarios = 10^5\npvs = zeros(Float32,1,num_scenarios)\n3cashflows = rand(Float32, 100)\n4discount_matrices = rand(Float32, 100, num_scenarios)\n\n# copy the data to the GPU\npvs_GPU = MtlArray(pvs)\n5cashflows_GPU = MtlArray(cashflows)\ndiscount_matrices_GPU = MtlArray(discount_matrices)\n\n@btime calculate_present_values!($pvs,$cashflows, $discount_matrices)\n6@btime calculate_present_values!($pvs_GPU,$cashflows_GPU, $discount_matrices_GPU)\n\n\n1\n\nThe function calculate_present_values! is written the same way as if we were just writing CPU code. Note that we are also passing a pre-allocated vector, present_values to store the result. This will allow us to isolate the performance of the computation, rather than including any overhead of allocating the array for the result.\n\n2\n\nThe code is broadcasted across the first dimension so that the single set of cashflows is discounted for each scenario’s discount vector.\n\n3\n\nMetal only supports 32 bit floating point (some CUDA hardware will support 64 bit floating point)\n\n4\n\nUsing 100 thousand scenarios for this example.\n\n5\n\nMtlArray(array) will copy the array values to the GPU.\n\n6\n\nNote that the data still lives on the GPU and is of the MtlMatrix (a type alias for a 2-D MtlArray).\n\n\n\n\n\n[ Info: Precompiling Metal [dde4c033-4e86-420c-a63e-0dd931031962] (cache misses: wrong dep version loaded (4), incompatible header (10))\n\n[ Info: Precompiling AtomixMetalExt [368c01e8-f8da-5dca-a1ea-818da1f33961] (cache misses: wrong dep version loaded (4), incompatible header (12))\n\n[ Info: Precompiling SpecialFunctionsExt [05d8ebbe-653a-54ed-ba56-24759129d732] (cache misses: wrong dep version loaded (4), incompatible header (12))\n\n\n\n\n  2.625 ms (6 allocations: 38.56 MiB)\n  64.166 μs (515 allocations: 13.93 KiB)\n\n\n1×100000 MtlMatrix{Float32, Metal.PrivateStorage}:\n 27.0273  24.1975  27.7725  26.4674  …  25.4061  26.6451  26.8603  26.6654\n\n\nThe testing suggests approximately 200 times faster computation when performed on the GPU. Note however, that does not include the overhead of (1) moving the data to the GPU (in the initial MtlArray(cashflows) call), or (2) returning the data to the CPU (since the return type for the GPU version is MtlArray). We can measure this overhead by wrapping the data transfer inside another function and benchmarking it:\n\nfunction GPU_overhead_test(present_values, cashflows,discount_matrices)\n    pvs_GPU = MtlArray(present_values)\n    cashflows_GPU = MtlArray(cashflows)\n    discount_matrices_GPU = MtlArray(discount_matrices)\n\n    calculate_present_values!(pvs_GPU,cashflows_GPU, discount_matrices_GPU)\n\n    Array(pvs_GPU) # convert to CPU array\nend\n\n@btime GPU_overhead_test($pvs,$cashflows,$discount_matrices)\n\n  4.668 ms (993 allocations: 443.91 KiB)\n\n\n1×100000 Matrix{Float32}:\n 27.0273  24.1975  27.7725  26.4674  …  25.4061  26.6451  26.8603  26.6654\n\n\nWith the additional overhead, the computation on the GPU takes more total time than if the work were done just on the CPU. This is a very simple example, and the balance tips heavily in favor of the GPU when:\n\nThe computational demands are significantly higher (e.g. we were to do more calculations than just a simple multiply/divide/sum).\nThe data size grows bigger.\n\n\n\n\n\n\n\nNote\n\n\n\nThe previous example can be translated to CUDA by simply exchanging MtlArray for CuArray.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis example again underscores that hardware parallelization is not an automatic “win” for performance. A lot of uninformed discussion around modeling performance is to simply try to get things to run on the GPU and it is often not the case that the models will run faster. Further, as the modeling logic gets more complex, it does require greater care to keep in mind GPU constraints (acceptable data types, memory limitations, avoiding scalar operations, data transfer between CPU and GPU, etc.). A best practice is to contemplate sequential performance and memory usage before leveraging GPU accelerators.\n\n\n\n\n11.6.2.3 Kernel Programming on the GPU\nAnother approach to GPU programming is often referred to as kernel programming, or being much more explicit about how a computation is performed. This is as opposed to the declarative approach in the array-oriented style (Section 11.6.2.2) wherein we specified what we wanted the computation to be.\nThe key ideas here are that we need to manually specify several aspects which came ‘free’ in the array-oriented style. The tradeoff is that we can be more fine-tuned about how the computation leverages our hardware, potentially increasing performance.\nThe GPU libraries in Julia abstract much of the low level programming typically necessary for this style of programming, but we still need to explicitly look at:\n\nHow the GPU will iterate across different cores/threads threads.\nHow many threads to utilize, the optimal number depends on the shape of the computation (long vectors, multi-dimensional arrays), memory constraints, and hardware specifics.\n\n\nGPU threads: Individual units of execution within a kernel. Each thread runs the same kernel code but operates on a different portion of the data.\n\n\nHow to chunk (group) the data to distribute the data to the different GPU threads\n\nOur strategy for the present values example will be to distribute the work such that different GPU threads are working on different scenarios. Within a scenario, the loop is a very familiar approach: initialize a subtotal to zero and then accumulate the calculated present values.\n\nfunction calculate_present_values_kernel!(present_values,cashflows, discount_matrices)\n1    idx = thread_position_in_grid_1d()\n\n2    pv = 0.0f0\n    for t in 1:size(cashflows, 1)\n3        pv += cashflows[t] * discount_matrices[t, idx]\n    end\n\n4    present_values[idx] = pv\n5    return nothing\nend\n\n\n1\n\nAs the work is distributed across threads, thread_position_in_grid_1d() will give the index of the current thread so that we can index data appropriately for the work as we decide to split it up (we’ve split up the work by scenario in this example).\n\n2\n\nRecall that we are working with Float32 on the GPU here, so the zero value is set via the f0 notation indicating a 32-bit floating point number.\n\n3\n\nThe loop is across timesteps within each thread, while the thread index is tracked with idx.\n\n4\n\nThe result is written to the pre-allocated array of present values, and we avoid race conditions because the different threads are working on different scenarios.\n\n5\n\nWe don’t explicitly have to return nothing here, but it makes it extra clear that the intention of the function is to mutate the present_values array given to it. This mutation intention is also signaled by the ! convention in the function name.\n\n\n\n\ncalculate_present_values_kernel! (generic function with 1 method)\n\n\nThe kernel above was fairly similar to how we might write code for CPU-threaded approaches, but we now need to specify the technicals of launching this on the GPU. The threads defines how many independent calculations to run at a given time, and the maximum will be dependent on the hardware used. The groups argument defines the number of threads that share memory and synchronize results together (meaning that group will wait for all threads to finish before moving onto the next chunk of data). The push-pull here is that threads that can share data avoid needing to create duplicate copies of that data in memory. However, if there is variability in how long each calculation will take, then the waiting time for synchronizing results may slow the overall computation down.\nOur task utilizes shared memory of the cashflows for each thread, so through some experimentation in advance, we find that a relatively large group size of ~512 is optimal.\nWe bring this all together through the use of the kernel macro @metal:\n\nthreads = 1024\ngroups = cld(num_scenarios, 512)\n\n@btime @metal threads=$threads groups=$groups calculate_present_values_kernel!(\n    $pvs_GPU,\n    $cashflows_GPU, \n    $discount_matrices_GPU\n    )\n\n  16.834 μs (149 allocations: 3.33 KiB)\n\n\nMetal.HostKernel{typeof(calculate_present_values_kernel!), Tuple{MtlDeviceMatrix{Float32, 1}, MtlDeviceVector{Float32, 1}, MtlDeviceMatrix{Float32, 1}}}(Main.calculate_present_values_kernel!, Metal.MTL.MTLComputePipelineStateInstance (object of type AGXG16XFamilyComputePipeline))\n\n\nThis is approximately seven times faster than the array-oriented style above, meaning that the GPU kernel version’s computation is over 1000 times faster than the CPU version. However, we saw previously that the cost of moving the data to the GPU memory and then back to the CPU memory was the biggest time sink of all - again we’d need to have more scale in the problem to make offloading to the GPU beneficial overall.\n\n\n\n\n\n\nNote\n\n\n\nThe Metal GPUs are able to iterate threads across three different dimensions. In the prior example, we only used one dimension and thus used thread_position_in_grid_1d(). If we were distributing the threads across, say, three dimensions then we would use thread_position_in_grid_2d().\nHow do you determine how many dimensions to use? A good approach is to mimic the data you are trying to parallelize. In the example of calculating a vector of present values across 100k scenarios, that was the primary ‘axis of parallelization’. If instead of a one-dimensional set of cashflows (e.g. a single asset with fixed cashflows), we had a two-dimensional set of cashflows (e.g. a portfolio of many assets), then we may find the best balance of code simplicity and performance to iterate across two dimensions of threads (but we are still limited by the same number of total available threads).\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe above example would be translated to CUDA by changing just a few things:\n\nThe thread indexing would be idx = threadIdx().x instead of i = thread_position_in_grid_1d()\nThe GPU arrays should be created with CuArray instead of MtlArray.\nThe kernel macro would be @cuda threads=1024 calculate_present_values_kernel!(...) instead of @metal threads=threads groups=groups calculate_present_values_kernel(...). The memory sharing and synchronizing between threads is more manual than Metal.jl, but this is not strictly necessary for our example.\n\nfunction calculate_present_values_kernel!(present_values,cashflows, discount_matrices)\n    idx = threadIdx().x \n\n    pv = 0.0f0\n    for t in 1:size(cashflows, 1)\n        pv += cashflows[t] * discount_matrices[t, idx]\n    end\n\n    present_values[idx] = pv\n    return nothing\nend\n\ngroups = cld(num_scenarios, 512)\n\n@cuda threads=threads calculate_present_values_kernel!(\n    pvs_GPU,\n    cashflows_GPU, \n    discount_matrices_GPU\n    )",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Parallelization</span>"
    ]
  },
  {
    "objectID": "parallelization.html#sec-multi-device",
    "href": "parallelization.html#sec-multi-device",
    "title": "11  Parallelization",
    "section": "11.7 Multi-Processing / Multi-Device",
    "text": "11.7 Multi-Processing / Multi-Device\nMultiple device, or multi-device computer refers to using separate groups of memory/processor combinations to accomplish tasks in parallel. This can be as simple as multiple distinct cores on within a single desktop computer, or many separate computers networked across the internet, or many processors within a high performance cluster or a computing-as-a-service provider like Amazon Web Services or JuliaHub.\nEverything discussed previously related to hardware (Chapter 9, Section 11.5, Section 11.6) continues to apply. The additional complexity is attempting to synchronize the computation across multiple sets of the same (homogeneous) or different (heterogeneous) hardware.\nAs you might imagine, approaches to multi-device computing can vary widely. Julia’s approach tries to strike the balance between capability and user-friendliness and uses a primary/worker model wherein one of the processors is the main coordinator while other processors are “workers”. If only one processor is started, then the main processor is also a worker processor. This main/worker approach uses a “one-sided” approach to coordination. The main worker utilizes high level calls and the workers respond, with some of the communication and hand-off handled by Julia transparently from the user’s perspective.\nA useful mental model is the asynchronous task-based concepts discussed in Section 11.5.1, as the main worker will effectively queue work with the worker processors. Because there may be a delay associated with the computation or the communication between the processors, the worker runs asynchronously.\n\n\n\nDescription\nTask API\nDistributed Analogue\n\n\n\n\nCreate a new task\nTask()\n@spawnat\n\n\nRun task asynchronously\n@async\n@spawnat\n\n\nRetrieve task result\nfetch\nfetch\n\n\nWait for task completion\n@sync\nsync\n\n\nCommunication between tasks\nChannel\nRemoteChannel\n\n\n\nAdapting the trade producer and monitor example from above to run on multiple processors (see #sec-channels to review the base model and algorithm), we make a few key changes:\n\nusing Distributed loads the Distributed standard Julia library, providing the interface for multi-processing across different hardware.\naddprocs(n) will add n worker processors (the main processor is already counted as one worker). When adding local machine processors, the processors are part of the local machine. This starts new Julia processes (you can see this in the task manager of the machine) which inherit the package environment (i.e. Project.toml and environment variables) from the main process; this does not occur automatically if not part of the same local machine.\n\nTo add processors from other machines, see the Distributed Computing section of the Julia docs.\n\nmyid() is the identification number of the given processor that’s been spun up.\nWe use a RemoteChannel instead of a Channel to facilitate communication across processors.\nInstead of @async, we use @spawnat n to create a task for processor number n (or :any will automatically assign a processor).\n\nSee\n\nusing Distributed\nlet\n\n    # Add worker processes if not already added\n    if nworkers() == 0\n        addprocs(4)  # Add 4 worker processes\n    end\n    \n    @everywhere function trade_producer(channel, i)\n        sleep(rand())\n        profit = randn()\n        put!(channel, profit)\n        println(\"Producer $(myid()): Trade Result #$i $(round(profit, digits=3))\")\n    end\n    \n    @everywhere function portfolio_monitor(channel, n)\n        sum = 0.0\n        for _ in 1:n \n            profit = take!(channel)\n            sum += profit\n            println(\"Monitor $(myid()): Received $(round(profit, digits=3)), Cumulative profit: $(round(sum, digits=3))\")\n        end\n    end\n    \n    function run_distributed_simulation()\n        channel = RemoteChannel(() -&gt; Channel{Float64}(32))\n    \n        # Start producer and consumer tasks\n        @sync begin\n            for i in 1:5\n                @spawnat :any trade_producer(channel, i)\n            end\n            @spawnat :any portfolio_monitor(channel, 5)\n        end\n    \n        # Close the channel and wait for tasks to finish\n        close(channel)\n    end\n    \n    # Run the simulation\n    run_distributed_simulation()\nend\n\nProducer 1: Trade Result #3 -0.697\nMonitor 1: Received -0.697, Cumulative profit: -0.697\nProducer 1: Trade Result #1 0.29\nMonitor 1: Received 0.29, Cumulative profit: -0.407\nProducer 1: Trade Result #5 0.657\nMonitor 1: Received 0.657, Cumulative profit: 0.25\nProducer 1: Trade Result #4 -0.906\nMonitor 1: Received -0.906, Cumulative profit: -0.656\nProducer 1: Trade Result #2 -1.882\nMonitor 1: Received -1.882, Cumulative profit: -2.537\n\n\nGiven the similarity to the single-process task-based version above, what’s the motivation for this bothering with a distributed approach? A few differences:\n\nIn this simplified example, we are simply starting additional Julia processes on the same machine. Like a threaded approach, the work will be split across the same multi-core processor. In this context, the main difference is that the processes do not share memory.\n\nCommunicating across processes generally has a little bit more overhead than communicating across threads.\n\nIf distributing across machines, avoiding memory sharing is advantageous if using the different machines that have their own memory stores, which need not compete with the main process (such as distributed chunks of large datasets). This essentially helps with memory constrained problems since you are no longer limited by the memory size or throughput of a single machine.\nThe worker processors don’t need to be the same architecture as the main processor, allowing usage of different machines or cloud computing that is communicating with a local main process.",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Parallelization</span>"
    ]
  },
  {
    "objectID": "parallelization.html#sec-parallel-programming-models",
    "href": "parallelization.html#sec-parallel-programming-models",
    "title": "11  Parallelization",
    "section": "11.8 Parallel Programming Models",
    "text": "11.8 Parallel Programming Models\nThe previous sections have explained the different parallel programming models and how to directly utilize them to harness additional computing power. Each approach (multi-threading, GPU, distributed processing, etc.) has unique considerations and trade-offs. These approaches in Julia are generally much more accessible to beginning and intermediate users than other languages, but admittedly still requires a decent amount of thought and care.\nIt is possible, if you are willing to give up some fine-grained control, to utilize some higher level approaches which look to abstract away some of the particularities of the implementation.\n\n11.8.1 Map-Reduce\nMap-Reduce (Section 6.4.4) operations are inherently parallelizable and various libraries provide parallelized versions of the base mapreduce. This is the workhorse function of many ‘big data’ workloads and many statistical operations are versions of mapreduce.\n\n11.8.1.1 Multi-Threading\n\n11.8.1.1.1 OhMyThreads\nOhMyThreads.jl provides the threaded versions of essential functions such as tmap, tmapreduce,tcollect, and tforeach (see ?tbl-funcional-methods). In most cases, the chunking and data sharing is handled automatically for you.\n\nimport OhMyThreads\n@btime OhMyThreads.tmapreduce(x -&gt; x, +, 1:100_000)\n\n  5.375 μs (75 allocations: 5.77 KiB)\n\n\n5000050000\n\n\n\n\n11.8.1.1.2 ThreadsX\nThreadsX.jl is built off of the wonderful Transducers.jl package, though the latter is a bit more advanced (more abstract, but as a result more composable and powerful). ThreadsX provides threaded versions of many popular base functions. It offers a wider set of ready-made threaded functions, but has a much more complex codebase. For the vast majority of threading needs, OhMyThreads.jl should be sufficient and performant. See the documentation for all of the implemented functions, but for our illustrative example:\n\nimport ThreadsX\n@btime ThreadsX.mapreduce(x -&gt; x, +, 1:100_000)\n\n\n[ Info: Precompiling DistributionsTestExt [ffbe0ea5-a612-5ff7-aaf5-cac02eef3019] (cache misses: wrong dep version loaded (8), incompatible header (8), mismatched flags (2))\n\n\n\n\n  42.042 μs (890 allocations: 67.48 KiB)\n\n\n5000050000\n\n\n\n\n\n11.8.1.2 Multi-Processing\nreduce(op,pmap(f,collection)) will use a distributed map and reduce the resulting map on the main thread. This pattern works well if each application of f to elements of collection is costly.\n@distributed (op) for x in collection; f(x); end is a way to write the loop with the reduction op for which the f need not be costly.\nThe difference between the two approaches is that with pmap, collection is made available to all workers. In the @distributed approach, the collection is partitioned and only a subset is sent to the designated workers.\nHere’s an example of both of these, calculating a simple example of counting coin flips:\n\n# this is a example of poor utilization of pmap, since the operation is \n# fast and the overhead of moving the whole collection dominates\n@btime reduce(+,pmap(x -&gt; rand((0,1)),1:10^3))\n\n  24.064 ms (5076 allocations: 144.66 KiB)\n\n\n504\n\n\n\nfunction dist_demo()\n    @distributed (+) for _ in 1:10^5\n        rand((0,1))\n    end\nend\n\n@btime dist_demo()\n\n  75.542 μs (21 allocations: 1.11 KiB)\n\n\n50176\n\n\n\n\n\n11.8.2 Array-Based\nArray based approaches will often utilize the parallelism of SIMD on the CPU or many cores on the GPU. It’s as simple as using generic library calls which will often be optimized at the compiler level. Examples:\n\nlet\n    x = rand(Float32,10^8)\n    x_GPU = MtlArray(x)\n    @btime sum($x)\n    @btime sum($x_GPU)\nend\n\n  4.906 ms (0 allocations: 0 bytes)\n  1.250 ms (800 allocations: 19.99 KiB)\n\n\n5.000343f7\n\n\nsum(x) compiles to SIMD instructions on the CPU, while using the GPU array type in sum(x_GPU) is enough to let the compiler dispatch on the GPU type and emit efficient, parallelized code for the GPU.\nDistributed array types allow for large datasets to effectively be partitioned across multiple processors, and have implementations in the DistributedArrays.jl and Dagger.jl libraries.\n\n\n11.8.3 Loop-Based\nLoops which don’t have race conditions can easily become multi-threaded. Here, we have three versions of updating a collection to square the contained values:\nBasic single-threaded:\n\nlet v = collect(1:10000)\n\n    for i in eachindex(v)\n        v[i] = v[i]^2\n    end\n    v[1:3]\nend\n\n3-element Vector{Int64}:\n 1\n 4\n 9\n\n\nUsing multi-threading\n\nlet v = collect(1:10000)\n    Threads.@threads  for i in eachindex(v)\n        v[i] = v[i]^2\n    end\n    v[1:3]\nend\n\n3-element Vector{Int64}:\n 1\n 4\n 9\n\n\nUsing multi-processing:\n\nusing SharedArrays\nlet\n    v = collect(1:10_000)\n    sV = SharedArray{eltype(v)}(length(v))\n    @sync Distributed.@distributed for i in eachindex(v)\n        sV[i] = v[i]^2\n    end\n    sV[1:3]\nend\n\n3-element Vector{Int64}:\n 1\n 4\n 9\n\n\nFor more advanced usage, including handling shared memory see Section 11.7 and Section 11.5.\n\n\n11.8.4 Task-Based\nTask based approaches attempt to abstract the scheduling and distribution of work from the user. Instead of saying how the computation should be done, the user specifies the intended operations and allows the library to handle the workflow. The main library for this in Julia is Dagger.jl.\nEffectively, the library establishes a network topology (a map of how different processor nodes can communicate) and models the work as a directed, acyclic graph (a DAG, which is like a map of how the work is related). The library is then able to assign the work in the appropriate order to the available computation devices. The benefit of this is most apparent with complex workflows or network topologies where it would be difficult to manually assign, communicate, and schedule the workflow.\nHere’s a very simple example which demonstrates Dagger waiting for the two tasks which work in parallel (we already added multiple processors to this environment in Section 11.7):\n\nimport Dagger\n\n# This runs first:\na = Dagger.@spawn rand(100, 100)\n\n# These run in parallel:\nb = Dagger.@spawn sum(a)\nc = Dagger.@spawn prod(a)\n\n# Finally, this runs:\nwait(Dagger.@spawn println(\"b: \", fetch(b), \", c: \", fetch(c)))\n\n\n[ Info: Precompiling Dagger [d58978e5-989f-55fb-8d15-ea34adc7bf54] (cache misses: wrong dep version loaded (2), incompatible header (4))\n\n[ Info: Precompiling DistributionsExt [c815768c-a342-5444-8dea-b375f31b26a5] (cache misses: wrong dep version loaded (2), incompatible header (4))\n\n[ Info: Precompiling GraphVizSimpleExt [8f367522-86d3-5221-bdf5-df974a7b0ff8] (cache misses: wrong dep version loaded (2), incompatible header (4))\n\n[ Info: Precompiling MetalExt [749d1e2e-b2a6-505a-add8-46168822f7b1] (cache misses: wrong dep version loaded (2), incompatible header (2))\n\n\n\n\nb: 4962.361919535384, c: 0.0",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Parallelization</span>"
    ]
  },
  {
    "objectID": "parallelization.html#choosing-a-parallelization-strategy",
    "href": "parallelization.html#choosing-a-parallelization-strategy",
    "title": "11  Parallelization",
    "section": "11.9 Choosing a Parallelization Strategy",
    "text": "11.9 Choosing a Parallelization Strategy\nThere is no one-size-fits-all strategy to parallelization. Here are some general guides to thinking about what parallelization technique to try given the circumstances:\n\nCPU-bound workloads with manageable memory demands: If your entire dataset fits comfortably in RAM and your operations are primarily arithmetic or straightforward loops, start by optimizing your single-threaded performance and consider vectorization (SIMD) for inner loops and multi-threading for parallelizable tasks. This approach leverages your CPU cores efficiently without introducing significant complexity.\nLarge-scale linear algebra or highly data-parallel computations: If your problem involves large matrix operations, linear algebra routines, or embarrassingly parallel computations that can be batched over many independent elements, a GPU or other specialized accelerators may be beneficial. GPUs excel at uniform computations over large datasets and can provide substantial speedups—assuming data transfer overhead and memory constraints are managed effectively. Note that standard linear algebra libraries are likely to already parallelize on the CPU without any explicit parallelization needed to be coded on your part.\nDistributing work across multiple machines or heterogeneous resources: If you need to scale beyond a single machine’s CPU and GPU capabilities—whether due to extremely large datasets, the need for concurrent access to geographically distributed resources, or leveraging specialized hardware—then consider distributed computing. Spreading tasks across multiple processes, servers, or clusters can scale performance horizontally. Just keep in mind the overhead of communication, potential data partitioning strategies, and the complexity of managing a distributed environment.\n\nIn practice, you may find that a combination of these approaches is ideal: start simple, measure performance, and iterate. By understanding your workload and hardware constraints, you can make informed decisions that balance complexity, cost, and the performance gains of parallel computing.",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Parallelization</span>"
    ]
  },
  {
    "objectID": "parallelization.html#references",
    "href": "parallelization.html#references",
    "title": "11  Parallelization",
    "section": "11.10 References",
    "text": "11.10 References\n\nhttps://book.sciml.ai/notes/06-The_Different_Flavors_of_Parallelism/\nhttps://docs.julialang.org/en/v1/manual/parallel-computing/\nhttps://enccs.github.io/julia-for-hpc/",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Parallelization</span>"
    ]
  },
  {
    "objectID": "parallelization.html#footnotes",
    "href": "parallelization.html#footnotes",
    "title": "11  Parallelization",
    "section": "",
    "text": "Technically, it’s possible that the second task doesn’t write to the array first. This could happen if there’s enough tasks (from our program or others on the computer) that saturate the CPU during the first task’s sleep period such that the first task gets picked up again before the second one does.↩︎\nThere are some chips which do not have access to the same memory in a multi-threading context, and are known as non-uniform memory access (NUMA). These architectures work more like those in Section 11.7.↩︎\nThe KernelAbstractions.jl library actually allows you to write generic kernels which then get compiled into different code depending on the backend you are using.↩︎",
    "crumbs": [
      "Foundations: Building Performant Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Parallelization</span>"
    ]
  },
  {
    "objectID": "computational-thinking.html",
    "href": "computational-thinking.html",
    "title": "Interdisciplinary Concepts and Applications",
    "section": "",
    "text": "This section explores concepts from related fields that enhance financial modeling. We examine how ideas from computer science, statistics, and other disciplines intersect with and improve modeling practices. Through examples, we’ll uncover the theoretical foundations supporting advanced techniques. This interdisciplinary approach aims to broaden your perspective and equip you with diverse tools for tackling complex financial problems.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "12  Applying Software Engineering Practices",
    "section": "",
    "text": "12.1 Chapter Overview\nModern software engineering practices such as version control, testing, documentation, and pipelines which makes modeling more robust and automated. Data practices and workflow advice.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Software Engineering Practices</span>"
    ]
  },
  {
    "objectID": "software.html#introduction",
    "href": "software.html#introduction",
    "title": "12  Applying Software Engineering Practices",
    "section": "12.2 Introduction",
    "text": "12.2 Introduction\nIn addition to the core concepts from computer science described so far, there’s also a similar set of ideas about the practice and experience of working with a code-based workflow that makes the end-to-end approach more powerful than the code itself.\nIt’s likely that the majority of a professional financial modeler’s time is often spent doing things other than building models, such as testing the model’s results, writing documentation, collaborating with others on the design, and figuring out how to share the model with others inside the company. This chapter covers how a code-first workflow makes each one of those responsibilities easier or more effective.\n\n12.2.1 Regulatory Compliance and Software Practices\nFinancial models often face regulatory requirements around model validation, change management, and reproducibility. Software engineering practices directly support these requirements - version control provides a complete audit trail of all model changes, automated testing helps validate model behavior and demonstrates ongoing quality control, and comprehensive documentation meets regulatory demands for model transparency. For example, the European Central Bank’s Targeted Review of Internal Models (TRIM) Guide explicitly requires financial institutions to maintain documentation of model changes and validation procedures, which is naturally supported by Git commit history and continuous integration test reports which will be discussed in this chapter.\n\n\n12.2.2 Chapter Structure\nThere are three essential topics covered in this chapter:\n\nTesting is the practice of implementing automated checks for desired behavior and outputs in the system.\nDocumentation is the practice of writing plain English (or your local language) to compliment the computer code for better human understanding.\nVersion Control is the systematic practice of tracking changes and facilitating collaborative workflows on projects.\n\nAs the chapter progresses, some highly related topics are covered, bridging some of the conceptual ideas into practical implementations for your own code and models.\nAs a reminder, this chapter is heavily oriented to concepts that are applicable in any language, though the examples are illustrated using Julia for consistency and its clarity. The code examples would have direct analogues in other languages. In Chapter 21 many of these concepts will be reinforced and expanded upon with Julia-specific workflows and tips.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Software Engineering Practices</span>"
    ]
  },
  {
    "objectID": "software.html#sec-software-testing",
    "href": "software.html#sec-software-testing",
    "title": "12  Applying Software Engineering Practices",
    "section": "12.3 Testing",
    "text": "12.3 Testing\nTesting is a crucial aspect of software engineering that ensures the reliability and correctness of code. In financial modeling, where accuracy is paramount, implementing robust testing practices is essential, and in many cases now legally required by the regulatory body or financial reporting authority. It’s good practice regardless of requirements.\nA test is implemented by writing a boolean expression after a @test macro:\n@test model_output = desired_output\nIf the expression evaluates to true, then the test passes. If the expression is anything else (false, or produces an error, or nothing, etc.) then the test will fail.\nHere is an example of modeled behavior being tested. We have a function which will discount the given cashflows at a given annual effective interest rate. The cashflows are assumed to be equally spaced at the end of each period:\n\nfunction present_value(discount_rate, cashflows)\n    v = 1.0\n    pv = 0.0\n    for cf in cashflows\n        v = v / (1 + discount_rate)\n        pv = pv + v * cf\n    end\n    return pv\nend\n\npresent_value (generic function with 1 method)\n\n\nWe might test the implementation like so:\n\nusing Test\n\n@test present_value(0.05, [10]) ≈ 10 / 1.05\n@test present_value(0.05, [10, 20]) ≈ 10 / 1.05 + 20 / 1.05^2\n\n\nTest Passed\n\n\n\nThe above test passes because the expression is true. However, the following will fail because we have defined the function assuming the given discount_rate is compounded once per period. This test will fail because the test target presumes a continuous compounding convention. The failing test will show the stacktrace of where the error occurred.\n\n@test present_value(0.05, [10]) ≈ 10 * exp(-0.05 * 1)\n\n\n\n\n\n\n\nTip\n\n\n\nWhen testing results of floating point math, it’s a good idea to use the approximate comparison (≈, typed in a Julia editor with by entering \\approx&lt;TAB&gt;). Recall that floating point math is a discrete, computer representation of continuous real numbers. As perfect precision is not efficient, very small differences can arise depending on the specific numbers involved or the order in which the operations are applied.\nIn tests (as in the isapprox/≈ function), you can also further specify a relative tolerance and an absolute tolerance:\n\n@test 1.02 ≈ 1.025 atol = 0.01\n@test 1.02 ≈ 1.025 rtol = 0.005\n\n\nTest Passed\n\n\n\n\n\nThe testing described in this section is sort of a ‘sampling’ based approach, wherein the modeler decides on some pre-determined set of outputs to test and determines the desired outcomes for that chosen set of inputs. That is, testing that 2 + 2 == 4 versus testing that a positive number plus a positive number will always equal another positive number. There are some more advanced techniques that cover the latter approach in Section 13.6.1.\n\n\n\n\n\n\nTip\n\n\n\nMore Julia-specific testing workflows are covered in Section 21.14.\n\n\n\n12.3.1 Test Driven Design\nTest Driven Design (TDD) is a software development approach where tests are written before the actual code. The process typically follows these steps:\n\nWrite a test that defines a desired function or improvement.\nRun the test, which should fail since the feature hasn’t been implemented.\nWrite the minimum amount of code necessary to pass the test.\nRun the test again. If it passes, the code meets the test criteria.\nRefactor the code for optimization while ensuring the test still passes.\n\nTDD can be particularly useful in financial modeling as it helps clarify (1) intended behavior of the system and (2) how you think the system should work.\nFor example, if we want to create a new function which calculates an interpolated value between two numbers, we might first define the test like this:\n# interpolate between two points (0,5) and (2,10)\n@test interp_linear(0,2,5,10,1) ≈ (10-5)/(2-0) * (1-0) + 5\nWe’ve defined how it should work for a value inside the bounding \\(x\\) values, but writing the test has us wondering… should the function error if x is outside of the left and right \\(x\\) bounds? Or should the function extrapolate outside the observed interval? The answer to that depends on exactly how we want our system to work. Sometimes the point of such a scheme is to extrapolate, other times extrapolating beyond known values can be dangerous. For now, let’s say we would like to have the function extrapolate, so we can define our test targets to work like that:\n@test interp_linear(0,2,5,10,-1) ≈ (10-5)/(2-0) * (-1 - 0) + 5\n@test interp_linear(0,2,5,10,3) ≈ (10-5)/(2-0) * (3 - 0) + 5\nBy thinking through what the correct result for those different functions is, we have forced ourselves to think how the function should work generically:\n\nfunction interp_linear(x1,x2,y1,y2,x)\n    # slope times difference from x1 + y1\n    return (y2 - y1) / (x2 - x1) * (x - x1) + y1\nend\n\ninterp_linear (generic function with 1 method)\n\n\nAnd we can see that our tests defined above would pass after writing the function.\n\n@testset \"Linear Interpolation\" begin\n    @test interp_linear(0,2,5,10,1) ≈ (10-5)/(2-0) * (1-0) + 5\n    @test interp_linear(0,2,5,10,-1) ≈ (10-5)/(2-0) * (-1 - 0) + 5\n    @test interp_linear(0,2,5,10,3) ≈ (10-5)/(2-0) * (3 - 0) + 5\nend;\n\n\nTest Summary:        | Pass  Total  Time\nLinear Interpolation |    3      3  0.0s\n\n\n\n\n\n\n12.3.2 Test Coverage\nTesting is great, but what if some things aren’t tested? For example, we might have a function that has a branching if/else condition and only ever test one branch. Then when the other branch is encountered in practice it is more vulnerable to having bugs because its behavior was never double checked. Wouldn’t it be great to tell whether or not we have tested all of our code?\nThe good news is that there is! Test coverage is a measurement related to how much of the codebase is covered by at least one associated test case. In the following example, code coverage would flag that the ... other logic is not covered by tests and therefore encourage the developer to write tests covering that case:\nfunction asset_value(strike,current_price)\n    if current_price &gt; strike\n    # ... some logic\n    else\n    # ... other logic\n    end\nend\n\n@test asset_value(10,11) ≈ 1.5   \nFrom this, it’s possible to determine a score for how well a given set of code is tested. 100% coverage means every line of code has at least one test that double checked its behavior.\n\n\n\n\n\n\nWarning\n\n\n\nTesting is only as good as the tests that are written. You could have 100% code coverage for a codebase with only a single rudimentary test covering each line. Or the test itself could be wrong! Testing is not a cure-all, but does encourage best practices.\n\n\nTest coverage is also a great addition when making modification to code. It can be set up such that you receive reports on how the test coverage changes if you were to make a certain modification to a codebase. An example might look like this for a proposed change which added 13 lines of code, of which only 11 of those lines were tested (“hit”). The total coverage percent has therefore gone down (-0.49%) because the proportion of new lines covered by tests is \\(11/13 = 84\\%\\), which is lower than the original coverage rate of 90%.\n@@            Coverage Diff             @@\n##         original    modif    +/-   ##\n==========================================\n- Coverage   90.00%   89.51%   -0.49%     \n==========================================\n  Files           2        2              \n  Lines         130      143      +13     \n==========================================\n+ Hits          117      128      +11     \n- Misses         13       15       +2     \n\n\n12.3.3 Types of Tests\nDifferent tests can emphasize different aspects of model behavior. You could be testing a small bit of logic, or test that the whole model runs if hooked up to a database. The variety of this kind of testing has given rise to various named types of testing, but it’s somewhat arbitrary and the boundaries between the types can be fuzzy.\n\n\n\n\n\n\n\nTest Type\nDescription\n\n\n\n\nUnit Testing\nVerifies the functionality of individual components or functions in isolation. It ensures that each unit of code works as expected.\n\n\nIntegration Testing\nChecks if different modules or services work together correctly. It verifies the interaction between various components of the system.\n\n\nEnd-to-End Testing\nSimulates real user scenarios to test the entire application flow from start to finish. It ensures the system works as a whole.\n\n\nFunctional Testing\nValidates that the software meets specified functional requirements and behaves as expected from a user’s perspective.\n\n\nRegression Testing\nEnsures that new changes or updates to the code haven’t broken existing functionality. It involves re-running previously completed tests.\n\n\n\nThere are other types of testing that can be performed on a model, such as performance testing, security testing, acceptance testing, etc., but these types of tests are outside of the scope of what we would evaluate with an @test check. It is possible to create more advanced, mathematical-type checks and tests, which is introduced in Section 13.6.1.\n\n\n\n\n\n\nTipFinancial Modeling Pro Tip\n\n\n\nTest reports and test coverage are a wonderful way to demonstrate regular and robust testing for compliance. It is important to read and understand any limitations related to testing in your choice of language and associated libraries.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Software Engineering Practices</span>"
    ]
  },
  {
    "objectID": "software.html#documentation",
    "href": "software.html#documentation",
    "title": "12  Applying Software Engineering Practices",
    "section": "12.4 Documentation",
    "text": "12.4 Documentation\nThe most important part of code for maintenance purposes is plainly written notes for humans, not the compiler. This includes in-line comments, docstrings, reference materials, and how-to pages, etc. Even as a single model developer, writing comments for your future self is critical for model maintenance and ongoing productivity.\n\n12.4.1 Comments\nComments are meant for the developer to aid in understanding a certain bit of code. A bit of time-tested wisdom is that after several weeks, months, or years away from a piece of code, something that seemed ‘obvious’ at the time tends to become perplexing at a later time. Writing comments is as much for yourself as it is your colleagues or successors.\nHere’s an example of documentation with single-line comments (indicated with the preceding #) and multi-line comments (enclosed by #= and =#):\nfunction calculate_bond_price(face_value, coupon_rate, years_to_maturity, market_rate)\n    # Convert annual rates to semi-annual\n    semi_annual_coupon = (coupon_rate / 2) * face_value\n    semi_annual_market_rate = market_rate / 2\n    periods = years_to_maturity * 2\n\n    # Calculate the present value of coupon payments\n    pv_coupons = 0\n    for t in 1:periods\n        pv_coupons += semi_annual_coupon / (1 + semi_annual_market_rate)^t\n    end\n\n    # Calculate the present value of the face value\n    pv_face_value = face_value / (1 + semi_annual_market_rate)^periods\n\n    #=\n    Sum up the components for total bond price\n    1. Present value of all coupon payments\n    2. Present value of the face value at maturity\n    =#\n    bond_price = pv_coupons + pv_face_value\n\n    return bond_price\nend\n\n\n12.4.2 Docstrings\nDocstrings (documentation strings) are intended to be a user-facing reference and help text. In Julia, docstrings are just strings placed in front of definitions. Markdown1 is available (and encouraged) to add formatting within the docstring.\nHere’s an example with some various features of documentation shown:\n\"\"\"\n1    calculate_bond_price(face_value, coupon_rate, years_to_maturity, market_rate)\n\nCalculate the price of a bond using discounted cash flow method.\n\nParameters:\n- `face_value`: The bond's par value\n- `coupon_rate`: Annual coupon rate as a decimal\n- `years_to_maturity`: Number of years until the bond matures\n- `market_rate`: Current market interest rate as a decimal\n\nReturns:\n- The calculated bond price\n\n2## Examples:\n\n```julia-repl\njulia&gt; calculate_bond_price(1000, 0.05, 10, 0.06)\n925.6126256977221\n```\n\n3## Extended help:\n\nThis function uses the following steps to calculate the bond price:\n1. Convert annual rates to semi-annual rates\n2. Calculate the present value of all future coupon payments\n3. Calculate the present value of the face value at maturity\n4. Sum these components to get the total bond price\n\nThe calculation assumes semi-annual coupon payments, which is standard in many markets.\n\"\"\"\nfunction calculate_bond_price(face_value, coupon_rate, years_to_maturity, market_rate)\n    # ... function definition as above\nend\n\n1\n\nThe typical docstring on a method includes the signature, indented so it’s treated like code in the Markdown docstring.\n\n2\n\nIt’s good practice to include a section that includes examples of appropriate usage of a function and the expected outcomes.\n\n3\n\nThe Extended help section is a place to put additional detail that’s available on generated docsites and in help tools like the REPL help mode.\n\n\nThe last point, the Extended help section is shown when using help mode in the REPL and including an extra ?. For example, in the REPL, typing ?calculate_bond_price will show the docstring up through the examples. Typing ??calculate_bond_price will show the docstring in its entirety.\n\n\n12.4.3 Docsites\nDocsites, or documentation sites, are websites that are generated to host documentation related to a project. With modern tooling around programming projects, a really rich set of interactive documentation can be created while the developer/modeler focuses on simple documentation artifacts.\nSpecifically, modern docsite tooling generally takes in markdown text pages along with the in-code docstrings and generates a multi-page site that has navigation and search.\nTypical contents of a docsite include:\n\nA Quickstart guide that introduces the project and provides an essential or common use case that the user can immediately run in their own environment. This helps to convey the scope and capability of a project and showcase how to model is intended to be used.\nTutorials are typically worked examples that introduce basic aspects of a concept or package usage and work up to more complex use cases.\nDeveloper documentation is intended to be read by those who are interested in understanding, contributing, or modifying the codebase.\nReference documentation describes concepts and available functionality. A subset of this is API, or Application Programming Interface, documentation which is the detailed specification of the available functionality of a package, often consisting largely of docstrings like the previous example.\n\nDocsite generators are generally able to look at the codebase and from just the docstrings create a searchable, hierarchical page with all of the content from the docstrings in a project. This basic docsite feature is incredibly beneficial for current and potential users of a project.\nTo go beyond creating a searchable index of docstrings requires additional effort (time well invested!). Creating the other types of documentation (quick start, tutorials, etc.) is mechanically as simple as creating a new markdown file. The hard part is learning how to write quality documentation. Good technical writing is a skill developed over time - but at least the technical and workflow aspects have been made as easy as possible!",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Software Engineering Practices</span>"
    ]
  },
  {
    "objectID": "software.html#version-control",
    "href": "software.html#version-control",
    "title": "12  Applying Software Engineering Practices",
    "section": "12.5 Version Control",
    "text": "12.5 Version Control\nVersion control systems (VCS) refer to the tracking of changes to a codebase in a verifiable and coordinated way across project contributors. VCS underpins many aspects of automating the mundane parts of a modeler’s job. Benefits of VCS include (either directly, or contribute significantly to):\n\nAccess control and approval processes\nVersioning of releases\nReproducibility across space and time of a model’s logic\nContinuous testing and validation of results\nMinimization of manual overrides, intervention, and opportunity for user error\nCoordinating collaboration in parallel and in sequence between one or many contributors\n\nThese features are massively beneficial to a financial modeler! A lot of the overhead in a modeler’s job becomes much easier or automated through this tooling.\nAmong several competing options (CVS, Mercurial, Subversion, etc.), Git is the predominant choice as of this book’s writing and therefore we will focus on Git concepts and workflows.\n\n12.5.1 Git Overview\nThis section will introduce Git related concepts at a level deeper than “here’s the five commands to run” but not at a reference-level of detail. The point of this is to reveal some of the underlying technology and approaches so that you can recognize where similar ideas appear in other areas and understand some of the limitations of the technology.\nGit is a free and open source software that tracks changes in files using a series of snapshots. Git itself is the underlying software tool and is a command-line tool at its core. However, you will often interact with it through various interfaces (such as a graphical user interface in VS Code, GitKraken, or other tool).\nEach snapshot, or commit, stores references to the files that have changed2. All of this is stored in a .git/ subfolder of a project, which is automatically created upon initializing a repository. This folder may be hidden by default by your filesystem. You generally never need to modify anything inside the .git/ folder yourself as Git handles this for you. Git tracks files that are contained in sibling directories to the .git/ subfolder.\n\n\n\n\n\n\nNote\n\n\n\nThink of Git as a series of ‘save points’ in a video game. Each time you reach a milestone, you create a ‘commit’ - a snapshot of your entire project. If you make a mistake later, you can always reload a previous save point. ‘Branches’ are like alternate timelines, allowing you to experiment with new features without affecting your main saved game.\n\n\nHashes indicate a verifiable snapshot of a codebase. For example, a full commit ID (a hash)40f141303cec3d58879c493988b71c4e56d79b90 will always refer to a certain snapshot of the code, and if there is a mismatch in the git history between the contents of the repository and the commit’s hash then the git repository is corrupted and it will not function. A corrupted repository usually doesn’t happen in practice, of course! You might see hashes shortened to the last several characters (e.g. 6d79b90) and in the following examples we’ll shorten the hypothetical hashes to just three characters (e.g. b90).\nA codebase can be branched, meaning that two different version of the same codebase can be tracked and switched between. Git lends itself to many different workflows, but a common one is to designate a primary branch (call it main) and make modifications in a new, separate branch. This allows for non-linear and controlled changes to occur without potentially tainting the main branch. It’s common practice to always have the main branch be a ‘working’ version of the code that can always ‘run’ for users, while branches contain works-in-progress or piecemeal changes that temporarily make the project unable to run.\nThe commit history forms a directed acyclic graph (DAG) representing the project’s history. That is, there is an order to the history: the second commit is dependent on the first commit. From the second commit, two child commits may be created which both have the second commit as their parent. Each one of this commits represents a stored snapshot of the project at the time of the commit.\n\n\n\n\n\n\nNote\n\n\n\nA directory structure demonstrating where git data is stored and what content is tracked after initializing a repository in the tracked-project directory. Note how untracked folder is not the parent directory of the .git directory so it does not get tracked by Git.\n/home/username\n├── untracked-folder\n│   ├── random-file.txt\n│   └── ...\n└── tracked-project\n    ├── .git\n    │   ├── config\n    │   ├── HEAD\n    │   └── objects\n    │       └── ...\n    ├── .gitignore\n    ├── README.md\n    └── src\n        ├── MainProject.jl\n        ├── module1.jl\n        └── module2.jl\nUnder the hood, the Git data stored inside the .git/ include things such as binary blobs, trees, commits, and tags. Blobs store file contents, trees represent directories, commits point to trees and parent commits, and tags provide human-readable names for specific commits. This structure allows Git to efficiently handle branching, merging, and maintaining project history.\n\n\nTable 12.1 shows an example workflow for trying to fix an erroneous function present_value which was written as part of a hypothetical FileXYZ.jl file. This example is trivial, but in a larger project where a ‘fix’ or ‘enhancement’ may span many files and take several days or weeks to implement this type of workflow becomes like a superpower compared to traditional, manual version control approaches. It sure beats an approach where you end up with filenames like FileXYZ v23 July 2022 Final.jl!\n\n\n\n\nTable 12.1: A workflow demonstrating branching, staging, committing, and merging in order to fix an incorrect function definition for a present value (pv) function. The branch name/commit ID shows which version you would see on your own computer/filesystem. The inactive branches are tracked in Git but do not manifest themselves on your filesystem unless you checkout that branch.\n\n\n\n\n\n\n\n\n\n\n\nBranch main , FileXYZ.jl file\nBranch fix_function , FileXYZ.jl file\nAction\nActive Branch\n(Abbreviated Commit ID)\n\n\n\n\nfunction pv(rate,amount,time)\n    amount / (1+rate)\nend\nDoes not yet exist\nWrite original function which forgets to take into account the time. Stage and commit it to the main branch.\n\nGit commands:\ngit add FileXYZ.jlgit commit -m 'add pv function'\nmain\n(...58b)\n\n\n”\nfunction pv(rate,amount,time)\n    amount / (1+rate)\nend\nCreate a new branch fix_function.\n\nGit command:\ngit branch fix_function\nmain\n(...58b)\n\n\n”\n”\nCheckout the new branch and make it active for editing. The branch is different but is starting from the existing commit.\n\nGit command: git checkout fix_function\nfix_function\n(...58b)\n\n\n”\nfunction pv(rate,amount,time)\n    amount / (1+rate)^time\nend\nEdit and save the changed file. No git actions taken.\nfix_function\n(...58b)\n\n\n”\n”\n“Stage” the modified file, telling git that you are ready to record (“commit”) a new snapshot of the project.\n\nGit command: git add FileXYZ.jl\nfix_function\n(...58b)\n\n\n”\n”\nCommit a new snapshot of the project by committing with a note to collaborators saying fix: present value logic\nGit command: git commit -m 'fix: present value logic\nfix_function\n(...6ac)\n\n\n”\n”\nSwitch back to the primary branch\n\nGit command: git checkout main\nmain\n(...58b)\n\n\nfunction pv(rate,amount,time)\n    amount / (1+rate)^time\nend\n”\nMerge changes from other branch into the main branch, incorporating the corrected version of the code.\n\nGit command: git merge fix_function\nmain\n(...b90)\n\n\n\n\n\n\n\nA visual representation of the git repository and commits for the actions described in Table 12.1 might be as follows, where the ...XXX is the shortened version of the hash associated with that commit.\nmain branch         :  ...58b       →   ...b90\n                          ↓           ↗\nfix_function branch :  ...58b → ...6ac\nThe “staging” aspect will be explained next.\n\n12.5.1.1 Git Staging\nStaging (sometimes called the “staging area” or “index”) is an intermediate step between making changes to files and recording those changes in a commit. Think of staging as preparing a snapshot - you’re choosing which modified files (or even which specific changes within files) should be included in your next commit. When you modify files in your Git repository, those changes are initially “unstaged.” Using the git add command (or your Git GUI’s staging interface) moves changes into the staging area. This two-step process - staging followed by committing - gives you precise control over which changes get recorded together. Here’s a typical workflow:\n\nYou modify several files in your modeling project\nYou review the changes and decide which ones are ready to commit\nYou stage only the changes that belong together as a logical unit\nYou create a commit with just those staged changes\nRepeat as needed with other modified files\n\nThis is particularly useful when you’re working on multiple features or fixes simultaneously. For example, imagine you’re updating a financial model and you:\n\nFix a bug in your present value calculation\nAdd comments to improve documentation\nStart working on a new feature\n\nYou could stage and commit the bug fix and documentation separately, keeping the work-in-progress feature changes unstaged until they’re complete. This creates a cleaner, more logical project history where each commit represents one coherent change.\nThink of staging as preparing a shipment: you first gather and organize the items (staging), then seal and send the package (committing). This extra step helps maintain a well-organized project history where each commit represents a logical, self-contained change.\n\n\n12.5.1.2 Git Tooling\nGit is traditionally a command-line based tool, however we will not focus on the command line usage as more beginner friendly and intuitive interfaces are available from different available software. The branching and nodes are well suited to be represented visually.\n\n\n\n\n\n\nNote\n\n\n\nSome recommend Git tools with a graphical user interface (GUI) :\n\nGithub Desktop interfaces nicely with Github and provides a GUI for common operations.\nVisual Studio Code has an integrated Git pane, providing a powerful GUI for common operations.\nGitKraken is free for open source repositories but requires payment for usage in enterprise or private repository environments. GitKraken provides intuitive interfaces for handling more advanced operations, conflicts, or issues that might arise when using Git.\n\n\n\n\n\n\n12.5.2 Collaborative Workflows\nGit is a distributed VCS, meaning that copies of the repository and all its history and content can live in multiple locations, such as on two colleagues’ computer as well as a server. In this distributed model, what happens if you make a change locally?\nGit maintains a local repository on each user’s machine, containing the full project history. This local repository includes the working directory (current state of files), staging area (changes prepared for commit), and the .git directory (metadata and object database). When collaborating, users push and pull changes to and from remote repositories. Git uses a branching model, allowing multiple lines of development to coexist and merge when ready.\n\n\n\n\n\n\nNote\n\n\n\nThe primary branch of a project is typically named the ⁠main branch. This change reflects a shift from the older ⁠master branch name, which some projects may still use.\n\n\n\n12.5.2.1 Pull Requests\nLayered onto core Git functionality, services like Github provide interfaces which enhance the utility of VCS. A major example of this is Pull Requests (or PRs), which is a process of merging git branches in a way that allows for additional automation and governance.\nThe following is an example of a PR on a repository adding a small bit of documentation to help future users. We’ll walk through several elements to describe what’s going on:\n\n\n\n\n\n\nFigure 12.1: A Pull Request on Github, demonstrating several utilities which enhance change management, automation, and governance.\n\n\n\nReferencing Figure 12.1, several elements are worth highlighting. In this pull request the author of this change, alecloudenback, is proposing to modify the QuartoNotebookRunner repository, which is not a repository for which the user alecloudenback has any direct rights to modify. After having made a copy of the repository (a “fork”), creating a new branch, making modifications, and committing those changes… a pull request has been made to modify the primary repository.\n\nAll changes are recorded using Git, keeping track of authorship, timestamps, and history.\nAt the top of the screenshot, the title “Note that Quarto…” allows the author to summarize what is changed in the branch to be merged.\nThe “Conversation” tab allows for additional details about the change to be discussed.\nIn the top right, a +1 -1 ■■□□□ is an indication of what’s changed. In this case, a single line of code (documentation, really) was removed (-1) and replaced with another (+1)\nNot shown in the Figure 12.1, the “Files Changed” tab shows a file-by-file and line-by-line comparison of what has changed (see Figure 12.2).\nThe reviewer (user MichaelHatherly) is a collaborator with rights to modify the destination repository has been assigned to review this change before merging it into the main branch.\n“CodeCoverage” was discussed above in testing, and in this case tests were automatically run when the PR was created, and the coverage indicates that there was no added bit of code that was untested.\nA section of “9 checks” that pass, which validated that tests within the repository still passed, on different combinations of operating systems and Julia versions. Additionally, the repository was checked to ensure that formatting conventions were followed.\nAdditionally, there are a number of project management features not really showcased here. A few to note:\n\nThe cross-referencing to another related issue in a different repository (the reference to issue #156).\nAssignees (assigned doers), labels, milestones, and projects are all functionality to help keep track of codebase development.\n\n\nThe features described here can take modelers many manhours of time - testing, review of changes, sign-off tracking, etc. In this Git-based workflow, the workflow above happens frictionlessly and much of it is automated. This is such a powerful paradigm that should be adopted within the financial industry and especially amongst modelers.\n\n\n\n\n\n\nFigure 12.2: A diff shows a file-by-file and line-by-line comparison of what has changed between commits in a codebase. Red indicates something was removed or changed, and green shows what replaced it. Note that even within a line, there’s extra green highlighting to show the newly added text while the unchanged text remains a lighter shade.\n\n\n\n\n\n\n12.5.3 Data Version Control\nGit is not well suited for large files (images, videos, datasets) that change regularly. Instead, the approach is to combine git with a data version control tool. These tool essentially replace the data (often called binary data or a blob) with a content hash in the git snapshots. The actual content referred to by the hash is then located elsewhere (e.g. a hosted server).\n::: {#note-content-hash callout-note} ## Content Hashes\nContent hashes are the output of a function that transforms arbitrary data into a string of data. Hashes are very common in cryptography and in areas where security/certainty is important. Eliding the details of how they work exactly, what’s important to understand for our purposes is that a content hash function will take arbitrary bits and output the same set of data each time the function is called on the original bits.\nFor example, it might look something like this:\n\ndata = \"A data file contents 123\"\nBase.hash(data)\n\n0x7a1f7a6e6f61fa64\n\n\nIf it’s run again on the same inputs, you get the same output:\n\nBase.hash(data)\n\n0x7a1f7a6e6f61fa64\n\n\nAnd if the data is changed even slightly, then the output is markedly different:\n\ndata2 = \"A data file contents 124\"\nBase.hash(data2)\n\n0xb0b70b13a7d93dcb\n\n\nThis is used in content addressed systems like Data Version Control (Section 12.5.3) and Artifacts (sec (artifacts?)) to ask for, and confirm the accuracy of, data instead of trying to address the data by its location. That is, instead of trying to ask to get data from a given URL (e.g. http://adatasource.com/data.csv) you can set up a system which keeps track of available locations for the data that matches the content hash. Something like (in Julia-ish pseudocode):\nstorage_locations = Dict(\n    0x4d7e8e449afd1c48 =&gt; [\n        \"http://datasets.com/1231234\",\n        \"http://companyintranet.com/admin.csv\",\n        \"C:/Users/your_name/Documents/Data/admin.csv\"\n    ]\n)\n\nfunction get_data(hash,locations)\n    for location in locations[hash]\n        if is_available(location)\n            return get(location)\n        end\n    end\n    \n    # if loop didn't return data\n    return nothing\nend\n:::",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Software Engineering Practices</span>"
    ]
  },
  {
    "objectID": "software.html#distributing-the-package",
    "href": "software.html#distributing-the-package",
    "title": "12  Applying Software Engineering Practices",
    "section": "12.6 Distributing the Package",
    "text": "12.6 Distributing the Package\nOnes you have created something, the next best feeling after having it working is having someone else also use the tool. Julia has a robust way to distribute and manage dependencies. This section will cover essential and related topics to distributing your project publicly or with an internal team.\n\n12.6.1 Registries\nRegistries are a way to keep track of packages that are available to install, which is more complex than it might seem at first. The registry needs to keep track of:\n\nWhat dependencies your package has.\nWhat versions of the dependencies your package is compatible with.\nMetadata about the package (name, unique identifier, authors).\nVersions of your package that your have made available and the Git hash associated with the that version for provenance and tracking\nThe location where your package’s repository lives so that the user can grab the code from there.\n\nThe Julia General Registry (“General”) is the default registry that comes loaded as the default registry when installing Julia. From a capability standpoint, there’s nothing that separates General from other registries, including ones that you can create yourself. At its core, the registry is can be seen as a Git repository where each new commit just adds information associated with the newly registered package or version of a package.\nFor distributing packages in a private, or smaller public group see Section 23.9.2.1.\n\n12.6.1.1 General Registry and other Hosted Registries\nAt its core, General is essentially the same as a local registry described in the prior section. However, there’s some additional infrastructure supporting General. Registered packages get backed up, cached for speed, and multiple servers across the globe are set up to respond to Pkg requests for redundancy and latency. Nothing would stop you from doing the same for your own hosted registry if it got popular enough!\n\n\n\n\n\n\nTip\n\n\n\nA local registry is a great way to set up internal sharing of packages within an organization. Services do exist for “managed” package sharing, adding enterprise features like whitelisted dependencies, documentation hosting, a ‘hub’ of searchable packages.\n\n\n\n\n\n12.6.2 Versioning\nVersioning is an important part of managing a complex web of dependencies. Versioning is used to let both users and the computer (e.g. Pkg) understand which bits of code are compatible with others. For this, consider your model/program’s Application Programming Interface (API). The API is essentially defined by the outputs produced by your code given the same inputs. If the same inputs are provided, then for the same API version the same output should be provided. However, this isn’t the only thing that matters. Another is the documentation associated with the functionality. If the documentation said that a function would work a certain way, then your program should follow that (or fix the documentation)!\nAnother case to consider is what if new functionality was added? Old code need not have changed, but if there’s new functionality, how to communicate that as an API change? This is where Semantic Versioning (SemVer) comes in: semantic means that your version something is intended to convey some sort of meaning over and above simply incrementing from v1 to v2 to v3, etc.\n\n12.6.2.1 Semantic Versioning\nSemantic Versioning (SemVer) is one of the most popular approaches to software versioning. It’s not perfect but has emerged as one of the most practical ways since it get’s a lot about version numbering right. Here’s how SemVer is defined3:\nGiven a version number MAJOR.MINOR.PATCH, increment the:\n\n- MAJOR version when you make incompatible API changes\n- MINOR version when you add functionality in a backward compatible manner\n- PATCH version when you make backward compatible bug fixes\nSo here are some examples of SemVer, if our package’s functionality for v1.0.0 is like this:\n\"\"\" my_add(x,y)\n\nAdd the numbers x and y together\n\"\"\"\nmy_add(x,y) = x - y\nPatch change (v1.0.1): Fix the bug in the implementation:\n\"\"\" my_add(x,y)\nAdd the numbers x and y together\n\"\"\"\nmy_add(x,y) = x + y\nThis is a patch change because it fixes a bug without changing the API. The function still takes two arguments and returns their sum, as originally documented.\nMinor change (v1.1.0): Add new functionality in a backward-compatible manner:\n\"\"\" my_add(x,y)\nAdd the numbers x and y together\n\nmy_add(x,y,z)\nAdd the numbers x, y, and z together\n\"\"\"\nmy_add(x,y) = x + y\nmy_add(x,y,z) = x + y + z\nThis is a minor change because it adds new functionality (the ability to add three numbers) while maintaining backward compatibility with the existing two-argument version.\nMajor change (v2.0.0): Make incompatible API changes:\n\"\"\" add_numbers(numbers...)\nAdd any number of input arguments together. This \nfunction replaces `my_add` from prior versions.\n\"\"\"\nadd_numbers(numbers...) = sum(numbers)\nThis is a major change because it fundamentally alters the API. The function name has changed, and it now accepts any number of arguments instead of specifically two or three. This change is not backward compatible with code using the previous versions, so a major version increment is necessary.\n\n\n\n\n\n\nNote\n\n\n\nNumbers need not roll over to the next digit when they hit 10. That is, it’s perfectly valid to go from v1.09.0 to v1.10.0 in SemVer.\n\n\n\n\n\n\n\n\nTip\n\n\n\nSometimes you’ll see a package with a version that starts with zero, such as v0.23.1. We recommend that as soon as you register a package, to make it a v1. v1 need not indicate the package is “complete” (what software is?), so don’t hold back on calling it v1. You’re letting users install it easily, so you might as well call it the first version and move on!\nAccording to SemVer’s rules, there are no patch versions when the major version is zero. This means that you have one less meaningful digit to communicate to users what’s going on with a change. Most packages put an upper bound on compatibility so that major or minor changes in upstream packages are less likely to cause issues in their own packages. This can be somewhat painful to depend on a package which has a v0 and is iterating through ‘fixes’ but is incrementing the minor version. You have to assume it’s making backward incompatible changes and should have skepticism of just upgrading to the new version of the dependency. It takes work on the downstream dependencies to decide if they should upgrade, adding mental and time loads to other authors and users.\n\n\n\n\n\n12.6.3 Artifacts\nArtifacts are a way to distribute content-addressed ((note-content-hash?)) data and other dependencies. An example use case is if you want to distribute some demonstration datasets with a package. When a package in added or updated, the associated data is pulled and un-archived by Pkg instead of the author of the package needing to manually handle data dependencies. Aside from this convenience, it means that different packages could load the same data without duplicating the data download or storage (since the data is content-addressed). The use-case is not real-time data, as the content-hash can only be updated per package version.\nFor example, the MortalityTable.jl package redistributes various publicly available, industry mortality tables. Inside the repository, there’s an Artifacts.toml file specified like:\n#/Artifacts.toml\n\n[\"mort.soa.org\"]\n1git-tree-sha1 = \"6164a6026f108fe95828b689fcd3b992acbff7c3\"\n\n    [[\"mort.soa.org\".download]]\n2    sha256 = \"6f5eb4909564b55a3397ccf4f4c74290e002f7e2e2474cebeb224bb23a9a2606\"\n    url = \"https://github.com/JuliaActuary/Artifacts/raw/v2020-02-15/mort.soa.org/2020-02-15.tar.gz\"\n\n1\n\nThe sha1 hash of the un-archived data once downloaded, used to verify that extraction was successful.\n\n2\n\nThe sha256 hash of the archived (compressed) data to ensure that the data downloaded was as intended.\n\n\nThen, within the package the data artifact can be referenced and handled by the artifact system rather than needing to manually handle it. That is, the data is reference-able like this:\n1table_dir = artifact\"mort.soa.org\"\n\n1\n\nartifact”…” is a string macro, which is a special syntax for macros that interact with strings. md\"...\" is another example, specifying that the content of the string is Markdown content.\n\n\nAs opposed to something like this:\n# pseudo Julia code\ntable_dir = if is_first_package_run\n    \n    data_path = download(\"url_of_data.tar.gz\") # download to a temp location\n    mv(data_path, \"/somewhere/to/keep/data.tar.gz\") # move to a 'permanent' location\n    extract(\"/somewhere/to/keep/data.tar.gz\") # extract contents\n    \"/somewhere/to/keep/data/\" # return data path\nelse\n    \"/somewhere/to/keep/data/\" # return pre-processed path\n\nend\n\n\n\n\n\n\nNote\n\n\n\nUtility Packages such as ArtifactUtils.jl can assist in creating correct entries for Artifact.toml files.\n\n\n\n\n\n\n\n\nNote\n\n\n\nArtifacts support .tar (uncompressed) and .tar.gz because that compression format enjoys more universal support and features than the .zip format most common on Windows systems.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Software Engineering Practices</span>"
    ]
  },
  {
    "objectID": "software.html#example-repository",
    "href": "software.html#example-repository",
    "title": "12  Applying Software Engineering Practices",
    "section": "12.7 Example Repository",
    "text": "12.7 Example Repository\nThe is a good example of a repository which shows the organization of files and code, setting up testing, and documentation.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Software Engineering Practices</span>"
    ]
  },
  {
    "objectID": "software.html#example-repository-structure",
    "href": "software.html#example-repository-structure",
    "title": "12  Applying Software Engineering Practices",
    "section": "12.8 Example Repository Structure",
    "text": "12.8 Example Repository Structure\nA well-structured Julia package demonstrates key software engineering principles in action. The JuliaTemplateRepo repository demonstrates best practices for: - Logical file organization and code structure - Comprehensive test coverage and continuous integration - Clear, accessible documentation with examples - Standard tooling configuration for package development\nThis open-source template serves as a reference implementation.\nThe PkgTemplates.jl package will allow you to create an empty repository with all of the testing, documentation, Git, and continuos integration scaffolding already in place.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Software Engineering Practices</span>"
    ]
  },
  {
    "objectID": "software.html#footnotes",
    "href": "software.html#footnotes",
    "title": "12  Applying Software Engineering Practices",
    "section": "",
    "text": "Markdown is a type of plain text which can be styled for better communication and aesthetics. E.g. **some text** would render as boldface: some text. This book was written in Markdown and all of the styling arose as a result of plain text files written with certain key elements.↩︎\nGit uses a content-addressable filesystem, meaning it stores data as key-value pairs. The key is a hash of the content, ensuring data integrity and allowing efficient storage of identical content. For more on hashes, jump to (note-content-hash?).↩︎\nYou can read more at SemVer.org.↩︎",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Applying Software Engineering Practices</span>"
    ]
  },
  {
    "objectID": "elements-of-compsci.html",
    "href": "elements-of-compsci.html",
    "title": "13  Elements of Computer Science",
    "section": "",
    "text": "13.1 Chapter Overview\nAdapting computer science concepts to work for financial professionals. Computability, computational complexity, and the language of algorithms and problem solving. A survey of important data structures. More advanced testing and verification topics.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Elements of Computer Science</span>"
    ]
  },
  {
    "objectID": "elements-of-compsci.html#computer-science-for-financial-professionals",
    "href": "elements-of-compsci.html#computer-science-for-financial-professionals",
    "title": "13  Elements of Computer Science",
    "section": "13.2 Computer Science for Financial Professionals",
    "text": "13.2 Computer Science for Financial Professionals\nComputer science as a term can be a bit misleading because of the overwhelming association with the physical desktop or laptop machines that we call “computers”. The discipline of computer science is much richer than consumer electronics: at its core, computer science concerns itself with areas of research and answering tough questions:\n\nAlgorithms and Optimization. How can a problem be solved efficiently? How can that problem be solved at all? Given constraints, how can one find an optimal solution?\nTheory of Computation. What sorts of questions are even answerable? Is an answer easy to compute or will resolving it require more resources than the entire known universe? Will a computation ever stop calculating?\nData Structures. How to encode, store, and use data? How does that data relate to each other and what are the trade-offs between different representations of that data?\nInformation Theory1. Given limited data, what can be known or inferred from it?\n\nFor a reader in the twenty-first century we hope that it is patently obvious how impactful the applied computer science has been as end-users of the internet, artificial intelligence, computational photography, safety control systems, etc. have been to our lives. It is a testament to the utility of being able to harness computer science ideas for practical use.\nIt’s common for beneficial advances in knowledge and application to occur at the boundary between two disciplines. It’s here in this chapter that we desire to bring together the financial discipline with computer science and to provide the financial practitioner with the language and concepts to leverage some of computer science’s most relevant ideas.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Elements of Computer Science</span>"
    ]
  },
  {
    "objectID": "elements-of-compsci.html#algorithms",
    "href": "elements-of-compsci.html#algorithms",
    "title": "13  Elements of Computer Science",
    "section": "13.3 Algorithms",
    "text": "13.3 Algorithms\nAn Algorithm is a general term for a process that transforms an input to an output. It’s the set of instructions dictating how to carry out a process. That process needs to be specified in sufficient detail to be able to call itself an algorithm (versus a heuristic which lacks that specificity).\nAn algorithm might be the directions to accomplish the following task: summing a series of consecutive integers from \\(1\\) to \\(n\\). There are multiple ways that this might be accomplished, each one considered a distinct algorithm:\n\niterating over each number and summing them up (starting with the smallest number)\niterating over each number and summing them up (starting with the largest number)\nsumming up the evens and then the odds, then adding the two subtotals\nand many more distinct algorithms…\n\nWe will look at some specific examples of these alternate algorithms as we introduce the next topic, computational complexity.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Elements of Computer Science</span>"
    ]
  },
  {
    "objectID": "elements-of-compsci.html#complexity",
    "href": "elements-of-compsci.html#complexity",
    "title": "13  Elements of Computer Science",
    "section": "13.4 Complexity",
    "text": "13.4 Complexity\n\n13.4.1 Computational Complexity\nWe can characterize the computational complexity of a problem by looking at how long an algorithm takes to complete a task when given an input of size \\(n\\). We can then compare two approaches to see which is computationally less complex for a given \\(n\\). This is a way of systematically evaluating an algorithm to determine its efficiency when being computed.\n\nNote that computational complexity isn’t quite the same as how fast an algorithm will run on your computer, but it’s a very good guide. Modern computer architectures can sometimes execute multiple instructions in a single cycle of the CPU making an algorithm that is, on paper slower than another, actually run faster in practice. Additionally, sometimes algorithms are able to substantially limit the number of computations to be performed, at the expense of using a lot more memory and thereby trading CPU usage with RAM usage.\nYou can think of computational complexity as a measure of how much work is to be performed. Sometimes the computer is able to perform certain kinds of work more efficiently.\nFurther, when we analyze an algorithm recall that ultimately our code gets translated into instructions for the computer hardware. Some instructions are implemented in a way that for any type of number (e.g. floating point), it doesn’t matter if the number is 1.0 or 0.41582574300044717, the operation will take the exact same time and number of instructions to execute (e.g. for the addition operation).\nSometimes a higher level operation is implemented in a way that takes many machine instructions. For example, division instructions may require many CPU cycles when compared to addition or multiplication. Sometimes this is an important distinction and sometimes not. For this book we will ignore this granularity of analysis.\n\n\n13.4.1.1 Example: Sum of Consecutive Integers\nTake for example the problem of determining the sum of integers from \\(1\\) to \\(n\\). We will explore three different algorithms and the associated computational complexity for them.\n\n13.4.1.1.1 Constant Time\nA mathematical proof can show a simple formula for the result. This allows us to compute the answer in constant time, which means that for any \\(n\\), our algorithm is essentially the same amount of work.\n\nnsum_constant(n) = n * (n + 1) ÷ 2\n\nnsum_constant (generic function with 1 method)\n\n\nIn this we see that we perform three operations: a multiplication, a sum, and a division, no matter what n is. If n is 10_000_000 we’d expect this to complete in about a single unit of time.\n\n\n13.4.1.1.2 Linear Time\nThis algorithm performs a number of operations which grows in proportion with \\(n\\) by individually summing up each element in \\(1\\) through \\(n\\):\n\nfunction nsum_linear(n)\n    result = 0\n    for i in 1:n\n        result += i\n    end\n\n    result\nend\n\nnsum_linear (generic function with 1 method)\n\n\nIf \\(n\\) were 10_000_000, we’d expect it to run with roughly 10 million operations, or about 3 million times as many operations as the constant time version. We can say that this version of the algorithm will take approximately \\(n\\) steps to complete.\n\n\n13.4.1.1.3 Quadratic Time\nWhat if we were less efficient, and instead we were only ever able to increment our subtotal by one. That is, instead of adding up \\(1+3\\), we had to instead do four operations: \\(1+1+1+1\\) . We can add a second loop which increments our result by a unit instead of simply adding the current i to the running total result. This makes our algorithm work much harder since it has to add numbers so many more times (Recall that to a computer adding two numbers is the same computational effort regardless of what the numbers are).\n\nfunction nsum_quadratic(n)\n    result = 0\n1    for i in 1:n\n2        for j in 1:i\n            result += 1\n        end\n    end\n\n    result\nend\n\n\n1\n\nThe outer loop with iterator i. loops over the integers \\(1\\) to \\(n\\).\n\n2\n\nThe inner loop with iterator j does the busy work of adding \\(1\\) to our subtotal i times.\n\n\n\n\nnsum_quadratic (generic function with 1 method)\n\n\nBreaking down the steps:\n\nWhen i is 1 there is 1 addition in the inner loop\nWhen i is 2 there are 2 additions in the inner loop\n…\nWhen i is n there are n additions in the inner loop\n\nTherefore, this computation takes \\((1 + 2 + \\cdots + n\\) steps to complete. Algebraically, this simplifies down to our constant time formula: it requires \\(n * (n + 1) ÷ 2\\) or \\(n^2 + n ÷ 2\\) steps to complete.\n\n\n\n13.4.1.2 Comparison\n\n13.4.1.2.1 Big-O Notation\nWe can categorize the above implementations using a convention called Big-O Notation2 which is a way of distilling and classifying computational complexity. We characterize the algorithms by the most significant term in the total number of operations. Table 13.1 shows for the examples constructed above what the description, order, and order of magnitude complexity is.\n\n\n\nTable 13.1: Complexity comparison for the three sample cases of summing integers from \\(1\\) to \\(n\\).\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nComputational Cost\nComplexity Description\nBig-O Order\nSteps (\\(n=10,000\\))\n\n\n\n\nnsum_constant\nfixed\nConstant\n\\(O(1)\\)\n~1\n\n\nnsum_linear\n\\(n\\)\nLinear\n\\(O(n)\\)\n~10,000\n\n\nnsum_quadratic\n\\(n^2 + n ÷ 2\\)\nQuadratic\n\\(O(n^2)\\)\n~100,000,000\n\n\n\n\n\n\nTable 13.2 shows a comparison of a more extended set of complexity levels. For the most complex categories of problems, the cost to compute grows so fast that it boggles the mind. What sorts of problems fall into the most complex categories? \\(O(2^n)\\), or exponential complexity, examples include the traveling salesman problem3 if solved with dynamic programming or the recursive approach to calculating the \\(nth\\) Fibonacci number. The beastly \\(O(n!)\\) algorithms include brute force solving the traveling salesman problem or enumerating all partitions of a set. In financial modeling, we may encounter these sorts of problems in portfolio optimization (using the brute-force approach of testing every potential combination assets to optimize a portfolio).\n\n\n\nTable 13.2: Different Big-O Orders of Complexity\n\n\n\n\n\n\n\n\n\n\n\n\nBig-O Order\nDescription\n\\(n=10\\)\n\\(n=1,000\\)\n\\(n=1,000,000\\)\n\n\n\n\n\\(O(1)\\)\nConstant Time\n1\n1\n1\n\n\n\\(O(n)\\)\nLinear Time\n10\n1,000\n1,000,000\n\n\n\\(O(n^2)\\)\nQuadratic Time\n100\n1,000,000\n10^12\n\n\n\\(O(log(n))\\)\nLogarithmic Time\n3\n7\n14\n\n\n\\(O(n\\times log(n))\\)\nLinearithmic Time\n30\n7,000\n14,000,000\n\n\n\\(O(2^n)\\)\nExponential Time\n1,024\n~10^300\n~10^301029\n\n\n\\(O(n!)\\)\nFactorial Time\n3,628,800\n~10^2567\n~10^5565708\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe care only about the most significant term because when \\(n\\) is large, the most significant term tends to dominate. For example, in our quadratic time example which has \\(n^2 + n ÷ 2\\) steps, if \\(n\\) is a large number like \\(10^6\\), then we see that it will result in:\n\\[\\begin{align*}\nn^2 + \\frac{n}{2} &= (10^6)^2 + \\frac{10^6}{2} \\\\\n                  &= 10^{12} + \\frac{10^6}{2}\n\\end{align*}\\]\n\\(10^{12}\\) is significantly more important than \\(\\frac{10^6}{2}\\) (sixty-four million times as important, to be precise). This is why Big-O notation reduces the problem down to only the most significant complexity cost term.\nIf n is small then we don’t really care about computational complexity in general. This is a lesson for our efforts as developers: focus on the most intensive parts of calculations when looking to optimize, and don’t worry about seldom used portions of the code.\n\n\n\n\n13.4.1.2.2 Empirical Results\nThe preceding examples of constant, linear, and quadratic times are conceptually correct but if we try to run them in practice we see that the description doesn’t seem to hold at all for the linear time version, as it runs as quickly as the constant time version.\n\nusing BenchmarkTools\n@btime nsum_constant(10_000)\n\n  0.833 ns (0 allocations: 0 bytes)\n\n\n50005000\n\n\n\n@btime nsum_linear(10_000)\n\n  1.500 ns (0 allocations: 0 bytes)\n\n\n50005000\n\n\n\n@btime nsum_quadratic(10_000)\n\n  1.104 μs (0 allocations: 0 bytes)\n\n\n50005000\n\n\nWhat happened was that the compiler was able to understand and optimize the linear version such that it effectively transformed it into the constant time version and avoid the iterative summation that we had written. For examples that are simple enough to use as a teaching problem, the compiler can often optimize different written code down to the same efficient machine code (this is the same Triangular Number optimization we saw in Section 5.4.3.4).\n\n\n\n13.4.1.3 Expected versus worst-case complexity\nAnother consideration is that there may be one approach which performs better in the majority of cases, at the expense of having very poor performance in specific cases. Sometimes we may risk those high cost cases if we expect the benefit to be worthwhile on the rest of the problem set.\nThis often happens when the data we are working with has some concept of “distance”. For example, in multi-stop route planning we can use the idea that it’s likely to be more efficient to visit nearby destinations first. Generally this works, but sometimes the nearest distance actually has a high cost (such as needing to avoid real-world obstacles in the way which force you to drive past other further away locations to get there).\n\n\n\n13.4.2 Space Complexity\nSo far we have focused on computational complexity, however similar analysis could be performed for space complexity, which is how much computer memory is required to solve a problem. Sometimes, an algorithm will trade computational complexity for space complexity. That is, we might be able to solve a problem much faster if we have more memory available.\nFor example, there has been research to improve the computational efficiency of matrix multiplication which do indeed run faster than traditional techniques. However, those algorithms don’t get implemented in general linear algebra libraries because they require way more memory than is available!\n\n\n13.4.3 Complexity: Takeaways\nThe idea of algorithmic complexity is important because it grounds us in the harsh truth that some problems are very difficult to compute. It’s in these cases that a lot of the creativity and domain specific heuristics can become the foremost consideration.\nWe must remember to be thoughtful about the design of our models. When searching for additional performance to look for the “loops-within-loops” which is where combinatorial explosions tend to happen. Focusing on the places that have large \\(n\\) or poor Big-O order that you can transform the performance of the overall model. Sometimes though, the fundamental complexity of the problem at hand forbids greater efficiency.\nSee the end of this chapter, Section 13.7 for an example demonstrating computational complexity in the context of portfolio selection.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Elements of Computer Science</span>"
    ]
  },
  {
    "objectID": "elements-of-compsci.html#data-structures",
    "href": "elements-of-compsci.html#data-structures",
    "title": "13  Elements of Computer Science",
    "section": "13.5 Data Structures",
    "text": "13.5 Data Structures\nThe science of data structures is about how data is represented conceptually and in computer applications.\nFor example, how should the string “abcdef” be represented and analyzed?\nThere are many common data structures and many specialized subtypes. We will describe some of the most common ones here. Julia has many data structures available in the Base library, but an extensive collection of other data structures can be found in the DataStructures.jl package.\n\n13.5.1 Arrays\nAn array is a contiguous block of memory containing elements of the same type, accessed via integer indices. Arrays have fast random access and are the fastest data structure for linear/iterated access of data.\nIn Julia, an array is a very common data structure and is implemented with a simple declaration, such as:\nx = [1,2,3]\nIn memory, the integers are stored as consecutive bits representing the integer values of 1, 2, and 3, and would look like this (with the different integers shown on new lines for clarity):\n0000000000000000000000000000000000000000000000000000000000000001\n0000000000000000000000000000000000000000000000000000000000000010\n0000000000000000000000000000000000000000000000000000000000000011\nThis is great for accessing the values one-by-one or in consecutive groups, but it’s not efficient if values need to be inserted in between. For example, if we wanted to insert 0 between the 1 and 2 in x, then we’d need to overwrite the second position in the array, ask the operating system to allocate more memory4, and re-write the bytes that come after our new value. Inserting values at the end (push!(array, value)) is usually fast unless more memory needs to be allocated.\n\n\n13.5.2 Linked Lists\nA linked list is a chain of nodes where each node contains a value and a pointer to the next node. Linked lists allow for efficient insertion and deletion but slower random access compared to arrays.\nIn Julia, a simple linked list node could be implemented as:\nmutable struct Node\n    value::Any\n1    next::Union{Node, Nothing}\nend\n\nz = Node(3,nothing)\ny = Node(2,z)\nx = Node(1,y)\n\n1\n\nNothing represents the end of the linked list.\n\n\nInserting a new node between existing nodes is efficient - if we wanted to insert a new node between the ones with value 2 and 3, we could do this:\na = Node(0,z) # &lt;1&gt; Create a new `Node` with `next` equal to `z`\ny.next = a # &lt;2&gt; Set the reference to `next` in `y` to be the new `Node` `a`.\nAccessing the nth element requires traversing the list from the beginning to check each Node’s next value. This iterative approach makes it \\(O(n)\\) time complexity for random access. This is in contrast to an array where you know right away where the \\(n\\)th item will be in the data structure.\nAlso, the linked list is one-directional. Items further down the chain don’t know what node points to them, so it’s impossible to traverse the list backwards.\nThere are many related implementations which make random access or traversal in reverse order more efficient such as doubly-linked lists or “trees” (Section 13.5.6) which organize the data not as a chain but as a tree with branching nodes.\n\n\n13.5.3 Records/Structs\nAn aggregate of named fields, typically of fixed size and sequence. Records group related data together. We’ve encountered structs in Section 5.4.7, but here we’ll add that simple structs with primitive fields can themselves be represented without creating pointers to the data stored:\n\nstruct SimpleBond\n    id::Int\n    par::Float64\nend\n\nstruct LessSimpleBond\n    id::String\n    par::Float64\nend\n\na = SimpleBond(1, 100.0)\nb = LessSimpleBond(\"1\", 100.0)\nisbits(a), isbits(b)\n\n(true, false)\n\n\nBecause a is comprised of simple elements, it can be represented as a contiguous set of bits in memory. It would look something like this in memory:\n0000000000000000000000000000000000000000000000000000000000000001\n0100000001011001000000000000000000000000000000000000000000000000\n\n1\n\nThe bits of 1\n\n2\n\nThe bits of 100.0\n\n\nIn contrast, the LessSimpleBond uses a String to represent the ID of the bond. In Julia, String is an immutable type that internally references a buffer of bytes; because it holds a reference, a struct containing a String is not an isbits type.\n.... a pointer ...\n0100000001011001000000000000000000000000000000000000000000000000\n\n1\n\na pointer/reference to the array of characters that comprise the string ID\n\n2\n\nThe bits of 100.0\n\n\nIn performance critical code, having data that is represented with simple bits instead of references/pointers can be much faster (see Chapter 30 for an example).\n\n\n\n\n\n\nNote\n\n\n\nFor many mutable types, there are immutable, bits-types alternatives. For example:\n\nArrays have a StaticArray counterpart (from the StaticArrays.jl package).\nStrings have InlineStrings (from the InlineStrings.jl package) which use fixed-width representations of strings.\n\nThe downsides to the immutable alternatives (other than the loss of potentially desired flexibly that mutability provides) are that they can be harder on the compiler (more upfront compilation cost) to handle the specialized cases involved.\n\n\n\n\n13.5.4 Dictionaries (Hash Tables)\n\n13.5.4.1 Hashes and Hash Functions.\nHashes are the result of a hash function that maps arbitrary data to a fixed size value. It’s sort of a “one way” mapping to a simpler value which has the benefits of:\n\nOne way so that if someone knows the hashed value, it’s very difficult to guess what the original value was. This is most useful in cryptographic and security applications.\nCreating (probabilistically) unique IDs for a given set of data.\n\nFor example, we can calculate an SHA hash on any data:\n\nimport SHA\nlet\n    a = SHA.sha256(\"hello world\") |&gt; bytes2hex\n    b = SHA.sha256(rand(UInt8, 10^6)) |&gt; bytes2hex\n    println(a)\n    println(b)\nend\n\nb94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9\n32ec5eb51b20244052dc016beb11c5e256ddabe3b64ac2f3a8388ab08d0c3ea6\n\n\nWe can easily verify that the sha256 hash of \"hello world\" is the same each time, but it’s virtually impossible to guess \"hello world\" if we are just given the resulting hash. This is the premise of trying to “crack” a password when the stored password hash is stolen.\nOne way to check if two sets of data are the same is to compute the hash and see if the resulting hashes are equal. For example, maybe you want to see if two data files with different names contain the same data - comparing the hashes is a sure way to determine if they contain the same data.\n\n\n13.5.4.2 Dictionaries\nDictionaries map a key to a value. More specifically, they use the hash of a key to store a reference to the value.\nDictionaries offer constant-time average case access but must handle potential collisions of keys (generally, the more robust the collision handling means higher fixed cost for access).\nHere’s an illustrative portfolio of assets indexed by CUSIPs:\nassets = Dict(\n    # CUSIP =&gt; Asset\n    \"037833AH4\" =&gt; Bond(\"General Electric\",...),\n    \"912828M80\" =&gt; Equity(\"AAPL\",...),\n    \"594918BQ1\" =&gt; Bond(\"ENRON\",...),\n)\nThen, lookup is performed by indexing the dictionary by the desired key:\nassets[\"037833AH4\"] # gives the General Electric Bond\n\n\n\n13.5.5 Graphs\nA graph is a collection of nodes (also called vertices) connected by edges to represent relationships or connections between entities. Graphs are versatile data structures that can model various real-world scenarios such as social networks, transportation systems, or computer networks.\nIn Julia, a simple graph could be implemented using a dictionary where keys are nodes and values are lists of connected nodes:\nstruct Graph\n    nodes::Dict{Any, Vector{Any}}\nend\n\nfunction add_edge!(graph::Graph, node1, node2)\n    push!(get!(graph.nodes, node1, []), node2)\n    push!(get!(graph.nodes, node2, []), node1)\nend\n\ng = Graph(Dict())\nadd_edge!(g, 1, 2)\nadd_edge!(g, 2, 3)\nadd_edge!(g, 1, 3)\nThis implementation represents an undirected graph. For a directed graph, you would only add the edge in one direction.\nGraphs can be traversed using various algorithms such as depth-first search (DFS) or breadth-first search (BFS). These traversals are useful for finding paths, detecting cycles, or exploring connected components.\nFor more advanced graph operations, the Graphs.jl package provides a comprehensive set of tools for working with graphs in Julia.\n\n\n13.5.6 Trees\nA tree is a hierarchical data structure with a root node and child subtrees. Each node in a tree can have zero or more child nodes, and every node (except the root) has exactly one parent node. Trees are widely used for representing hierarchical relationships, organizing data for efficient searching and sorting, and in various algorithms.\nA simple binary tree node in Julia could be implemented as:\nmutable struct TreeNode\n    value::Any\n    left::Union{TreeNode, Nothing}\n    right::Union{TreeNode, Nothing}\nend\n\n# Creating a simple binary tree\nroot = TreeNode(1, \n    TreeNode(2, \n        TreeNode(4, nothing, nothing), \n        TreeNode(5, nothing, nothing)\n    ), \n    TreeNode(3, \n        nothing, \n        TreeNode(6, nothing, nothing)\n    )\n)\nTrees have various specialized forms, each with its own properties and use cases:\n\nBinary Search Trees (BST): Each node has at most two children, with all left descendants less than the current node, and all right descendants greater.\nAVL Trees: Self-balancing binary search trees, ensuring that the heights of the two child subtrees of any node differ by at most one.\nB-trees: Generalization of binary search trees, allowing nodes to have more than two children. Commonly used in databases and file systems.\nTrie (Prefix Tree): Used for efficient retrieval of keys in a dataset of strings. Each node represents a common prefix of some keys.\n\nTrees support efficient operations like insertion, deletion, and searching, often with \\(O(log n)\\) time complexity for balanced trees. They are fundamental in many algorithms and data structures, including heaps, syntax trees in compilers, and decision trees in machine learning.\n\n\n13.5.7 Data Structures Conclusion\nData structures have strengths and weakness depending on whether you want to prioritize computational efficiency, memory (space) efficiency, code simplicity, and/or mutability. Due to the complexity of real world modeling needs, it can be the case that different representations of the data are more natural or more efficient for the use case at hand.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Elements of Computer Science</span>"
    ]
  },
  {
    "objectID": "elements-of-compsci.html#verification-and-advanced-testing",
    "href": "elements-of-compsci.html#verification-and-advanced-testing",
    "title": "13  Elements of Computer Science",
    "section": "13.6 Verification and Advanced Testing",
    "text": "13.6 Verification and Advanced Testing\n\n13.6.1 Formal Verification\nFormal verification is a technique used to prove or disprove the correctness of algorithms with respect to a certain formal specification or property. In essence, it’s a mathematical approach to ensuring that a system behaves exactly as intended under all possible conditions.\nIn formal verification, we use mathematical methods to:\n\nCreate a formal model of the system\nSpecify the desired properties or behaviors\nProve that the model satisfies these properties\n\nThis process can be automated using specialized software tools called theorem provers or model checkers.\n\n13.6.1.1 Formal Verification in Practice\nIt sounds like the perfect risk management and regulatory technique: prove that the system works exactly as intended. However, there has been very limited deployment of formal verification in industry. This is for several reasons:\n\nIncomplete Coverage: It’s often impractical to formally verify entire large-scale financial systems. Verification, if at all, is typically limited to critical components.\nIncomplete Specification: Actually reasoning through how the system should behave in all scenarios requires actually contemplating mathematically complete and rigorous possibilities that could occur.\nModel-Reality Gap: The formal model may not perfectly represent the real-world system, especially in finance where market behavior can be unpredictable.\nChanging Requirements: Financial regulations and market conditions change rapidly, potentially outdating formal verifications.\nPerformance Trade-offs: Systems designed for easy formal verification might sacrifice performance or flexibility.\nCost: The process can be expensive in terms of time and specialized labor.\n\n\n\n\n13.6.2 Property Based Testing\nTesting will be discussed in more detail in Chapter 12, but an intermediate concept between Formal Verification and typical software testing is property-based testing, which tests for general rules instead of specific examples.\nFor example, a function which is associative (\\((a + b) + c = a + (b + c)\\)) or commutative (\\(a + b\\) = \\(b + a\\)) can be tested with simple examples like:\nusing Test\n\nmyadd(a,b) = a + b\n\n@test myadd(1,2) == myadd(2,1) # commutative\n@test myadd(myadd(1,2),3) == myadd(1,myadd(2,3)) # associative\nHowever, we really haven’t proven the associative and commutative properties in general. There are techniques to do this, which is a more comprehensive alternative to testing specific examples above. Packages like Supposition.jl provide functionality for this. Note that like Formal Verification, property-based testing is a more advanced topic.\n\n\n\n\n\n\nWarning\n\n\n\nProperty-based testing is also tied in with concepts related to types we talked about in Section 5.4.1: floating point numbers are not associative when summing more than two numbers at a time: the order in which the floats get added affects the accumulated imprecision.\n\n\n\n\n13.6.3 Fuzzing\nFuzzing is kind of like property based testing, but instead of testing general rules, we generalize the simple examples using randomness. For example, we could test the commutative property using random numbers instead, therefore statistically checking that the property holds:\n@testset for i in 1:10000\n    a = rand()\n    b = rand()\n\n    @test myadd(a,b) == myadd(b,a)\nend\nThis is a good advancement over the simple @test myadd(1,2) == myadd(2,1), in terms of checking the correctness of myadd, but it comes at the cost of more computational time and non-deterministic tests. Fuzzing typically seeks to find edge-cases: crashes or unexpected behavior by exploring random/invalid inputs.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Elements of Computer Science</span>"
    ]
  },
  {
    "objectID": "elements-of-compsci.html#sec-portfolio-complexity",
    "href": "elements-of-compsci.html#sec-portfolio-complexity",
    "title": "13  Elements of Computer Science",
    "section": "13.7 Finance Example: Complexity and Portfolio Selection",
    "text": "13.7 Finance Example: Complexity and Portfolio Selection\nMany real portfolio decisions include discrete choices: buy or skip a lot, include or exclude an ETF, cap the number of names, or enforce minimum ticket sizes. These discrete elements quickly turn an otherwise smooth optimization into a combinatorial search.\nConsider a simplified subset selection problem with \\(n\\) candidate assets. Each asset \\(i\\) has an expected return \\(r_i\\) and a “cost” \\(c_i\\) (e.g., capital required for a minimum lot). We wish to select a subset that maximizes total expected return subject to a budget \\(B\\). This is the classic 0–1 knapsack problem:\n\\[\n\\max_{x \\in \\{0,1\\}^n} \\sum_{i=1}^{n} r_i\\, x_i \\quad \\text{subject to} \\quad \\sum_{i=1}^{n} c_i\\, x_i \\le B\n\\]\n\nBrute force enumerates all \\(2^n\\) subsets to find the best feasible one. Complexity: \\(O(2^n \\cdot n)\\) work, so it explodes rapidly.\nA greedy heuristic sorts by value-to-cost ratio \\(r_i/c_i\\) and takes items while budget remains. Complexity: \\(O(n \\log n)\\); fast, but not guaranteed optimal.\nBranch-and-bound (B&B) remains worst‑case exponential, but prunes large parts of the search using an optimistic upper bound (here, the fractional knapsack relaxation), often exploring far fewer nodes in practice.\n\nRather than relying on wall‑clock times (which depend on your machine), we count “visited” nodes—how many partial solutions each method examines—to illustrate growth in work.\n\nusing Random\n\n# Generate a synthetic instance. Pass an RNG for reproducibility.\nfunction synth_instance(rng::AbstractRNG, n::Int; budget_ratio::Float64=0.35)\n    r = 0.04 .+ 0.08 .* rand(rng, n)            # expected returns in [4%, 12%]\n    c = Float64.(rand(rng, 50:150, n))          # costs (min lot \"sizes\"), as reals\n    B = budget_ratio * sum(c)                   # budget as a real, not floored\n    return r, c, B\nend\n\n# Greedy heuristic: sort by value-to-cost ratio and pack until budget is exhausted.\nfunction greedy_knapsack(r::AbstractVector{&lt;:Real},\n    c::AbstractVector{&lt;:Real},\n    B::Real)\n    n = length(r)\n    ratio = r ./ c\n    idx = sortperm(ratio, rev=true)           # highest ratio first\n    total = 0.0\n    cost = 0.0\n    chosen = falses(n)\n    @inbounds for i in idx\n        if cost + c[i] &lt;= B + 1e-12\n            chosen[i] = true\n            total += r[i]\n            cost += c[i]\n        end\n    end\n    return total, chosen\nend\n\n# Brute-force: enumerate all 2^n subsets (only feasible for small n).\nfunction brute_force_knapsack(r::AbstractVector{&lt;:Real},\n    c::AbstractVector{&lt;:Real},\n    B::Real)\n    n = length(r)\n    best = -Inf\n    bestmask = 0\n    visited = 0\n    total_subsets = (Int(1) &lt;&lt; n)\n    @inbounds for mask in 0:(total_subsets-1)\n        visited += 1\n        value = 0.0\n        cost = 0.0\n        for i in 1:n\n            if ((mask &gt;&gt;&gt; (i - 1)) & 0x1) == 1\n                value += r[i]\n                cost += c[i]\n                if cost &gt; B\n                    break\n                end\n            end\n        end\n        if cost &lt;= B && value &gt; best\n            best = value\n            bestmask = mask\n        end\n    end\n    chosen = falses(n)\n    for i in 1:n\n        chosen[i] = ((bestmask &gt;&gt;&gt; (i - 1)) & 0x1) == 1\n    end\n    return best, chosen, visited\nend\n\n# Branch-and-Bound with a fractional-knapsack upper bound (optimistic).\n# The upper bound relaxes x_i ∈ {0,1} to x_i ∈ [0,1] and \"fills\" the remaining budget fractionally.\nfunction branch_and_bound_knapsack(r::AbstractVector{&lt;:Real},\n    c::AbstractVector{&lt;:Real},\n    B::Real)\n    n = length(r)\n    rs = Float64.(r)\n    cs = Float64.(c)\n    Bf = float(B)\n\n    ratio = rs ./ cs\n    order = sortperm(ratio, rev=true)\n    rs = rs[order]\n    cs = cs[order]\n\n    best = Ref(-Inf)            # current best objective\n    best_choice = falses(n)     # in sorted order\n    visited = Ref(0)            # node counter\n    choice = falses(n)          # current partial choice (sorted order)\n\n    @inline function ubound(idx::Int, value::Float64, cost::Float64)\n        if cost &gt; Bf\n            return -Inf\n        end\n        cap = Bf - cost\n        ub = value\n        @inbounds for j in idx:n\n            if cs[j] &lt;= cap + 1e-12\n                ub += rs[j]\n                cap -= cs[j]\n            else\n                ub += rs[j] * (cap / cs[j])     # fractional \"fill\" for bound\n                break\n            end\n        end\n        return ub\n    end\n\n    function dfs(idx::Int, value::Float64, cost::Float64)\n        visited[] += 1\n        if idx &gt; n\n            if cost &lt;= Bf && value &gt; best[]\n                best[] = value\n                best_choice .= choice\n            end\n            return\n        end\n        # Prune if even the optimistic bound cannot beat current best.\n        if ubound(idx, value, cost) &lt;= best[] + 1e-12\n            return\n        end\n        # Branch: include item idx\n        choice[idx] = true\n        dfs(idx + 1, value + rs[idx], cost + cs[idx])\n        # Branch: exclude item idx\n        choice[idx] = false\n        dfs(idx + 1, value, cost)\n    end\n\n    dfs(1, 0.0, 0.0)\n\n    # Map best_choice back to the original indexing\n    selected = falses(n)\n    @inbounds for (k, i) in pairs(order)\n        selected[i] = best_choice[k]\n    end\n    return best[], selected, visited[]\nend\n\n# Small demonstration: brute force is feasible at n ≈ 20; for n ≈ 40 we skip brute force.\nlet\n    rng = MersenneTwister(1234)\n\n    # Small instance where brute force is still possible\n    n_small = 20\n    r, c, B = synth_instance(rng, n_small; budget_ratio=0.35)\n\n    bf_val, bf_sel, bf_visited = brute_force_knapsack(r, c, B)\n    bb_val, bb_sel, bb_visited = branch_and_bound_knapsack(r, c, B)\n    gr_val, gr_sel = greedy_knapsack(r, c, B)\n\n    println(\"n = $n_small\")\n    println(\" total subsets = \", 1 &lt;&lt; n_small)                 # 1,048,576\n    println(\" brute force visited = \", bf_visited,\n        \", optimal value = \", bf_val)\n    println(\" branch-and-bound visited = \", bb_visited,\n        \", optimal value = \", bb_val)\n    println(\" greedy value = \", gr_val,\n        \", optimality gap = \", bf_val - gr_val)\n\n    # Larger instance—note how the search space explodes (2^40 ≈ 10^12)\n    n_large = 40\n    r2, c2, B2 = synth_instance(rng, n_large; budget_ratio=0.35)\n    bb2_val, bb2_sel, bb2_visited = branch_and_bound_knapsack(r2, c2, B2)\n    gr2_val, gr2_sel = greedy_knapsack(r2, c2, B2)\n\n    println(\"\\nn = $n_large\")\n    println(\" total subsets = \", BigInt(1) &lt;&lt; n_large)         # ≈ 1.1×10^12\n    println(\" branch-and-bound visited = \", bb2_visited,\n        \", optimal value = \", bb2_val)\n    println(\" greedy value = \", gr2_val,\n        \", optimality gap = \", bb2_val - gr2_val)\nend\n\nn = 20\n total subsets = 1048576\n brute force visited = 1048576, optimal value = 0.7605388597519076\n branch-and-bound visited = 317, optimal value = 0.7605388597519077\n greedy value = 0.703897494058528, optimality gap = 0.05664136569337963\n\nn = 40\n total subsets = 1099511627776\n branch-and-bound visited = 115, optimal value = 1.4909468614490333\n greedy value = 1.4909468614490333, optimality gap = 0.0\n\n\nWhat this shows in practice:\n\nBrute force scales as \\(2^n\\). At \\(n=20\\), there are \\(1{,}048{,}576\\) subsets—still manageable. At \\(n=40\\), there are \\(\\approx 10^{12}\\) subsets—completely infeasible to enumerate.\nGreedy is fast and often near‑optimal, especially when ratios \\(r_i/c_i\\) align with the true optimum. However, it can be arbitrarily sub‑optimal in adversarial cases.\nBranch‑and‑bound exploits structure. The fractional‑knapsack bound is optimistic—it assumes you could take fractions of remaining items—so any subtree whose bound cannot beat the current best is safely pruned. Worst‑case complexity remains exponential, but typical search can drop from billions of nodes to thousands or millions.\n\n\n\n\n\n\n\nTipFinancial Modeling Pro Tip\n\n\n\nIn realistic portfolio construction, practitioners frequently avoid pure subset selection by relaxing to continuous weights (e.g., mean–variance) or by formulating a mixed‑integer model and using modern MILP solvers. These solvers implement sophisticated branch‑and‑bound/branch‑and‑cut strategies, presolve, and cutting planes. When exact optimality is not essential, heuristics (greedy, local search, genetic/evolutionary methods) can deliver high‑quality portfolios quickly under operational constraints (cardinality, sector caps, turnover budgets).\nSee Chapter 27 for more discussion.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Elements of Computer Science</span>"
    ]
  },
  {
    "objectID": "elements-of-compsci.html#footnotes",
    "href": "elements-of-compsci.html#footnotes",
    "title": "13  Elements of Computer Science",
    "section": "",
    "text": "This topic will be covered in Chapter 14.↩︎\n“Big-O”, so named because the ‘O’ denotes the order of growth, as in \\(O(1)\\). \\(O(n)\\), etc.↩︎\nThe Traveling Salesperson Problem is a classic computer science problem where you need to find the shortest possible route that visits each city in a given set exactly once and returns to the starting city. It’s a seemingly simple problem that becomes computationally intensive very quickly as the number of cities increases.↩︎\nIn practice, the operating system may have already allocated space for an array that’s larger than what the program is actually using so far, so this step may be ‘quick’ at times, while other times the operating system may actually need to extend the block of memory allocated to the array.↩︎",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Elements of Computer Science</span>"
    ]
  },
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "14  Statistical Inference and Information Theory",
    "section": "",
    "text": "14.1 Chapter Overview\nA brief introduction to information theory and its foundational role in statistics. Entropy and probability distributions. Bayes’ rule and model selection comparison via likelihoods. A brief tour of modern Bayesian statistics.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical Inference and Information Theory</span>"
    ]
  },
  {
    "objectID": "statistics.html#introduction",
    "href": "statistics.html#introduction",
    "title": "14  Statistical Inference and Information Theory",
    "section": "14.2 Introduction",
    "text": "14.2 Introduction\nStatistics has an invaluable role in any data-driven modeling enterprise. As financial professionals dealing inherently with risk and uncertainty - we use probability and statistics to understand, model, and communicate these aspects.\nStatistics curricula and practice is undergoing a significant transformation, with a larger focus on information theory and Bayesian methods (as opposed to the common Frequentist methods that have dominated the statistics field for more than a century). Why the change? In short, these methods work better in a wider range of situations and convey more meaningful information about model performance and uncertainty. Until now, computational challenges have limited Bayesian methods to simpler problems, but newer algorithms and better hardware are overcoming this limitation.\nIn our experience, it is rare that a financial professional has had exposure to non-Frequentist theory and methods. Given how central probability and statistics is to this endeavor, we have drafted this chapter as a introduction - a proper treatment is beyond the scope of this book. Armed with this knowledge, some of the terminology and tools should be more accessible, and possibly included in new financial models.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical Inference and Information Theory</span>"
    ]
  },
  {
    "objectID": "statistics.html#information-theory",
    "href": "statistics.html#information-theory",
    "title": "14  Statistical Inference and Information Theory",
    "section": "14.3 Information Theory",
    "text": "14.3 Information Theory\nProbability, statistics, machine learning, signal processing, and even physics have a foundational link in information theory which is the description and analysis of how much useful data is contained within something. We will work through this with concrete examples.\n\n14.3.1 Example: The Missing Digit\nLet’s consider the following situation: we are studying a poorly made copy of a financial statement. Amongst many associated exhibits, we are interested in the par value of a particular asset class. Unfortunately, for one reason or another one of the digits is completely indecipherable. Here’s what you can read, with the _ indicating that one of the digit is missing from the scanned copy:\n32,000,_00\nIt is likely that you quickly formed an opinion on what the missing number is, but let us make that intuition more formal and quantitative.\nGiven that we know that par values of assets tend to be nice round numbers, our prior assumption for what the probability of the missing digit is may be something like the \\(p(x_i)\\) row of Table 14.1. This prior distribution assumes that the missing digit is most likely a 0. We shall call the individual outcomes \\(x_i\\) and the overall set of probabilities \\(\\{x_0,x_1,...x_9\\}\\) is called \\(X\\).\nThe information content of an outcome, \\(h(x)\\) is measured in bits and defined as1:\n\\[\nh(x_i) = \\text{log}_2\\frac{1}{p(x_i)}\n\\tag{14.1}\\]\nLooking at Table 14.1, we can see that the information content of an outcome is lower when that outcome has a higher probability than the other potential outcomes. Specifically, If the digit was indeed 0, we have gained less information relative to our expectation than if the missing digit were anything other than 0 .\n\n\n\n\n\n\nTip\n\n\n\nThe information content is sometimes referred to as a measure of surprise that one would have when observing a realized outcome. In our missing digit example (@tbl-digit-information-human), we would not be surprised at all to find out that the missing digit were 0. In contrast, we would be more surprised to find out the digit were an 8.\n\n\nWe can characterize the entire distribution \\(X\\) via the entropy, \\(H(X)\\), of a probability set is the ensemble’s average information content:\n\\[\nH(X) = \\sum{p(x_i)\\text{log}_2}\\frac{1}{p(x_i)}\n\\tag{14.2}\\]\nThe entropy \\(H(X)\\) of the presumed outcomes in Table 14.1 distribution of outcomes is \\(0.722 \\text{bits}\\). In just\n\n\n\nTable 14.1: Probability distribution of missing digit, knowing the human inclination to prefer round numbers for par values of assets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\\(p(x_i)\\)\n.91\n.01\n.01\n.01\n.01\n.01\n.01\n.01\n.01\n.01\n\n\n\\(h(x_i)\\)\n0.136\n6.644\n6.644\n6.644\n6.644\n6.644\n6.644\n6.644\n6.644\n6.644\n\n\n\n\n\n\nTo be clear, we have take a non-uniform view on the probability distribution for the missing digit, and we’ll refer to this as the prior assumption (or just prior). This is unashamedly an opinionated assumption, just like your intuition when you encountered 32,000,_00! All we are doing is giving a quantitative basis for describing this assumption. Taking a view on a prior distribution is a quantitatively incorporating previously encountered data and professional judgment. Having a prior assumption like this is completely compatible with information theory.\nOur professional judgment notwithstanding: what if we had another colleague who believed humans are completely rational and without bias for certain numbers? They think an asset’s par value need not be rounded at all. They argue for a prior distribution consistent with Table 14.2.\nWith the uniform prior assumption, \\(H(X) = 3.322 \\text{bits}\\) and \\(h(x_i)\\) is also uniform. Note that \\(H\\) is higher for the uniform prior than the prior in Table 14.1. We will not prove it here, but a uniform probability over a set of outcomes is the highest entropy distribution that can be assumed for this problem. A higher entropy prior distribution can typically be viewed as a less biased prior assumption than a lower entropy prior.\n\n\n\nTable 14.2: Probability distribution of missing digit with uniform, maximal entropy for the assumed probability distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\\(p(x_i)\\)\n.10\n.10\n.10\n.10\n.10\n.10\n.10\n.10\n.10\n.10\n\n\n\\(h(x_i)\\)\n3.322\n3.322\n3.322\n3.322\n3.322\n3.322\n3.322\n3.322\n3.322\n3.322\n\n\n\n\n\n\nThe choice of prior assumption can significantly impact the interpretation and analysis of the missing information. If we have strong reasons to believe that the human bias prior is more appropriate given the context (e.g., knowing that the number is likely a round number), then we would expect the missing digit to be ‘0’ with high probability. However, if we have no specific knowledge about the nature of the number and prefer to make a more conservative assumption, the uniform prior may be more suitable.\nIn real-world scenarios, the choice of prior assumptions often depends on domain knowledge, available data, and the specific problem at hand. It is important to carefully consider and justify the prior assumptions used in information-theoretic and statistical analyses.\n\n\n14.3.2 Example: Classificaiton\nIn this example, we will determine the optimal splits for a decision tree2 based on the information gained at each node in the tree.\n\nusing DataFrames\n\nemployed = [true, false, true, true, true, false, false, true]\ngood_credit = [true, true, false, true, false, false, false, true]\ndefault = [true, false, true, true, true, true, false, true]\ndefault_data = DataFrame(; employed, good_credit, default)\n\n\n\nTable 14.3: Fictional data regarding loan attributes and whether or not a loan defaulted before its maturity.\n\n\n\n8×3 DataFrame\n\n\n\nRow\nemployed\ngood_credit\ndefault\n\n\n\nBool\nBool\nBool\n\n\n\n\n1\ntrue\ntrue\ntrue\n\n\n2\nfalse\ntrue\nfalse\n\n\n3\ntrue\nfalse\ntrue\n\n\n4\ntrue\ntrue\ntrue\n\n\n5\ntrue\nfalse\ntrue\n\n\n6\nfalse\nfalse\ntrue\n\n\n7\nfalse\nfalse\nfalse\n\n\n8\ntrue\ntrue\ntrue\n\n\n\n\n\n\n\n\n\nThe entropy of the default rate data is, per Equation 14.2:\n\nH0 = let\n    p_default = sum(default_data.default) / nrow(default_data)\n    p_good = 1 - p_default\n    p_default * log2(1 / p_default) + p_good * log(1 / p_good)\nend\n\n0.6578517147391054\n\n\nOur goal is to determine which attribute (employed or good_credit) to use as the first split in the decision tree. Intuitively, we are looking for the most important factor in predicting default rates. We will quantitatively evaluate this by calculating the information gain, which is the difference in entropy between the prior node and the candidate node. Whichever criteria gains us the most information is the preferred attribute to create a decision split.\nIn our case we start with H0 as calculated above for the output variable default and calculate the difference in entropy between it and the average entropy of the data if we split on that node. The name for this is the information gain, \\(IG(inputs, attributes)\\):\n\\[\nIG(T,a) = H{(T)} - H{(T|a)}\n\\]\nIn words, the the information gain is simply the difference in entropy before and after learning the value of an outcome \\(a\\). We will illustrate that by determining the first branch in the decision tree.\nLet’s first consider splitting the tree based on the employed status. We will calculate the entropy of each subset: with employment and without employment.\nIf we split the data based on being employed, we’d get two sub-datasets:\n\ndf_employed = filter(:employed =&gt; ==(true), default_data)\n\n5×3 DataFrame\n\n\n\nRow\nemployed\ngood_credit\ndefault\n\n\n\nBool\nBool\nBool\n\n\n\n\n1\ntrue\ntrue\ntrue\n\n\n2\ntrue\nfalse\ntrue\n\n\n3\ntrue\ntrue\ntrue\n\n\n4\ntrue\nfalse\ntrue\n\n\n5\ntrue\ntrue\ntrue\n\n\n\n\n\n\nand\n\ndf_unemployed = filter(:employed =&gt; ==(false), default_data)\n\n3×3 DataFrame\n\n\n\nRow\nemployed\ngood_credit\ndefault\n\n\n\nBool\nBool\nBool\n\n\n\n\n1\nfalse\ntrue\nfalse\n\n\n2\nfalse\nfalse\ntrue\n\n\n3\nfalse\nfalse\nfalse\n\n\n\n\n\n\nLet’s call its entropy H_employed, which should be zero because there is no variability in the default outcome for this subset.\n\nH_employed = let\n    p_default = sum(df_employed.default) / nrow(df_employed)\n    p_good = 1 - p_default\n    # p_default * log2(1 / p_default) + p_good * log(1 / p_good) \n1    p_default * log2(1 / p_default) + 0\nend\n\n\n1\n\nIn the case of \\(p_i = 0\\) the value of \\(h\\) (the second term in the sum above) is taken to be \\(0\\), which is consistent with the \\(\\lim_{p\\to0^+}p\\log (p) = 0\\).\n\n\n\n\n0.0\n\n\nAnd the corresponding candidate leaf is H_unemployed:\n\nH_unemployed = let\n    p_default = sum(df_unemployed.default) / nrow(df_unemployed)\n    p_good = 1 - p_default\n    p_default * log2(1 / p_default) + p_good * log(1 / p_good)\nend\n\n0.7986309056458281\n\n\nTo balance these two results, we weight them according to the amount of data (number of observations) that would fall into each leaf:\n\nH1_employment = let\n    p_emp = nrow(df_employed) / nrow(default_data)\n    p_unemp = 1 - p_emp\n\n    p_emp * H_employed + p_unemp * H_unemployed\nend\n\n0.29948658961718555\n\n\nThe information gain for splitting the tree using employment status is the difference between the root entropy and the entropy of the employment split:\n\nIG_employment = H0 - H1_employment\n\n0.35836512512191987\n\n\nWe could repeat the analysis to determine the information gain if we were to split the tree based on having good credit. However, given that there are only two attributes we can already conclude that employed is a better attribute to split the data on. This is because the information gain of IG_employment (0.358) is the majority of the overall entropy H0 (0.658). Entropy is always additive and you cannot have negative entropy, therefore no other other attribute could have greater information gain. This also matches our intuition when looking at Table 14.3 as the eye can spot a higher correlation between employed and default than good_credit and default.\nThe above example demonstrates how we can use information theory to create more optimal inferences on data.\n\n\n14.3.3 Maximum Entropy Distributions\nWhy is information theory a useful concept? Many financial models are statistical in nature and concepts of randomness and entropy are foundational. For example, when trying to estimate parameter distributions or assume a distribution for a random process you can lean on information theory to use the most conservative choice: the distribution with the highest entropy given known constraints. These distributions are referred to as maximum entropy (maxent) distributions.\nIn many real-world problems, we seek a distribution that is “least biased” or “most conservative” given certain known information (such as a mean or a range). Maximum entropy distributions accomplish this by spreading out probability as widely as possible under the constraints we know to be true (e.g., average value, bounded domain). They make no additional assumptions beyond those constraints, thereby avoiding unwarranted specificity.\nBy using a maxent distribution, we effectively acknowledge that everything else about the system’s behavior is unknown and should remain as “random” (unconstrained) as possible. This is a powerful principle because it aligns well with real-world modeling scenarios where we might know just a few key facts—like a process’s average rate or finite variance—but have no strong reason to assume anything else about its structure.\nMany of the most common probability distributions (Normal, Exponential, Gamma, etc.) can be derived by applying the maximum entropy principle under simple, natural constraints: - Normal distribution when the mean and variance are finite but otherwise unconstrained.\n- Exponential distribution when we know only that the mean is positive, with outcomes over ([0,)).\n- Uniform distribution when outcomes are bounded within a certain interval, and we have no further information about how likely each point is.\nAdditional maxent distributions and associated constraints are listed in Table 14.4. Those distributions arise again and again in nature because of the second law of thermodynamics - nature likes to have constantly increasing entropy and therefore it should be no surprise (random) processes that maximize entropy pop up all over the place. The second law of thermodynamics in physics is an analogy: it states that a closed system tends to move toward higher entropy states. Similarly, in purely probabilistic settings, when few constraints are imposed, the system’s “natural” distribution tends to be the one that maximizes entropy.\nMaxent distributions have practical modeling use:\n\nConservative Assumptions: Using a maxent distribution guards against over-fitting or adding hidden assumptions. It essentially says: “Given only these constraints, let the data spread out in the most uniform (least structured) way consistent with what I know.”\n\nSimplicity and Clarity: It’s often easier to justify a maxent model to stakeholders or regulators. If you only know a mean and a variance, a Normal distribution may be the least-biased fit. If you only know a mean and posit that values must be positive, the Exponential distribution is your maxent choice.\n\nBuilt-In Neutrality: In financial or actuarial contexts, adopting a maxent framework can prevent overly optimistic or pessimistic models. By sticking to the distribution with the fewest assumptions, the risk analysis remains transparent and more robust to model mis-specification.\n\nSome discussion of maximum entropy distributions in the context of risk assessment is available in an article by Duracz3.\n\n\n\nTable 14.4: Maximum Entropy Distributions and the conditions under which they are applicable. For example, if you know that a probability must be continuous and have a positive mean (and can’t be normalized), then the MED is the Exponential Distribution.\n\n\n\n\n\n\n\n\n\n\n\nConstraint\nDiscrete Distribution\nContinuous Distribution\n\n\n\n\n\nBounded range\nUniform (discrete)\nUniform (continuous)\n\n\n\nBounded range (0 to 1) with information about the mean or variance\n\nBeta\n\n\n\nMean is finite, two possible values\nBinomial\n\n\n\n\nMean is finite and positive\nGeometric\nExponential\n\n\n\nMean is finite and range is &gt; zero\n\nGamma\n\n\n\nMean and Variance is finite\n\nGaussian (Normal)\n\n\n\nPositive and equal mean and variance\nPoisson\n\n\n\n\n\n\n\n\nAs an example, let’s look at processes that behave like the Gaussian (Normal) distribution.\n\n14.3.3.1 Processes that give rise to certain distributions\nA random walk can be viewed as the cumulative impact of nudges pushing in opposite directions. This behavior culminates in the random, terminal position being able to be described by a Gaussian distribution. The center of a Gaussian distribution is “thick” because there are many more ways for the cumulative total nudges to mostly cancel out, while its increasingly rare to end up further and further from the starting point (mean). The distribution then spreads out as flat (randomly) as it can while still maintaining the constraint of having a given, finite variance. Any other continuous distribution that has the same mean and variance has lower entropy than the Gaussian.\n\n\n\nTable 14.5: Underlying processes create typical probability distributions. That there is significant overlap with the distributions in Section 14.3.3 is not a coincidence.\n\n\n\n\n\n\n\n\n\n\nProcess\nDistribution of Data\nExamples\n\n\n\n\nMany additive pluses and minus that move an outcome in one dimension\nNormal\nSum of many dice rolls, errors in measurements, sample means (Central Limit Theorem)\n\n\nMany multiplicative pluses and minus that move an outcome in one dimension\nLog-normal\nIncomes, sizes of cities, stock prices\n\n\nWaiting times between independent events occurring at a constant average rate\nExponential\nTime between radioactive decay events, customer arrivals\n\n\nDiscrete trials each with the same probability of success, counting the number of successes\nBinomial\nCoin flips, defective items in a batch\n\n\nDiscrete trials each with the same probability of success, counting the number of trials until the first success\nGeometric\nNumber of job applications until getting hired\n\n\nContinuous trials each with the same probability of success, measuring the time until the first success\nExponential\nTime until a component fails, time until a sales call results in a sale\n\n\nWaiting time until the r-th event occurs in a Poisson process\nGamma\nTime until the 3rd customer arrives, time until the 5th defect occurs\n\n\n\n\n\n\n\n\n\n\n\n\nTipProbability Distributions\n\n\n\nThere are a lot of specialized distributions. There are lists of distributions you can find online or in references such as Leemis and McQueston (2008) which has a full-page network diagram of the relationships.\nThe information-theoretic and Bayesian perspective on it is to eschew memorization of a bunch of special cases and statistical tests. If you pull up the aforementioned diagram in Leemis and McQueston (2008), you can see just a handful of distributions that have the most central roles in the universe of distributions. Many distributions are simply transformations, limiting instances, or otherwise special cases of a more fundamental distribution. Instead of trying to memorize a bunch of probability distributions, it’s better to think critically about:\n\nThe fundamental processes that give rise to the randomness we are interested in modeling.\nTransformations of the data to make it nicer to work with, such as translations, scaling, or other non-destructive changes.\n\nThen when you encounter an unusual dataset, you don’t need to comb the depths of Wikipedia to find the perfect probability distribution for that situation.\n\n\n\n\n14.3.3.2 Additive and Multiplicative Processes\nTable 14.5 describes some examples, let us discuss further what it means to have a process that arises via an additive vs multiplicative effect4. Additive processes result in a normal distribution while multiplicative processes give rise to a log-normal distribution5.\nAn outcome is additive and results in a normal distribution if it’s the sum or difference of multiple independent processes. Examples of this include:\n\nRolling multiple dice and taking their sum.\nA random walk along the natural numbers wherein with equal probability you take a step left or right.\nCalculating the arithmetic mean of samples (the Central Limit Theorem).\n\nHowever, many processes are multiplicative in nature. For example the population density of cities is distributed in a log-normal fashion. If we think about the factors that contribute to choice of place to live, we can see how these factors multiply: an attractive city might make someone 10% more likely to move, a city with water features 15% more likely, high crime 30% less likely, etc. These forces combine in a multiplicative way in the generative process of deciding where to move. In finance, many price processes are considered multiplicative.\n\n\n\n\n\n\nTip 14.1: Logarithms\n\n\n\nThe logarithm of a geometric process transforms the outcomes into “log-space”. The information is the same, but is often a more convenient form for the analysis. That is, if:\n\\[\nY = x_1 \\times x_2 \\times \\ldots \\times x_i\n\\]\nThen,\n\\[\nlog(Y) = log(x_1) + log(x_2) + \\ldots + \\log(x_i)\n\\]\nThis is effectively the transformation that gives rise to the Normal versus Log-Normal distribution.\n\nIn the context of computational thinking:\nFirst, we should think about how to transform data or modeling outcomes into a more convenient format. The log transform doesn’t eliminate any information but may map the information into a shape that is easier for an optimizer or Monte Carlo simulation to explore.\nSecond, per Chapter 5, floating point math is a lossy transformation of real numbers into a digital computer representation. Some information (in the literal Shannon information sense) is lost when computing and this tends to be worst with very small real numbers, such as those we encounter frequently in probabilities and likelihoods. Logarithms map very small numbers into negative numbers that don’t encounter the same degree of truncation error that tiny numbers do\nThird, modern CPUs are generally much faster at adding or subtracting numbers than multiplying or dividing. Therefore working with the logarithm of processes may be computationally faster than the direct process itself.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical Inference and Information Theory</span>"
    ]
  },
  {
    "objectID": "statistics.html#sec-bayes-rule",
    "href": "statistics.html#sec-bayes-rule",
    "title": "14  Statistical Inference and Information Theory",
    "section": "14.4 Bayes’ Rule",
    "text": "14.4 Bayes’ Rule\nWith some of the foundational concepts laid down, we now turn to the perpetual challenge of attempting to make inferences and predictions given a set of data. We covered basic information theory, probability distributions, log transformations, and random processes because modern statistical analysis relies heavily on those concepts and techniques. We’ll introduce Bayes’ Rule, but then analysis beyond trivial applications one will typically quickly encounter challenges posed by those ideas.\nThe remainder of this chapter will re-introduce Bayes’ Rule and then build up modeling applications that illustrate some core concepts of applying Bayes Rule to complex, data-intensive problems.\n\n14.4.1 Bayes’ Rule Formula\nThe minister and statistician Thomas Bayes derived a relationship of conditional probabilities that we today know as Bayes’ Rule. Laplace6 furthered the notion, and developed the modern formulation. commonly written as:\n\\[\nP(H|D) = \\frac{P(D|H) \\times P(H)}{P(D)}\n\\]\nThe components of this are:\n\n\\(P(H∣D)\\) is the conditional probability of event \\(H\\) occurring given that \\(D\\) is true.\n\\(P(D∣H)\\) is the conditional probability of event \\(D\\) occurring given that \\(H\\) is true.\n\\(P(H)\\) is the prior probability of event \\(H\\).\n\\(P(D)\\) is the prior probability of event \\(D\\).\n\nIf we take the following:\n\n\\(D\\) is the available data\n\\(H\\) is our hypothesis\n\nThen we can draw conclusions about the probability of a hypothesis being true given the observed data. When thought about this way, Bayes’ rule is often described as:\n\\[\n\\text{posterior} = \\frac{\\text{likelihood} \\times \\text{prior}}{\\text{evidence}}\n\\]\nThis is a very useful framework, which we’ll return to more completely in Section 14.5. First, let’s look at combining information theory and Bayes’ rule in an applied example.\n\n\n14.4.2 Model Selection via Likelihoods\nLet’s say that we have competing hypothesis about a data generating process, such as: “given a set of data representing risk outcomes, what distribution best fits the data”? We will not be able to determine an absolute probability of a model given the data, but amazingly we can determine relative probability of models given the data. This is powerful because often times one of the most difficult modeling tasks is to select a model formulation - and Bayes gives us a powerful tool to help choose.\nWe can compare these models using Bayes’ rules by observing the following: Suppose we have two models, \\(H_1\\) and \\(H_2\\), and we want to compare their likelihoods given the observed data, D. We can use Bayes’ rule to calculate the posterior probability of each model:\n\\[\nP(H_1\\|D) = (P(D\\|H_1) \\times P(H_1)) / P(D)\n\\]\n\\[\nP(H_2\\|D) = (P(D\\|H_2) \\times P(H_2)) / P(D)\n\\]\nWhere:\n\n\\(P(H_1|D)\\) and \\(P(H_2|D)\\) are the posterior probabilities of models \\(H_1\\) and \\(H_2\\), respectively, given the data \\(D\\).\n\\(P(D|H_1)\\) and \\(P(D|H_2)\\) are the likelihoods of the data \\(D\\) under models \\(H_1\\) and \\(H_2\\), respectively.\n\\(P(H_1)\\) and \\(P(H_2)\\) are the prior probabilities of models \\(H_1\\) and \\(H_2\\), respectively.\n\\(P(D)\\) is the marginal likelihood of the data, which serves as a normalizing constant.\n\nTo compare the likelihoods of the two models, we can calculate the ratio of their posterior probabilities, known as the Bayes factor, \\(BF\\):\n\\[\nBF = \\frac{P(H_1|D)}{P(H_2|D)}\n\\]\nSubstituting the expressions for the posterior probabilities from Bayes’ rule, we get:\n\\[\nBF = \\frac{P(D|H_1) \\times P(H_1)} {P(D|H_2) \\times P(H_2)}\n\\]\nThe marginal likelihood \\(P(D)\\) cancels out since it appears in both the numerator and denominator. If we assume equal prior probabilities for the models, i.e., \\(P(H_1)\\) = \\(P(H_2)\\), then the Bayes factor simplifies to the likelihood ratio: \\[\nBF = \\frac{P(D|H_1)}{P(D|H_2)}\n\\]\nThe Bayes factor then is a statement about the relative probability of two competing models for the given data. We can interpret the results as:\n\nIf \\(BF &gt; 1\\), the data favors model \\(H_1\\) over model \\(H_2\\).\nIf \\(BF &lt; 1\\), the data favor model \\(H_2\\) over model \\(H_1\\).\nIf \\(BF = 1\\), the data do not provide evidence in favor of either model.\n\nIn practice, the likelihoods \\(P(D|H_1)\\) and \\(P(D|H_2)\\) are often calculated using the probability density or mass functions of the models, evaluated at the observed data points. The prior probabilities \\(P(H_1)\\) and \\(P(H_2)\\) can be assigned based on prior knowledge or assumptions about the models. By comparing the likelihoods of the models using the Bayes factor, we can quantify the relative support for each model given the observed data, while taking into account the prior probabilities of the models.\nAnother way of interpreting this is the evaluation of which model has the higher likelihood given the data.\n\n\n\n\n\n\nWarningNull Hypothesis Statistical Test\n\n\n\nNull Hypothesis Statistical Tests (NHST) is the idea of trying to statistically support an alternative hypothesis over a null hypothesis. The support in favor of alternative versus the null is reported via some statistical power, such as the p-value (the probability that the test result is as, or more extreme, than the value computed). The idea is that there’s some objective way to push science towards greater truths and NHST was seen as a methodology that avoided the subjectivity of the Bayesian approach. However, while pure in intention, the NHST choices of both null hypothesis and model contain significant amounts of subjectivity! There is subjectivity in the null hypothesis, data collection methodologies, study design, handling of missing data, choice of data not to include, which statistical tests to perform, and interpretation of relationships.\nWe might as well call the null hypothesis a prior and stop trying to disprove it absolutely. Instead: focus on model comparison, model structure, and posterior probabilities of the competing theories.\nOver 100 statistical tests have been developed in service of NHST Lewis (2013), but it’s widely viewed now that a focus on NHST has led to worse science due to a multitude of factors, such as:\n\n“P-hacking” or trying to find subsets of data which can (often only by chance) support rejecting some null.\nCognitive anchoring to the importance of a p-value of 0.05 or less – why choose that number versus 0.01 or 0.001 or 0.49?\nBias in research processes where one may stop data collection or experimentation after achieving a favorable test result.\nInappropriate application of the myriad of statistical tests.\nFocus on p-values rather than effects that simply matter more or have greater effect.\n\nFor example, which is of more interest to doctors? A study indicating a 1 in a billion chance of serious side effect , with p-value of 0.0001 or a study indicating a 1 in 3 chance with p-value 0.06? Many journals would only publish the former study, even though the latter study intuitively suggests a potentially more risky drug.\n\nDifficulty to determine causal relationships.\n\nThe authors of this book recommend against basic NHST and memorization of statistical tests in favor of principled Bayesian approaches. For the actuarial readers, NHST is analogous to traditional credibility methods (of which the authors also prefer more modern statistical approaches).\n\n\n\n14.4.2.1 Example: Rainfall Risk Model Comparison\nThe example we’ll look at relates to the annual rainfall totals for a specific location in California7, which could be useful for insuring flood risk or determining the value of a catastrophe bond. Acknowledging that we are attempting to create a geocentric model8 instead of a scientifically accurate weather model, we narrow the problem to finding a probability distribution that matches the historical rainfall totals.\nOur goal is to recommend a model that best fits the data and justify that recommendation quantitatively. Before even looking at the data, Table 14.6 shows three competing models based on thinking about the real-world outcome we are trying to model. These three are chosen for the increasingly sophisticated thought process that might lead the modeler to recommend them - but which is supportable by the statistics?\n\n\n\nTable 14.6: Three alternative hypothesis about the distribution of annual rainfall totals.\n\n\n\n\n\n\n\n\n\n\nHypothesis\nProcess\nPossible Rationale\n\n\n\n\n\\(H_1\\)\nA Normal (Gaussian) distribution\nThe sum of independent rainstorms creates annual rainfall totals that are normally distributed\n\n\n\\(H_2\\)\nA LogNormal distribution\nSince it’s normal-ish, but skewed and can’t be negative\n\n\n\\(H_3\\)\nA Gamma Distribution\nSince rainfall totals would be the sum of exponentially-distributed independent rainfall events\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the literature for rainfall modeling, \\(H_3\\) (the Gamma distribution) is known as the “Log-Pearson Type III distribution”. It’s actually recommended by the US Corps of Army Engineers as the recommended way to model rainfall totals.\nWe are able to avoid learning and memorizing specialty distributions and statistical tests, which are so common in Frequentist approaches. First-principles reasoning on the probabilistic processes can get one to a reasonable hypothesis, comparable to ‘specialist’ knowledge one would encounter in the literature for a particular applied field.\n\n\nHere’s the data:\n\nrain = [\n    39.51, 42.65, 44.09, 41.92, 28.42, 58.65, 30.18, 64.4, 29.02,\n    37.00, 32.17, 36.37, 47.55, 27.71, 58.26, 36.55, 49.57, 39.84,\n    82.22, 47.58, 51.18, 32.28, 52.48, 65.24, 51.12, 25.03, 23.27,\n    26.11, 47.3, 31.8, 61.45, 94.95, 34.8, 49.53, 28.65, 35.3, 34.8,\n    27.45, 20.7, 36.99, 60.54, 22.5, 64.85, 43.1, 37.55, 82.05, 27.9,\n    36.55, 28.7, 29.25, 42.32, 31.93, 41.8, 55.9, 20.65, 29.28, 18.4,\n    39.31, 20.36, 22.73, 12.75, 23.35, 29.59, 44.47, 20.06, 46.48,\n    13.46, 9.34, 16.51, 48.24\n];\n\nPlotted, we see some of the characteristics that align with our prior assumptions and knowledge about the system itself, such as: the data being constrained to positive values and a skew towards having some extreme weather years with lots of rainfall.\n\nusing CairoMakie\nhist(rain)\n\n\n\n\n\n\nFigure 14.1: Annual rainfall totals for a specific location in California.\n\n\n\n\nWe will show the likelihood of the three models after deriving the maximum likelihood (MLE), which is simply finding the parameters that maximize the calculated likelihood. In general, this can be accomplished by an optimization routine, but here we will just use the functions built into Distributions.jl:\n\nusing StatsBase\nusing Distributions\n\nn = fit_mle(Normal, rain)\nln = fit_mle(Normal, log.(rain))\nlg = fit_mle(Gamma, log.(rain))\n@show n\n@show ln\n@show lg;\n\nn = Distributions.Normal{Float64}(μ=38.91442857142857, σ=16.643603630714306)\nln = Distributions.Normal{Float64}(μ=3.5690550009062663, σ=0.44148379736539156)\nlg = Distributions.Gamma{Float64}(α=61.58531301458412, θ=0.05795302201453571)\n\n\nLet’s look at the likelihoods by applying the maximum likelihood distribution to the observed data. For the practical reasons described in Tip 14.1, we will compare the the log-likelihoods to maintain convention with what you’d likely see or deal with in practice. Taking the log of the likelihood does not change the ranking of the likelihoods.\n\n let\n    n_lik = sum(log.(pdf.(n, rain)))\n    ln_lik = sum(log.(pdf.(ln, log.(rain))))\n    lg_lik = sum(log.(pdf.(lg, log.(rain))))\n\n    @show n_lik\n    @show ln_lik\n    @show lg_lik\nend;\n\nn_lik = -296.16751566478115\nln_lik = -42.09272021737913\nlg_lik = -43.79151806348801\n\n\nThe results indicate that the LogNormal and the Gamma model for rainfall distribution are very superior to the Normal model, consistent with the visual inspection of the quantiles in Figure 14.2. We reach that conclusion by noting how much more likely the latter two are, as the likelihoods of \\(-42\\) and \\(-44\\) is much greater than \\(-296\\)9.\n\nlet x = rain\n\n    range = 1:0.1:100\n    fig, ax, _ = lines(range, cdf.(n, range), label=\"Normal\", axis=(xgridvisible=false, ygridvisible=false,))\n    lines!(ax, range, cdf.(ln, log.(range)), label=\"LogNormal\")\n    lines!(range, cdf.(lg, log.(range)), label=\"LogGamma\")\n    lines!(quantile.(Ref(x), 0.01:0.01:0.99), 0.01:0.01:0.99, label=\"Data\", color=(:black, 0.6), linewidth=3)\n    fig[1, 2] = Legend(fig, ax, \"Model\", framevisible=false)\n    fig\nend\n\n\n\n\n\n\nFigure 14.2\n\n\n\n\nWe evaluated the likelihood at a single point estimate of the parameters, but a true posterior probability of the parameters of the distributions will be represented by a distribution rather than a point. The rest of chapter will describe how to express the posterior probabilities of the the parameters for \\(H_1\\), \\(H_2\\), and \\(H_3\\) using Bayesian statistical methods.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical Inference and Information Theory</span>"
    ]
  },
  {
    "objectID": "statistics.html#sec-modern-bayes",
    "href": "statistics.html#sec-modern-bayes",
    "title": "14  Statistical Inference and Information Theory",
    "section": "14.5 Modern Bayesian Statistics",
    "text": "14.5 Modern Bayesian Statistics\n\n14.5.1 Background\nBayesian statistics is generally not taught in undergraduate statistics. Bayes’ rule is introduced, basic probability exercises are assigned, and then statistics moves on to a curriculum of regression and NHSTs (of the Frequentist school). Why is the applied practice of statistics currently gravitating towards Bayesian approaches? There are both philosophical and practical reasons why.\n\n14.5.1.1 Philosophical Motivations\nPhilosophically, one of the main reasons why Bayesian thinking is appealing is its ability to provide straightforward interpretations of statistical conclusions.\nFor example, when estimating an unknown quantity, a Bayesian probability interval can be directly understood as having a high probability of containing that quantity. In contrast, a Frequentist confidence interval is typically interpreted only in the context of a series of similar inferences that could be made in repeated practice. In recent years, there has been a growing emphasis on interval estimation rather than hypothesis testing in applied statistics. This shift has strengthened the Bayesian perspective since it is likely that many users of standard confidence intervals intuitively interpret them in a manner consistent with Bayesian thinking.\n\n\n\n\n\n\nNote“The fallacy of placing confidence in confidence intervals”\n\n\n\n“The fallacy of placing confidence in confidence intervals” is the title of an article (Morey_Hoekstra_Rouder_Lee_Wagenmakers_2016?) describing the issues with confidence intervals, with one of the primary issues being what they call the “Fundamental Confidence Fallacy,” or FCF.\nThe FCF is the belief that if you have a \\(X\\%\\) confidence interval, that the probability that the true value of interest lies within that interval is \\(X\\%\\). This is false, and an abbreviated explanation of why is as follows:\n\nAn \\(X\\%\\) confidence interval refers to a procedure that results in a range \\(R\\) wherein \\(X\\%\\) of the time the true value of interest lies within \\(R\\).\nHowever, once the data is observed we can know with higher probability than \\(X\\) whether the true value lies within that range.\n\nHere’s an example taken from the article referenced above: we wish to estimate the mean of a continuous random variable. We observe two data points \\(y_1\\) and \\(y_2\\). If \\(y_1 &lt; y_2\\) then we say our interval is \\((-\\infty,\\infty)\\) otherwise the interval is empty. Our credibility procedure creates an interval for which 50% of the time it contains the true value. However, once we’ve observed the data and created the interval, then “post-data” we can tell with certainty whether our interval contains the variable of interest. We can only say “pre-data”, or pre-observations, that the credibility interval is really \\(X\\%\\) probable to contain the right value. Similar (and other) issues arises with more “real world” examples of confidence intervals.\nIn contrast, the Bayesian procedure of estimating an interval using the posterior distribution of the data can be interpreted as an interval for which you can say that you believe the true value is contained within the interval. Typically, this is referred to as a credibility interval to distinguish from a confidence interval.\n\n\nAnother meaningful way to understand the contrast between Bayesian and Frequentist approaches is through the lens of decision theory, specifically how each view treats the concept of randomness. This perspective pertains to whether you regard the data being random or the parameters being random.\nFrequentist statistics treats parameters as fixed and unknown, and the data as random — that data you collect is but one realization of an infinitely repeatable random process. Consequently, Frequentist procedures, like hypothesis testing or confidence intervals, are generally based on the idea of long-run frequency or repeatable sampling.\nConversely, Bayesian statistics turns this on its head by treating the data as fixed — after all, once you’ve collected your data, it’s no longer random but a fixed observed quantity. Parameters, which are unknown, are treated as random variables. The Bayesian approach then allows us to use probability to quantify our uncertainty about these parameters.\nThe Bayesian approach tends to align more closely with our intuitive way of reasoning about problems. Often, you are given specific data and you want to understand what that particular set of data tells you about the world. You’re likely less interested in what might happen if you had infinite data, but rather in drawing the best conclusions you can from the data you do have.\n\n\n14.5.1.2 Practical Motivations\nPractically, recent advances in computational power, algorithm development, and open-source libraries have enabled practitioners to adapt the Bayesian workflow.\nFor most real-world problems, deriving the posterior distribution is analytically intractable and computational methods must be used. Advances in raw computing power only in the 1990’s made non-trivial Bayesian analysis possible, and recent advances in algorithms have made the computations more efficient. For example, one of the most popular algorithms, NUTS, was only published in the 2010’s.\nMany problems require the use of compute clusters to manage runtime, but if there is any place to invest in understanding posterior probability distributions, it’s financial companies trying to manage risk!\nThe availability of open-source libraries, such as Turing.jl, PyMC3, and Stan provide access to the core routines in an accessible interface. To get the most out of these tools requires the mindset of computational thinking described in this book - understanding model complexity, model transformations and structure, data types and program organization, etc.\n\n\n14.5.1.3 Advantages of the Bayesian Approach\nThe main advantages of this approach over traditional actuarial techniques are:\n\nFocus on distributions rather than point estimates of the posterior’s mean or mode. We are often interested in the distribution of the parameters and a focus on a single parameter estimate will understate the risk distribution.\nModel flexibility. A Bayesian model can be as simple as an ordinary linear regression, but as complex as modeling a full insurance mechanics.\nSimpler mental model. Fundamentally, Bayes’ theorem could be distilled down to an approach where you count the ways that things could occur and update the probabilities accordingly.\nExplicit Assumptions.: Enumerating the random variables in your model and explicitly parameterizing prior assumptions avoids ambiguity of the assumptions inside the statistical model.\n\n\n\n14.5.1.4 Challenges with the Bayesian Approach\nWith the Bayesian approach, there are a handful of things that are challenging. Many of the listed items are not unique to the Bayesian approach, but there are different facets of the issues that arise.\n\nModel Construction. One must be thoughtful about the model and how variables interact. However, with the flexibility of modeling, you can apply (actuarial) science to makes better models!\nModel Diagnostics. Instead of R^2 values, there are unique diagnostics that one must monitor to ensure that the posterior sampling worked as intended.\nModel Complexity and Size of Data. The sampling algorithms are computationally intensive - as the amount of data grows and model complexity grows, the runtime demands cluster computing.\nModel Representation. The statistical derivation of the posterior can only reflect the complexity of the world as defined by your model. A Bayesian model won’t automatically infer all possible real-world relationships and constraints.\n\n\n\n\n\n\n\nNoteSubjectivity of the Priors?\n\n\n\nThere are two ways one might react to subjectivity in a Bayesian context: It’s a feature that should be embraced or it’s a flaw that should be avoided.\n\n14.5.1.5 Subjectivity as a Feature\nA Bayesian approach to defining a statistical model is an approach that allows for explicitly incorporating professional judgment. Encoding assumptions into a Bayesian model forces the actuary to be explicit about otherwise fuzzy predilections. The explicit assumption is also more amenable to productive debate about its merits and biases than an implicit judgmental override.\n\n\n14.5.1.6 Subjectivity as a Flaw\nSubjectivity is inherent in all useful statistical methods. Subjectivity in traditional approaches include how the data was collected, which hypothesis to test, what significant levels to use, and assumptions about the data-generating processes.\nIn fact, the “objective” approach to null hypothesis testing is so prone to abuse and misinterpretation that in 2016, the American Statistical Association issued a statement intended to steer statistical analysis into a “post p&lt;0.05 era.” That “p&lt;0.05” approach is embedded in most traditional approaches to actuarial credibility10 and therefore should be similarly reconsidered.\n\n\n\n\n\n\n14.5.2 Implications for Financial Modeling\nLike Bayes’ Formula itself, another aspect of financial literature that is taught but often glossed over in practice is the difference between process risk (volatility), parameter risk, and model formulation risk. When performing analysis that relies on stochastic results, in practice typically only process/volatility risk is assessed.\nBayesian statistics provides the tools to help financial modelers address parameter risk and model formulation. The posterior distribution of parameters derived is consistent with the observed data and modeled relationships. This posterior distribution of parameters can then be run as an additional dimension to the risk analysis.\nAdditionally, best practices include skepticism of the model construction itself, and testing different formulation of the modeled relationships and variable combinations to identify models which are best fit for purpose. Tools such as Information Criterion, posterior predictive checks, Bayes factors, and other statistical diagnostics can inform the actuary about trade-offs between different choices of model.\n\n\n\n\n\n\nNoteBayesian Versus Machine Learning\n\n\n\nMachine learning (ML) is fully compatible with Bayesian analysis - one can derive posterior distributions for the ML parameters like any other statistical model and the combination of approaches may be fruitful in practice.\nHowever, to the extent that actuaries have leaned on ML approaches due to the shortcomings of traditional actuarial approaches, Bayesian modeling may provide an attractive alternative without resorting to notoriously finicky and difficult-to-explain ML models. The Bayesian framework provides an explainable model and offers several analytic extensions beyond the scope of this introductory chapter:\n\nCausal Modeling: Identifying not just correlated relationships, but causal ones, in contexts where a traditional designed experiment is unavailable.\nBayes Action: Optimizing a parameter for, e.g., a CTE95 level instead of a parameter mean.\nInformation Criterion: Principled techniques to compare model fit and complexity.\nMissing data: Mechanisms to handle the different kinds of missing data.\nModel averaging: Posteriors can be combined from different models to synthesize different approaches.\nCredibility Intervals: A posterior representation around the likely range of values for parameters of interest.\n\n\n\n\n\n14.5.3 Basics of Bayesian Modeling\nA Bayesian statistical model has four main components to focus on:\n\nPrior encoding assumptions about the random variables related to the problem at hand, before conditioning on the data.\nA Model that defines how the random variables give rise to the observed outcome.\nData that we use to update our prior assumptions.\nPosterior distributions of our random variables, conditioned on the observed data and our model\n\nWhile this is simply stating Bayes’ formula in words, it’s also the blueprint for a workflow to implement more advanced Bayesian methods.\nHaving defined a prior assumption, selected a model, and collected our data, the computation of the posterior can be the most challenging. The workflow involves computationally sampling the posterior distribution, often using a technique called Markov Chain Monte-Carlo (MCMC). The result is a series of values that are sampled statistically from the posterior distribution. Introducing this process is the focus of the rest of this chapter.\n\n\n14.5.4 Markov-Chain Monte Carlo\nComputing the posterior distribution for most model parameters is analytically intractable. However, we can probabilistically sample from the posterior distribution and achieve an approximation of the posterior distribution. MCMC samplers, as they are called, do this by moving through the parameter space (the set of possible values for the parameters) in a special way. The statistical marvel is that they travel to different points in proportion to the posterior probability. It is a “Markov-Chain” because the probability of the next point’s location is influenced by the prior sampling point’s location.\n\n14.5.4.1 Example: MCMC from Scratch\nHere is a simple example demonstrated with one of the oldest MCMC algorithm, called Metropolis-Hastings. The general idea is this:\n\nStart at an arbitrary point and make that the current_state.\nPropose a new point which is the current_state plus some movement that comes from a random distribution, proposal_dist.\nCalculate the likelihood ratio of the proposed versus current point (acceptance_ratio below).\nDraw a random number - if that random number is less than the acceptance_ratio, then move to that new point. Otherwise do not move.\nRepeat steps 2-4 until the distribution of points converges to a stable posterior distribution.\n\nThis gets us what we desire because the resulting distribution of samples has frequency that’s proportional to the posterior distribution.\nWe will try to find the posterior of an arbitrary set of normally distributed asset returns. We set the true, (unobserved in reality) values for \\(\\mu\\) and \\(\\sigma\\) and then draw 250 generated observations:\n\n# In reality, we don't observe the parameters \n# we are interested in determining the values for.\nσ = 0.15\nμ = 0.1\n\n\nn_observations = 250\nreturn_dist = Normal(μ,σ)\nreturns = rand(return_dist,n_observations)\n\n# plot the distribution of returns\nμ_range = LinRange(-0.5, 0.5, 400)\nσ_range = LinRange(0.0, 3.0, 400)\n\nf = Figure()\nax1 = Axis(f[1,1],title=\"True Distribution of Returns\")\nax2 = Axis(f[2,1],title=\"Simulated Outcomes\",xlabel=\"Return\")\nplot!(ax1,return_dist)\nvlines!(ax1,[μ],color=(:black,0.7))\ntext!(ax1,μ,0;text=\"mean ($μ)\",rotation=pi/2)\nhist!(ax2,returns)\nvlines!(ax2,[mean(returns)],color=(:black,0.7))\ntext!(ax2,mean(returns),0;text=\"mean ($(round(mean(returns);digits=3)))\",rotation=pi/2)\n\nlinkxaxes!(ax1,ax2)\n\nf\n\n\nThe target probability densities which we will attempt to infer via MCMC.\n\n\nHaving generated sample data, we will next define a probability distribution for the random step that we take from the current_point on the Markov chain. We choose a 2D Gaussian for this, since the parameter space to explore is two-dimensional ( \\(\\mu\\) and \\(\\theta\\)). The proposal_std controls how big of a movement is taken at each step.\n\n# Define the proposal step distribution\nproposal_std = 0.05\nproposal_dist = Normal(0,proposal_std)\n\nDistributions.Normal{Float64}(μ=0.0, σ=0.05)\n\n\nWe next define how many steps we want the chain to sample for, and implement the algorithm’s main loop containing the logic in the steps above.\n\n# MCMC parameters\nnum_samples = 5000\nburn_in = 500\n\n# Define priors\nμ_prior = Normal(0, 0.25)\nσ_prior = Gamma(0.5)\n\n# Initialize the Markov chain\nμ_current, σ_current =  0.0, 0.25\ncurrent_prob = sum(logpdf(Normal(μ_current, σ_current), r) for r in returns) + \n               logpdf(μ_prior, μ_current) + \n               logpdf(σ_prior, σ_current)\n\nchain = zeros(num_samples, 2)\n\ncount = 0\n\n# MCMC sampling loop\nwhile count &lt; num_samples\n    \n    # Generate a new proposal\n    μ̇, σ̇ = μ_current + rand(proposal_dist), σ_current + rand(proposal_dist)\n    if σ̇ &gt; 0 \n        \n        # Calculate the acceptance ratio\n        \n        proposal_prob = sum(logpdf(Normal(μ̇, σ̇), r) for r in returns) +\n                           logpdf(μ_prior, μ̇) + logpdf(σ_prior, σ̇)\n        log_acceptance_ratio = proposal_prob - current_prob\n        \n        # Accept or reject the proposal\n        if log(rand()) &lt; log_acceptance_ratio\n            μ_current, σ_current = μ̇, σ̇\n            current_prob = proposal_prob\n        end\n        \n        # Store the current state as a sample\n        count += 1\n        chain[count, :] .= μ_current, σ_current\n    else\n        # skip because σ can't be negative\n    end\nend\n\nchain\n\n5000×2 Matrix{Float64}:\n 0.0          0.25\n 0.0          0.25\n 0.0          0.25\n 0.0          0.25\n 0.0          0.25\n 0.0          0.25\n 0.000133385  0.211403\n 0.000133385  0.211403\n 0.000133385  0.211403\n 0.0376266    0.207245\n ⋮            \n 0.111205     0.131187\n 0.111205     0.131187\n 0.115948     0.136149\n 0.115948     0.136149\n 0.115948     0.136149\n 0.115948     0.136149\n 0.115948     0.136149\n 0.115948     0.136149\n 0.115948     0.136149\n\n\nThe resulting chain contains a list of points that the algorithm has moved along during the sampling process. Note that there is a burn-in parameter. This is because we want the chain iterate long enough to be effectively independent of both (1) the starting point for the sample, and (2) that different chains are effectively independent.\nAfter having performed the sampling, we can now visualize the chain versus the target_distribution. A few things to note:\n\nThe red line indicates the “warm up” or “burn-in” phase and we do not consider that as part of the sampled chain because those values are too correlated with the arbitrary starting point.\nThe blue line indicates the path traveled by the Metropolis-Hasting algorithm. Long asides into low-probability regions are possible, but in general the path will traverse areas in proportion to the probability of interest.\n\n\n# Plot the chain\nlet \n    f = Figure()\n    \n    # μ lines\n    ax1 = Axis(f[1, 1], ylabel=\"σ\", xticklabelsvisible=false)\n\n    # burn in lines\n    scatterlines!(ax1,chain[1:burn_in,1], \n                      chain[1:burn_in,2],\n                      color=(:red,0.1),\n                      markercolor=(:red,0.1))\n\n    # sampled lines\n    scatterlines!(ax1,chain[burn_in+1:end,1],\n                      chain[burn_in+1:end,2],\n                      color=(:blue,0.1),\n                      markercolor=(:blue,0.1))\n\n    # μ histogram\n    ax2 = Axis(f[2,1],xlabel=\"μ\", yticklabelsvisible=false)\n    hist!(ax2,chain[burn_in+1:end,1],color=:blue)\n    linkxaxes!(ax1, ax2)\n\n    # σ histogram\n    ax3 = Axis(f[1,2],xticklabelsvisible=false)\n    hist!(ax3,chain[burn_in+1:end,2],color=:blue, direction=:x)\n    linkyaxes!(ax1, ax3)\n\n    f\nend\n\n\n\n\n\n\nFigure 14.3: The blue lines of the MCMC chain explore the posterior density of interest (after discarding the burn-in samples in red). Note that locations where the sampler remained longer (rejected more proposals) show up as darker points.\n\n\n\n\nIn this example, \\(\\mu\\) and \\(\\sigma\\) are independent, but if there were a correlation (such as when \\(\\mu\\) were higher, \\(\\sigma\\) were also higher) then the sampler would pick up on this, and we would see a skew in the plotted chain.\nThe point of this short, ground-up introduction to MCMC is that the technique is not magic by demonstrating that we could do-it-from-scratch with small amounts of code. The challenge is that it’s just computationally intensive. Modern libraries perform the sampling for you with more advanced algorithms than Metropolis-Hastings.\n\n\n\n14.5.5 MCMC Algorithms\nThe Metropolis-Hasting algorithm is simple, but somewhat inefficient. Some challenges with MCMC sampling are both mathematical and computational:\n\nOften times the algorithm will back-track (take a “U-Turn”), wasting steps in regions already explored.\nThe algorithm can have a very high rate of rejecting proposals if the proposal mechanism generates steps that would move the current state into a low-probability regions.\nThe choice of proposal distribution and parameters can greatly influence the speed of convergence. Too large of movement and key regions can be entirely skipped over, while small movements can take much longer than necessary to explore the space.\nAs the number of parameters grows, the dimensionality of the parameter space to explore also grows making posterior exploration much harder.\nThe shape of the posterior space can be more or less difficult to explore. Complex models may have regions of density that are not nicely “round” - regions may be curved, donut shaped, or disjointed.\n\nThe issues above mean that MCMC sampling is very computationally expensive for more complex examples. Compared with Metropolis-Hastings, modern algorithms (such as the No-U-Turn (NUTS)) algorithm explore the posterior distribution more efficiently by avoiding back-tracking to already explored regions and dynamically adjusting the proposals to adaptively fit the posterior. Many of them take direct influence from particle physics, with the algorithm keeping track of the energy of the current state as it explores the posterior space.\nAlgorithms have only brought so much relief to the modeler with finite resources and compute. There is still a lot of responsibility for modeler to design models that are computationally efficient, transformed to eliminate oddly-shaped density regions, or find the right simplifications to the analysis in order to make the problem tractable.\n\n\n\n\n\n\nNote\n\n\n\nWhat does it mean to transform the parameter space?\nAn example will be shown in Chapter 31 where we want to ensure that a binomial variable is constrained to the region \\([0,1]\\) but the underlying factors are allowed to vary across the entire real numbers. We use a logit (or inverse logit, a.k.a. logistic) to transform the parameters to the required probability range for the binomial outcome.\nAnother common transform is “Normalizing” the data to center the data around zero and to scale the outcomes such that the sample standard deviation is equal to one.\n\n\n\n\n14.5.6 Rainfall Example (Continued)\nWe will construct a Bayesian model using the Turing.jl library. Using a battle-tested library allows us to step back from the intricacies of defining our own sampler and routine and focus on the models and analysis. The goal is to fit the parameters of one of the competing models from above in order to demonstrate an MCMC analysis workflow and essential concepts.\nThe first thing that we will do is use Turing’s @model macro to define a model. This has a few components:\n\nThe “model” is really just a Julia function that takes in data and relates the data to the statistical outcomes modeled.\nThe ~ is the syntax to either relate a parameter to a prior assumptions.\nA loop (or broadcasted .~) that ties specific data observations to the random process.\n\nThink of the @model block really as a model constructor. It isn’t until we pass data to the model that you get a fully instantiated Model type11.\nHere’s what defining the LogNormal model looks like in Turing. We have to specify prior distributions for LogNormal parameters.\n\nusing Turing\n\n1@model function rainLogNormal(logdata)\n    \n    # Prior Assumptions for the (Log) Normal Parameters\n2    μ ~ Normal(4,1)\n3    σ ~ Exponential(0.5)\n\n    # Link observations to the random process\n    for i in 1:length(logdata)\n        logdata[i] ~ Normal(μ, σ)\n    end\nend\n\nm = rainLogNormal(log.(rain));\n\n\n1\n\nDefining the model uses the @model macro from Turing.\n\n2\n\nWe presume that there will be positive rainfall and 96% of mean annual rainfall will be somewhere between \\(exp(2)\\) and \\(exp(6)\\), or 7 and 403 inches.\n\n3\n\nIn a LogNormal model, 0.5 deviations covers a lot of variation in outcomes.\n\n\n\n\n\n14.5.6.1 Setting Priors\nIn the example above, we used “weakly informative” priors. We constrained the prior probability to plausible ranges, knowing enough about the system of study (rainfall) that it would be completely implausible for there to be a Uniform(0,Inf) distribution of mean log-rainfall total, knowing that rain can’t fall in infinite quantities.\nAdmittedly, we haven’t confirmed with a meteorologist that \\(exp(20)\\) (485 million) inches of rain per year is impossible. But such is the beauty of the transparency of Bayesian analysis that the prior assumption is right there! Front and center and ready to be debated by other modelers! If you think that 485 million inches of rain is possible next year than you can challenge this assumption and propose another explicit alternative.\n“Strongly informative” priors would be something where we want to encode a stronger assumption about the plausible range of outcomes, such as if we knew enough about the problem domain that we could tell given the location of the rainfall, we’d expect 95% of the rainfall to be between, say, 10 and 30 inches per year. Then we could constrain the prior even more than we did above.\n“Uninformative” priors use only maximum entropy or uniform priors to avoid encoding other assumptions into the model.\n\n\n14.5.6.2 Sampling\nAnalysis should begin by evaluating the prior assumptions for reasonability and coverage over possible outcomes of the process we are trying to model. The top plot in Figure 14.7 shows the modeled rainfall outcomes taking on a wide range of possible outcomes. If we had more knowledge of the system we could enforce a stronger (narrower) prior assumption to constrain the model to a smaller set of values.\nThe object returned is an MCMCChains structure containing the samples as well as diagnostic information. Summary information gets printed below.\n\nchain_prior = sample(m, Prior(), 1000)\n\n\n\n\nChains MCMC chain (1000×3×1 Array{Float64, 3}):\n\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 0.22 seconds\nCompute duration  = 0.22 seconds\nparameters        = μ, σ\ninternals         = lp\n\nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ⋯\n      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ⋯\n\n           μ    3.9561    1.0009    0.0307   1066.1120   1025.2091    0.9992   ⋯\n           σ    0.5094    0.5210    0.0159   1043.4048    926.2347    1.0002   ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           μ    2.0119    3.3067    3.9263    4.6431    5.9645\n           σ    0.0159    0.1508    0.3362    0.7000    1.9804\n\n\n\n\nFigure 14.4: Model output for the sampled prior. This isn’t running an MCMC algorithm, it’s simply taking draws from the defined prior assumptions.\n\n\n\n\nAssessment of samples from the prior should include:\n\nConfirming that the model’s behavior is reasonable that\nConfirming that the model covers the range of possible data that might be observed.\n\nThe sample outcomes from the modeled prior are shown in Figure 14.7.\nNext, we sample the posterior by using the No-U-Turns (NUTS) algorithm and drawing 1000 samples (not including the warm-up phase). This is the primary result we will analyze further.\n\nchain_posterior = sample(m,NUTS(),1000)\n\n\n\n\nChains MCMC chain (1000×14×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 2.13 seconds\nCompute duration  = 2.13 seconds\nparameters        = μ, σ\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n\n           μ    3.5662    0.0549    0.0018   952.7601   662.0594    1.0003     ⋯\n           σ    0.4517    0.0419    0.0014   979.5344   560.1388    0.9996     ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           μ    3.4553    3.5294    3.5667    3.6051    3.6746\n           σ    0.3825    0.4207    0.4476    0.4786    0.5405\n\n\n\n\nFigure 14.5: Model output for the sampled posterior.\n\n\n\n\n\n\n14.5.6.3 Diagnostics\nBefore analyzing the result itself, we should check a few things to ensure the model and sampler were well behaved. MCMC techniques are fundamentally stochastic and randomness can cause an errant sampling path. Or a model may be mis-specified such that the parameter space to explore is incompatible with the current algorithm (or any known so far).\nA few things we can check:\nFirst, the ess or effective sample size which adjusts the number of samples for the degree of autocorrelation in the chain.Ideally, we would be able to draw independent samples from the posterior, but due to the Markov-Chain approach the samples can have autocorrelation between neighboring samples. We collect less information about the posterior in the presence of positive autocorrelation.\nAn ess greater than our sample indicates that there was less (negative) autocorrelation than we would have expected for the chain. An ess much less than the number of samples indicates that the chain isn’t sampling very efficiently but, aside from needing to run more samples, isn’t necessarily a problem.\n\ness(chain_posterior)\n\n\nESS\n  parameters        ess   ess_per_sec \n      Symbol    Float64       Float64 \n\n           μ   952.7601      447.7256\n           σ   979.5344      460.3075\n\n\n\n\nSecond, the rhat (\\(\\hat{R}\\)) is the Gelman-Rubin convergence diagnostic and its value should be very close to 1.0 for a chain that has converged properly. Even a value of 1.01 may indicate an issue and quickly gets worse for higher values.\n\nrhat(chain_posterior)\n\n\nR-hat\n  parameters      rhat \n      Symbol   Float64 \n\n           μ    1.0003\n           σ    0.9996\n\n\n\n\nNext, we can look at the “trace” plots for the parameters being sampled (Figure 14.6). These are sometimes called “hairy caterpillar” plots because in a healthy chain sample, we should see a series without autocorrelation and that the values bounce around randomly between individual samples.\n\nlet\n    f = Figure()\n    ax1 = Axis(f[1,1],ylabel=\"μ\")\n    lines!(ax1,vec(get(chain_posterior,:μ).μ.data))\n    ax2 = Axis(f[2,1],ylabel=\"σ\")\n    lines!(ax2,vec(get(chain_posterior,:σ).σ.data))\n    f\nend\n\n\n\n\n\n\nFigure 14.6: The trace plots indicate low autocorrelation which is desirable for an MCMC sample.\n\n\n\n\nThe ess, rhat, and trace plots all look good for our sampled chain so we can next we will analyze the results in the context of our rainfall problem.\n\n\n14.5.6.4 Analysis\nLet’s see how it looks compared to the data first. Figure 14.7 shows 200 samples from the prior and posterior. The prior (top) shows how wide the range of possible rainfall outcomes could be using our weakly informative prior assumptions. The bottom shows that after having learned from the data, the posterior probability of rainfall has narrowed considerably.\n\nfunction chn_cdf!(axis,chain,rain)    \n    n = 200\n    s = sample(chain, n)\n    vals = get(s, [:μ, :σ])\n    ds = Normal.(vals.μ, vals.σ) # ,get(s,:σ)[i])\n    rg = 1:200\n    for (i, d) in enumerate(ds)\n        lines!(axis, rg,cdf.(d,log.(rg)),color=(:gray,0.3))\n    end\n\n    # plot the actual data\n    percentiles= 0.01:0.01:0.99\n    lines!(axis,quantile.(Ref(rain),percentiles),percentiles,linewidth=3)\nend\n\nlet \n    f = Figure()\n    ax1 = Axis(f[1,1],title=\"Prior\", xgridvisible=false, ygridvisible=false,ylabel=\"Quantile\")\n    chn_cdf!(ax1,chain_prior,rain)\n\n    ax2 = Axis(f[2,1],title=\"Posterior\", xgridvisible=false, ygridvisible=false,xlabel=\"Annual Rainfall (inches)\",ylabel=\"Quantile\")\n    chn_cdf!(ax2,chain_posterior,rain)\n\n    linkxaxes!(ax1, ax2)\n\n    f\nend\n\n\n\n\n\n\nFigure 14.7: The prior model show a wide range of possible outcomes, and the shape of the distribution is reasonable: there’s a nice ‘S’ shape to the CDF, indicating a dense region where most outcomes would fall in the PDF. The fitted posterior model (bottom) has good coverage of the observed data (shown in blue).\n\n\n\n\nComparing to the maximum likelihood analysis from before by plotting the MLE point estimate onto the marginal densities in Figure 14.8. The peak of the the posterior is referred to as the maximum a posteriori (MAP) and would be the point estimate proposed by this Bayesian analysis. However, the Bayesian way of thinking about distributions of outcomes rather than point estimates is one of the main aspects we encourage for financial modelers. Using the posterior distribution of the parameters, we can assess parameter uncertainty directly instead of ignoring it as we tend to do with point estimates.\n\nlet\n    # get the parameters from the earlier MLE approach\n    p = params(ln)\n\n    f = Figure()\n\n    # plot μ posterior\n    ax1 = Axis(f[1,1],title=\"μ posterior\",xgridvisible=false)\n    hideydecorations!(ax1)\n    d = density!(ax1,vec(get(chain_posterior,:μ).μ.data))\n    l = vlines!(ax1,[p[1]],color=:red)\n    \n\n    # plot σ posterior\n    ax2 = Axis(f[2,1],title=\"σ posterior\", xgridvisible=false)\n    hideydecorations!(ax2)\n    density!(ax2,vec(get(chain_posterior,:σ).σ.data))\n    vlines!(ax2,[p[2]],color=:red)\n    \n    Legend(f[1,2],[d,l],[\"Posterior Density\", \"MLE Estimate\"])\n\n    f\nend\n\n\n\n\n\n\n\nFigure 14.8: The MLE point estimate need not necessarily align with the peak or center of posterior densities (e.g. in the case of a bimodal distribution).\n\n\n\n\n\n\n14.5.6.5 Model Limitations\nWe have built and assessed a simple statistical model that could be used in the estimation of risk for a particular location. Nowhere in our model did we define a mechanism to capture a more sophisticated view of the world. There is no parameter for changes over time due to climate change, or inter-annual seasonality for El Niño or La Niña cycles, or any of a multitude of other real-world factors that can influence the forecasting. All we’ve defined is that there is a LogNormal process generating rainfall in a particular location. This may or may not be sufficient to capture the dynamics of the problem at hand.\nPart of the benefits of the Bayesian approach is that it allows us to extend the statistical model to be arbitrarily complex in order to capture our intended dynamics. We are limited by the availability of data, computational power and time, and our own expertise in the modeling. Regardless of the complexity of the model, the same fundamental techniques and idea apply in the Bayesian approach.\n\n\n14.5.6.6 Continuing the Analysis\nLike any good model, you can often continue the analysis in any number of directions, such as: collecting more data, evaluating different models, creating different visualizations, making predictions about future events, creating a multi-level model that predicts rainfall for multiple related locations simultaneously, among many other threads of analysis.\nEarlier we discussed model comparison. To compute a real Bayes Factor in comparing the different models, we would take the average likelihood across the posterior samples instead of just comparing the maximum likelihood points as we did earlier. There are more sophisticated tools for estimating out of sample performance of the model, or measures that evaluate a model for over-fitting by penalizing the diagnostic statistic for the model having too many free parameters. See LOO (leave-one-out) cross-validation and various “information criteria” in the resources listed in Section 14.5.8.\n\n\n\n14.5.7 Conclusion\nThis chapter has attempted to make accessible the foundations of statistical inference and the modern tools and approaches available. Underlying this approach to thinking about statistical problems are informational theoretic and mathematical concepts that can be challenging to learn. This is especially true when traditional finance and actuarial curricula is not centered on the necessary computational foundations associated with modern statistical analysis. Most importantly, moving beyond single ‘best estimate’ values to embrace full probability distributions leads to richer financial analysis and more comprehensive risk assessment.\n\n\n14.5.8 Further Reading\nBayesian approaches to statistical problems are rapidly changing the professional statistical field. To the extent that the actuarial profession incorporates statistical procedures, financial professionals should consider adopting the same practices. The benefits of this are a better understanding of the distribution of risk and return, results that are more interpretable and explainable, and techniques that can be applied to a wider range of problems. The combination of these things would serve to enhance best practices related to understanding and communicating about financial quantities.\nTextbooks recommended by the author are:\n\nStatistical Rethinking (McElreath)\nBayes Rules! (Johnson, Ott, Dogucu)\nBayesian Data Analysis (Gelman, et. al.)\n\nChi Feng has an interactive demonstration of different MCMC samplers available at: https://chi-feng.github.io/mcmc-demo/.\n\n\n\n\nLeemis, Lawrence M, and Jacquelyn T McQueston. 2008. “Univariate Distribution Relationships.” The American Statistician 62 (1): 45–53. https://doi.org/10.1198/000313008x270448.\n\n\nLewis, N D. 2013. 100 Statistical Tests. Createspace.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical Inference and Information Theory</span>"
    ]
  },
  {
    "objectID": "statistics.html#footnotes",
    "href": "statistics.html#footnotes",
    "title": "14  Statistical Inference and Information Theory",
    "section": "",
    "text": "Log base two turns out to be the most natural representation of information content as it mimics the fundamental 0 or 1 value bit. A more complete introduction is available in “Information Theory, Inference, and Learning Algorithms” by David MacKay.↩︎\nA decision tree is a classification algorithm which attempts to optimally classify an output based on if/else type branches on the input variables.↩︎\nhttps://www.researchgate.net/publication/239752412_Derivation_of_Probability_Distributions_for_Risk_Assessment↩︎\nMultiplicative process are often referred to as “geometric”, as in “geometric Brownian motion” or “geometric mean”. Additive processes are sometimes referred to as “arithmetic”. This root of this confusing terminology appears to be due to the fact that series involving repeated multiplication were solved via geometric (triangles, angles, etc.) methods while those using sums and differences were solved via arithmetic.↩︎\nSee Tip 14.1.↩︎\nLaplace actually deserves most of the credit, as it was he who formalized the modern notion of Bayes’ rule and cemented the mathematical formulation. Bayes just described it first, in a way that actually had almost no direct impact on math or science. See “The Theory That Would Not Die”.↩︎\nhttps://data.ca.gov/dataset/annual-precipitation-data-for-northern-california-1944-current↩︎\nSee @sec-predictive-vs-explanatory.↩︎\nThe values are negative because we are taking the logarithm of a number less than \\(1\\). The likelihoods are less than \\(1\\) because the likelihood is the joint (multiplicative) probability of observing each of the individual outcomes.↩︎\nNote that the approach discussed here is much more encompassing than the Bühlmann-Straub Bayesian approach described in the actuarial literature.↩︎\nSpecifically: a DynamicPPL.Model type (PPL = Probabilistic Programming Language).↩︎",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Statistical Inference and Information Theory</span>"
    ]
  },
  {
    "objectID": "stochastic.html",
    "href": "stochastic.html",
    "title": "15  Stochastic Modeling",
    "section": "",
    "text": "15.1 Chapter Overview\nBrief overview of stochastic modeling concepts. Random inputs vs. embedded randomness in stepwise models. Key components: distributions, time horizons, state space. Evaluating outputs with expectation, variance, and risk measures. Special properties like Markov, stationarity, and SDEs. Practical finance examples with Julia code for scenario generation and macroeconomic simulations. An overview of pseudo random number generators (PRNGs).",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Stochastic Modeling</span>"
    ]
  },
  {
    "objectID": "stochastic.html#introduction",
    "href": "stochastic.html#introduction",
    "title": "15  Stochastic Modeling",
    "section": "15.2 Introduction",
    "text": "15.2 Introduction\nStochastic modeling is the technique of running many similar scenarios through a model in order to evaluate different aspects of a given model. Some contexts in which stochastic modeling arises in the financial context include:\n\nValuation. Market consistent pricing often involves calibrating a set of stochastic economic scenarios to market-observed prices. Those calibrated scenarios are then used to determine the value of another set of contracts for which market prices are not directly observable.\nScenario Analysis. Many financial models have a high degree of complexity and interactions between components not well understood a priori. Using a set of calibrated or otherwise arbitrary scenarios can illuminate unexpected or interrelated behavior that arises from the model. Sometimes financial assets and liabilities have a path-dependency, meaning that the overall behavior or value is a function of the entire path taken (e.g. an asian option).\nRisk and Capital Analysis. Whether model parameters are point estimates (“an asset has a \\(1\\%\\) probability of default”) or distributions (“an asset has a probability of default distributed as \\(Beta(1,99)\\)”), stochastic modeling can be utilized to determine projected capital levels for a company or line of business at certain security levels.\n\nThe reliability of modeled outcomes is fundamentally dependent on the quality of both the underlying scenarios and the model framework itself. For example, the 2008 American Academy of Actuaries’ Economic Scenario Generator, which has served as the predominant tool for US regulated insurance liabilities, has demonstrated limitations in its calibration. Specifically, the generator has historically produced interest rate paths that diverge significantly from market-implied forward rates, often skewing toward higher values (Strommen 2022). In such instances, characterizing outputs at the 99th percentile as representing “extreme” scenarios would be methodologically inappropriate, as the baseline calibration already embeds an upward bias relative to market expectations2.\nRecall the kinds of uncertainties in Table 4.2 - stochastic modeling implementations often mechanically only address the aleatory (process) volatility component of outcome uncertainty. Through techniques like developing a posterior distribution for model parameters (@sec-bayes-rule) we can also explicitly attempt to model epistemic (parameter) uncertainty as well.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Stochastic Modeling</span>"
    ]
  },
  {
    "objectID": "stochastic.html#fundamentals-of-stochastic-modeling",
    "href": "stochastic.html#fundamentals-of-stochastic-modeling",
    "title": "15  Stochastic Modeling",
    "section": "15.3 Fundamentals of Stochastic Modeling",
    "text": "15.3 Fundamentals of Stochastic Modeling\nStochastic modeling represents a significant departure from deterministic approaches by explicitly incorporating randomness and uncertainty. Before diving into specific modeling techniques, it’s essential to understand the fundamental concepts that form the foundation of any stochastic model.\n\n15.3.1 Random Variables and Probability\nAt the core of stochastic modeling is the concept of random variables—quantities whose values depend on random outcomes. In financial contexts, random variables might represent:\n\nFuture interest rates\nStock price movements\nInsurance claim frequencies\nDefault probabilities\nMortality rates\n\nThese random variables are characterized by probability distributions that describe the likelihood of different outcomes. For example, stock returns might follow a normal distribution, while insurance claims might follow a Poisson distribution for frequency and a LogNormal distribution for severity.\n\n\n15.3.2 The Role of Monte Carlo Simulation\nDespite the opening quote’s humorous skepticism, Monte Carlo methods have become indispensable in modern financial modeling. Named after the famous casino, these techniques use repeated random sampling to obtain numerical results when analytical solutions are impractical or impossible.\nThe basic procedure involves:\n\nGenerating random inputs based on specified probability distributions\nRunning the model with these inputs\nCollecting and analyzing the outputs\nRepeating until sufficient samples are obtained for statistical significance\n\nAs computational power has increased, Monte Carlo methods have moved from “last resort” to standard practice in many financial applications. In addition to this conceptual chapter, Chapter 25 demonstrates this process to generate stochastic economic scenarios.\n\n\n15.3.3 Time Dimensions in Stochastic Models\nStochastic models can be categorized based on how they treat time:\n\nDiscrete-time models: The system evolves in distinct time steps (daily, monthly, annually)\nContinuous-time models: The system evolves continuously, often described by stochastic differential equations\n\nFinancial applications frequently use both approaches. For example, option pricing might use continuous-time models (Black-Scholes), while asset-liability management might use discrete-time models with monthly or quarterly steps.\n\n\n15.3.4 State Spaces and Transitions\nA stochastic model’s state space represents all possible states the system can occupy. This might be:\n\nDiscrete: Limited number of possible states (e.g., credit ratings: AAA, AA, A, etc.)\nContinuous: Infinite possible states (e.g., stock prices can take any positive real value)\n\nThe transitions between states are governed by probability distributions or transition matrices, depending on the model type. Understanding the state space is crucial for properly formulating and solving stochastic problems in finance.\n\n\n15.3.5 Aleatory vs. Epistemic Uncertainty\nStochastic models address two fundamental types of uncertainty: - Aleatory uncertainty: The inherent randomness in the system (e.g., market volatility) - Epistemic uncertainty: Uncertainty due to limited knowledge about model parameters\nMost basic stochastic models focus on aleatory uncertainty, assuming model parameters are known with certainty. More sophisticated approaches incorporate epistemic uncertainty by treating parameters themselves as random variables, often using Bayesian methods.\n\n\n15.3.6 Calibration and Validation\nBefore a stochastic model can be applied, it must be properly calibrated to match observed data or market prices. This typically involves:\n\nEstablishing initial parameters via professional judgment or statistical means.\nAdjusting parameters to match desired calibration data: e.g. minimizing a loss function with respect to the predicted outcomes versus the calibration data (see @sec-optimization).\nValidating the model against out-of-sample data.\n\nFor financial applications, models are often calibrated to be “market-consistent,” meaning they reproduce the prices of traded instruments before being used to value more complex products.\n\n\n15.3.7 Computational Considerations\nStochastic modeling is computationally intensive, particularly when: - Many scenarios are required for convergence - The model has high dimensionality - Path dependencies require storing entire trajectories\nModern implementations leverage parallel processing, variance reduction techniques, and sometimes machine learning approximations to make complex stochastic models computationally feasible.\nWith these fundamental concepts in mind, we can now explore specific types of stochastic models and their applications in finance and actuarial science.\n\n\n15.3.8 Kinds of Stochastic Models\n\n15.3.8.1 Input Ensemble\nIn this approach, you first generate many sets of random inputs (scenarios) and then run the same model repeatedly—one run per scenario3:\n\nInterest Rate Scenarios for Bond Portfolios\nGenerate thousands of yield-curve scenarios (e.g., parallel shifts, twists) to test how a bond portfolio might perform over time. By averaging the results over all simulations, you can estimate the portfolio’s expected return or risk metrics.\nCommodity Price Paths for Hedging\nCreate multiple possible paths for commodity prices under different macro conditions. Use these to evaluate various hedging strategies and see which one consistently mitigates risk across a wide range of market conditions.\n\nGenerally, models programmed in code can easily map across a large number of stochastic inputs.\n\n\n15.3.8.2 Random State\nHere, the model itself “rolls the dice” at each step. Instead of pre-generating input scenarios, the randomness is embedded in the logic that evolves the state over time4:\n\nProperty & Casualty Claims Simulation\nIn each simulated month, randomly draw the number of claims (from a Poisson or negative binomial distribution), then randomly assign the severity of each claim (often a lognormal or gamma distribution). The insurer’s financial state (reserves, surplus) evolves based on these simulated losses.\nCall-Center (Queueing) Model\nModel the random arrival of customer calls following a Poisson process, and at each arrival decide if the call is answered immediately or queued. Service times for each call can be drawn from an exponential distribution. By simulating many days, you can estimate average waiting times or the probability that a caller has to wait more than a certain threshold.\n\n\n\n\n\n\n\nNote\n\n\n\nMany financial models project outcomes using expected values of random variables. Many actuarial models utilize this approach to determine expected outcomes given an assumption (such as expected present value of life insurance claims). This is different than evaluating many possible discrete realized scenarios wherein each insured life either dies or survives each period, and models designed for projected expected payments need to be adapted to handle a random state approach.\n\n\n\n\n15.3.8.3 Closed-Form\nSome stochastic processes admit analytical solutions or formulas for key quantities, so you do not need to run a simulation for each scenario. For example, here a a couple of common models that have closed form solutions to the underlying stochastic differential equation.\n\nBlack Scholes Option Price\nThe Black-Scholes European call and put-call price parity are results for analytically pricing options despite the underlying theory being based on a stochastic evolution of asset prices.\nOrnstein-Uhlenbeck Interest Rate Model\nAn Ornstein-Uhlenbeck process is often used to model mean-reverting interest rates. Despite being fundamentally stochastic, it has closed-form expressions for zero-coupon bond prices and other interest-rate derivatives, saving you from running thousands of random simulations for valuation.\n\nThe focus of this section will be on the other kinds of computational-driven, stochastic models.\n\n\n15.3.8.4 Special Cases or Properties\nStochastic processes often combine several structural or theoretical properties to reflect real-world phenomena in financial and actuarial models. Before diving into each property—Markov, stationarity, martingales, and SDEs—it can be useful to see how they serve as building blocks. For example, Markov processes allow today’s decisions or valuations to depend only on the current state, whereas stationarity implies that the underlying statistical characteristics (mean or variance) do not change over time. Martingales capture a notion of “fairness” in gambling and risk-neutral pricing, and SDEs inject random shocks in a continuous-time setting. By understanding these properties, one can better see which assumptions are embedded in derivatives pricing, credit migration models, or mortality forecasts.\n\n15.3.8.4.1 Markov Models\nA stochastic process has the Markov property if the future state depends only on the current state and not on the past states (memorylessness). Markov chains and hidden Markov models contain this property. The Markov property simplifies modeling and computation by reducing dependencies on past states. For instance, a credit rating model might assume that the probability of going from BBB to BB depends only on today’s rating, without direct reference to prior years’ ratings.\n\n\n15.3.8.4.2 Stationary Models\nA stochastic process is stationary if its statistical properties, such as mean and variance, do not change over time. Strict stationarity means all moments of the process are invariant over time, while weak Stationarity means only the mean and variance are invariant over time. Stationarity simplifies the analysis and modeling of stochastic processes, especially for time series data. Sometimes a non-stationary model can be made stationary by transformation (such as removing a constant trend component).\n\n\n15.3.8.4.3 Martingales\nA martingale is a stochastic process where the expected future value, given all prior information, is equal to the current value. Examples include fair gambling games or financial asset prices in efficient markets. Martingales are important in financial modeling and risk-neutral pricing, especially for derivatives.\n\n\n15.3.8.4.4 Stochastic Differential Equations (SDEs)\nThese are differential equations that incorporate stochastic terms (typically driven by Brownian motion or other noise sources). The well-known Black-Scholes equation for option pricing is one example of how SDE can be applied in real world. SDEs are essential for modeling systems where both deterministic and random factors drive behavior over time.\nA special kind of SDE is Brownian Motion. This is a continuous-time stochastic process where changes over time are independent and normally distributed. This is extensively used in financial models for asset price movements (e.g., the Black-Scholes model). In addition to financial markets, Brownian motion is also widely used in modeling physical systems (diffusion).\nIn practice, these properties often occur together in financial models.\n\n\n\n\n15.3.9 Components of Stochastic Models\nStochastic modeling has additional terminology to introduce.\n\n15.3.9.1 Random variables\nA random variable represents a quantity whose value is determined by the outcome of a random event. It can be discrete or continuous. It is essentially a mapping from event space to numerical values. Examples include stock prices, waiting time in queues and number of claims in insurance, etc. Random variables form the basis of stochastic models by introducing uncertainty into the model.\n\n\n15.3.9.2 Probability distributions\nA probability distribution describes the likelihood of different outcomes for a random variable. Examples include normal distribution, Poisson distribution and exponential distribution, etc. Probability distributions help in modeling the behavior of random variables and in defining how likely different events or outcomes are.\n\n\n15.3.9.3 State space\nThe state space represents all possible states or values that a stochastic process can take. For example, in a Markov process, the state space might be the set of all possible values that the system can occupy at any given time. The state space helps in defining the scope of the model by specifying possible outcomes.\n\n\n15.3.9.4 Stochastic processes\nA stochastic process is a collection of random variables indexed by time (or some other variable) that evolve in a probabilistic manner. Examples include Brownian motion, Markov chains and Poisson processes. Stochastic processes model how random variables change over time, which is essential for understanding dynamic systems influenced by randomness.\n\n\n15.3.9.5 Transition probabilities\nThese represent the probabilities of transitioning from one state to another in a stochastic process. For example, in a Markov chain, the transition matrix contains the probabilities of moving from one state to another. Transition probabilities determine how the system evolves from one time step to the next, reflecting the underlying randomness.\n\n\n15.3.9.6 Time horizon\nThe time horizon refers to the period over which the stochastic process is observed. It can be discrete (e.g., steps in a Markov chain) or continuous (e.g., continuous-time models like Brownian motion). The time horizon helps in determining how the process behaves over short or long periods.\n\n\n15.3.9.7 Initial conditions\nThese are the starting points or initial values of the random variables or the system at time zero. They may be initial price of a stock, initial number of customers in a queue, etc. The starting condition influences the future evolution of the process, and different initial conditions can lead to different outcomes.\n\n\n15.3.9.8 Noise (random shocks)\nRandom noise represents unpredictable random fluctuations that can affect the outcome of a stochastic process. This can be market volatility, measurement errors or environmental variations. Noise is a critical element in stochastic models as it introduces randomness and uncertainty into otherwise deterministic systems.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Stochastic Modeling</span>"
    ]
  },
  {
    "objectID": "stochastic.html#evaluating-and-applying-stochastic-models",
    "href": "stochastic.html#evaluating-and-applying-stochastic-models",
    "title": "15  Stochastic Modeling",
    "section": "15.4 Evaluating and Applying Stochastic Models",
    "text": "15.4 Evaluating and Applying Stochastic Models\n\n15.4.1 Evaluating Stochastic Results\nThese types of models are evaluated simply by running them many times until the measure of interest converges on a stable result: for example, we might run a model until the mean of the results no longer varies materially as we add more scenarios.\n\n15.4.1.1 Expectation and variance\nThe expected value (mean) represents the average or mean outcome of a random variable over many trials or realizations. The variance measures the spread or variability of outcomes around the expected value. These statistical measures provide insights into the central tendency and the uncertainty or risk in a stochastic model.\n\n\n15.4.1.2 Covariance and correlation\nCovariance measures how two random variables change together. Positive covariance indicates that the variables tend to increase together. On the other hand correlation is the standardized version of covariance that measures the strength of the linear relationship between two variables. Understanding how different random variables interact helps in building more complex models, especially in multivariate stochastic processes.\n\n\n15.4.1.3 Risk Measures\nRisk measures encompass the set of functions that map a set of outcomes to an output value characterizing the associated riskiness of those outcomes. As is usual when attempting to compress information (e.g. condensing information into a single value), there are multiple ways we can characterize this riskiness.\n\n15.4.1.3.1 Coherence & Other Desirable Properties\nFurther, it is desirable that a risk measure has certain properties, and risk measures that meet the first four criteria are called “Coherent” in the literature. From “An Introduction to Risk Measures for Actuarial Applications” ((Hardy 2006)), she describes four properties. Using \\(H\\) as a risk measure and \\(X\\) as the associated risk distribution:\n\n15.4.1.3.1.1 1. Translation Invariance\nFor any non-random \\(c\\)\n\\(H(X + c) = H(X) + c\\) This means that adding a constant amount (positive or negative) to a risk adds the same amount to the risk measure. It also implies that the risk measure for a non-random loss, with known value c, say, is just the amount of the loss c.\n\n\n15.4.1.3.1.2 2. Positive Homogeneity\nFor any non-random \\(λ &gt; 0\\):\n\\[H(λX) = λH(X)\\]\nThis axiom implies that changing the units of loss does not change the risk measure.\n\n\n15.4.1.3.1.3 3. Subadditivity\nFor any two random losses \\(X\\) and \\(Y\\),\n\\[H(X + Y) ≤ H(X) + H(Y)\\]\nIt should not be possible to reduce the economic capital required (or the appropriate premium) for a risk by splitting it into constituent parts. Or, in other words, diversification (ie consolidating risks) cannot make the risk greater, but it might make the risk smaller if the risks are less than perfectly correlated.\n\n\n15.4.1.3.1.4 4. Monotonicity\nIf \\(Pr(X ≤ Y) = 1\\) then \\(H(X) ≤ H(Y)\\).\nIf one risk is always bigger then another, the risk measures should be similarly ordered.\n\n\n15.4.1.3.1.5 Other Properties\nIn “Properties of Distortion Risk Measures” (Balbás, Garrido, and Mayoral 2009) also note other properties of interest:\n\nComplete\nCompleteness is the property that the distortion function associated with the risk measure produces a unique mapping between the original risk’s survival function \\(S(x)\\) and the distorted \\(S*(x)\\) for each \\(x\\). See Distortion Risk Measures for more detail on this.\nIn practice, this means that a non-complete risk measure ignores some part of the risk distribution (e.g. CTE and VaR don’t use the full distribution and have the same)\n\n\nExhaustive\nA risk measure is “exhaustive” if it is coherent and complete.\n\n\nAdaptable\nA risk measure is “adapted” or “adaptable” if its distortion function (see Distortion Risk Measures).\n\n\\(g\\) is strictly concave, that is \\(g\\) is strictly decreasing.\n\\(lim_{u\\to0+} g\\prime(u) = \\inf\\) and \\(lim_{u\\to1-} g\\prime(u) = 0\\).\n\nAdaptive risk measures give more weight to larger losses. They are exhaustive but the converse is not true.\n\n\n\n\n\n15.4.1.4 Summary of Risk Measure Properties\n\n\n\nMeasure\nCoherent\nComplete\nExhaustive\nAdaptable\n\n\n\n\nVaR\nNo\nNo\nNo\nNo\n\n\nCTE\nYes\nNo\nNo\nNo\n\n\nDualPower \\((y &gt; 1)\\)\nYes\nYes\nYes\nNo\n\n\nProportionalHazard \\((γ &gt; 1)\\)\nYes\nYes\nYes\nNo\n\n\nWangTransform\nYes\nYes\nYes\nYes\n\n\n\n\n\n\n\n\n\nNoteDistortion Risk Measures\n\n\n\nDistortion Risk Measures are a way of remapping the probabilities of a risk distribution in order to compute a risk measure \\(H\\) on the risk distribution \\(X\\).\nAdapting (Wang 2002), there are two key components:\n\n15.4.1.5 Distortion Function \\(g(u)\\)\nThis remaps values in the [0,1] range to another value in the [0,1] range, and in \\(H\\) below, operates on the survival function \\(S\\) and \\(F=1-S\\).\nLet \\(g:[0,1]\\to[0,1]\\) be an increasing function with \\(g(0)=0\\) and \\(g(1)=1\\). The transform \\(F^*(x)=g(F(x))\\) defines a distorted probability distribution, where “\\(g\\)” is called a distortion function.\nNote that \\(F^*\\) and \\(F\\) are equivalent probability measures if and only if \\(g:[0,1]\\to[0,1]\\) is continuous and one-to-one. Definition 4.2. We define a family of distortion risk-measures using the mean-value under the distorted probability \\(F^*(x)=g(F(x))\\):\n\n\n15.4.1.6 Risk Measure Integration\nTo calculate a risk measure \\(H\\), we integrate the distorted \\(F\\) across all possible values in the risk distribution (i.e. \\(x \\in X\\)):\n\\[H(X) = E^*(X) = - \\int_{-\\infty}^0 g(F(x))dx + \\int_0^{+\\infty}[1-g(F(x))]dx\\]\nThat is, the risk measure (\\(H\\)) is equal to the expected value of the distortion of the risk distribution (\\(E^*(X)\\)).\n\n\n\n\n\n15.4.1.7 Risk Measures: Examples\n\n15.4.1.7.1 Basic Comparison\nWe won’t re-implement the logic here, but here’s a very simple example demonstrating the relative values of VaR, CTE, and a Wang Transform at the 90% level. These functions come from the public library ActuaryUtilities.jl.\n\nusing ActuaryUtilities\n\nlet outcomes = rand(1000)\n\n    @show VaR(0.9)(outcomes)\n    @show CTE(0.9)(outcomes)\n    @show WangTransform(0.9)(outcomes)\nend\n\n(VaR(0.9))(outcomes) = 0.9153942745262581\n(CTE(0.9))(outcomes) = 0.9539937290941024\n(WangTransform(0.9))(outcomes) = 0.8198688288859242\n\n\n0.8198688288859242\n\n\n\n\n\n15.4.1.8 Plotting a Range of Values\nWe will generate a random outcome and show how the risk measures behave:\n\nusing Distributions\nusing ActuaryUtilities\nusing CairoMakie\n\n# the assumed distribution of outcomes\noutcomes = Weibull(1, 5)\n\n\nαs = range(0.0, 0.99; length=100)\n\nlet\n    f = Figure()\n    ax = Axis(\n        f[1, 1],\n        xlabel=\"α\",\n        ylabel=\"Loss\",\n        title=\"Comparison of Risk Measures\",\n        xgridvisible=false,\n        ygridvisible=false,\n    )\n\n    lines!(\n        ax,\n        αs,\n        [quantile(outcomes, α) for α in αs],\n        label=\"Quantile α of Outcome\",\n        color=:grey10,\n        linewidth=3,\n    )\n\n    lines!(\n        ax,\n        αs,\n        [VaR(α)(outcomes) for α in αs],\n        label=\"VaR(α)\",\n        linestyle=:dash\n    )\n    lines!(\n        ax,\n        αs,\n        [CTE(α)(outcomes) for α in αs],\n        label=\"CTE(α)\",\n    )\n    lines!(\n        ax,\n        αs[2:end],\n        [WangTransform(α)(outcomes) for α in αs[2:end]],\n        label=\"WangTransform(α)\",\n    )\n    lines!(\n        ax,\n        αs,\n        [ProportionalHazard(2)(outcomes) for α in αs],\n        label=\"ProportionalHazard(2)\",\n    )\n\n    lines!(\n        ax,\n        αs,\n        [DualPower(2)(outcomes) for α in αs],\n        label=\"DualPower(2)\",\n    )\n    lines!(\n        ax,\n        αs,\n        [RiskMeasures.Expectation()(outcomes) for α in αs],\n        label=\"Expectation\",\n    )\n    axislegend(ax, position=:lt)\n\n    f\nend\n\n\n\n\n\n\n\n15.4.2 Variance Reduction Techniques\nWhile Monte Carlo simulation is a powerful approach for modeling complex financial or actuarial systems, it can require a very large number of random draws for the results to converge. When there have been a limited number of runs, the variance of the sampled results can be high, and vary materially between different sets of stochastic runs. This can be addressed by simply running more scenarios, but can be computationally expensive, particularly when path-dependent processes—or high-dimensional models—are involved. For this reason, several “variance reduction techniques” have been established to improve efficiency without reducing accuracy. They work by designing the way we draw samples (or process them) so that desired statistics converge more quickly.\n\n15.4.2.1 Control Variates\nA control variate is a random variable whose expected value is already known (or can be estimated with high precision). By simulating the main process alongside the control variate and leveraging the difference in outcomes, the overall variance of the estimator is reduced. In finance, for instance, one might use a simpler instrument whose price is analytically known as a control variate when simulating a more complex derivative.\n\n\n15.4.2.2 Antithetic Variates\nIn this technique, each random draw (X) is paired with a corresponding draw (-X) or another symmetric transformation. The idea is that positive and negative deviations tend to cancel out, thereby reducing overall variability. For example, if simulating geometric Brownian motion paths, each path with a particular series of random shocks could be matched with a path using the same shocks negated. Updating estimates with the average result of these pairs generally yields a lower variance for the same number of total draws.\n\n\n15.4.2.3 Stratified Sampling\nInstead of sampling purely at random across the entire probability distribution, the distribution is divided into “strata” (or sub-intervals), and draws are forced to fall into each of these strata with a pre-specified frequency. By covering the distribution more systematically, the resulting estimates often converge faster. This is sometimes called quasi-Monte Carlo and involves techniques to span the sample space that is less random but still unbiased. Julia has a package for this called QuasiMonteCarlo.jl\nTogether, these techniques reflect a crucial principle of computational thinking: by leveraging structure or known properties of the simulation, you can achieve more accurate estimates with fewer overall scenarios. That translates into faster runs, less strain on computational resources, and better performance when embedding these methods in large-scale models or iterative processes.\n\n\n\n15.4.3 Stochastic Modeling: Examples\n\n15.4.3.1 Macroeconomic Analysis\nHere we show still another stochastic process in macroeconomic analysis. Stochastic macroeconomic analysis often involves modeling random shocks and their effects on macroeconomic variables such as output, consumption, inflation, and employment. One common approach is through Dynamic Stochastic General Equilibrium (DSGE) models, which are widely used in macroeconomic analysis. These models incorporate randomness (stochastic elements) to capture real-world uncertainty in economic systems.\n\nusing Random, CairoMakie, Distributions\n\n# Parameters\nα = 0.33                # Capital share of output\nδ = 0.05                # Depreciation rate\ns = 0.2                 # Savings rate\nn = 0.01                # Population growth rate\ng = 0.02                # Technology growth rate\nσ = 0.01                # Standard deviation of productivity shocks\nT = 100                 # Number of periods to simulate\nK0 = 1.0                # Initial capital stock\nA0 = 1.0                # Initial productivity\n\n# Shock distribution (normal distribution for productivity shocks)\nshock_distribution = Normal(0, σ)\n\n# Random number generator (Xoshiro as default with a seed to ensure reproduciabiity)\nrng_gen = Xoshiro(1234)\n\n# Function to simulate the model\nfunction simulate_stochastic_solow(T, α, δ, s, n, g, σ, K0, A0)\n    K = zeros(T)        # Capital over time\n    Y = zeros(T)        # Output over time\n    A = zeros(T)        # Productivity shocks over time\n    A[1] = A0           # Initial productivity\n    K[1] = K0           # Initial capital\n\n    for t in 1:(T-1)\n        # Apply random productivity shock\n        ε_t = rand(rng_gen, shock_distribution)\n        A[t+1] = A[t] * exp(ε_t)  # Productivity evolves stochastically\n\n        # Output based on Cobb-Douglas production function\n        Y[t] = A[t] * K[t]^α\n\n        # Capital accumulation equation\n        K[t+1] = s * Y[t] + (1 - δ) * K[t]\n\n        # Population and technology growth\n        K[t+1] *= (1 + n) * (1 + g)\n    end\n\n    Y[T] = A[T] * K[T]^α  # Final output\n    return K, Y, A\nend\n\n# Simulate the model\nK, Y, A = simulate_stochastic_solow(T, α, δ, s, n, g, σ, K0, A0)\n\n# Plot the results\nf = Figure()\naxis = Axis(f[1, 1], xlabel=\"Time\", ylabel=\"Output\", title=\"Stochastic Solow Growth Model\")\nscatter!(1:T, Y, label=\"Output (Y)\")\nf\n\n\n\n\n\n\n15.4.3.2 Other Examples\nStochastic examples can be found in many other sections of this book, such as:\n\nPortfolio optimization applications (Chapter 27).\nGenetic algorithms for certain optimization problems (Chapter 17).\nStochastic state-based mortality projections (Chapter 30).",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Stochastic Modeling</span>"
    ]
  },
  {
    "objectID": "stochastic.html#pseudo-random-number-generators",
    "href": "stochastic.html#pseudo-random-number-generators",
    "title": "15  Stochastic Modeling",
    "section": "15.5 Pseudo-Random Number Generators",
    "text": "15.5 Pseudo-Random Number Generators\nModern computers utilize pseudo-random number generators (PRNGs) to generate random-like numbers. PRNGs are algorithms used to generate sequences of numbers that appear to be random but are actually determined by an initial value, known as the seed. These generators are called “pseudo-random” because the sequences they produce are deterministic; if you provide the same seed, you’ll get the same sequence of numbers. In addition, they have a finite period, which means that after a certain number of generated values, the sequence will repeat. It’s important to choose or design PRNGs with a long enough period for practical applications.\nFinancial modelers should understand how PRNGs work because many financial models rely on Monte Carlo simulations, risk analysis, and other stochastic modeling techniques that require random sampling. A good PRNG is essential for robust financial modeling. Choosing the right PRNG ensures results with high statistical quality and efficiency, which is critical when making financial decisions. The ability to specify and control the seed of a random number generator is a fundamental requirement in computational modeling, as it enables exact reproducibility of results. By using a fixed seed, simulations can be rerun with identical random sequences, allowing researchers and stakeholders to verify outcomes, conduct consistent sensitivity analyses, and perform rigorous debugging. This reproducibility is critical for transparency, auditability, and compliance with regulatory standards in fields such as quantitative finance, risk management, and scientific computing. Without a seedable PRNG, the inherent randomness of simulations would preclude the possibility of replicating results precisely, undermining confidence in the validity and reliability of the modeling process.\nDifferent choices of random number generators typically have a low impact on the mean outputs of stochastic models, since most generators are designed to approximate uniform distributions adequately. However, they can have a substantial effect on risk measures such as standard deviation, Value-at-Risk (VaR), and tail quantiles, particularly when the PRNG has a short period or exhibits hidden correlations. In such cases, the variability and uncertainty estimates around risk measures can be severely biased or understated, leading to misleading conclusions about the model’s risk profile. For this reason, it is essential to rerun the stochastic model multiple times with different seeds—or even different PRNGs—to test the stability and convergence of the estimated risk measures and ensure that the results are robust to the choice of random number source.\n\n15.5.1 Common PRNGs\n\n15.5.1.1 Mersenne Twister\nFor many years, the Mersenne Twister was a standard and highly recommended PRNG for financial modeling purposes. One of its greatest strengths is its exceptionally long period of (2^{19937} - 1), which is crucial for applications requiring a large number of independent random numbers. It is also known for its good statistical properties, passing many standard tests for randomness. Moreover, it was designed with features for creating multiple streams, though ensuring their statistical independence in parallel applications requires careful management.\n\n\n15.5.1.2 Xorshift\nXorshift is a family of PRNGs known for their simplicity and extremely fast operation. The name “xorshift” comes from the bitwise XOR (exclusive or) and bit-shifting operations that are the core of the algorithm. Xorshift generators are often used in applications where speed is a priority and cryptographic-strength randomness is not a strict requirement. One of the main advantages of xorshift is that its core operations can be efficiently implemented in hardware. However, a typical xorshift generator has a relatively short period compared to the Mersenne Twister.\n\n\n15.5.1.3 Xoshiro\nXoshiro is a family of high-performance PRNGs with excellent statistical properties. The name “Xoshiro” is a portmanteau of the core operations it uses: XOR, SHIFT, and ROTATE. Xoshiro algorithms, including the highly-regarded Xoshiro256++ variant, use a combination of bitwise XOR, bit-shifting, and addition/rotation operations. They generally have more complex update rules and longer periods than basic Xorshift algorithms.\nWhen selecting seeds at random to initialize multiple instances of the Mersenne Twister generator, there is a significant likelihood of producing streams that are statistically correlated in parallel computations. This arises because the default seeding mechanism may not sufficiently de-correlate the internal states across different instances. In contrast, generators such as Xoshiro256++ offer markedly improved suitability for parallel environments, with carefully managed methodologies for ensuring stream independence.\n\n\n15.5.1.4 Comparing PRNGs\n\n\n\n\n\n\n\n\n\nGenerator\nTypical Period\nRelative Speed\nParallel Safety\n\n\n\n\nMersenne Twister\nVery Long ((2^{19937}-1))\nGood\nPoor (risk of correlation)\n\n\nXorshift\nShorter\nVery Fast\nPoor (not designed for it)\n\n\nXoshiro256++\nVery Long ((2^{256}-1))\nExcellent\nExcellent (designed for it)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSince Julia 1.7, the default random number generator for the language has been Xoshiro256++, which replaced an implementation of Mersenne Twister.\nImportantly, Julia’s implementation of Xoshiro256++ is thread safe (meaning you can use the RNG in multiple threads without losing RNG quality).\n\n\n\n\n\n15.5.2 Consistent Interface\nJulia offers a consistent interface for random numbers due to its design and multiple dispatch principles. Consider the following random numbers in different data types.\n\nusing Random\n\nrng = MersenneTwister(1234)\nrand(Int, (2, 3))\n\n2×3 Matrix{Int64}:\n 7463082635820835149   4907488282503000330  -3871884353034667600\n 2697929313060509323  -3942530201469858828    -24814452901646670\n\n\n\nusing Random\n\nrng = MersenneTwister(1234)\nrand(Float64, (2, 3))\n\n2×3 Matrix{Float64}:\n 0.650796  0.840087  0.320374\n 0.651955  0.796333  0.540216\n\n\n\nusing Random\n\nrng = Xoshiro(1234)\nrand(Bool, (2, 3))\n\n2×3 Matrix{Bool}:\n 0  0  0\n 1  1  1",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Stochastic Modeling</span>"
    ]
  },
  {
    "objectID": "stochastic.html#conclusion",
    "href": "stochastic.html#conclusion",
    "title": "15  Stochastic Modeling",
    "section": "15.6 Conclusion",
    "text": "15.6 Conclusion\nStochastic modeling involves the study of systems influenced by random factors and uncertainty. By combining random variables, probability distributions, and processes like Markov chains or Brownian motion, stochastic models provide insights into systems that cannot be described purely deterministically.\n\n\n\n\nBalbás, Alejandro, José Garrido, and Silvia Mayoral. 2009. “Properties of Distortion Risk Measures.” Methodology and Computing in Applied Probability 11 (3): 385–99. https://doi.org/10.1007/s11009-008-9089-z.\n\n\nHardy, Mary R. 2006. “An Introduction to Risk Measures for Actuarial Applications.” SOA Syllabus Study Note 19.\n\n\nStrommen, Stephen J. 2022. “Low Interest Rates and the NAIC Economic Scenario Generator Project.” Risk & Rewards. https://www.soa.org/sections/investment/investment-newsletter/2022/june/rr-2022-06-strommen/.\n\n\nWang, Shaun S. 2002. “A Universal Framework for Pricing Financial and Insurance Risks.” ASTIN Bulletin 32 (2): 213–34. https://doi.org/10.2143/AST.32.2.1027.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Stochastic Modeling</span>"
    ]
  },
  {
    "objectID": "stochastic.html#footnotes",
    "href": "stochastic.html#footnotes",
    "title": "15  Stochastic Modeling",
    "section": "",
    "text": "Kalos was a pioneer in Monte Carlo techniques, quoted via https://doi.org/10.1007/978-3-540-74686-7_3↩︎\nIt should be noted that the as of mid-2025, the NAIC undertaking a comprehensive project to replace this economic scenario generator, recognizing the need for enhanced modeling capabilities.↩︎\nAn extended example of this approach is discussed in Chapter 25.↩︎\nA worked example of this approach is illustrated in Chapter 30.↩︎",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Stochastic Modeling</span>"
    ]
  },
  {
    "objectID": "autodiff.html",
    "href": "autodiff.html",
    "title": "16  Automatic Differentiation",
    "section": "",
    "text": "16.1 Chapter Overview\nHarnessing the chain rule to compute derivatives not just of simple functions, but of complex programs.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#motivation-for-automatic-derivatives",
    "href": "autodiff.html#motivation-for-automatic-derivatives",
    "title": "16  Automatic Differentiation",
    "section": "16.2 Motivation for (Automatic) Derivatives",
    "text": "16.2 Motivation for (Automatic) Derivatives\nDerivatives are one of the most useful analytical tools we have. Determining the rate of change with respect to an input is effectively sensitivity testing. Knowing the derivative lets you optimize things faster (see Chapter 17). You can test properties and implications (monotonicity, maxima/minima). These applications make derivatives foundational for machine learning, generative models, scientific computing, and more.\nWe will work up concepts on computing derivatives, from the most basic (finite differentiation) to automatic differentiation (forward mode and then reverse mode). These tools can be used in many modeling applications. Automatic Differentiation (“AD” or “autodiff”) refers to innovative techniques to get computers to perform differentiation of complex algorithms (code) that would be intractable for a human to perform manually.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#finite-differentiation",
    "href": "autodiff.html#finite-differentiation",
    "title": "16  Automatic Differentiation",
    "section": "16.3 Finite Differentiation",
    "text": "16.3 Finite Differentiation\nFinite differentiation is evaluating a function \\(f(x)\\) at a value \\(x\\) and then at a nearby value \\(x+\\epsilon\\). The line drawn through these two points effectively estimates the line that is tangent to the function \\(f\\) at \\(x\\) - effectively the derivative has been found by approximation. That is, we are looking to approximate the derivative using the property:\n\\[\nf'(x) = \\lim_{{\\epsilon \\to 0}} \\frac{{f(x_0 + \\epsilon) - f(x_0)}}{{\\epsilon}}\n\\]\nWe can approximate the result by simply choosing a small \\(\\epsilon\\).\nThere’s also flavors of finite differentiation to approximate derivatives to be aware of:\n\nforward difference is as defined in the above equation, where \\(\\epsilon\\) is added to \\(x_0\\)\nreverse difference is as defined in the above equation, where \\(\\epsilon\\) is subtracted from \\(x_0\\)\ncentral difference is where we evaluate at \\(x_0 \\pm \\epsilon\\) and then divide by \\(2\\epsilon\\)\n\nThe benefit of the central difference is that it limits issues around minima and maxima where the trough or peak respectively would seem much steeper if using forward or reverse. Here’s a picture of this:\n\n\n\n\n\nOne benefit of the central difference method is that is often more accurate than forward or reverse differences. However, it comes at the cost of needing an additional function evaluation/computation in many circumstances. Take, for example, the process of optimizing a function to find a maxima or minima.\nMaxima-finding algorithms usually involve guessing an initial point, evaluating the function at that point, and determining what the derivative of the function is at that point. Both items are used to update the guess to one that’s closer to the solution. This approach is used in many optimization algorithms such as Newton’s Method. At each step you need to evaluate the function three times: for \\(x\\), \\(x+\\epsilon\\), and \\(x-\\epsilon\\). With forward or reverse finite differences, you can reuse the prior function evaluation of the prior guess \\(x\\). As one of the components in the estimation of the derivative, thereby saving an evaluation of the function for each iteration.\nThere are additional challenges with the finite differentiation method. In practice, we are often interested in much more complex functions than \\(x^2\\). For example, we may actually be interested in the sum of a series that is many elements long or contains more complex operations than basic algebra. In the prior example, the \\(\\epsilon\\) is set unusually wide for demonstration purposes. As \\(\\epsilon\\) grow smaller generally, the accuracy of all three finite different methods increases. However, that’s not always the case due to both the complexity of the function that you may be trying to differentiate or due to numerical inaccuracies of floating point math.\nTo demonstrate, here is a more complex example using an arbitrary function\n\\[\nf(x) = exp(x)\n\\]\nfor this example we’ll show the results of the three methods calculated at different values of \\(\\epsilon\\):\n\nf(x) = exp(x)\nϵ = 10 .^ (range(-16, stop=0, length=100))\nx0 = 1\nestimate = @. (f(x0 + ϵ) - f(x0 - ϵ)) / 2ϵ\n1actual = f(x0)\n\nfig = Figure()\nax = Axis(fig[1, 1], xscale=log10, yscale=log10, xlabel=\"ϵ\", ylabel=\"absolute error\")\nscatter!(ax, ϵ, abs.(estimate .- actual))\nfig\n\n\n1\n\nThe derivative of \\(f(x) = exp(x)\\) is itself. That is \\(f'(x) = f(x)\\) in this special case.\n\n\n\n\n\n\n\n\n\nFigure 16.1: A log-log plot showing the absolute error of the finite differences. Further to the left, roundoff error dominates while further to the right, truncation error dominates.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe @. in the code example above is a macro that applies broadcasting each function to its right. @. (f(x0 + ϵ) - f(x0 - ϵ)) / 2ϵ is the same as (f.(x0 .+ ϵ) .- f.(x0 .- ϵ)) ./ (2 .* ϵ)\n\n\nA few observations:\n\nAt virtually every value of ϵ we observe some error from the true derivative.\nThat error is the sum of two parts:\n\nTruncation error is inherent in that we are using a given non-zero value for ϵ and not determining the limiting analytic value as \\(\\epsilon \\to 0\\). The larger \\(\\epsilon\\) is, the larger the truncation error.\nRoundoff error which arises due to the limited precision of floating point math.\n\n\nThe implications of this are that we need to often be careful about the choice of ϵ, as the optimal choice will vary depending on the function and the point we are attempting to evaluate. This presents a number of practical difficulties in various algorithms.\nAdditionally, when computing the finite difference we must evaluate the function multiple times to determine a single estimate of the derivative. When performing something like optimization, the process typically involves iteratively making many guesses requiring many evaluations of the approximate derivative. Further, the efficiency of the algorithm usually depends on the accuracy of computing the derivative!\nDespite the accuracy and computational overhead, finite differences can be very useful in many circumstances. However, a more appealing alternative approach will be covered next.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#automatic-differentiation",
    "href": "autodiff.html#automatic-differentiation",
    "title": "16  Automatic Differentiation",
    "section": "16.4 Automatic Differentiation",
    "text": "16.4 Automatic Differentiation\nAutomatic differentiation is essentially the practice of defining algorithmically what the derivatives of function should be. We are able to do this through a creative application of the chain rule. Recall that the chain rule allows us to compute the derivative of a composite function using the derivatives of the component functions:\n\\[\nh(x)=f(g(x))\n\\] \\[\nh'(x) = f'(g(x)) g'(x)\n\\]\nUsing this rule, we can define how elementary operations act when differentiated. Combined with the fact that most computer code is building up from a bunch of elementary operations, we can get a very long way in differentiating complex functions.\n\n16.4.1 Dual Numbers\nTo understand where we are going, let’s remind ourselves about complex numbers. Complex numbers are of the form which has a real part (\\(r\\)) and an imaginary part (\\(iq\\)):\n\\[\nr+iq\n\\]\nBy definition we say that \\(i^2 = -1\\). This is useful because it allows us to perform certain types of operations (e.g. finding a square root of a negative number) that is otherwise unsolvable with just the real numbers1. After defining how the normal algebraic operations (addition, multiplication, etc.) work for the imaginary number, we are able to utilize the imaginary numbers for a variety of practical mathematical tasks.\nWhat is meant by extending the algebraic operations for imaginary numbers? For example, stating how addition should work for imaginary numbers:\n\\[\n(r+iq) + (s+iu) = (r+s) + i(q+u)\n\\]\nIn a similar fashion as extending the Real (\\(\\mathbb{R}\\)) numbers with an imaginary part, for automatic differentiation we will extend them with a dual part. A dual number is one of the form:\n\\[\na + \\epsilon b\n\\]\nWhere \\(\\epsilon^2 = 0\\) and \\(\\epsilon \\neq 0\\) by definition. While \\(a\\) represents the function value, \\(b\\) carries its derivative. An example should make this clearer. First let’s define a DualNumber:\n\n1struct DualNumber{T,U}\n    a::T\n    b::U\n2    function DualNumber(a::T, b::U=zero(a)) where {T,U}\n        return new{T,U}(a, b)\n    end\nend\n\n\n1\n\nWe define this type parametrically to handle all sorts of &lt;:Real types and allow a and b to vary types in case a mathematical operation causes a type change (e.g. as in the case of integers becoming a floating point number like 10/4 == 2.5)\n\n2\n\nIn the constructor, we set the default value of b to be zero(a). zero(a) is a generic way to create a value equal to zero with the same type of the argument a. E.g. zero(12.0) == 0.0 and zero(12) == 0.\n\n\n\n\nNow let’s define how dual numbers work under addition. The mathematical rule is:\n\\[\n(a+\\epsilon b)+(c+\\epsilon d)=(a+c)+(b+d)\\epsilon\n\\]\nWe then need to define how it works for the combinations of numbers that we might receive as arguments to our function (this is an example where multiple dispatch greatly simplifies the code compared to object oriented single dispatch!):\n\nBase.:+(d::DualNumber, e::DualNumber) = DualNumber(d.a + e.a, d.b + e.b)\nBase.:+(d::DualNumber, x) = DualNumber(d.a + x, d.b)\nBase.:+(x, d::DualNumber) = d + x\n\nAnd here’s how we would get the derivative of a very simple function:\n\nf1(x) = 5 + x\n\nf1(DualNumber(10, 1))\n\nDualNumber{Int64, Int64}(15, 1)\n\n\nThat’s not super interesting though - the derivative of f1 is just 1 and we supplied that in the construction of DualNumber. We did at least prove that we can add the 10 and 5!\nLet’s make this more interesting by also defining the multiplication operation on dual numbers. We’ll follow the product rule:\n\\[\n(u v)' = u' v + u v'\n\\]\n\nBase.:*(d::DualNumber, e::DualNumber) = DualNumber(d.a * e.a, d.b * e.a + d.a * e.b)\nBase.:*(x, d::DualNumber) = DualNumber(d.a * x, d.b * x)\nBase.:*(d::DualNumber, x) = x * d\n\nNow what if we evaluate this function:\n\nf2(x) = 5 + 3x\n\nf2(DualNumber(10, 1))\n\nDualNumber{Int64, Int64}(35, 3)\n\n\nWe have found that the second component is 3, which is indeed the derivative of \\(5+3x\\) with respect to \\(x\\). And in the first part we have the value of f2 evaluated at 10.\n\n\n\n\n\n\nNote\n\n\n\nWhen calculating the derivative, why do we start with 1 in the dual part of the number? Because the derivative of a variable with respect to itself is 1. From this unitary starting point, the various operations applied accumulate the derivative of the various operations in the \\(b\\) part of \\(a + \\epsilon b\\).\n\n\nWe can also define this for things like transcendental functions:\n\nBase.exp(d::DualNumber) = DualNumber(exp(d.a), exp(d.a) * d.b)\nBase.sin(d::DualNumber) = DualNumber(sin(d.a), cos(d.a) * d.b)\nBase.cos(d::DualNumber) = DualNumber(cos(d.a), -sin(d.a) * d.b)\nexp(DualNumber(1, 1))\n\nDualNumber{Float64, Float64}(2.718281828459045, 2.718281828459045)\n\n\n\n\nsin(DualNumber(0, 1))\n\nDualNumber{Float64, Float64}(0.0, 1.0)\n\n\n\ncos(DualNumber(0, 1))\n\nDualNumber{Float64, Float64}(1.0, -0.0)\n\n\nAnd finally, to put it all together in a more usable wrapper, we can define a function which will calculate the derivative of another function at a certain point. This function applies f to an initialized DualNumber and then returns the \\(b\\) component from the result:\n\nderivative(f, x) = f(DualNumber(x, one(x))).b\n\nderivative (generic function with 1 method)\n\n\nAnd then evaluating it on a more complex function like \\(f(x) = 5e^{sin(x)}+3x\\) at \\(x = 0\\), we would analytically derive \\(8\\), which matches what we calculate next:\n\nf3(x) = 5 * exp(sin(x)) + 3x\nderivative(f3, 0)\n\n8.0\n\n\nWe have demonstrated that through the clever use of dual numbers and the chain rule that complex expressions can be automatically differentiated by a computer to an exact level, limited only by the same machine precision that applies to our primary function of interest as well.\nLibraries exist (such as ChainRules.jl) which define large numbers of predefined rules for many more operations, even beyond basic algebraic functions. This allows complex programs to be differentiated automatically.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#performance-of-automatic-differentiation",
    "href": "autodiff.html#performance-of-automatic-differentiation",
    "title": "16  Automatic Differentiation",
    "section": "16.5 Performance of Automatic Differentiation",
    "text": "16.5 Performance of Automatic Differentiation\nRecall that in the finite difference method, we generally had to evaluate the function two or three times to approximate the derivative. Here we have a single function call that provides both the value and the derivative at that value. How does this compare performance-wise to simply evaluating the function a single time? Let’s check how long it takes to compute a Float64 versus a DualNumber:\n\nusing BenchmarkTools\n@btime f3(x) setup = x = rand()\n\n  4.833 ns (0 allocations: 0 bytes)\n\n\n8.565453981297253\n\n\n\n@btime f3(DualNumber(x, 1)) setup = x = rand()\n\n  8.800 ns (0 allocations: 0 bytes)\n\n\nDualNumber{Float64, Float64}(5.898039836210173, 8.539477014273457)\n\n\nIn performing this computation, the compiler has been able to optimize it such that we effectively are able to compute the function and its derivative at less than two times the cost of the base function evaluation. As the function gets more complex, the overhead does increase but is still a generally preferred versus finite differentiation. This advantage becomes more pronounced as we contemplate derivatives with respect to many variables at once or for higher-order derivatives.\n\n\n\n\n\n\nNote\n\n\n\nIn fact, it’s largely due to the advances in applications of automatic differentiation that has led to the explosion of machine learning and artificial intelligence techniques in the 2010s/2020s. The “learning” process relies on solving parameter weights and would be too computationally expensive if using finite differences.\nThese applications of AD in specialized C++ libraries underpin the libraries like PyTorch, TensorFlow, and Keras. These libraries specialize in allowing for AD on a limited subset of operations. Julia’s available AD libraries are more general and can be applied to many more scenarios.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#automatic-differentiation-in-practice",
    "href": "autodiff.html#automatic-differentiation-in-practice",
    "title": "16  Automatic Differentiation",
    "section": "16.6 Automatic Differentiation in Practice",
    "text": "16.6 Automatic Differentiation in Practice\nWe have, of course, not defined an exhaustive list of operations, covering only +, *, exp, sin, and cos. There are only a few more arithmetic (-, /) and transcendental (log, more trigonometric functions, etc.) before we would have a very robust set of algebraic operations defined for our DualNumber. In fact, it’s possible to go even further and to define the behavior through conditional expressions and iterations to differentiate fairly complex functions or to extend the mechanism to partial derivatives and higher-order derivatives as well.\n\nimport Distributions\nimport ForwardDiff\n\nN(x) = Distributions.cdf(Distributions.Normal(), x)\n\nfunction d1(S, K, τ, r, σ, q)\n    return (log(S / K) + (r - q + σ^2 / 2) * τ) / (σ * √(τ))\nend\n\nfunction d2(S, K, τ, r, σ, q)\n    return d1(S, K, τ, r, σ, q) - σ * √(τ)\nend\n\n\"\"\"\n    eurocall(parameters)\n\nCalculate the Black-Scholes implied option price for a european call where `parameters` is a vector with the following six elements:\n\n- `S` is the current asset price\n- `K` is the strike or exercise price\n- `τ` is the time remaining to maturity (can be typed with \\\\tau[tab])\n- `r` is the continuously compounded risk free rate\n- `σ` is the (implied) volatility (can be typed with \\\\sigma[tab])\n- `q` is the continuously paid dividend rate\n\"\"\"\nfunction eurocall(parameters)\n1    S, K, τ, r, σ, q = parameters\n    iszero(τ) && return max(zero(S), S - K)\n    d₁ = d1(S, K, τ, r, σ, q)\n    d₂ = d2(S, K, τ, r, σ, q)\n    return S * exp(-q * τ) * N(d₁) - K * exp(-r * τ) * N(d₂)\nend\n\n\n1\n\nWe put the various variables inside a single parameters vector to allow calling a single gradient call instead of multiple derivative calls for each parameter.\n\n\n\n\nMain.Notebook.eurocall\n\n\n\nS = 1.0\nK = 1.0\nτ = 30 / 365\nr = 0.05\nσ = 0.2\nq = 0.0\nparams = [S, K, τ, r, σ, q]\neurocall(params)\n\n0.024933768194037365\n\n\n\n\n\n\n\n\nTip\n\n\n\nSome terminology in differentiation:\n\nScalar-valued function: A function whose output is a single scalar.\n\nVector-valued (array-valued) function: A function whose output is a vector or array.\n\nDerivative (or partial derivative): The (instantaneous) rate of change of the output with respect to one input variable. In a multivariate context, these are partial derivatives, e.g., \\(\\frac{\\partial}{\\partial x} f(x,y,z)\\).\n\nGradient: For a scalar-valued function of several variables, the gradient is the vector of all first partial derivatives, e.g. \\(\\nabla f(x,y,z) = \\bigl[\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}\\bigr]^T\\).\n\nJacobian: For a vector-valued function, the Jacobian is the matrix of first partial derivatives. If $f:R^n R^m $, its Jacobian is an \\(m \\times n\\) matrix.\n\nHessian: For a scalar-valued function of several variables, the Hessian is the matrix of all second partial derivatives (i.e., it’s an \\(n \\times n\\) matrix).\n\n\n\nWith the above code, now we can get the partial derivatives with respect to each parameter. The first, third, fourth, fifth, and sixth correspond to the common “greeks” delta, theta, rho, vega, and epsilon respectively. The second term is the partial derivative with respect to the strike price:\n\nForwardDiff.gradient(eurocall, params)\n\n6-element Vector{Float64}:\n  0.5399635456230847\n -0.5150297774290475\n  0.16420676980838977\n  0.04233121458320943\n  0.11379886104405815\n -0.044380565393678295\n\n\n\nEntry 1: delta \\(\\partial C / \\partial S\\)\nEntry 2: sensitivity to strike \\(\\partial C / \\partial K\\) (no standard Greek)\nEntry 3: \\(\\partial C / \\partial \\tau\\), which is the negative of the market convention “theta” (\\(\\Theta = \\partial C / \\partial t = -\\partial C / \\partial \\tau\\))\nEntry 4: rho \\(\\partial C / \\partial r\\)\nEntry 5: vega \\(\\partial C / \\partial \\sigma\\)\nEntry 6: dividend rho (often denoted “phi”) \\(\\partial C / \\partial q\\)\n\nWe can also get the second order greeks with another call. This includes many uncommon second order partial derivatives, but the popular gamma is in the [1,1] position for example:\n\nForwardDiff.hessian(eurocall, params)\n\n6×6 Matrix{Float64}:\n  6.92276    -6.92276    0.242297   0.568994   -0.0853491   -0.613375\n -6.92276     6.92276   -0.07809   -0.526663    0.199148     0.568994\n  0.242297   -0.07809   -0.846846   0.521448    0.685306    -0.559878\n  0.568994   -0.526663   0.521448   0.0432874  -0.0163683   -0.0467667\n -0.0853491   0.199148   0.685306  -0.0163683   0.00245525   0.007015\n -0.613375    0.568994  -0.559878  -0.0467667   0.007015     0.0504144\n\n\n\n16.6.1 Performance\nEarlier we assessed the impact on performance for the derivatives using DualNumber on a very basic function. What about if we take a more realistic example like eurocall? We can observe approximately a 9x slowdown when computing all of the first order derivatives which isn’t bad considering we are computing 6x of the outputs!\n\n@btime eurocall($params)\n\n  17.410 ns (0 allocations: 0 bytes)\n\n\n0.024933768194037365\n\n\n\nlet\n1    g = similar(params)\n    @btime ForwardDiff.gradient!($g, eurocall, $params)\nend\n\n\n1\n\nTo avoid benchmarking memory allocation for the new array, we pre-allocate the array in memory to store the result and then call gradient! to fill in g for each result.\n\n\n\n\n  251.255 ns (3 allocations: 704 bytes)\n\n\n6-element Vector{Float64}:\n  0.5399635456230847\n -0.5150297774290475\n  0.16420676980838977\n  0.04233121458320943\n  0.11379886104405815\n -0.044380565393678295",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#forward-mode-and-reverse-mode",
    "href": "autodiff.html#forward-mode-and-reverse-mode",
    "title": "16  Automatic Differentiation",
    "section": "16.7 Forward Mode and Reverse Mode",
    "text": "16.7 Forward Mode and Reverse Mode\nThe approach outlined above is forward-mode AD, where derivatives are propagated alongside values. Reverse-mode AD evaluates the function, records a computation graph, and then accumulates sensitivities backwards.\nReverse mode requires more book-keeping because unlike the forward mode the derivative needs to be carried backwards, unlike the DualNumber approach of forward mode.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#practical-tips-for-automatic-differentiation",
    "href": "autodiff.html#practical-tips-for-automatic-differentiation",
    "title": "16  Automatic Differentiation",
    "section": "16.8 Practical tips for Automatic Differentiation",
    "text": "16.8 Practical tips for Automatic Differentiation\nHere are a few practical tips to keep in mind.\n\n16.8.1 Choosing between Reverse Mode and Forward Mode\nForward mode is more efficient when the number of outputs is much larger than the number inputs. When the number of inputs is much larger than the number of outputs, then reverse mode will generally be more efficient. Examples of the number of inputs being larger than the outputs might be in a statistical analysis where many features are used to predict a limited number of outcome variables or a complex model with a lot of parameters.\n\nUse forward mode when the number of inputs is small relative to the number of outputs.\nUse reverse mode when the number of inputs is large and the number of outputs is small (e.g., loss functions).\n\n\n\n16.8.2 Mutation\nAuto-differentiation works through most code, but a particularly tricky part to get right is when values within arrays are mutated (changed). It’s possible to do so but may require a little bit more boilerplate to setup. As of 2024, Enzyme.jl has the best support for functions with mutation inside of them.\n\n\n16.8.3 Custom Rules\nCustom rules for new or unusual functions can be defined, but this is an area that should be explored equipped with a bit of calculus and a deeper understanding of both forward-mode and reverse-mode. ChainRules.jl provides an interface for defining additional rules that hook into the AD infrastructure in Julia as well as provide a good set of documentation on how to extend the rules for your custom function.\n\n\n16.8.4 Available Libraries in Julia\n\nForwardDiff.jl provides robust forward-mode AD.\nZygote.jl is a reverse-mode package with the innovations of being able to differentiate structs in addition to arrays and scalars.\nEnzyme.jl is a newer package which allows for both forward and reverse mode, but has the advantage of supporting array mutation. Additionally, Enzyme works at the level of LLVM code (an intermediate level between high level Julia code and machine code) which allows for different, sometimes better, optimizations.\nDifferentiationInterface.jl is a wrapper library providing a consistent API while being able exchange different backends.\n\nIn the authors’ experience, they would probably recommend DifferentiationInterface.jl as a starting point, and diving into specific libraries if certain features are needed.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#references",
    "href": "autodiff.html#references",
    "title": "16  Automatic Differentiation",
    "section": "16.9 References",
    "text": "16.9 References\n\nhttps://book.sciml.ai/notes/08-Forward-Mode_Automatic_Differentiation_(AD)_via_High_Dimensional_Algebras/\nhttps://blog.esciencecenter.nl/automatic-differentiation-from-scratch-23d50c699555",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#footnotes",
    "href": "autodiff.html#footnotes",
    "title": "16  Automatic Differentiation",
    "section": "",
    "text": "Richard Feynman has a wonderful, short lecture on algebra here: https://www.feynmanlectures.caltech.edu/I_22.html↩︎",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "optimization.html",
    "href": "optimization.html",
    "title": "17  Optimization",
    "section": "",
    "text": "17.1 Chapter Overview\nOptimization as root finding or minimization/maximization of defined objectives. Differentiable programming and the benefits to optimization problems. Other non-gradient based optimization approaches. Model fitting as an optimization problem.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "optimization.html#introduction",
    "href": "optimization.html#introduction",
    "title": "17  Optimization",
    "section": "17.2 Introduction",
    "text": "17.2 Introduction\nLocal and global optimization: A local optimum value refers to a solution where the objective function (or cost function) has the best possible value in a neighborhood surrounding that solution. A global optimum value, on the other hand, is the best possible value of the objective function across the entire feasible domain. For smooth and convex functions, the gradient points towards the global minimum (or maximum), making it extremely efficient for finding the optimal solution. Even for non-convex functions, the gradient provides valuable information about the direction to move towards improving the objective function value locally.\n\nusing CairoMakie\nlet\n    f(x) = x^8 - 3x^4 + x\n    xs = range(-1.5, 1.5; length=1001)\n    ys = f.(xs)\n\n    fig = Figure(resolution=(600, 400))\n    ax = Axis(fig[1, 1],\n        xlabel=\"x\",\n        ylabel=\"f(x)\",\n        title=\"Function with Local and Global Optima (over [-1.5, 1.5])\",\n        limits=(-1.5, 1.5, minimum(ys) - 0.5, maximum(ys) + 0.5),\n    )\n\n    lines!(ax, xs, ys, color=:blue)\n\n    # Global minimum on the sampled grid\n    i_global = argmin(ys)\n    x_global_min = xs[i_global]\n    y_global_min = ys[i_global]\n    scatter!(ax, [x_global_min], [y_global_min], color=:red, markersize=10, label=\"Global minimum (on grid)\")\n\n    # Detect local minima by comparing neighbors (simple discrete test)\n    local_min_inds = findall((ys[2:end-1] .&lt; ys[1:end-2]) .& (ys[2:end-1] .&lt; ys[3:end])) .+ 1\n    # Exclude the global index to highlight a different local minimum if present\n    local_min_inds = filter(i -&gt; i != i_global, local_min_inds)\n    if !isempty(local_min_inds)\n        i_local = first(local_min_inds)\n        scatter!(ax, [xs[i_local]], [ys[i_local]], color=:black, markersize=10, label=\"Local minimum (on grid)\")\n    end\n\n    axislegend(ax, position=:rt)\n    fig\nend\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/PpRC2/src/scenes.jl:264\n\n\n\n\n\n\n\nOptimization techniques are incredibly important and find uses in many areas:\n\nMachine learning relies on “training”, which is essentially optimizing parameters to match target data.\nDetermining parameters for market consistent models where the modeled price is fit to market-observed prices.\nMinimizing a certain risk outcome by optimizing asset allocations\nMaximizing risk-adjusted yield in a portfolio.\n\nWe will introduce some basic concepts and categories of optimization techniques in this chapter:\n\nObjective and Loss Functions\nOptimization Techniques\n\nNonlinear Optimization\n\nGradient Based Techniques\nBracketing Methods\nOther Techniques\n\nLinear Optimization\n\n\nBelow is an expanded version of the section that aims to be more thorough while keeping the writing concise and direct.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "optimization.html#objective-and-loss-functions",
    "href": "optimization.html#objective-and-loss-functions",
    "title": "17  Optimization",
    "section": "17.3 Objective and Loss Functions",
    "text": "17.3 Objective and Loss Functions\nAny optimization algorithm needs to know what is being optimized. We call this function the objective function. Usually, the objective function is something we want to minimize—though maximizing a function is mathematically equivalent to minimizing its negative. Concretely, if you want to maximize \\(f(x)\\), you can minimize \\(-f(x)\\). The optimal point (also known as the \\(\\arg\\min\\)) is simply the input value that produces the smallest objective value.\nIn many practical situations—especially those with noisy or uncertain data—the function we want to optimize is called a loss function. This term is common in statistical modeling and machine learning, where the aim is to measure how far a model’s predictions deviate from real observations. For instance, the following squared loss function computes the sum of squared errors between a model’s predictions and the targets:\nloss(f, guess, target) = sum((f.(guess) .- target).^2)\nHere, guess might represent model parameters (or input values), and target is the observed data. Minimizing this loss aligns the model’s predictions with reality as closely as possible.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "optimization.html#optimization-techniques",
    "href": "optimization.html#optimization-techniques",
    "title": "17  Optimization",
    "section": "17.4 Optimization Techniques",
    "text": "17.4 Optimization Techniques\n\n17.4.1 Nonlinear Optimization\nNonlinear optimization refers to problems where the relationship between the inputs and outputs is not constrained to be a linear relationship, which is incredibly common in financial modeling.\nIn many financial settings, variables must obey certain bounds or relationships (e.g., portfolios cannot exceed a total budget, a particular risk measure must remain below a threshold). Handling constraints often involves methods like Lagrange multipliers, projected gradient, or analytic penalty functions. We won’t cover these in depth here, but be aware that most general optimization libraries can handle constraints in nonlinear problems as well.\n\n17.4.1.1 Gradient-Based Optimization\nThe most efficient nonlinear optimization algorithms tend to utilize the gradient (i.e. multivariable derivative) in order to make the optimization substantially more efficient. The gradient is useful because it can tell you if you are at a maxima or minima (when the derivative is zero, the function is at a maxima or minima), but also because algorithms can be ‘smarter’ about searching for a solution using the additional information. The gradient provides the direction of the steepest ascent of a function. Optimization algorithms often iteratively update parameters in the direction opposite to the gradient (for minimization problems), which tends to converge towards a local minimum (or maximum for maximization problems). Besides, computing the gradient is often computationally feasible and relatively inexpensive compared to other methods for determining function behavior, such as higher-order derivatives or finite-difference methods. Beyond just the direction, the magnitude (or norm) of the gradient also indicates how steep the function change is in that direction. This information is used to adjust step sizes in optimization algorithms, balancing between convergence speed and stability.\nCalculating gradients in the context of computer algorithms is discussed at length in Chapter 16, but a quick recap of the available approaches:\n\nFinite differences: evaluate the function at nearby points and determine the rates of change associated with each change in direction.\nAnalytic derivatives: A human-derived or computer tool (such as Mathematica) is able to analytically determine a derivative that is coded into the optimization algorithm\nAutomatic differentiation (AD): elementary rules are applied to decompose elementary code operations into derivatives, allowing for very efficient computation of complex algorithms.\n\nTo compare the approaches, here is an example of determining the derivative of a simple function at a certain point:\n\nusing Zygote\n\n# Define a differentiable function\nf(x) = 3x^2 + 2x + 1\n# Define an input value\nx = 2.0\n\nfinite_diff = (f(x + 0.001) - f(x)) / 0.001\n\nprintln(\"Value of f(x) at x=\", f(x))\n1println(\"Gradient of f(x) at x=\", finite_diff)\n2println(\"Gradient of f(x) at x=\", gradient(f, x))\n3println(\"Gradient of f(x) at x=\", 6 * x + 2)\n\n\n1\n\nA finite-difference approach to determining a gradient. Note that this method required two function calls of f to calculate an approximation.\n\n2\n\nAn automatic, exact derivative determined by the rules in the Zygote library.\n\n3\n\nAn analytic derivative using a user-coded derivative.\n\n\n\n\nValue of f(x) at x=17.0\nGradient of f(x) at x=14.002999999998877\nGradient of f(x) at x=(14.0,)\nGradient of f(x) at x=14.0\n\n\n\n17.4.1.1.1 Root finding\nRoot finding, also known as root approximation or root isolation, is the process of finding the values of the independent variable (usually denoted as \\(x\\)) for which a given function equals zero. In mathematical terms, if we have a function \\(f(x)\\), root finding involves finding values of \\(x\\) such that \\(f(x)=0\\).\nThere are various algorithms for root finding, each with its own advantages and disadvantages depending on the characteristics of the function and the requirements of the problem. One notable approach is Newton’s method, an iterative method that uses the derivative or gradient of the function to approximate the root with increasing accuracy in each iteration.\nWe will again use a simple function to illustrate the process:\n\nusing Zygote\n\n# Define a differentiable function\nf(x) = 2x^2 - 3x + 1\n# Define an initial value\nx = 0.0\n# tolerance of difference in value\ntol = 1e-6\n# maximum number of iteration of the algorithm\nmax_iter = 1000\niter = 0\nwhile abs(f(x)) &gt; tol && iter &lt; max_iter\n    x -= f(x) / gradient(f, x)[1]\n    iter += 1\nend\nif iter == max_iter\n    println(\"Warning: Maximum number of iterations reached.\")\nelse\n    println(\"Root found after \", iter, \" iterations.\")\nend\nprint(\"Approximate root: \", x)\n\nRoot found after 5 iterations.\nApproximate root: 0.4999999998835846\n\n\nAlthough it might look different, root-finding (i.e. finding \\(x\\) such that \\(g(x)=0\\)) can be cast as a minimization problem by defining an objective function such as \\(g(x))^2\\). In this view, driving \\(g(x)^2\\) to zero compels \\(g(x)\\) itself to be zero, so the methods and algorithmic ideas from minimization apply naturally to root-finding scenarios as well.\n\n\n17.4.1.1.2 BFGS\nBFGS—named for Broyden, Fletcher, Goldfarb, and Shanno—is a popular member of the quasi–Newton family of optimization algorithms. Although it does require first‐order gradient information, BFGS does not need the exact Hessian (i.e., second derivatives). Instead, it updates an approximation to the inverse Hessian at each step using the gradients from previous iterations. This extra curvature information allows BFGS to converge much faster than steepest descent in practice. Moreover, because it never explicitly forms the full Hessian matrix, it remains efficient for moderately sized problems. In finance and actuarial settings, BFGS can be especially useful for model calibration or parameter estimation tasks where one needs to handle nonlinear functions relatively quickly but cannot afford the computational overhead of second derivatives.\nThe following example uses the Optim package available in Julia to do the BFGS optimization. We don’t illustrate the complete algorithm as it is a bit longer, but wanted to ensure that this workhorse of an algorithm was mentioned.\n\nusing Optim\n\n# Define the objective function to minimize\nfunction objective_function(x)\n    return sum(abs2, x)\nend\n\n# Initial guess for the minimization\ninitial_x = [1.0]\n# Perform optimization using BFGS method\nresult = optimize(objective_function, initial_x, BFGS())\n# Extract the optimized solution\nsolution = result.minimizer\nminimum_value = result.minimum\n\n# Print the result\nprintln(\"Optimized solution: x = \", solution)\nprintln(\"Minimum value found: \", minimum_value)\n\nOptimized solution: x = [-6.359357485052897e-13]\nMinimum value found: 4.0441427622698303e-25\n\n\n\n\n\n17.4.1.2 Gradient-Free Optimization\nThis category includes algorithms that do not rely on gradients or derivative information. They often explore the objective function using heuristics or other types of probes to guide the search.\n\n17.4.1.2.1 Bracketing Methods\nA bracketed search algorithm is a technique used in optimization and numerical methods to confine or “bracket” a minimum or maximum of a function within a specified interval. The primary goal is to reduce the search space systematically until a satisfactory solution or range containing the optimal value is found.\nFor 1D minimization with bracketing, golden-section search and Brent’s method are standard. The bisection code below is for root finding, not minimization.\n\nfunction bisection_method(f, a, b; tol=1e-6, max_iter=100)\n    \"\"\"\n    Bisection method to find a root of the function f(x) within the interval [a, b].\n\n    Parameters:\n    - f: Function to find the root of.\n    - a, b: Initial interval [a, b] where the root is expected to be.\n    - tol: Tolerance for the root (default is 1e-6).\n    - max_iter: Maximum number of iterations allowed (default is 100).\n\n    Returns:\n    - root: Approximate root found within the tolerance.\n    - iterations: Number of iterations taken to converge.\n    \"\"\"\n    fa = f(a)\n    fb = f(b)\n    if fa * fb &gt; 0\n        error(\"The function values at the endpoints must have opposite signs.\")\n    end\n    iterations = 0\n    while (b - a) / 2 &gt; tol && iterations &lt; max_iter\n        c = (a + b) / 2\n        fc = f(c)\n        if fc == 0\n            return c, iterations\n        end\n        if fa * fc &lt; 0\n            b = c\n            fb = fc\n        else\n            a = c\n            fa = fc\n        end\n        iterations += 1\n    end\n    root = (a + b) / 2\n    return root, iterations\nend\n\n# Define the function we want to find the root of\nfunction f(x)\n    return x^3 - 6x^2 + 11x - 6.1\nend\n\n# Initial interval [a, b] and tolerance\na = 0.5\nb = 10\ntolerance = 1e-6\n# Apply the bisection method\nroot, iterations = bisection_method(f, a, b, tol=tolerance)\n\n# Print results\nprintln(\"Approximate root: \", root)\nprintln(\"Iterations taken: \", iterations)\nprintln(\"Function value at root: \", f(root))\n\nApproximate root: 3.046680122613907\nIterations taken: 23\nFunction value at root: -9.356632642010254e-7\n\n\nA popular practical algorithm is called Brent’s Method, which uses additional heuristics to accelerate the optimization routine in most cases.\n\n\n\n17.4.1.3 Other Non-Gradient Based Optimization Techniques\n\n17.4.1.3.1 Nelder-Mead simplex method\nThe Nelder-Mead simplex method is a popular optimization algorithm used for minimizing (or maximizing) nonlinear functions that are not necessarily differentiable. It’s particularly useful when gradient-based methods cannot be applied. It is often used in low-dimensional problems due to its simplicity and robustness. We will use the Rosenbrock function which can be useful in certain portfolio optimization problems to illustrate the process.\n\nusing Optim\n\n# Define the Rosenbrock function\nfunction rosenbrock(v)\n    x, y = v[1], v[2]\n    return (1 - x)^2 + 100 * (y - x^2)^2\nend\n\n# Initial guess for (x, y)\ninitial_guess = [-1.5, 2.0]\n\n# Perform optimization using the Nelder-Mead method\nresult = optimize(rosenbrock, initial_guess, NelderMead())\n\n# Extract results\noptimal_point = Optim.minimizer(result)\nminimum_value = Optim.minimum(result)\n\nprintln(\"Optimal Point: \", optimal_point)\nprintln(\"Minimum Value: \", minimum_value)\n\nOptimal Point: [0.9999913430783984, 0.9999847519696222]\nMinimum Value: 5.016695917763382e-10\n\n\n\n\n17.4.1.3.2 Simulated annealing\nSimulated Annealing (SA) is a probabilistic optimization technique inspired by the annealing process in metallurgy. It is used to find near-optimal solutions to optimization problems, particularly in cases where traditional gradient-based methods may get stuck in local minima/maxima. SA accepts worse solutions with a certain probability, allowing it to explore the search space more broadly initially and then gradually narrow down towards better solutions as it progresses. In this section the Rastrigin function is used to illustrate the process. The function can be useful for asset modeling.\n\nusing Random\n\nRandom.seed!(1234)\n\n# Parameters\nmax_iterations = 1000            # Number of iterations\ninitial_temperature = 100.0      # Starting temperature\ncooling_rate = 0.99              # Cooling rate (temperature multiplier)\nbounds = (-5.12, 5.12)           # Bounds for the search space\ndimension = 5                    # Number of dimensions in the search space\n\n# Objective function: Rastrigin function\nfunction rastrigin(x)\n    A = 10\n    n = length(x)\n    return A * n + sum(xi^2 - A * cos(2 * π * xi) for xi in x)\nend\n\n# Random initialization within bounds\nfunction initialize_solution()\n    return rand(bounds[1]:0.01:bounds[2], dimension)\nend\n\n# Random perturbation within bounds\nfunction perturb_solution(solution)\n    perturbed = copy(solution)\n    index = rand(1:dimension)\n    perturb_amount = rand(-0.1:0.01:0.1)  # Small random change\n    perturbed[index] += perturb_amount\n    # Ensure perturbed solution is within bounds\n    perturbed[index] = clamp(perturbed[index], bounds[1], bounds[2])\n    return perturbed\nend\n\n# Simulated Annealing main function\nfunction simulated_annealing()\n    current_solution = initialize_solution()\n    current_value = rastrigin(current_solution)\n    best_solution = copy(current_solution)\n    best_value = current_value\n    temperature = initial_temperature\n\n    for iteration in 1:max_iterations\n        # Generate new candidate solution by perturbation\n        candidate_solution = perturb_solution(current_solution)\n        candidate_value = rastrigin(candidate_solution)\n\n        # Acceptance probability (Metropolis criterion)\n        ΔE = candidate_value - current_value\n        if ΔE &lt; 0 || rand() &lt; exp(-ΔE / temperature)\n            current_solution = candidate_solution\n            current_value = candidate_value\n        end\n\n        # Update best solution found so far\n        if current_value &lt; best_value\n            best_solution = copy(current_solution)\n            best_value = current_value\n        end\n\n        # Decrease temperature\n        temperature *= cooling_rate\n        if iteration % 100 == 0\n            println(\"Iteration $iteration: Best Value = $best_value, Temperature = $temperature\")\n        end\n    end\n\n    return best_solution, best_value\nend\n\n# Run the simulated annealing algorithm\nbest_solution, best_value = simulated_annealing()\nprintln(\"Best Solution: \", best_solution)\nprintln(\"Best Value (Minimum): \", best_value)\n\nIteration 100: Best Value = 79.6456061966745, Temperature = 36.60323412732294\nIteration 200: Best Value = 68.74807121999402, Temperature = 13.397967485796167\nIteration 300: Best Value = 35.612137805380314, Temperature = 4.9040894071285726\nIteration 400: Best Value = 22.630909076712427, Temperature = 1.7950553275045138\nIteration 500: Best Value = 22.116366182140844, Temperature = 0.6570483042414603\nIteration 600: Best Value = 22.11468680700349, Temperature = 0.24050092913110663\nIteration 700: Best Value = 21.995266535865557, Temperature = 0.08803111816824594\nIteration 800: Best Value = 21.93833868942773, Temperature = 0.03222223628802339\nIteration 900: Best Value = 21.918338689427735, Temperature = 0.011794380589564411\nIteration 1000: Best Value = 21.918338689427735, Temperature = 0.004317124741065788\nBest Solution: [-0.9999999999999993, -1.3357370765021415e-16, -1.9799999999999993, 3.979999999999999, -0.9899999999999995]\nBest Value (Minimum): 21.918338689427735\n\n\n\n\n17.4.1.3.3 Particle swarm optimization (PSO)\nParticle swarm optimization is a metaheuristic optimization algorithm inspired by the social behavior of birds flocking or fish schooling. It is used to solve optimization problems by iteratively improving a candidate solution based on the velocity and position of particles (potential solutions) in the search space. The PSO algorithm differs from other methods in a key way, that instead of updating a single candidate solution at each iteration, we update a population (set) of candidate solutions, called a swarm. Each candidate solution in the swarm is called a particle. We think of a swarm as an apparently disorganized population of moving individuals that tend to cluster together while each individual seems to be moving in a random direction. The PSO algorithm aims to mimic the social behavior of animals and insects.\n\nusing Random, CairoMakie\n\n# Define the Rosenbrock function\nfunction rosenbrock(x)\n    return (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2\nend\n\n# PSO Implementation\nfunction particle_swarm_optimization(objective, n_particles, n_iterations, bounds, dim)\n    # Initialize particles\n    positions = [rand(bounds[1]:0.1:bounds[2], dim) for _ in 1:n_particles]\n    velocities = [rand(-1.0:0.1:1.0, dim) for _ in 1:n_particles]\n    personal_best_positions = deepcopy(positions)\n    personal_best_scores = [objective(p) for p in positions]\n    global_best_position = personal_best_positions[argmin(personal_best_scores)]\n    global_best_score = minimum(personal_best_scores)\n\n    # PSO parameters\n    ω = 0.5            # Inertia weight\n    c1, c2 = 2.0, 2.0  # Cognitive and social learning factors\n\n    # Optimization loop\n    for iter in 1:n_iterations\n        for i in 1:n_particles\n            # Update velocity\n            r1, r2 = rand(), rand()\n            velocities[i] .= ω .* velocities[i] +\n                             c1 * r1 .* (personal_best_positions[i] - positions[i]) +\n                             c2 * r2 .* (global_best_position - positions[i])\n\n            # Update position\n            positions[i] .= positions[i] .+ velocities[i]\n\n            # Clamp positions within bounds\n            positions[i] .= clamp.(positions[i], bounds[1], bounds[2])\n\n            # Evaluate fitness\n            score = objective(positions[i])\n            if score &lt; personal_best_scores[i]\n                personal_best_positions[i] = deepcopy(positions[i])\n                personal_best_scores[i] = score\n            end\n\n            if score &lt; global_best_score\n                global_best_position = deepcopy(positions[i])\n                global_best_score = score\n            end\n        end\n        if iter % 100 == 0\n            println(\"Iteration $iter: Best Score = $global_best_score\")\n        end\n    end\n    return global_best_position, global_best_score\nend\n\n# Parameters\nn_particles = 30\nn_iterations = 100\nbounds = (-2.0, 2.0)\ndim = 2\n\n# Run PSO\nbest_position, best_score = particle_swarm_optimization(rosenbrock, n_particles, n_iterations, bounds, dim)\n\nprintln(\"Best Position: $best_position\")\nprintln(\"Best Score: $best_score\")\n\n# Visualization\nx = -2.0:0.05:2.0\ny = -1.0:0.05:3.0\nZ = [(1 - xᵢ)^2 + 100 * (yᵢ - xᵢ^2)^2 for yᵢ in y, xᵢ in x]\n\n# Heatmap and Scatter Plot\nfig = Figure()\nax = Axis(fig[1, 1], title=\"PSO Optimization of Rosenbrock\", xlabel=\"x\", ylabel=\"y\")\nheatmap!(ax, x, y, Z, colormap=:viridis)\nscatter!(ax, [best_position[1]], [best_position[2]], color=:red, markersize=10, label=\"Optimal Solution\")\naxislegend(ax)\nfig\n\nIteration 100: Best Score = 5.829480589621494e-19\nBest Position: [1.0000000001510196, 1.0000000003768819]\nBest Score: 5.829480589621494e-19\n\n\n\n\n\n\n\n17.4.1.3.4 Evolutionary Algorithm\nAn evolutionary algorithm (EA) is a family of optimization algorithms inspired by the principles of biological evolution. They are particularly useful for solving complex optimization problems where traditional gradient-based methods may struggle due to nonlinearity, multimodality, or high dimensionality of the search space.\nThe following shows an example to maximize population fitness in terms of an objective function, with common crossover and mutation processes throughout all generations.\n\nusing Random\n\nRandom.seed!(1234)\n\n# Parameters\npopulation_size = 50           # Number of individuals in the population\nchromosome_length = 5          # Number of genes in each individual (dimensionality)\ngenerations = 100              # Number of generations\nmutation_rate = 0.1            # Probability of mutation\ncrossover_rate = 0.7           # Probability of crossover\nbounds = (-5.12, 5.12)         # Boundaries for each gene\n\n# Target function: Rastrigin function\nfunction rastrigin(x)\n    A = 10\n    n = length(x)\n    return A * n + sum(xᵢ^2 - A * cos(2 * π * xᵢ) for xᵢ in x)\nend\n\n# Initialize population randomly within bounds\nfunction initialize_population()\n    return [rand(bounds[1]:0.01:bounds[2], chromosome_length) for _ in 1:population_size]\nend\n\n# Fitness function (negative because we are minimizing)\nfunction fitness(individual)\n    return -rastrigin(individual)\nend\n\n# Selection: Tournament selection\nfunction tournament_selection(population, fitnesses)\n    candidates = rand(1:population_size, 2)\n    return ifelse(fitnesses[candidates[1]] &gt; fitnesses[candidates[2]],\n        population[candidates[1]], population[candidates[2]])\nend\n\n# Crossover: Single-point crossover\nfunction crossover(parent1, parent2)\n    if rand() &lt; crossover_rate\n        point = rand(1:chromosome_length)\n        child1 = vcat(parent1[1:point], parent2[point+1:end])\n        child2 = vcat(parent2[1:point], parent1[point+1:end])\n        return child1, child2\n    else\n        return parent1, parent2\n    end\nend\n\n# Mutation: Randomly change genes with some probability\nfunction mutate(individual)\n    for i in 1:chromosome_length\n        if rand() &lt; mutation_rate\n            individual[i] = rand(bounds[1]:0.01:bounds[2])\n        end\n    end\n    return individual\nend\n\n# Main Genetic Algorithm loop\nfunction genetic_algorithm()\n    population = initialize_population()\n    best_individual = nothing\n    best_fitness = -Inf\n\n    for gen in 1:generations\n        # Evaluate fitness\n        fitnesses = [fitness(ind) for ind in population]\n\n        # Find best individual in current population\n        current_best = argmax(fitnesses)\n        if fitnesses[current_best] &gt; best_fitness\n            best_fitness = fitnesses[current_best]\n            best_individual = population[current_best]\n        end\n\n        # Generate new population\n        new_population = []\n        while length(new_population) &lt; population_size\n            # Selection\n            parent1 = tournament_selection(population, fitnesses)\n            parent2 = tournament_selection(population, fitnesses)\n\n            # Crossover\n            child1, child2 = crossover(parent1, parent2)\n\n            # Mutation\n            child1 = mutate(child1)\n            child2 = mutate(child2)\n\n            # Add children to new population\n            push!(new_population, child1, child2)\n        end\n        population = new_population[1:population_size]\n\n        if gen % 100 == 0\n            println(\"Generation $gen: Best Fitness = \", best_fitness)\n        end\n    end\n\n    return best_individual, -best_fitness\nend\n\n# Run the genetic algorithm\nbest_solution, best_value = genetic_algorithm()\nprintln(\"Best Solution: \", best_solution)\nprintln(\"Best Value (Minimum): \", best_value)\n\nGeneration 100: Best Fitness = -2.9892957379930465\nBest Solution: [-0.02, -1.02, -0.04, 0.02, -1.04]\nBest Value (Minimum): 2.9892957379930465\n\n\n\n\n17.4.1.3.5 Bayesian optimization\nBayesian Optimization (BO) is a powerful technique for global optimization of expensive-to-evaluate black-box functions. It leverages probabilistic models to predict the objective function’s behavior across the search space and uses these models to make informed decisions about where to evaluate the function next. This approach efficiently balances exploration (searching for promising regions) and exploitation (exploiting regions likely to yield optimal values), making it particularly suitable for optimization problems where function evaluations are costly, such as tuning hyperparameters of machine learning models or optimizing parameters of complex simulations.\n\nusing Random\n\n# Define your objective function to be optimized\nfunction objective(x::Float64)\n    return -(x^2 + 0.1 * sin(5 * x))  # Example objective function (negative because we seek maximum)\nend\n# Bayesian optimization function\nfunction bayesian_optimization(objective, bounds::Tuple{Float64,Float64}, num_iterations::Int)\n    Random.seed!(1234)  # Setting a seed for reproducibility\n    X = Float64[]  # List to store evaluated points\n    Y = Float64[]  # List to store objective values\n    # Initial random point (you can choose other initial points as well)\n    x_init = rand() * (bounds[2] - bounds[1]) + bounds[1]\n    push!(X, x_init)\n    push!(Y, objective(x_init))\n    # Main loop\n    for i in 1:num_iterations\n        # Fit a model to the observed data (Gaussian Process in this case)\n        # For simplicity, let's just use the current best observed value\n        x_next = rand() * (bounds[2] - bounds[1]) + bounds[1]  # Random sampling\n        # Evaluate the objective function at the chosen point\n        y_next = objective(x_next)\n        # Update the data with the new observation\n        push!(X, x_next)\n        push!(Y, y_next)\n        # Here, we will just print the current best observed value\n        println(\"Iteration $i: Best value = $(maximum(Y))\")\n    end\n    # Return the best observed value and corresponding parameter\n    best_idx = argmax(Y)\n    return X[best_idx], Y[best_idx]\nend\n\nbest_x, best_value = bayesian_optimization(objective, (-5.0, 5.0), 10)\nprintln(\"Best x found: $best_x, Best value: $best_value\")\n\nIteration 1: Best value = -0.3041807254074535\nIteration 2: Best value = -0.3041807254074535\nIteration 3: Best value = -0.3041807254074535\nIteration 4: Best value = -0.3041807254074535\nIteration 5: Best value = -0.3041807254074535\nIteration 6: Best value = -0.3041807254074535\nIteration 7: Best value = -0.3041807254074535\nIteration 8: Best value = 0.02504980758369785\nIteration 9: Best value = 0.02504980758369785\nIteration 10: Best value = 0.02504980758369785\nBest x found: -0.057501331095793695, Best value: 0.02504980758369785\n\n\nWhile gradient-free methods help optimize complex nonlinear or noisy functions, some financial and operational problems are best modeled with linear relationships. The next section shows how to set up such problems, including constraints, using linear and integer programming.\n\n\n\n\n17.4.2 Linear optimization\nLinear optimization, also known as linear programming (LP), is a mathematical method for finding the best outcome in a mathematical model with linear relationships. It involves optimizing a linear objective function subject to a set of linear equality and inequality constraints. Linear programming has a wide range of applications across various fields, including operations research, economics, engineering, and logistics.\nWe will use linear optimization to solve the following problem, with \\(n\\) the number of elements in \\(b\\):\n\\[\n\\begin{aligned}\n\\max_{x} \\quad & c \\cdot x \\\\\n\\text{subject to} \\quad & x \\geq 0 \\\\\n& A_i \\cdot x \\leq b_i \\quad \\forall i \\in \\{1, 2, \\ldots, n\\}\n\\end{aligned}\n\\]\n\nusing JuMP, GLPK, LinearAlgebra\n\n# Define the objective coefficients\nc = [1.0, 2.0, 3.0]\n# Define the constraint matrix (A) and right-hand side (b)\nA = [1.0 1.0 0.0;\n    0.0 1.0 1.0]\nb = [10.0, 20.0]\n# Create a JuMP model\nlinear_model = Model(GLPK.Optimizer)\n# Define decision variables\n@variable(linear_model, x[1:3] &gt;= 0)\n# Define objective function\n@objective(linear_model, Max, dot(c, x))\n# Add constraints\n@constraint(linear_model, constr[i=1:2], dot(A[i, :], x) &lt;= b[i])\n# Solve the optimization problem\noptimize!(linear_model)\n\n# Print results\nprintln(\"Objective value: \", objective_value(linear_model))\nprintln(\"Optimal solution:\")\nfor i in 1:3\n    println(\"\\tx[$i] = \", value(x[i]))\nend\n\nWARNING: using JuMP.objective_function in module Notebook conflicts with an existing identifier.\nObjective value: 70.0\nOptimal solution:\n    x[1] = 10.0\n    x[2] = 0.0\n    x[3] = 20.0\n\n\n\n17.4.2.1 Integer programming\nInteger Programming (IP) is a type of optimization problem where some or all of the variables are restricted to be integers. Although the problem definition seems similar to an LP, the complexity of solving an IP hugely increases as the solution space is not continuous but discrete.\nLet us use IP to solve this problem. A factory produces two types of products \\(x_1\\) and \\(x_2\\) with the following details:\n\\[\n\\begin{aligned}\n\\max_{x} \\quad & 40x_1 + 50x_2 \\\\\n\\text{subject to} \\quad & x_1, x_2 \\in \\mathbb{Z} \\\\\n& 4x_1 + 3x_2 \\leq 200 \\quad \\text{(labor)} \\\\\n& x_1 + 2x_2 \\leq 40 \\quad \\text{(material)}\n\\end{aligned}\n\\]\n\n# Import necessary packages\nusing JuMP, GLPK\n\n# Create a model with the GLPK solver\nmodel = Model(GLPK.Optimizer)\n\n# Define decision variables (x₁ and x₂ are integers)\n@variable(model, x₁ &gt;= 0, Int)\n@variable(model, x₂ &gt;= 0, Int)\n\n# Define the objective function (maximize profit)\n@objective(model, Max, 40 * x₁ + 50 * x₂)\n\n# Add constraints\n@constraint(model, 4x₁ + 3x₂ &lt;= 200)  # Labor constraint\n@constraint(model, x₁ + 2x₂ &lt;= 40)   # Material constraint\n\n# Solve the model\noptimize!(model)\n\n# Check the solution status\nif termination_status(model) == MOI.OPTIMAL\n    println(\"Optimal solution found!\")\n    println(\"x₁ (Product x₁ units): \", value(x₁))\n    println(\"x₂ (Product x₂ units): \", value(x₂))\n    println(\"Maximum Profit: \", objective_value(model))\nelse\n    println(\"No optimal solution found.\")\nend\n\nOptimal solution found!\nx₁ (Product x₁ units): 40.0\nx₂ (Product x₂ units): 0.0\nMaximum Profit: 1600.0",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "optimization.html#example-model-fitting",
    "href": "optimization.html#example-model-fitting",
    "title": "17  Optimization",
    "section": "17.5 Example: Model Fitting",
    "text": "17.5 Example: Model Fitting\nIn model fitting, the “best fitting curve” refers to the curve or function that best describes the relationship between the independent and dependent variables in the data. The goal of model fitting is to find the parameters of the chosen curve or function that minimize the difference between the observed data points and the values predicted by the model.\nThe process of finding the best fitting curve typically involves:\n\nChoosing a model: Based on the nature of the data and the underlying relationship between the variables, a suitable model or family of models are selected.\nEstimating parameters: Using the chosen model, one estimates the parameters that best describe the relationship between the variables. This is often done using optimization techniques such as least squares regression, maximum likelihood estimation, or Bayesian inference.\nEvaluating the fit: Once the parameters are estimated, one evaluates the goodness of fit of the model by comparing the predicted values to the observed data. Common metrics for evaluating fit, or error functions, include the residual sum of squares, the coefficient of determination (R-squared), and visual inspection of the residuals.\nIterating if necessary: If the fit is not satisfactory, one may need to iterate on the model or consider alternative models until you find a satisfactory fit to the data.\n\nWhile we have shown general approaches like BFGS or gradient‐free schemes, libraries such as LsqFit wrap these concepts into convenient functions. Under the hood, these packages may employ gradient‐based methods (including automatic differentiation) to refine parameters. Below is a demonstration:\n\nusing LsqFit, CairoMakie\n\nx_data = 0:0.1:10\ny_data = 2 .* sin.(x_data) .+ 0.5 .* randn(length(x_data))\n# Define the model function\ncurve_model(x, p) = p[1] * x .^ 5 + p[2] * x .^ 4 .+ p[3] * x .^ 3 .+ p[4] * x .^ 2 .+ p[5] * x .+ p[6]\n# Initial parameter guess\np₀ = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n# Fit the model to the data\nfit_result = curve_fit(curve_model, x_data, y_data, p₀)\n# Extract the fitted parameters\nparams = coef(fit_result)\n# Evaluate the model with the fitted parameters\ny_fit = curve_model(x_data, params)\n# Plot the data and the fitted curve\nfig = Figure()\nAxis(fig[1, 1], title=\"Curve Fitting - a polynomial guess-fitting a sinusoidal function\")\nscatter!(x_data, y_data, label=\"Data\")\nlines!(x_data, y_fit, label=\"Fitted Curve\", linestyle=:dash, color=:red)\nfig",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "optimization.html#more-resources",
    "href": "optimization.html#more-resources",
    "title": "17  Optimization",
    "section": "17.6 More Resources",
    "text": "17.6 More Resources\nThe textbook “Algorithms For Optimization” (by Kochenderfer And Wheeler) is a comprehensive introduction to optimization and uses Julia for its examples.\nIn Julia, Optimization.jl provides a unified front-end for all kinds of general optimization problems. JuMP.jl provides a unified front-end in a specialized optimization mini-domain specific language.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "18  Visualizations",
    "section": "",
    "text": "18.1 Chapter Overview\nThe evolved brain and pattern recognition, a general guide for creating and iterating on visualizations, and principles for creating good visualizations while avoiding common mistakes.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "visualization.html#introduction",
    "href": "visualization.html#introduction",
    "title": "18  Visualizations",
    "section": "18.2 Introduction",
    "text": "18.2 Introduction\nVisualization is a cornerstone of data analysis, statistical modeling, and decision-making. It transforms raw data into something we can see and understand, making it easier to uncover patterns, communicate ideas, and make informed decisions.\nThe human brain can only parse a relatively small number of textual datapoints at a single time. We are incredibly visual creatures, with our brains able to process visually many orders of magnitude more information per second than through text.\nConsider the following example of tabular data, with four sets of paired \\(x\\) and \\(y\\) coordinates.\n\nusing DataFrames\n\n# Define the Anscombe Quartet data\nanscombe_data = DataFrame(\n    x1=[10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0],\n    y1=[8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68],\n    x2=[10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0],\n    y2=[9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74],\n    x3=[10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0],\n    y3=[7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73],\n    x4=[8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 19.0, 8.0, 8.0, 8.0],\n    y4=[6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n)\n\n11×8 DataFrame\n\n\n\nRow\nx1\ny1\nx2\ny2\nx3\ny3\nx4\ny4\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n10.0\n8.04\n10.0\n9.14\n10.0\n7.46\n8.0\n6.58\n\n\n2\n8.0\n6.95\n8.0\n8.14\n8.0\n6.77\n8.0\n5.76\n\n\n3\n13.0\n7.58\n13.0\n8.74\n13.0\n12.74\n8.0\n7.71\n\n\n4\n9.0\n8.81\n9.0\n8.77\n9.0\n7.11\n8.0\n8.84\n\n\n5\n11.0\n8.33\n11.0\n9.26\n11.0\n7.81\n8.0\n8.47\n\n\n6\n14.0\n9.96\n14.0\n8.1\n14.0\n8.84\n8.0\n7.04\n\n\n7\n6.0\n7.24\n6.0\n6.13\n6.0\n6.08\n8.0\n5.25\n\n\n8\n4.0\n4.26\n4.0\n3.1\n4.0\n5.39\n19.0\n12.5\n\n\n9\n12.0\n10.84\n12.0\n9.13\n12.0\n8.15\n8.0\n5.56\n\n\n10\n7.0\n4.82\n7.0\n7.26\n7.0\n6.42\n8.0\n7.91\n\n\n11\n5.0\n5.68\n5.0\n4.74\n5.0\n5.73\n8.0\n6.89\n\n\n\n\n\n\nSomething not obvious by looking at the tabular data above is that each set of data has the same summary statistics. That is, the four sets of data are all described by the same linear features.\n\nusing Statistics, Printf\nlet d = anscombe_data\n    map([[:x1, :y1], [:x2, :y2], [:x3, :y3], [:x4, :y4]]) do pair\n        x, y = eachcol(d[:, pair])\n\n        # calculate summary statistics\n        mean_x, mean_y = mean(x), mean(y)\n        intercept, slope = ([ones(size(y)) x] \\ y)\n        correlation = cor(x, y)\n\n        (; mean_x, mean_y, intercept, slope, correlation)\n    end |&gt; DataFrame\nend\n\n4×5 DataFrame\n\n\n\nRow\nmean_x\nmean_y\nintercept\nslope\ncorrelation\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n9.0\n7.50091\n3.00009\n0.500091\n0.816421\n\n\n2\n9.0\n7.50091\n3.00091\n0.5\n0.816237\n\n\n3\n9.0\n7.5\n3.00245\n0.499727\n0.816287\n\n\n4\n9.0\n7.50091\n3.00173\n0.499909\n0.816521\n\n\n\n\n\n\nAnalytical summarization alone is not enough to understand the data. We need to visualize the data to see the patterns emerge, wherein each of the four datasets tells a very different story:\n\nusing CairoMakie\n\n\n# Create the plots\nfig = Figure(resolution=(800, 800))\n\nax1 = Axis(fig[1, 1], title=\"Dataset 1\")\nscatter!(ax1, anscombe_data.x1, anscombe_data.y1)\nlines!(ax1, 2:14, x -&gt; 3 + 0.5x, color=:red)\n\nax2 = Axis(fig[1, 2], title=\"Dataset 2\")\nscatter!(ax2, anscombe_data.x2, anscombe_data.y2)\nlines!(ax2, 2:14, x -&gt; 3 + 0.5x, color=:red)\n\nax3 = Axis(fig[2, 1], title=\"Dataset 3\")\nscatter!(ax3, anscombe_data.x3, anscombe_data.y3)\nlines!(ax3, 2:14, x -&gt; 3 + 0.5x, color=:red)\n\nax4 = Axis(fig[2, 2], title=\"Dataset 4\")\nscatter!(ax4, anscombe_data.x4, anscombe_data.y4)\nlines!(ax4, 2:14, x -&gt; 3 + 0.5x, color=:red)\n\nfig\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\nThis dataset is known as Anscombe’s Quartet and is a famous statistical example which is used here to underscore the importance of visualization when seeking to understand or communicate data. However, there are more reasons to refine your experience in the art and science of data visualization which we list in Table 18.1.\n\n\n\nTable 18.1: A list of reasons to practice the art and science of data visualization.\n\n\n\n\n\n\n\n\n\nPurpose\nDescription\n\n\n\n\nSimplify Complexity\nRaw data can be overwhelming, especially with large datasets or many variables. A single visualization can convey thousands of points of data into a clear picture.\n\n\nReveal Patterns and Relationships\nSome insights are hidden in plain sight until you visualize them, such as the relationships in the Anscombe’s Quartet example.\n\n\nSupport Better Decisions\nUnderstanding patterns and relationships can then translate into better decision making, such as highlighting trends or risks at a glance.\n\n\nCommunicate Effectively\nConveying information to others in a visual manner is one of the most effective way at aiding in understanding. The best visualizations don’t just inform— they tell a story that’s useful for understanding and decision making\n\n\nEncourage Exploration\nVisual exploration is at the heart of understanding data, uncovering distributions, relationships, or unusual patterns before diving into formal models.\n\n\n\n\n\n\nVisualization isn’t just about making data look pretty—it’s about making it useful. Whether you’re exploring data for the first time, presenting findings to stakeholders, or refining a model, good visualizations are essential tools for turning information into insight.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "visualization.html#developing-visualizations",
    "href": "visualization.html#developing-visualizations",
    "title": "18  Visualizations",
    "section": "18.3 Developing Visualizations",
    "text": "18.3 Developing Visualizations\nHow does one develop effective visualizations? While a specialty all its own, we present the following considerations when creating quantitative displays of visual information. Consider this a guide of ‘how’ to create visualizations of data.\n\n18.3.1 Define Your Message\n\nClarify the Objective: Start with a clear purpose. What is the key insight you want to communicate? Whether it’s forecasting trends, assessing risk, analyzing variances, or comparing financial scenarios, your visualization should be laser-focused on delivering that message, stripping out unnecessary details.\nKnow Your Audience: Tailor every aspect of your visualization to the needs and expertise of your audience. For financial professionals or actuaries, this means ensuring that the visual elements align with their analytical requirements and technical proficiency.\n\n\n\n18.3.2 Emphasize Accuracy and Integrity\n\nMaintain Consistent Scales: Ensure axes are uniformly scaled and proportional to avoid misleading interpretations. For instance, when showing growth rates or volatility, avoid truncating or exaggerating axes. Consider using logarithmic axes when plotting growth or exponential relationships.\nThink about human perception of the shapes. For example:\n\nWe have a hard time comparing arc distances compared to linear distances, so pie charts are almost always a bad idea.\nWhen using area to convey data (such as the size of a circle), note that the area scales quadratically, so small changes in diameter can lead to large perceptual differences in area.\n\nData-Driven Design: Strip away unnecessary decorative elements. Every visual component should serve the core purpose of guiding interpretation based on the data.\n\n\n\n18.3.3 Prioritize Clarity Over Complexity\n\nSimplify Graphics: Use straightforward charts, clean lines, and precise labels. Avoid embellishments that detract from the data’s message such as color variation for aesthetics’ sake.\nEliminate “Chartjunk”: Remove distracting elements like excessive gridlines, complex legends, or overly varied colors. Each element should have a clear role in supporting the narrative.\nLeverage White Space: Thoughtful use of white space can help separate key elements and make comparisons more intuitive.\n\n\n\n18.3.4 Organize Data Thoughtfully\n\nDecompose Complexity: When dealing with multi-variable or time-series data, consider breaking it into small multiples or related charts for side-by-side comparison.\nLayered Information: Combine related datasets (e.g., financial performance vs. risk exposure), but ensure each layer is visually distinct and does not obscure others.\nProvide Multiple Views: Offer both high-level summaries for quick insights and detailed views for deeper analysis.\n\n\n\n18.3.5 Enhance Readability\n\nClear Annotations: Label axes, data points, and key takeaways explicitly. Use annotations to highlight critical insights, such as shifts in trends or activation of risk triggers.\nConsistent Design Elements: Stick to legible fonts and cohesive color schemes. For example, use consistent colors across charts to represent comparable data points for easier pattern recognition.\n\n\n\n18.3.6 Validate and Iterate\n\nTest for Clarity: Share your visuals with peers or stakeholders to ensure they are interpreted as intended. Feedback can help identify areas of confusion or misrepresentation.\nIterate Continuously: Treat visualization design as an evolving process. Refine layouts, scales, and annotations based on feedback and changing analytical needs.\n\nEffective financial visualizations are built on clarity, accuracy, and thoughtful organization. By adhering to these principles—streamlined design, data integrity, and iterative refinement—you can transform complex datasets into actionable insights that empower decision-making in financial modeling and actuarial work.\n\n\n\n\n\n\nTip\n\n\n\nMost financial modelers are familiar with putting together plots in Excel….\n\n\n\n\n18.3.7 Example: Improving a Disease Funding Visualization\nWe will take a visualization (?fig-vox-plot) which has a number of issues and apply some of the principles above to improve the communication.\nThis example was found via (Schwarz 2016), which also identifies several of the following issues with the graphic:\n\nMisleading Circle Areas: Using circle diameters to represent values distorts perception because people intuit a comparison of area, not diameter. This results in misleading comparisons, such as the area for Breast Cancer funds appearing four times larger than Prostate Cancer, despite being only twice the value.\nData Dimensionality: There are two dimensions of data conveyed: deaths and funding, while it’s presented with four degrees of variation: (1) color, (2) vertical ranking, (3) horizontal categorization, and (4) bubble size.\nLabeling Issue: Disease names should be placed directly on circles to improve readability and assist color-blind individuals.\nComparison Clarity: To effectively compare funds raised to deaths caused, these metrics should be displayed side-by-side or connected with lines.\nPrecision Overload: Excessive precision in numerical data, such as using eight digits for funds raised, is unnecessary and can confuse interpretation.\nMissing Data Label: The graph omits a label for the last dollar amount, likely related to Diabetes, which could be avoided by labeling directly on the graph.\n\n\n\n\nVox Media Infographic that inappropriately and ineffectively conveys data. From Vox Media (Accessed via Archive.org) (Matthews 2014). {#fig-vox-plot}\n\n\nIn this revised version, we take the data as accurate and simply recast the visualization of the data. The revised plot takes the following steps:\n\nUse a simpler 2D scatterplot mirroring the two dimensional data.\nEliminate unnecessary color and let the labels themselves sit within the plot to avoid the eye needing to jump between the legend and the datapoints.\nRemove precision in the axis ticks, since decimal level precision is not necessary to tell the story.\nRemove unnecessary plot elements including gridlines and axes without tick labels.\n\n\nusing CairoMakie\n\n# Data\ndiseases = [\"Breast Cancer\", \"Prostate Cancer\", \"Heart Disease\", \"Motor Neuron/ALS\",\n    \"HIV / AIDS\", \"Chronic Obstructive Pulmonary Disease\", \"Diabetes\", \"Suicide\"]\nmoney_raised = [257, 147, 54.1, 22.9, 14, 7, 4.2, 3.2]\ndeaths_us = [41.374, 21.176, 596.577, 6.849, 7.683, 142.942, 73.831, 39.518]\n\n# Create the scatter plot\nfig = Figure()\nax = Axis(\n    fig[1, 1],\n    xlabel=\"Annual Money Raised (\\$millions)\",\n    ylabel=\"Annual Deaths in US (thousands)\",\n    limits=(0, 350, -30, 770),\n    xgridvisible=false,\n    ygridvisible=false,\n)\nhidespines!(ax, :t, :r)\nscatter!(ax, money_raised, deaths_us)\n\n# Annotate each point with the disease name\nfor (i, disease) in enumerate(diseases)\n    # avoid overlapping labels\n    offset = if disease == \"Motor Neuron/ALS\"\n        (0, -15)\n    else\n        (3, 2)\n    end\n    text!(ax, money_raised[i], deaths_us[i], text=disease, fontsize=12, offset=offset)\nend\n\n# Display the plot\nfig\n\n\n\n\nFrom the revised plot, a few key insights emerge naturally and immediately:\n\nThe cancers receive outsized funding relative to the deaths caused.\nHeart disease remains an outsized killer compared to all other causes of death present.\n\nAnd it raises some interesting questions:\n\nIs there an inverse relationship between the perceived “control” one has over a disease and how much funding people are willing to allocate to a cause?\nGiven the wide dispersion in funding, does it matter? How does funding correlate with progress? E.g. has there been faster progress in extending lifespan from avoiding cancer deaths than other diseases?\n\nThe new visualization is easier to understand and draw comparisons. From the clarity, relationships are revealed and the visualizations itself reveals interesting followup questions and suggests follow-on analysis to be performed.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "visualization.html#principles-of-good-visualization",
    "href": "visualization.html#principles-of-good-visualization",
    "title": "18  Visualizations",
    "section": "18.4 Principles of Good Visualization",
    "text": "18.4 Principles of Good Visualization\nExtending the above “how”, we now present the “what”; principles of good visualization (some elements taken from (Tufte 2001)):\n\nClearly represent the data without distortions of size or space.\n\nRefrain from clipping axes.\nDo not rely on features such as shape area unless you have fully considered how viewers perceive them.\n\nUtilize variations of features to represent data dimensionality with purpose.\n\nIf colors vary in a plot, the different colors should have meaning.\nDon’t jump to a 3D plot - use variations in marker/line styles or small multiples to convey higher dimensions.\n\nEncourage the eye to compare different pieces of data.\nReveal the data at several levels of detail, from a broad overview to the fine structure.\n\nInstead of summary statistics, try plotting all of the data with reduced transparency and let the viewer draw summary conclusions.\n\nMaintain consistency throughout the exhibit.\n\nAny change in font, color, size, weight, etc. can be interpreted as an intentional choice that the viewer will try to interpret - don’t overburden the viewer.\n\nServe a reasonably clear purpose: description, exploration, tabulation, or decoration and cut out what’s not purposeful.\n\nMaximize the data to ink ratio.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "visualization.html#types-of-visualization-tools",
    "href": "visualization.html#types-of-visualization-tools",
    "title": "18  Visualizations",
    "section": "18.5 Types of visualization tools",
    "text": "18.5 Types of visualization tools\nWhile not an exhaustive list by any means, we take a brief tour through some very common plots and the associated Julia code.\n\n\n\n\n\n\nNote\n\n\n\nIn several of the examples, we could go further to abide by the previously listed principles of good visualizations, such as removing unnecessary gridlines or chart elements. Here, the intention is to provide a sense of how these types of plots might be constructed programmatically. Therefore, we seek not just to streamline the plots themselves, but to also ensure that the code examples are simple and understandable.\n\n\n\nBasic Charts and Graphs: Bar charts, line graphs, scatter plots, histograms, pie charts.\n\nBar charts are best for comparing categorical data or discrete values across different categories. Sometimes categories can be grouped for a stacked bar chart to show for example how each category changes over time.\nLine graphs are best for showing trends over time or when we want to highlight the rate of change. It is very intuitive to use line graphs to track trends or patterns.\nScatter plots are best for showing relationships or correlations between two variables. It is used a lot when one looks for patterns, clusters or outliers, or would like to explore the distribution of data points across different dimensions.\nHistograms are best for showing the distribution of a single continuous variable, or visualizing the distribution of data points across different ranges or intervals.\n\n\n\nusing Random, CairoMakie\n\n# Data for the plots\ncategories = [\"Product A\", \"Product B\", \"Product C\", \"Product D\"]\nsales = [150, 250, 200, 300]  # For bar chart\n\nx = randn(100)  # For scatter plot\n\n# Combine individual plots into a 2x2 layout\nf = Figure()\nbarplot(f[1, 1], 1:4, sales, axis=(xticks=(1:4, categories), title=\"bar\", xticklabelsize=10))\naxis = Axis(f[1, 2], title=\"line\")\nlines!(f[1, 2], cumsum(x))\naxis = Axis(f[2, 1], title=\"scatter\")\nscatter!(axis, x)\naxis = Axis(f[2, 2], title=\"histogram\")\nhist!(axis, x)\nf\n\n\n\n\n\nMultivariate Visualizations: Heatmaps, parallel coordinates plots, radar charts, bubble charts.\n\nHeatmaps are best for visualizing the intensity, interactions or relationships of values across two dimensions.\nBubble charts which are variants of scatter plots are best for showing relationships between three variables. One can easily highlight relative importance or magnitude using the size of bubbles (e.g., revenue, population).\nParallel coordinates plots are best for comparing multiple variables across different observations. They are often used for detecting patterns, correlations or relationships across multiple dimensions.\nRadar charts are best for comparing multiple variables for a single or few observations, especially when one needs to show comparisons of several quantitative variables for one or more items, with each variable represented on an axis.\n\n\n\nusing Random, CairoMakie\n\nRandom.seed!(1234)\n\n# Data for plots\n# For heatmap\nxs = range(0, π, length=10)\nys = range(0, π, length=10)\nzs = [sin(x * y) for x in xs, y in ys]\n\nbubble_x = rand(10) * 10\nbubble_y = rand(10) * 10\nbubble_size = rand(10) * 100\n\n# Dummy data for radar chart\nradar_data = [0.7, 0.9, 0.4, 0.6, 0.8]\n\n# Dummy data for parallel coordinates plot\nparallel_data = rand(10, 5)\n\nf = Figure()\n\n# Heatmap (1,1)\nax1 = Axis(f[1, 1], title=\"Heatmap\")\nheatmap!(ax1, xs, ys, zs)\n\n# Parallel coordinates (1,2)\nax2 = Axis(f[1, 2], title=\"Parallel Coordinates\")\nfor i in 1:size(parallel_data, 2)\n    lines!(ax2, 1:size(parallel_data, 2), parallel_data[i, :])\nend\n\n\n# Radar Chart (2,1)\nax3 = Axis(f[2, 1], title=\"Radar Chart\", aspect=1)\nangles = range(0, 2π, length=length(radar_data) + 1)\nr = [radar_data; radar_data[1]]\narc!(ax3, Point2f(0), 0.5, -π, π, color=:grey90)\narc!(ax3, Point2f(0), 1, -π, π, color=:grey90)\nlines!(ax3, cos.(angles) .* r, sin.(angles) .* r, color=:green)\npoly!(ax3, cos.(angles) .* r, sin.(angles) .* r, color=(:blue, 0.2))\nscatter!(ax3, cos.(angles) .* r, sin.(angles) .* r, color=:red)\nhidedecorations!(ax3)\nhidespines!(ax3)\n\n# Bubble Plot (2,2)\nax4 = Axis(f[2, 2], title=\"Bubble Plot\")\nscatter!(ax4, bubble_x, bubble_y, markersize=bubble_size, color=:orange, alpha=0.5)\n\nf\n\n\n\n\n\nDimensionality Reduction: PCA plots, t-SNE, and UMAP for visualizing high-dimensional data. Here we show an example how high-dimensional data can be shown on a t-SNE plot.\n\nHere we show an example to cluster synthetic stocks based on financial indicators like:\n– Volatility – Momentum (6-month return) – Market Cap – P/E Ratio – Dividend Yield\nt-SNE will reduce the dimensions and help us visualize clusters of stocks with similar characteristics.\n\nusing TSne, DataFrames, Random, Distributions, CairoMakie, StatsBase\n\n# Generate synthetic financial dataset\nRandom.seed!(42)\nnum_stocks = 100\n\ndf = DataFrame(\n    Stock=[\"Stock_$(i)\" for i in 1:num_stocks],\n    Volatility=rand(Uniform(10, 50), num_stocks),  # % annualized\n    Momentum=rand(Uniform(-10, 30), num_stocks),   # 6-month return\n    Market_Cap=1:num_stocks,  # in billion USD\n    P_E_Ratio=rand(Uniform(5, 50), num_stocks),\n    Dividend_Yield=rand(Uniform(0, 5), num_stocks) # in %\n)\n\n# Normalize features\nfeatures = [:Volatility, :Momentum, :Market_Cap, :P_E_Ratio, :Dividend_Yield]\nX = Matrix(df[:, features])\nX = StatsBase.standardize(ZScoreTransform, X, dims=1)  # Standardize data\n\n# Apply t-SNE\ntsne_result = tsne(X)\n\n# Add t-SNE components to DataFrame\ndf.TSNE_1 = tsne_result[:, 1]\ndf.TSNE_2 = tsne_result[:, 2]\n\n# A graph of somewhat randomly distributed but also small patterns of linearity.\nMakie.scatter(df.TSNE_1, df.TSNE_2)\n\n\n\n\n\nTime Series Visualization: Line charts, area charts, time-series decomposition plots. Refer to Chapter 20 for a time series plot.\nGeospatial Visualization: Maps, choropleth maps for visualizing spatial data. Here we show how spatial data can be visualized using a choropleth map.\nInteractive Dashboards: Tools like Tableau, Power BI, Pluto for interactive and dynamic data exploration.\n\n\n18.5.1 Additional Examples\nFor more kinds of visualizations, see:\n\nhttps://datavizproject.com\nA Tour Through the Visualization Zoo ((Heer, Bostock, and Ogievetsky 2010))",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "visualization.html#julia-plotting-packages",
    "href": "visualization.html#julia-plotting-packages",
    "title": "18  Visualizations",
    "section": "18.6 Julia Plotting Packages",
    "text": "18.6 Julia Plotting Packages\nJulia has several powerful packages for data visualization, each with different strengths depending on your needs (e.g., interactive vs. static plots, ease of use vs. customization). Each package has unique strengths depending on the use case, so the best choice depends on our specific needs, the type of data, and whether we need interactive or static visualizations. Below are some of the most common visualization packages in Julia:\n\n18.6.1 CairoMakie.jl and GLMakie.jl\nMakie is designed for high-performance, interactive, and 3D visualization. It supports real-time interaction and is highly customizable. It supports 2D and 3D plotting and real-time interactivity. It is also extremely fast with GPU acceleration for certain operations. CairoMakie is suitable for print-quality vector output, while GLMakie utilizes GPU acceleration for high quality 2D and 3D plots. CairoMakie was chosen as the tool for this book because it offers very sensible default behavior and aesthetics, is easy to customize, and generally straightforward code.\n\n\n18.6.2 Plots.jl\nPlots is one of the most versatile and popular Julia plotting libraries. It provides a high-level interface for different plotting backends (e.g., GR, Plotly, PyPlot, PGFPlotsX, etc.). It uses a high-level syntax that is easy to use. It supports multiple backends for both static and interactive plots, but can be more limited in its customization options.\n\n18.6.2.1 StatsPlots.jl\nStatsPlots extends Plots by adding statistical plot types such as boxplots, violin plots, histograms, and density plots. It’s ideal for users who frequently work with statistical data. It is specialized for statistical visualizations. It allows easy integration with Julia’s statistical packages like DataFrames and StatsBase.\n\n\n\n18.6.3 GraphPlot.jl\nThis package is used to plot graphs (networks), such as social network visualizations or other graph-related problems. It supports integration with the LightGraphs package for graph analytics.\n\n\n18.6.4 UnicodePlots.jl\nUnicodePlots provides simple plotting capabilities in the terminal using Unicode characters, making it lightweight and fast. There are no external dependencies. It is great for quick plotting within the terminal.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "visualization.html#references",
    "href": "visualization.html#references",
    "title": "18  Visualizations",
    "section": "18.7 References",
    "text": "18.7 References\nMuch of the principles and some of the examples are inspired by (Tufte 2001).\n\n\n\n\nHeer, Jeffrey, Michael Bostock, and Vadim Ogievetsky. 2010. “A Tour Through the Visualization Zoo.” Communications of the ACM. https://homes.cs.washington.edu/~jheer/files/zoo/.\n\n\nMatthews, Dylan. 2014. “The Ice Bucket Challenge, and Why We Give to Charity.” 2014. https://web.archive.org/web/20140823152725/https://www.vox.com/2014/8/20/6040435/als-ice-bucket-challenge-and-why-we-give-to-charity-donate.\n\n\nSchwarz, C. J. 2016. “A Short Tour of Bad Graphs.” Online. http://www.stat.sfu.ca/~cschwarz/posters/1999/absenteeism.pdf.\n\n\nTufte, Edward. 2001. The Visual Display of Quantitative Information. 2nd ed. Graphics Press.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "matrices.html",
    "href": "matrices.html",
    "title": "19  Matrices and Their Uses",
    "section": "",
    "text": "19.1 Chapter Overview\nMatrices and their myriad uses: reframing problems through the eyes of linear algebra, an intuitive refreshing on applicable maths, and recurring patterns of matrix operations in financial modeling.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Matrices and Their Uses</span>"
    ]
  },
  {
    "objectID": "matrices.html#matrix-manipulation",
    "href": "matrices.html#matrix-manipulation",
    "title": "19  Matrices and Their Uses",
    "section": "19.2 Matrix manipulation",
    "text": "19.2 Matrix manipulation\nWe first review basic matrix manipulation routines before going into more advanced topics.\n\n19.2.1 Addition and subtraction\nThink of each matrix as a data grid (like a spreadsheet). Adding or subtracting values element by element is analogous to combining two sets of financial figures—such as merging cash inflows and outflows.\nExample: Combining variations in A and B where each element represents a specific scenario’s impact when doing cash flow projections. We would like to see combined variations of A and B, and we also would like to know the difference in variations between A and B.\n\n# Define two matrices\nA = [1 2 3;\n    4 5 6;\n    7 8 9]\nB = [9 8 7;\n    6 5 4;\n    3 2 1]\n# Perform element-wise matrix addition and subtraction\nC = A .+ B\nD = A .- B\n# Display the result\nprintln(\"Result of matrix addition:\")\nprintln(C)\nprintln(\"Result of matrix subtraction:\")\nprintln(D)\n\nResult of matrix addition:\n[10 10 10; 10 10 10; 10 10 10]\nResult of matrix subtraction:\n[-8 -6 -4; -2 0 2; 4 6 8]\n\n\n\n\n19.2.2 Transpose\nTransposing a matrix is akin to flipping a dataset over its diagonal—turning rows into columns. This operation is useful when aligning data for regression or matching dimensions in financial models.\nExample: Converting a time-series (rows as time points) in A into a format suitable for cross-sectional analysis (columns as different variables) in B.\n\n# Define a matrix\nA = [1 2 3;\n    4 5 6;\n    7 8 9]\n# Perform matrix transpose\nB = A'\n# Display the result\nprintln(\"Result of matrix transpose:\")\nprintln(B)\n\nResult of matrix transpose:\n[1 4 7; 2 5 8; 3 6 9]\n\n\n\n\n19.2.3 Determinant\nThe determinant acts as a “volume-scaling” factor. It indicates how much a linear transformation stretches or compresses space. A zero determinant signals that the transformation collapses the space into a lower dimension, implying that the matrix cannot be inverted.\nGiven a matrix A \\[\n\\mathbf{A} =\n\\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nn}\n\\end{pmatrix}\n\\]\nthe determinant of matrix A can be calculated as \\[\n\\det(\\mathbf{A}) = \\sum_{j=1}^{n} (-1)^{1+j} a_{1j} \\det(\\mathbf{A}_{1j})\n\\]\nExample: In portfolio theory, a near-zero determinant of a covariance matrix might indicate multicollinearity among assets.\n\nusing LinearAlgebra\n\n# Define a matrix\nA = [1 2 3;\n    5 10 20;\n    7 8 9]\n# Perform matrix determinant calculation\nB = det(A)\n# Display the result\nprintln(\"Result of matrix determinant:\")\nprintln(B)\n\nResult of matrix determinant:\n30.000000000000007\n\n\n\n\n19.2.4 Trace\nThe trace, being the sum of the diagonal elements, offers a quick summary that can reflect the total variance or influence of a matrix.\nExample: In risk analysis, the trace of a covariance matrix may provide insights into the overall market volatility captured by the diagonal elements.\n\nusing LinearAlgebra\n\n# Define a matrix\nA = [1 2 3;\n    5 10 20;\n    7 8 9]\n# Perform matrix determinant calculation\nB = tr(A)\n# Display the result\nprintln(\"Result of matrix trace:\")\nprintln(B)\n\nResult of matrix trace:\n20\n\n\n\n\n19.2.5 Norm\nA matrix norm measures the “size” or “energy” of the matrix. It generalizes the concept of vector length to matrices, quantifying the overall magnitude.\nThe Frobenius norm of a matrix is defined as: \\[\n\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |A_{ij}|^2}\n\\]\nExample: A common usage of norms include error analysis, where the norm of the difference between two matrices measures how far an approximation has deviated from the true values. Another common usage in machine learning is during regularization where it allows the training process to know how large an error would be to guide the direction of updating parameters.\n\nusing LinearAlgebra\n\n# Define a matrix\nA = [1 2 3;\n    4 5 6;\n    7 8 9]\n# Perform matrix norm calculation\nB = norm(A)\n# Display the result\nprintln(\"Result of matrix norm:\")\nprintln(B)\n\nResult of matrix norm:\n16.881943016134134\n\n\n\n\n19.2.6 Multiplication\nMatrix multiplication (non-element-wise) represents the composition of linear transformations. It’s like applying a sequence of financial adjustments—first transforming the data with one factor and then modifying it with another. Other applications include:\n\nTransforming asset returns by a matrix representing factor loadings to obtain risk contributions.\nNeural network construction. Matrix multiplication is fundamental for training and using neural networks.\nSystems of linear equations. Many real-world problems reduce to solving systems of linear equations.\n\n\n# Define two matrices\nA = [1 2 3;\n    4 5 6;\n    7 8 9]\nB = [9 8 7;\n    6 5 4;\n    3 2 1]\n# Perform non-element-wise matrix multiplication\nC = A * B\n# Display the result\nprintln(\"Result of non-element-wise matrix multiplication:\")\nprintln(C)\n\nResult of non-element-wise matrix multiplication:\n[30 24 18; 84 69 54; 138 114 90]\n\n\nOn the other hand, element-wise multiplication multiplies corresponding elements directly (like applying a weight matrix).\nExample: Adjusting individual cash flow items by their respective risk weights in stress testing.\n\n# Define two matrices\nA = [1 2 3;\n    4 5 6;\n    7 8 9]\nB = [9 8 7;\n    6 5 4;\n    3 2 1]\n# Perform element-wise matrix multiplication\nC = A .* B\n# Display the result\nprintln(\"Result of element-wise matrix multiplication:\")\nprintln(C)\n\nResult of element-wise matrix multiplication:\n[9 16 21; 24 25 24; 21 16 9]\n\n\n\n\n19.2.7 Inversion\nMatrix inversion “reverses” a transformation. If a matrix transforms one set of financial assets into another state, its inverse would bring them back.\nExample: In solving linear systems for equilibrium pricing, obtaining the inverse of the coefficient matrix allows you to revert to the original asset prices.\n\n# Define a matrix\nA = [1 2; 3 4]\n# Compute the inverse of the matrix\nA_inv = inv(A)\n# Display the result\nprintln(\"Inverse of matrix A:\")\nprintln(A_inv)\n\nInverse of matrix A:\n[-1.9999999999999996 0.9999999999999998; 1.4999999999999998 -0.4999999999999999]\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor a matrix to be inverted, it must meet several important criteria. - Square Matrix. The matrix must be square, meaning it has the same number of rows and columns. - The determinant of the matrix must be non-zero.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Matrices and Their Uses</span>"
    ]
  },
  {
    "objectID": "matrices.html#matrix-decomposition",
    "href": "matrices.html#matrix-decomposition",
    "title": "19  Matrices and Their Uses",
    "section": "19.3 Matrix decomposition",
    "text": "19.3 Matrix decomposition\n\n19.3.1 Eigenvalues\nEigenvalue decomposition, also known as eigen decomposition, is a matrix factorization that decomposes a matrix into its eigenvectors and eigenvalues. This technique uncovers the intrinsic “modes” or principal directions in a dataset. The eigenvalues indicate the strength of each mode, while eigenvectors show the direction or pattern associated with that strength. Eigenvalues and eigenvectors are fundamental concepts in linear algebra and play key roles include:\n\nEigenvalues help in analyzing how linear transformations affect vectors in a vector space.\nEigenvalues facilitate the diagonalization of matrices and simplify the calculations.\nIn systems of differential equations, eigenvalues help determine the stability of equilibrium points.\nIdentifying the main factors that cause variance in a set of asset returns, which is critical for risk management or stress testing portfolios.\nIn graph theory, eigenvalues of the adjacency matrix provide insights into the properties of the graph, such as connectivity, stability, and clustering.\nMany algorithms in data science, like clustering and factorization methods, rely on eigenvalues to identify patterns and reduce dimensionality, which enhances computational efficiency and interpretability.\n\n\nusing LinearAlgebra\n\n# Create a square matrix\nA = [1 2 3;\n    4 5 6;\n    7 8 9]\n# Perform eigenvalue decomposition\neigen_A = eigen(A)\n# Extract eigenvalues and eigenvectors\nλ = eigen_A.values\nV = eigen_A.vectors\n\n# Display the results\nprintln(\"Original Matrix:\")\nprintln(A)\nprintln(\"\\nEigenvalues:\")\nprintln(λ)\nprintln(\"\\nEigenvectors:\")\nprintln(V)\n\nOriginal Matrix:\n[1 2 3; 4 5 6; 7 8 9]\n\nEigenvalues:\n[-1.1168439698070434, -8.582743335036247e-16, 16.11684396980703]\n\nEigenvectors:\n[-0.7858302387420671 0.4082482904638635 -0.2319706872462857; -0.0867513392566285 -0.8164965809277261 -0.5253220933012335; 0.6123275602288101 0.4082482904638627 -0.8186734993561815]\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor a matrix to get eigenvalues, it must be square, meaning it has the same number of rows and columns.\n\n\n\n\n19.3.2 Singular values\nSingular value decomposition (SVD) breaks a matrix into three matrices U, Σ, and V, representing the left singular vectors (analogous to the primary features), the singular values (diagonal matrix capturing the importance), and the right singular vectors (detail on how features interact), respectively. Singular values are key to:\n\nMatrix factorization, which simplifies many matrix operations, making it easier to analyze and manipulate data.\nDimensionality reduction. This is particularly useful in high-dimensional data scenarios, where reducing dimensions helps eliminate noise and improve computational efficiency.\nSVD can be used for data compression, particularly in image processing.\nSVD helps filter out noise in data analysis.\nSVD provides a robust method for solving linear equations, particularly when the matrix is ill-conditioned or singular.\nIn machine learning, SVD helps extract important features from datasets.\nSVD provides insights into the relationships within data. The singular values indicate the strength of the relationship, while the singular vectors offer a way to visualize and interpret those relationships.\n\n\nusing LinearAlgebra\n\n# Create a random matrix\nA = rand(4, 3)\n# Perform Singular Value Decomposition (SVD)\nU, Σ, V = svd(A)\n# U: Left singular vectors\n# Σ: Singular values (diagonal matrix)\n# V: Right singular vectors (transpose)\n# Reconstruct original matrix\nA_reconstructed = U * Diagonal(Σ) * V'\n\n# Display the results\nprintln(\"Original Matrix:\")\nprintln(A)\nprintln(\"\\nLeft Singular Vectors:\")\nprintln(U)\nprintln(\"\\nSingular Values:\")\nprintln(Σ)\nprintln(\"\\nRight Singular Vectors:\")\nprintln(V)\nprintln(\"\\nReconstructed Matrix:\")\nprintln(A_reconstructed)\n\nOriginal Matrix:\n[0.4489215164053002 0.46214548318738025 0.1219982249634336; 0.7935126360575298 0.669402656693519 0.7977853901465533; 0.3864460150311093 0.6668772535542175 0.5922671528861628; 0.2455008265712988 0.4364767459431822 0.31053831756656447]\n\nLeft Singular Vectors:\n[-0.3307641226910797 -0.9286025022459885 0.04778030188870862; -0.7148196083802631 0.2053552313743689 -0.662754614690138; -0.5271832791564495 0.3089721052851582 0.59567480319012; -0.31891348794514895 -0.007928425147902507 0.4512703095737221]\n\nSingular Values:\n[1.8162865612573293, 0.2827642518197883, 0.239638526348568]\n\nRight Singular Vectors:\n[-0.5493224179933457 -0.48260368018837935 -0.6821572904826102; -0.6178144863941182 -0.31509718176798407 0.7204297512203715; -0.5626278890097046 0.8171948689942359 -0.12506959901581707]\n\nReconstructed Matrix:\n[0.44892151640530015 0.4621454831873795 0.12199822496343322; 0.79351263605753 0.6694026566935187 0.7977853901465536; 0.38644601503110954 0.6668772535542175 0.592267152886163; 0.2455008265712989 0.4364767459431823 0.31053831756656464]\n\n\n\n\n19.3.3 Matrix Factorization and Factorization Machines\nMatrix factorization is a popular technique in recommendation systems for modeling user-item interactions and making personalized recommendations. The core idea behind matrix factorization is to decompose the user-item interaction matrix into two lower-dimensional matrices, capturing latent factors that represent user preferences and item characteristics. By learning these latent factors, the recommendation system can make predictions for unseen user-item pairs.\nFactorization Machines (FM) are a type of supervised machine learning model designed for tasks such as regression and classification, especially in the context of recommendation systems and predictive modeling with sparse data. FM models extend traditional linear models by incorporating interactions between features, allowing them to capture complex relationships within the data.\nExample: In credit scoring or recommendation systems for financial products, these techniques reveal latent factors that influence customer behavior.\n\nusing Recommendation, SparseArrays, MLDataUtils\n\n# Generate synthetic user-item interaction data\nnum_users = 100\nnum_items = 50\nnum_ratings = 500\nuser_ids = rand(1:num_users, num_ratings)\nitem_ids = rand(1:num_items, num_ratings)\nratings = rand(1:5, num_ratings)\n# Create a sparse user-item matrix\nuser_item_matrix = sparse(user_ids, item_ids, ratings)\n# Split data into training and testing sets\ntrain_data, test_data = splitobs(user_item_matrix, 0.8)\n# Set parameters for matrix factorization\nnum_factors = 10\nnum_iterations = 10\n# Train matrix factorization model\ndata = DataAccessor(user_item_matrix)\nrecommender = MF(data) # FactorizationMachines(data) alternatively\nfit!(recommender)\n# Predict ratings for the test set\nrec = Dict()\nfor user in 1:num_users\n    rec[user] = recommend(recommender, user, num_items, collect(1:num_items))\nend\n# Evaluate model performance\npredictions = []\nfor (i, j, v) in zip(findnz(test_data.data)[1], findnz(test_data.data)[2], findnz(test_data.data)[3])\n    for p in rec[i]\n        if p[1] == j\n            push!(predictions, p[2])\n            break\n        end\n    end\nend\nrmse = measure(RMSE(), predictions, nonzeros(test_data.data))\nprintln(\"Root Mean Squared Error (RMSE): \", rmse)\n\nRoot Mean Squared Error (RMSE): 1.285060774521402\n\n\n\n\n19.3.4 Principal component analysis\nPrincipal Component Analysis (PCA) is a widely used technique in various fields for dimensionality reduction, data visualization, feature extraction, and noise reduction. PCA can also be applied to detect anomalies or outliers in the data by identifying data points that deviate significantly from the normal patterns captured by the principal components. Anomalies may appear as data points with large reconstruction errors or as outliers in the low-dimensional space spanned by the principal components.\nExample: Compressing various economic indicators into a handful of principal components to illustrate predominant trends in market dynamics or risk factors.\n\nusing MultivariateStats\n\n# Generate some synthetic data\ndata = randn(100, 5)  # 100 samples, 5 features\n# Perform PCA\npca_model = fit(PCA, data; maxoutdim=2)  # Project to 2 principal components\n# Transform the data\ntransformed_data = transform(pca_model, data)\n# Access principal components and explained variance ratio\nprincipal_components = pca_model.prinvars\nexplained_variance_ratio = pca_model.prinvars / sum(pca_model.prinvars)\n\n# Print results\nprintln(\"Principal Components:\")\nprintln(principal_components)\nprintln(\"Explained Variance Ratio:\")\nprintln(explained_variance_ratio)\n\nPrincipal Components:\n[32.66071532330788, 28.874865183703864]\nExplained Variance Ratio:\n[0.5307614725367923, 0.46923852746320777]",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Matrices and Their Uses</span>"
    ]
  },
  {
    "objectID": "data-learning.html",
    "href": "data-learning.html",
    "title": "20  Learning from Data",
    "section": "",
    "text": "20.1 Chapter Overview\nYun-Tien Lee\nWe will touch on how to use data to inform a model: fitting parameters, forecasting, and fundamental limitations on prediction.",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Learning from Data</span>"
    ]
  },
  {
    "objectID": "data-learning.html#how-to-learn-from-data",
    "href": "data-learning.html#how-to-learn-from-data",
    "title": "20  Learning from Data",
    "section": "20.2 How to learn from data",
    "text": "20.2 How to learn from data\n\n20.2.1 Understand the problem and define goals\n\nClarify objectives: What we want to achieve with the data (e.g., prediction, classification, clustering, or insight extraction).\nIdentify key metrics: Determine how success will be measured (accuracy, RMSE, precision, etc.).\nKnow the context: Understand the domain and business problem one is addressing to shape the data analysis process.\n\n\n\n20.2.2 Collect data\nVarious data may be available in different formats. - Ensure data relevance: The data should be relevant to the problem. - Consider data quality: Collect data with high accuracy, completeness, and consistency.\n\n\n20.2.3 Explore and preprocess the data\nThis involves data cleaning and preparation to ensure the dataset is suitable for analysis.\n\nHandle missing data: We could impute missing values (mean, median, or KNN imputation), or drop rows/columns with excessive missing data.\nDeal with outliers: Use statistical techniques (e.g., z-scores) to detect and remove or cap extreme values.\nFeature scaling: Apply normalization or standardization to ensure features are on comparable scales (important for algorithms like SVM, K-means, etc.).\nEncode categorical data: Use techniques such as: one-hot encoding for nominal data, or label encoding or ordinal encoding for ordered categories.\nData visualization: Use tools like Makie.jl to visualize distributions, correlations, and missing values.\n\n\n\n20.2.4 Exploratory data analysis (EDA)\nEDA helps discover patterns, relationships, and insights within the data. One can do the following, but not limited, to these analyses:\n\nSummary statistics: Check mean, variance, skewness, and correlations between variables.\nVisualize relationships: Use histograms, scatter plots, box plots, and heatmaps to identify trends and correlations.\nDetect multicollinearity: Check correlations between independent variables (e.g., Pearson’s correlation matrix).\n\n\n\n20.2.5 Select and engineer features\nFeature selection and engineering help improve model performance by focusing on the most relevant information.\n\n\n20.2.6 Choose the right algorithm or model\nDepending on our problem type, choose appropriate algorithms for learning from the data:\n\nSupervised Learning (with labeled data):\n\nClassification: Logistic regression, SVM, decision trees, random forests, or neural networks.\nRegression: Linear regression, ridge regression, or gradient boosting.\n\nUnsupervised Learning (without labeled data):\n\nClustering: K-means, DBSCAN, hierarchical clustering.\nDimensionality Reduction: PCA, t-SNE, or UMAP.\n\nReinforcement Learning: Learn from interactions with an environment (e.g., Q-learning, Deep Q-Networks).\n\n\n\n20.2.7 Train and evaluate the model\n\nSplit the data: Use either a train-test split (e.g., 80/20 or 70/30 split) or a cross-validation (e.g., k-fold cross-validation).\nFit the model: Train the model on the training set.\nEvaluate the model: Use evaluation metrics appropriate to the task.\n\n\n\n20.2.8 Tune hyperparameters\nHyperparameters control how models learn. One can use techniques like the following to tune hyperparameters:\n\nGrid search: Test a range of hyperparameter values.\nRandom search: Randomly explore combinations of hyperparameters.\nBayesian optimization: Use probabilistic models to guide hyperparameter search.\n\n\n\n20.2.9 Deploy and Monitor the Model\nOnce the model performs well, deploy it to make predictions on new data.\n\nModel deployment platforms: Use tools like Flask, FastAPI, or MLOps platforms.\nMonitor performance: Continuously monitor metrics to detect concept drift or performance degradation.\n\n\n\n20.2.10 Draw Insights and Make Decisions\nFinally, interpret the results and use insights to make decisions or recommendations. Effective communication of findings is essential, especially for stakeholders.\n\nVisualization: Use dashboards or reports to communicate findings.\nInterpretability: Use explainable AI (e.g., SHAP values) to make model predictions transparent.\n\n\n\n20.2.11 Limitations\nHowever, there are certain fundamental limitations:\n\nThere may often be inherent uncertainty and noise in the data itself.\nEvery model has its own assumptions and simplifications.\nThere may be non-stationarity in the data, especially in financial data. Non-stationary processes change over time, meaning that patterns learned from past data may no longer be valid in the future.\nModels may be overfitting or underfitting. Overfitting occurs when a model is too complex and captures noise instead of the underlying pattern, leading to poor generalization to new data. Underfitting occurs when the model is too simple to capture the relevant structure in the data.\nSometimes in high-dimensional spaces, data becomes sparse, and meaningful patterns are harder to identify.\nSome predictions may be limited by ethical concerns (e.g., predicting criminal behavior) or legal restrictions (e.g., privacy laws that limit data collection).",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Learning from Data</span>"
    ]
  },
  {
    "objectID": "data-learning.html#applications",
    "href": "data-learning.html#applications",
    "title": "20  Learning from Data",
    "section": "20.3 Applications",
    "text": "20.3 Applications\n\n20.3.1 Parameter fitting\nRefer to Chapter 17 on Optimization for more details.\n\n\n20.3.2 Forecasting\nForecasting is the process of making predictions about future events or outcomes based on historical data, patterns, and trends. It involves the use of statistical methods, machine learning models, or expert judgment to estimate future values in a time series or predict the likelihood of specific events. Forecasting is widely used in fields like economics, finance, meteorology, supply chain management, and business planning.\nHere is an example how to do time series forecasting in Julia, where point sizes show covariance of predictions:\n\nusing CSV, DataFrames, CairoMakie, StateSpaceModels\n\nairp = CSV.read(StateSpaceModels.AIR_PASSENGERS, DataFrame)\nlog_air_passengers = log.(airp.passengers)\nsteps_existing = length(log_air_passengers)\nsteps_ahead = 30\n\n# SARIMA\nmodel_sarima = SARIMA(log_air_passengers; order=(0, 1, 1), seasonal_order=(0, 1, 1, 12))\nfit!(model_sarima)\nforec_sarima = forecast(model_sarima, steps_ahead)\n\nf = Figure()\naxis = Axis(f[1, 1], title=\"SARIMA\")\nscatter!(1:steps_existing, log_air_passengers)\nscatter!(steps_existing+1:steps_existing+steps_ahead, map(x -&gt; x[1], forec_sarima.expected_value), color=:red, markersize=map(x -&gt; x[1] * 1000, forec_sarima.covariance))\nf\n\n\n┌ Warning: f_tol is deprecated. Use f_abstol or f_reltol instead. The provided value (1.0e-6) will be used as f_reltol.\n└ @ Optim ~/.julia/packages/Optim/8dE7C/src/types.jl:120",
    "crumbs": [
      "Interdisciplinary Concepts and Applications",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Learning from Data</span>"
    ]
  },
  {
    "objectID": "julia-development.html",
    "href": "julia-development.html",
    "title": "Developing in Julia",
    "section": "",
    "text": "Aside from the essentials needed to convey the topics, the previous chapters have provided relatively limited insight into the workflow of developing in Julia. Extending our earlier analogy: up to this point we have discussed using hammers and saws to create widgets, and even how to make a widget run faster and more smoothly on different machines. This chapter turns to the process of building and designing larger systems—the practices that make your widget‑making factory run smoothly.\nThis chapter is heavy with Julia-specific tips and advice. We have deliberately delayed this content until well into the book, focusing on the concepts instead of getting bogged down on language-specific details. In this section, we dive into the messy business of building bigger, integrated things and tools to make that easier. Readers taking the concepts to other languages need not burden themselves with the details of Julia workflows and therefore can jump to the section beginning with 30  Stochastic Mortality Projections.\n\n\n\n\n\n\nNote\n\n\n\nThe chapters in this section are adapted from Modern Julia Workflows, originally written by G. Dalle, J. Smit, and A. Hill. These chapters derive from that work and are also licensed under CC BY‑SA 4.0.\nThe content has been modified to align with the rest of this book (e.g., adding cross‑references and removing duplicated material) and to add or remove elements that the authors deemed more appropriate for financial modelers.\n\n\n```",
    "crumbs": [
      "Developing in Julia"
    ]
  },
  {
    "objectID": "julia-writing.html",
    "href": "julia-writing.html",
    "title": "21  Writing Julia Code",
    "section": "",
    "text": "21.1 Chapter Overview\nInstalling and setting up your Julia environment. Text editor and REPL editing environments. Setting up your global environment for development. Creating packages. Logging and debugging code.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Writing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-writing.html#getting-help",
    "href": "julia-writing.html#getting-help",
    "title": "21  Writing Julia Code",
    "section": "21.2 Getting help",
    "text": "21.2 Getting help\nBefore you write any line of code, it’s good to know where to find help. The official help page is a good place to start. In particular, the Julia community is always happy to guide beginners.\nAs a rule of thumb, the Discourse forum is where you should ask your questions to make the answers discoverable for future users. If you just want to chat with someone, you have a choice between the open source Zulip and the closed source Slack.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Writing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-writing.html#installation",
    "href": "julia-writing.html#installation",
    "title": "21  Writing Julia Code",
    "section": "21.3 Installation",
    "text": "21.3 Installation\nThe most natural starting point to install Julia onto your system is the Julia downloads page, which will tell you to use juliaup.\n\nWindows users can download Julia and juliaup together from the Windows Store.\nMacOS or Linux users can execute the following terminal command:\n\ncurl -fsSL https://install.julialang.org | sh\nIn both cases, this will make the juliaup and julia commands accessible from the terminal (or Windows Powershell). On Windows this will also create an application launcher. All users can start Julia by running\njulia\nMeanwhile, juliaup provides various utilities to download, update, organize and switch between different Julia versions. As a bonus, you no longer have to manually specify the path to your executable. This all works thanks to adaptive shortcuts called “channels”, which allow you to access specific Julia versions without giving their exact number.\nFor instance, the release channel will always point to the current stable version, and the lts channel will always point to the long-term support version. Upon installation of juliaup, the current stable version of Julia is downloaded and selected as the default.\n\n\n\n\n\n\nTip\n\n\n\nTo use other channels, add them to juliaup and put a + in front of the channel name when you start Julia:\njuliaup add lts\njulia +lts\nYou can get an overview of the channels installed on your computer with\njuliaup status\nWhen new versions are tagged, the version associated with a given channel can change, which means a new executable needs to be downloaded. If you want to catch up with the latest developments, just do\njuliaup update",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Writing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-writing.html#repl",
    "href": "julia-writing.html#repl",
    "title": "21  Writing Julia Code",
    "section": "21.4 REPL",
    "text": "21.4 REPL\nThe Read-Eval-Print Loop (or REPL) is the most basic way to interact with Julia, check out its documentation for details. You can start a REPL by typing julia into a terminal, or by clicking on the Julia application in your computer. It will allow you to play around with arbitrary Julia code:\njulia&gt; a, b = 1, 2;\n\njulia&gt; a + b\n3\nThis is the standard (Julia) mode of the REPL, but there are three other modes you need to know. Each mode is entered by typing a specific character after the julia&gt; prompt. Once you’re in a non-Julia mode, you stay there for every command you run. To exit it, hit backspace after the prompt and you’ll get the julia&gt; prompt back.\n\n21.4.1 Help mode (?)\nBy pressing ? you can obtain information and metadata about Julia objects (functions, types, etc.) or unicode symbols. The query fetches the docstring of the object, which explains how to use it.\nhelp?&gt; println\nsearch: println print sprint pointer printstyled\n\n  println([io::IO], xs...)\n\n  Print (using print) xs to io followed by a newline. If io is not supplied, prints to the default output stream stdout.\n\n  See also printstyled to add colors etc.\n\n  Examples\n  ≡≡≡≡≡≡≡≡\n\n  julia&gt; println(\"Hello, world\")\n  Hello, world\n\n  julia&gt; io = IOBuffer();\n\n  julia&gt; println(io, \"Hello\", ',', \" world.\")\n\n  julia&gt; String(take!(io))\n  \"Hello, world.\\n\"\nIf you don’t know the exact name you are looking for, type a word surrounded by quotes to see in which docstrings it pops up.\n\n\n21.4.2 Package mode (])\nBy pressing ] you access Pkg.jl, Julia’s integrated package manager, whose documentation is an absolute must-read. Pkg.jl allows you to:\n\n]activate different local, shared or temporary environments;\n]instantiate them by downloading the necessary packages;\n]add, ]update (or ]up) and ]remove (or ]rm) packages;\nget the ]status (or ]st) of your current environment.\n\nAs an illustration, we download the package Example.jl inside a new environment we call demo (which will create an associated folder if it does not exist):\n(demo) pkg&gt; activate demo\n  Activating new project at `~/demo`\n\n(demo) pkg&gt; add Example\n   Resolving package versions...\n    Updating `~/demo/Project.toml`\n  [7876af07] + Example v0.5.5\n    Updating `~/demo/Manifest.toml`\n  [7876af07] + Example v0.5.5\n(demo) pkg&gt; status\nStatus `~/demo/Project.toml`\n  [7876af07] Example v0.5.5\nNote that the same keywords are also available in Julia mode:\njulia&gt; using Pkg\n\njulia&gt; Pkg.rm(\"Example\")\n    Updating `~/demo/Project.toml`\n  [7876af07] - Example v0.5.5\n    Updating `~/demo/Manifest.toml`\n  [7876af07] - Example v0.5.5\nThe package mode itself also has a help mode, accessed with ?, in case you’re lost among all these new keywords.\n\n\n21.4.3 Shell mode (;)\nBy pressing ; you enter a terminal, where you can execute any command you want, such as changing the working directory to the folder we just created:\nshell&gt; cd demo\n/Users/myself/demo",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Writing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-writing.html#editor",
    "href": "julia-writing.html#editor",
    "title": "21  Writing Julia Code",
    "section": "21.5 Editor",
    "text": "21.5 Editor\nIn theory, any text editor suffices to write and modify Julia code. In practice, an Integrated Development Environment (or IDE) makes the experience much more pleasant, thanks to code-related utilities and language-specific plugins.\nThe best IDE for Julia is Visual Studio Code, or VSCode, developed by Microsoft. The Julia VSCode extension is the most feature-rich of all Julia IDE plugins. You can download it from the VSCode Marketplace and read its documentation.\n\n\n\n\n\n\nTip\n\n\n\nIn what follows, we will sometimes mention commands and keyboard shortcuts provided by this extension. But the only shortcut you need to remember is Ctrl + Shift + P (or Cmd + Shift + P on Mac): this opens the VSCode command palette, in which you can search for any command. Type julia in the command palette to see what you can do.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Writing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-writing.html#running-code",
    "href": "julia-writing.html#running-code",
    "title": "21  Writing Julia Code",
    "section": "21.6 Running code",
    "text": "21.6 Running code\nYou can execute a Julia script from your terminal, but in most cases that is not what you want to do.\njulia myfile.jl  # avoid this\nJulia has a rather high startup and compilation latency. If you only use scripts, you will pay this cost every time you run a slightly modified version of your code. That is why many Julia developers fire up a REPL at the beginning of the day and run all of their code there, chunk by chunk, in an interactive way. Full files can be run interactively from the REPL with the include function.\njulia&gt; include(\"myfile.jl\")\nAlternatively, includet from the Revise.jl package can be used to “include and track” a file. This will automatically update changes to function definitions in the file in the running REPL session.\n\n\n\n\n\n\nTip\n\n\n\nRunning code is made much easier by the following commands:\n\nJulia: Restart REPL (shortcut Alt + J then Alt + R) - this will open or restart the integrated Julia REPL. It is different from opening a plain VSCode terminal and launching Julia manually from there.\nJulia: Execute Code in REPL and Move (shortcut Shift + Enter) - this will execute the selected code in the integrated Julia REPL, like a notebook.\n\n\n\nWhen keeping the same REPL open for a long time, it’s common to end up with a “polluted” workspace where the definitions of certain variables or functions have been overwritten in unexpected ways. This, along with other events like struct redefinitions, might force you to restart your REPL now and again, and that’s okay.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Writing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-writing.html#notebooks",
    "href": "julia-writing.html#notebooks",
    "title": "21  Writing Julia Code",
    "section": "21.7 Notebooks",
    "text": "21.7 Notebooks\nNotebooks are a popular alternative to IDEs when it comes to short and self-contained code, typically in data science. They are also a good fit for literate programming, where lines of code are interspersed with comments and explanations.\nThe most well-known notebook ecosystem is Jupyter, which supports Julia, Python and R as its three core languages. To use it with Julia, you will need to install the IJulia.jl backend. Then, if you have also installed Jupyter with pip install jupyterlab, you can run this command to launch the server:\njupyter lab\nIf you only have IJulia.jl on your system, you can run this snippet instead:\njulia&gt; using IJulia\n\njulia&gt; IJulia.notebook()\n\n1\n\nLaunches classic Jupyter Notebook; for JupyterLab use IJulia.jupyterlab() if JupyterLab is installed\n\n\n\n\n\n\n\n\nTip\n\n\n\nJupyter notebooks can be opened, edited, and run in VS Code via the Jupyter and Julia extensions, without installing Python’s Jupyter or IJulia.jl system-wide.\n\n\nPluto.jl is a newer, pure-Julia tool, adding reactivity and interactivity. It is also more amenable to version control than Jupyter notebooks because notebooks are saved as plain Julia scripts. Pluto is unique to Julia because of the language’s ability to introspect and analyze dependencies in its own code. Pluto also has built-in package/environment management, meaning that Pluto notebooks contains all the code needed to reproduce results (as long as Julia and Pluto are installed).\nTo try out Pluto, install the package and then run\njulia&gt; using Pluto\n\njulia&gt; Pluto.run()",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Writing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-writing.html#markdown",
    "href": "julia-writing.html#markdown",
    "title": "21  Writing Julia Code",
    "section": "21.8 Markdown",
    "text": "21.8 Markdown\nMarkdown is a markup language used to add formatting elements to plain text content, such as Julia docstrings. Additionally, other tools such as Quarto (described below) are built using Markdown notation as the basis for their formatting, so it’s useful to know about Markdown and the most essential\n\n21.8.1 Plain Text Markdown\nPlain text markdown files, which have the .md extension, are not used for interactive programming, meaning one cannot run code written in the file. As a result, plain text markdown files are usually rendered into a final product by other software.\nThis is an example of a plain text markdown file, including a code example contained within the `````````` block:\n# Title\n\n## Section Header\n\nThis is example text.\n\n```julia\nprintln(\"hello world\")\n```\n\n\n21.8.2 Quarto\nQuarto “is an open-source scientific and technical publishing system.” Quarto’s primary authoring format is the .qmd (Quarto Markdown) file. Quarto can also render plain .md files, but .qmd enables executable code cells and richer metadata.\nQuarto markdown files like plain text markdown files also integrate with editors, such as VSCode.\n\n\n\n\n\n\nTip\n\n\n\nInstall the Quarto extension for a streamlined experience.\n\n\nUnlike plain text markdown files, Quarto markdown files have executable code chunks. These code chunks provide a functionality similar to notebooks, thus Quarto markdown files are an alternative to notebooks. Additionally, Quarto markdown files give users additional control over output and styling via the YAML header at the top of the .qmd file.\nAs of Quarto version 1.5, users can choose from two Julia engines to execute code - a native Julia engine and IJulia.jl. The primary difference between the native Julia engine and IJulia.jl is that the native Julia engine does not depend on Python and can utilize local environments. For this reason it’s recommended to start with the native Julia engine. Learn more about the native Julia engine in Quarto’s documentation.\nThis book is built using Quarto documents to create the associated typeset book and website.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Writing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-writing.html#sec-environments",
    "href": "julia-writing.html#sec-environments",
    "title": "21  Writing Julia Code",
    "section": "21.9 Environments and Dependencies",
    "text": "21.9 Environments and Dependencies\nJulia comes bundled with Pkg.jl, an environment and package manager. It enables installation of packages from registries, pinning versions for compatibility, and analyzing your dependencies. Environment is meant to mean, in general, the computer you use and software installed in it. When we speak about environments in the Julia context, this means the Julia version and packages available to the current Julia code. For example, from the current code is a given package installed and usable?\nIf you open a Julia REPL, by default you will be in the global environment. If you hit ] to enter Pkg mode, you should see:\n(@v1.10) pkg&gt;\nThe (@1.10) indicates that you are using the global environment for the current Julia version (there is no global environment which applies across all Julia versions installed). You can activate a new environment with activate [environment name].\n(@v1.10) pkg&gt; activate MyNewEnv\n  Activating new project at `~/MyNewEnv`\nThis will not do anything, yet! When we add a package to this environment, then it will create a Project.toml and Manifest.toml file in that directory. Now that directory is a full fledged Julia project!\n\n\n\n\n\n\nTip\n\n\n\nActivate a temporary environment with activate --temp. This will give you a temporary environment with a random name, which is very useful for testing out things in a clean, simplified environment. Note that environment stacking still applies, so the global environment, like @1.10 will be available inside your temp environment.\n\n\n\n21.9.1 Project.toml\nA Project.toml file defines attributes about the current project and its dependencies. Julia uses this to understand how to reference your current project and what dependencies it should look for from registries when instantiating the project.\n\n\n\n\n\n\nNote\n\n\n\nTOML (Tom’s Obvious Markup Language) is a modern configuration file format used to store settings and data in a human-readable, plaintext format.\n\n\nThis is a bit abstract, so here is a quick, annotated tour of an example Project.toml file:\n1name = \"FinanceCore\"\n2uuid = \"b9b1ffdd-6612-4b69-8227-7663be06e089\"\nauthors = [\"alecloudenback &lt;alecloudenback@users.noreply.github.com&gt; and contributors\"] \n3version = \"2.1.0\"\n\n4[deps]\nDates = \"ade2ca70-3891-5945-98fb-dc099432e06a\"\nLoopVectorization = \"bdcacae8-1622-11e9-2a5c-532679323890\"\nRoots = \"f2b01f46-fcfa-551c-844a-d8ac1e96c665\"\n\n5[compat]\nDates = \"1\"\nLoopVectorization = \"^0.12\"\nRoots = \"^1.0, 2\"\njulia = \"1.6\"\n\n1\n\nThe name is the name of your current project which only matters if you turn your project into a package.\n\n2\n\nA UUID is a unique identifier and can be created with Julia’s UUIDs standard library.\n\n3\n\nThe version follows Semantic Versioning (“SemVer”) to convey to Pkg (and users!) information that ties a specific version to a specific code commit1.\n\n4\n\nThe deps section records the name of direct dependencies and their UUIDs so that Julia can know which packages to grab in order to make your project run.\n\n5\n\nThe compat section defines compatibility with packages can be enforced (via SemVer) to clarify which versions are allowed to be installed in case incompatibilities arise.\n\n\nWhen you instantiate a project (see Section 21.9 for more), Julia will essentially add the packages listed under deps, and will resolve the compatible versions, generally picking the highest version number for the packages so long as the compat section rule are not broken.\nWhen adding the dependencies, those packages themselves likely specify their own set of dependencies and Julia must resolve the entire dependency graph or dependency tree to allow your current project to work.\n\n\n\n\n\n\nNoteSemantic Versioning\n\n\n\nSemantic Versioning (“SemVer”) is a scheme which uses the three-component version code to convey meaning about different versions of a package to both users and computer systems. With the version scheme vMAJOR.MINOR.PATCH, the meaning is roughly as follows:\n\nMAJOR increments denote changes to the code which make it incompatible with prior versions.\nMINOR increments denote changes which add features that are compatible with the prior versions.\nPATCH increments denote changes which fix issues in prior versions and code written against the prior version is still compatible.\n\nAs an example, say we are currently using v2.10.4 of a package, and the following theoretical options are available for us to upgrade to:\n\nv2.10.5 - The 4 to 5 indicates that something may have been broken in the prior release and so we should upgrade without fear that we need to make changes to our code (unless we relied on the previously broken code!).\nv2.11.0 - The 10 to 11 bump suggests that the new release contains some features which should not require us to change any of our previously written code.\nv3.0.0 - The 2 to 3 indicates that we will potentially have to modify code that we have written that interfaces with this dependency.\n\nSemVer cannot distill all possible compatibility and upgrade information about a set of packages (e.g. an author may release an update with a MINOR version which also includes fixes).\n\n\n\n\n21.9.2 Manifest.toml\nThe Manifest.toml file includes a record of all external dependencies used by the project at hand. Unlike Project.toml, this file gets machine generated when Julia instantiates or updates the environment. The contents are basically a long list of your direct dependencies and the dependencies of those direct dependencies and looks something like this:\njulia_version = \"1.10.0\"\nmanifest_format = \"2.0\"\nproject_hash = \"5fea00df4808d89f9c977d15b8ee992bd408081b\"\n\n[[deps.AbstractFFTs]]\ndeps = [\"LinearAlgebra\"]\ngit-tree-sha1 = \"d92ad398961a3ed262d8bf04a1a2b8340f915fef\"\nuuid = \"621f4979-c628-5d54-868e-fcf4e3e8185c\"\nversion = \"1.5.0\"\nweakdeps = [\"ChainRulesCore\", \"Test\"]\n\n    [deps.AbstractFFTs.extensions]\n    AbstractFFTsChainRulesCoreExt = \"ChainRulesCore\"\n    AbstractFFTsTestExt = \"Test\"\n\n... many more lines\n\n\n\n\n\n\nNote\n\n\n\nStarting with Julia 1.11, Manifest files will include a version indication, making it nicer to work with multiple Julia versions at one time on a single system.\n\n\n\n\n21.9.3 Reproducibility\nReproducibility fulfills both practical and principled goals. Practical in that we can record the complex chain of dependencies that is used in modern computing in order to potentially re-create a result or demonstrate an audit trail of the tools used. Principled in that there are circumstances (like science research) in which we want to be able to replicate results. The combination of Project.toml and Manifest.toml go a long way towards accomplishing this, as you can share both and with the same hardware and Julia version should be able to get the exact same set of dependencies and therefore run the same code. In practice, this level of reproducibility isn’t usually needed, as most time a set of code can be run accurately without requiring the exact same set of dependencies.\nSince dependencies can have variation between systems (Windows/Mac) and architectures (x86 vs x64), you may not be able to recreate the Manifest exactly. Nevertheless, it’s a fairly low bar if you are trying to maintain the utmost level of rigor around the toolchain and Julia is one of the most robust languages regarding tools to support open replication of results.\n\n\n\n\n\n\nTipArtifacts\n\n\n\nJulia has a system called artifacts which allows specification of a location and hash (a cryptographic key) for data and binaries. The artifact system is used to download and verify the contents of a file match the hash. This is designed for more permanent data and less end-user workflows, but we call it out here as another example where Julia takes steps to promote consistency and reproducibility.\nFor more on data workflows for the end-user, see Chapter 12.\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can configure the environment in which a VSCode Julia REPL opens. Just click the Julia env: ... button at the bottom. By default the Julia extension uses juliaup’s default channel, but you can override the executable path in VS Code settings.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Writing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-writing.html#creating-local-packages",
    "href": "julia-writing.html#creating-local-packages",
    "title": "21  Writing Julia Code",
    "section": "21.10 Creating Local packages",
    "text": "21.10 Creating Local packages\nOnce your code base grows beyond a few scripts, you will want to create a package of your own. The first advantage is that you don’t need to specify the path of every file: using MyPackage is enough to get access to the names you define and export (or using MyPackage: myfunc1, myfunc2 to use bring non-exported functions into your environment). Furthermore, by structuring your project as a Pacakge, you can specify versions for your package and its dependencies, making your code easier and safer to reuse.\nTo create a new package locally, the easy way is to use ]generate .\nusing Pkg\nPkg.generate(\"MyPackage\"); \nThis command initializes a simple folder with a Project.toml and a src subfolder. As we have seen, the Project.toml specifies the dependencies. Meanwhile, the src subfolder contains a file MyPackage.jl, where a module called MyPackage is defined. It is the heart of your package, and will typically look like this when you’re done:\nmodule MyPackage\n\n# imported dependencies\nusing OtherPackage1\nusing OtherPackage2\n\n# files defining functions, types, etc.\ninclude(\"file1.jl\")\ninclude(\"subfolder/file2.jl\")\n\n# names you want to make public\nexport myfunc # e.g. defined in `file1.jl`\nexport MyType\n\nend\n\n21.10.1 PkgTemplates.jl\nPkgTemplates.jl is like ]generate from Pkg.jl but provides a number of options to pre-configure the repository for things such as continuous integration, testing, and compatibility. If you are not yet making use of that more advanced functionality, the ]generate method will work just fine for you.\nThis will walk you through an interactive prompt to create a package in the desired folder. ~/.julia/dev is a suggested location, but technically any folder will make do:\nusing PkgTemplates\ncd(\"~/.julia/dev\")\nTemplate(interactive=true)(\"MyPkg\")",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Writing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-writing.html#development-workflow",
    "href": "julia-writing.html#development-workflow",
    "title": "21  Writing Julia Code",
    "section": "21.11 Development workflow",
    "text": "21.11 Development workflow\nOnce you have created a package, your development routine might look like this:\n\nOpen a REPL in which you import MyPackage\nRun some functions interactively, either by writing them directly in the REPL or from a Julia file that you use as a notebook\nModify some files in MyPackage\nGo back to step 2\n\nFor that to work well, you need code modifications to be taken into account automatically. That is why Revise.jl exists. If you start every REPL session by explicitly Revise.jl (using Revise), then all the other packages you import after that will have their code tracked. Whenever you edit a source file and hit save, the REPL will update its state accordingly. To automatically do this for every session, see Section 21.12.\n\n\n\n\n\n\nNote\n\n\n\nThe Julia extension imports Revise.jl by default when it starts a REPL.\n\n\nThis is how you get started using your own package once it’s set up:\nusing Revise, Pkg\nPkg.activate(\"./MyPackage\")\nusing MyPackage\nmyfunc() # defined and exported in MyPackage\nMyPackage.myfunc2() # defined and *not* exported in MyPackage\n\n\n\n\n\n\nNote\n\n\n\nIf you are working on a set of interrelated packages, you may need to tell those packages to use the development version of the package which you are modifying, instead of using the latest available from a registry. For example, say you are working on revisions to PkgA in the following dependency tree:\nPkgB -- depends on -- &gt; PkgA\nIf you are modifying PkgA, then you might need to tell PkgB to use the development version. For this, then you would need to:\n\nCreate an outer environment where you want to run the packages for interactive use while developing (say activate @mydevenv).\n]dev PkgB which will download the associated repository into ~./julia/dev/PkgB\nGo into the environment ~/.julia/dev/PkgB and tell that environment to use the development version of PkgA with ]dev PkgB (assuming you are modifying PkgA also in the ~/.julia/dev/ folder)\n\nNow, in the @mydevenv environemnt, when you load PkgB it will load the version of PkgA",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Writing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-writing.html#sec-startup-config",
    "href": "julia-writing.html#sec-startup-config",
    "title": "21  Writing Julia Code",
    "section": "21.12 Configuration",
    "text": "21.12 Configuration\nJulia accepts startup flags to handle settings such as the number of threads available or the environment in which it launches. In addition, most Julia developers also have a startup file which is run automatically every time the language is started. It is located at ~/.julia/config/startup.jl.\nThe basic component that everyone puts in the startup file is Revise.jl. Users also commonly import packages that affect the REPL experience, as well as esthetic, benchmarking or profiling utilities. A typical example is OhMyREPL.jl which is widely used for syntax highlighting in the REPL. While other packages are often used, we suggest the following as a minimum:\n# save as a file in ~/.julia/config/startup.jl\ntry\n    using Revise\n    using OhMyREPL\ncatch e\n    @warn \"Error with startup packages\"\nend\nMore generally, the startup file allows you to define your own favorite helper functions and have them immediately available in every Julia session. StartupCustomizer.jl can help you set up your startup file.\n\n\n\n\n\n\nTip\n\n\n\nHere are a few more startup packages that can make your life easier once you know the language better:\n\nAbbreviatedStackTraces.jl allows you to shorten error stacktraces, which can sometimes get pretty long (beware of its interactions with VSCode)\nTerm.jl offers a completely new way to display things like types and errors (see the advanced configuration to enable it by default).",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Writing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-writing.html#interactivity",
    "href": "julia-writing.html#interactivity",
    "title": "21  Writing Julia Code",
    "section": "21.13 Interactivity",
    "text": "21.13 Interactivity\nThe Julia REPL comes bundled with InteractiveUtils.jl, a bunch of very useful functions for interacting with source code.\nHere are a few examples:\n\nusing InteractiveUtils # not necessary in a REPL session\nsupertypes(Int64)\n\n(Int64, Signed, Integer, Real, Number, Any)\n\n\n\nsubtypes(Integer)\n\n3-element Vector{Any}:\n Bool\n Signed\n Unsigned\n\n\n\n# first five methods that take an integer argument\nmethodswith(Integer)[1:5]\n\n5-element Vector{Method}: Array(s::LinearAlgebra.UniformScaling, m::Integer, n::Integer) in LinearAlgebra at /Users/alecloudenback/.julia/juliaup/julia-1.11.6+0.aarch64.apple.darwin14/share/julia/stdlib/v1.11/LinearAlgebra/src/uniformscaling.jl:420 Float16(x::Integer) in Base at float.jl:234 GenericMemoryRef(mem::GenericMemoryRef, i::Integer) in Core at boot.jl:527 GenericMemoryRef(mem::GenericMemory, i::Integer) in Core at boot.jl:526 Integer(x::Integer) in Core at boot.jl:925\n\n\n\n@which exp(1) # where the currently used function is defined\n\nexp(x::Real) in Base.Math at math.jl:1528\n\n\n\napropos(\"matrix exponential\") # search docstrings\n\nBase.exp\nBase.:^\n\n\nWhen you ask for help on a Julia forum, you might want to include your local Julia information:\n\nversioninfo()\n\nJulia Version 1.11.6\nCommit 9615af0f269 (2025-07-09 12:58 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: macOS (arm64-apple-darwin24.0.0)\n  CPU: 14 × Apple M4 Max\n  WORD_SIZE: 64\n  LLVM: libLLVM-16.0.6 (ORCJIT, apple-m1)\nThreads: 10 default, 0 interactive, 5 GC (on 10 virtual cores)\nEnvironment:\n  JULIA_NUM_THREADS = auto\n  JULIA_PROJECT = @.\n  JULIA_LOAD_PATH = @:@stdlib\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe following packages can give you even more interactive power:\n\nInteractiveCodeSearch.jl to look for a precise implementation of a function.\nInteractiveErrors.jl to navigate through stacktraces.\nCodeTracking.jl to extend InteractiveUtils.jl",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Writing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-writing.html#sec-julia-testing",
    "href": "julia-writing.html#sec-julia-testing",
    "title": "21  Writing Julia Code",
    "section": "21.14 Testing",
    "text": "21.14 Testing\nTesting in Julia primarily revolves around the built-in Test package, which provides a straightforward way to write and run tests using the @test macro. The basic syntax is simple - you write @test expression where the expression should evaluate to true for the test to pass.\nTo run tests in Julia, navigate to the package directory and run Pkg.test(), or use the ] test YourPackageName command in the Julia REPL. This will run the file in the package directory contained in test/runtests.jl. The test infrastructure automatically handles setting up the correct environment and dependencies for testing. Test coverage reports can be generated to see which lines of code are exercised by your tests.\nruntests.jl is just a normal Julia file and can include other files to help organize your tests. This structure integrates naturally with Julia’s package manager and testing tools.\nTest organization is handled through @testset blocks, which group related tests together and provide summary statistics when tests are run. For example, extending the introduction to testing from Section 12.3:\n\nusing Test\nfunction present_value(discount_rate, cashflows)\n    v = 1.0\n    pv = 0.0\n    for cf in cashflows\n        v = v / (1 + discount_rate)\n        pv = pv + v * cf\n    end\n    return pv\nend\n\n@testset \"Scalar Discount\" begin\n    @test present_value(0.05, 10) ≈ 10 / 1.05\n    @test present_value(0.05, 20) ≈ 20 / 1.05\nend\n@testset \"Vector Discount\" begin\n    @test present_value(0.05, [10]) ≈ 10 / 1.05\n    @test present_value(0.05, [10, 20]) ≈ 10 / 1.05 + 20 / 1.05^2\nend;\n\n\nTest Summary:   | Pass  Total  Time\nScalar Discount |    2      2  0.1s\nTest Summary:   | Pass  Total  Time\nVector Discount |    2      2  0.0s\n\n\n\n\nThere are many more related testing facilities described in the Julia Docs, such as combining for loops with test sets.\n\n\n\n\n\n\nTip\n\n\n\nFor floating-point comparisons, you’ll often want to use isapprox (as a shorter symol: ≈, typed as ) instead of == to handle small numerical differences. Here’s some examples:\n1@test 1/3  ≈ 0.33333333333333\n2@test 1/3  ≈ 0.333 atol = 1e-3\n3@test 1/3  ≈ 0.333 rtol = 1e-3\n\n1\n\nPasses because the values are sufficiently close.\n\n2\n\nPasses because the absolute difference between the values is less than \\(1/1000\\).\n\n3\n\nPasses because the difference between values is less than \\(1/1000\\) times the larger of the two values.\n\n\nHere’s the default behavior for isapprox, excerpted from its docstring:\n\nFor real or complex floating-point values, if an atol &gt; 0 is not specified, rtol defaults to the square root of eps of the type of x or y, whichever is bigger (least precise). This corresponds to requiring equality of about half of the significant digits. Otherwise, e.g. for integer arguments or if an atol &gt; 0 is supplied, rtol defaults to zero.\n\n\n\nThe testing workflow in Julia supports both test-driven development and continuous integration seamlessly. Tests can be run locally during development, and services like GitHub Actions can automatically run your test suite on multiple Julia versions and operating systems when you push changes.\nGood testing practices in Julia involve testing edge cases, using appropriate numerical tolerances, organizing tests logically, and ensuring adequate coverage of your code’s functionality. It’s also important to write tests that are clear and maintainable - each test should have a specific purpose and test one thing well.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Writing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-writing.html#footnotes",
    "href": "julia-writing.html#footnotes",
    "title": "21  Writing Julia Code",
    "section": "",
    "text": "When registering a package to a repository, the repository will record the version indicated in the Project.toml file to the git commit id of the package when it is registered.↩︎",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Writing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-debugging.html",
    "href": "julia-debugging.html",
    "title": "22  Troubleshooting Julia Code",
    "section": "",
    "text": "22.1 Chapter Overview\nDebugging in Julia involves a mix of strategies, including using print statements, the Debugger package for step-by-step inspection, logging with the Logging module, and interactive debugging with Infiltrator. These tools and techniques can help you identify and fix issues in your code efficiently.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Troubleshooting Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-debugging.html#error-messages-and-stack-traces",
    "href": "julia-debugging.html#error-messages-and-stack-traces",
    "title": "22  Troubleshooting Julia Code",
    "section": "22.2 Error Messages and Stack Traces",
    "text": "22.2 Error Messages and Stack Traces\nJulia’s error messages and stack traces can be quite informative. When an error occurs, Julia provides a traceback that shows the function call stack leading to the error, which helps in identifying where things went wrong.\n#| error: true\nfunction mysqrt(x)\n    return sqrt(x)\nend\n\nmysqrt(-1)  # This will raise a `DomainError`\nThe stacktrace will show us the sequence of function calls that led to the error. The print out will show the list of functions that were called (the callstack) which led to the code that errored. Additionally, help text is often printed, potentially offering some advice for resolving the issue. When you encounter errors in an interactive session, you can click on different parts of the stacktrace and be taken to the associated code in your editor.\n\n22.2.1 Error Types\nNotice that errors are given specific types and not just result in a generic Error. This aids in understanding for the user: if a DomainError then you know that you passed the right type (e.g. a Float64 to a function that takes a number), just that the value was not acceptable (as in the example above). Contrast that with a MethodError which will tell you that you’ve passed an invalid kind of thing to the function, not just that its value was off:\n#| error: true\nmysqrt(\"a string isn't OK\")",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Troubleshooting Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-debugging.html#logging",
    "href": "julia-debugging.html#logging",
    "title": "22  Troubleshooting Julia Code",
    "section": "22.3 Logging",
    "text": "22.3 Logging\nWhen you encounter a problem in your code or want to track progress, a common reflex is to add print statements everywhere.\n\nfunction printing_func(n)\n    for i in 1:n\n        println(i^2)\n    end\nend\n\nprinting_func (generic function with 1 method)\n\n\n\nprinting_func(3)\n\n1\n4\n9\n\n\nA slight improvement is given by the @show macro, which displays the variable name:\n\nfunction showing_func(n)\n    for i in 1:n\n        @show i^2\n    end\nend\n\nshowing_func (generic function with 1 method)\n\n\n\nshowing_func(3)\n\ni ^ 2 = 1\ni ^ 2 = 4\ni ^ 2 = 9\n\n\nBut you can go even further with the macros @debug, @info, @warn and @error. They have several advantages over printing:\n\nThey display variable names and a custom message\nThey show the line number they were called from\nThey can be disabled and filtered according to source module and severity level\nThey work well in multithreaded code\nThey can write their output to a file\n\n\nfunction warning_func(n)\n    for i in 1:n\n        @warn \"This is bad\" i^2\n    end\nend\n\nwarning_func (generic function with 1 method)\n\n\n\nwarning_func(3)\n\n\n┌ Warning: This is bad\n│   i ^ 2 = 1\n└ @ Main.Notebook ~/prog/julia-fin-book/julia-debugging.qmd:81\n┌ Warning: This is bad\n│   i ^ 2 = 4\n└ @ Main.Notebook ~/prog/julia-fin-book/julia-debugging.qmd:81\n┌ Warning: This is bad\n│   i ^ 2 = 9\n└ @ Main.Notebook ~/prog/julia-fin-book/julia-debugging.qmd:81\n\n\n\n\nRefer to the logging documentation for more information.\n\n\n\n\n\n\nNote\n\n\n\nIn particular, note that @debug messages are suppressed by default. You can enable them through the JULIA_DEBUG environment variable if you specify the source module name, typically Main or your package module.\n\n\nBeyond the built-in logging utilities, ProgressLogging.jl has a macro @progress, which interfaces nicely with VSCode and Pluto to display progress bars. And Suppressor.jl can sometimes be handy when you need to suppress warnings or other bothersome messages (use at your own risk).",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Troubleshooting Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-debugging.html#commonly-encountered-macros",
    "href": "julia-debugging.html#commonly-encountered-macros",
    "title": "22  Troubleshooting Julia Code",
    "section": "22.4 Commonly Encountered Macros",
    "text": "22.4 Commonly Encountered Macros\nAside from those mentioned in the context of Logging, there are a number of different useful macros, many of which are highlighted in the following table:\n\nUseful macros for modeling work. There are others related to parallelism which will be covered in Chapter 11.\n\n\n\n\n\n\nMacro\nDescription\n\n\n\n\nBenchmarkTools.@benchmark\nRuns the given expression multiple times, collecting timing and memory allocation statistics. Useful for benchmarking and performance analysis.\n\n\nBenchmarkTools.@btime\nSimilar to @benchmark, but focuses on the minimum execution time and provides a more concise output.\n\n\n@edit\nOpens the source code of a function or module in an editor for inspection or modification.\n\n\n@which\nDisplays the method that would be called for a given function call, helping to understand method dispatch.\n\n\n@code_warntype\nShows the type inference results for a given function call, highlighting any type instabilities or performance issues.\n\n\n@debug, @info, @warn, @error\nUsed for logging messages at different severity levels (info, warning, error) during program execution.\n\n\n@assert\nAsserts that a given condition is true, throwing an error if the condition is false. Useful for runtime checks and debugging.\n\n\n@view, @views\nAccess a subset of an array without copying the data in that slice. @views applies to all array slicing operations within the expressions that follow it.\n\n\nTest.@test, Test.@testset\nUsed for defining unit tests. @test checks that a condition is true, while @testset groups related tests together.\n\n\n@raw\nEncloses a string literal, disabling string interpolation and escape sequences. Useful for writing raw string data. This is especially helpful when working with filepaths where the \\ in Windows paths otherwise needs to be escaped with a leading slash (e.g. \\\\ ).\n\n\n@fastmath\nEnables aggressive floating-point optimizations within a block, potentially sacrificing strict IEEE compliance for performance. Avoid in numerically sensitive code unless you understand the implications for IEEE semantics.\n\n\n@inbounds\nDisables bounds checking for array accesses within a block, improving performance but removing safety checks.\n\n\n@inline\nSuggests to the compiler that a function should be inlined at its call sites, potentially improving performance by reducing function call overhead.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Troubleshooting Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-debugging.html#debugging",
    "href": "julia-debugging.html#debugging",
    "title": "22  Troubleshooting Julia Code",
    "section": "22.5 Debugging",
    "text": "22.5 Debugging\nThe limitation of printing or logging is that you cannot interact with local variables or save them for further analysis. The following two packages solve this issue (consider adding to your default environment @v1.X, like Revise.jl).\n\n22.5.1 Setting\nAssume you want to debug a function checking whether the \\(n\\)-th Fermat number \\(F_n = 2^{2^n} + 1\\) is prime:\n\nfunction fermat_prime(n)\n    k = 2^n\n    F = 2^k + 1\n    for d in 2:isqrt(F)  # integer square root\n        if F % d == 0\n            return false\n        end\n    end\n    return true\nend\n\nfermat_prime (generic function with 1 method)\n\n\n\nfermat_prime(4), fermat_prime(6)\n\n(true, true)\n\n\nUnfortunately, \\(F_4 = 65537\\) is the largest known Fermat prime, which means \\(F_6\\) is incorrectly classified. Let’s investigate why this happens!\n\n\n22.5.2 Infiltrator.jl\nInfiltrator.jl is a lightweight inspection package, which will not slow down your code at all. Its @infiltrate macro allows you to directly set breakpoints in your code. Calling a function which hits a breakpoint will activate the Infiltrator REPL-mode and change the prompt to infil&gt;. Typing ? in this mode will summarize available commands. For example, typing @locals in Infiltrator-mode will print local variables:\nusing Infiltrator\n\nfunction fermat_prime_infil(n)\n    k = 2^n\n    F = 2^k + 1\n    @infiltrate\n    for d in 2:isqrt(F)\n        if F % d == 0\n            return false\n        end\n    end\n    return true\nend\nWhat makes Infiltrator.jl even more powerful is the @exfiltrate macro, which allows you to move local variables into a global storage called the safehouse.\njulia&gt; fermat_prime_infil(6)\nInfiltrating fermat_prime_infil(n::Int64)\n  at REPL[2]:4\n\ninfil&gt; @exfiltrate k F\nExfiltrating 2 local variables into the safehouse.\n\ninfil&gt; @continue\n\ntrue\n\njulia&gt; safehouse.k\n64\n\njulia&gt; safehouse.F\n1\nThe diagnosis is a classic one: integer overflow. Indeed, \\(2^{64}\\) is larger than the maximum integer value in Julia:\ntypemax(Int)\n2^63-1\nAnd the solution is to call our function on “big” integers with an arbitrary number of bits:\nfermat_prime(big(6))\n\n\n22.5.3 Debugger.jl\nDebugger.jl allows us to interrupt code execution anywhere we want, even in functions we did not write. Using its @enter macro, we can enter a function call and walk through the call stack, at the cost of reduced performance.\nThe REPL prompt changes to 1|debug&gt;, allowing you to use custom navigation commands to step into and out of function calls, show local variables and set breakpoints. Typing a backtick ` will change the prompt to 1|julia&gt;, indicating evaluation mode. Any expression typed in this mode will be evaluated in the local context. This is useful to show local variables, as demonstrated in the following example:\njulia&gt; using Debugger\n\njulia&gt; @enter fermat_prime(6)\nIn fermat_prime(n) at REPL[7]:1\n 1  function fermat_prime(n)\n&gt;2      k = 2^n\n 3      F = 2^k + 1\n 4      for d in 2:isqrt(F)  # integer square root\n 5          if F % d == 0\n 6              return false\n\nAbout to run: (^)(2, 6)\n1|debug&gt; n\nIn fermat_prime(n) at REPL[7]:1\n 1  function fermat_prime(n)\n 2      k = 2^n\n&gt;3      F = 2^k + 1\n 4      for d in 2:isqrt(F)  # integer square root\n 5          if F % d == 0\n 6              return false\n 7          end\n\nAbout to run: (^)(2, 64)\n1|julia&gt; k\n64\n\n\n\n\n\n\nTip\n\n\n\nVSCode offers a nice graphical interface for debugging. Click left of a line number in an editor pane to add a breakpoint, which is represented by a red circle. In the debugging pane of the Julia extension, click Run and Debug to start the debugger. The program will automatically halt when it hits a breakpoint. Using the toolbar at the top of the editor, you can then continue, step over, step into and step out of your code. The debugger will open a pane showing information about the code such as local variables inside of the current function, their current values and the full call stack.\nThe debugger can be sped up by selectively compiling modules that you will not need to step into via the + symbol at the bottom of the debugging pane. It is often easiest to start by adding ALL_MODULES_EXCEPT_MAIN to the compiled list, and then selectively remove the modules you need to have interpreted.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Troubleshooting Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-sharing.html",
    "href": "julia-sharing.html",
    "title": "23  Distributing and Sharing Julia Code",
    "section": "",
    "text": "23.1 Chapter Overview\nApplying software engineering best practices (Chapter 12) in Julia, including testing, documentation, and coverage metrics. Collaborating on code. Publishing packages for others to use.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Distributing and Sharing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-sharing.html#setup",
    "href": "julia-sharing.html#setup",
    "title": "23  Distributing and Sharing Julia Code",
    "section": "23.2 Setup",
    "text": "23.2 Setup\nA vast majority of Julia packages are hosted on GitHub (although less common, other options like GitLab are also possible). GitHub is a platform for collaborative software development, based on the version control system Git (see Chapter 12 for an introduction).\nThe first step is therefore creating an empty GitHub repository on GitHub (don’t add a README License, etc. at this step).\n\n\n\n\n\n\nTip\n\n\n\nYou should try to follow package naming guidelines and add a “.jl” extension at the end, like so: “MyAwesomePackage.jl”.\n\n\nLocally, use PkgTemplates.jl (see Section 21.10.1) to then create the package’s folder locally on your computer, which will create a package with several subfolders (these will be described as the chapter progresses).\nTo sync this up with the newly created GitHub repository, you git push this new folder to the remote repository https://github.com/myuser/MyAwesomePackage.jl. GitHub should show you how to do this on the associated repository page (something like this):\n# terminal commands from inside your new package directory:\ngit init\ngit add .\ngit commit -m \"Initial commit\"\ngit branch -M main \ngit remote add origin https://github.com/myuser/MyAwesomePackage.jl.git\ngit push -u origin main",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Distributing and Sharing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-sharing.html#github-actions",
    "href": "julia-sharing.html#github-actions",
    "title": "23  Distributing and Sharing Julia Code",
    "section": "23.3 GitHub Actions",
    "text": "23.3 GitHub Actions\nThe most useful aspect of PkgTemplates.jl is that it automatically generates workflows for GitHub Actions. These are stored as YAML files in .github/workflows, with a slightly convoluted syntax that you don’t need to fully understand. For instance, the file CI.yml contains instructions that execute the tests of your package (see below) for each pull request, tag or push to the main branch. This is done on a GitHub server and should theoretically cost you money, but if your GitHub repository is public, you get an unlimited workflow budget for free.\nA variety of workflows and functionalities are available through optional plugins. The interactive setting Template(..., interactive=true) allows you to select the ones you want for a given package. Otherwise, you will get the default selection, which you are encouraged to look at.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Distributing and Sharing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-sharing.html#testing",
    "href": "julia-sharing.html#testing",
    "title": "23  Distributing and Sharing Julia Code",
    "section": "23.4 Testing",
    "text": "23.4 Testing\nThe purpose of the test subfolder in your package is unit testing: automatically checking that your code behaves the way you want it to. For instance, if you write your own square root function, you may want to test that it gives the correct results for positive numbers, and errors for negative numbers.\nusing Test\n\n@test sqrt(4) ≈ 2\n\n@testset \"Invalid inputs\" begin\n    @test_throws DomainError sqrt(-1)\n    @test_throws MethodError sqrt(\"abc\")\nend;\nSuch tests belong in test/runtests.jl, and they are executed with the ]test command (in the REPL’s Pkg mode). Unit testing may seem rather naive, or even superfluous, but as your code grows more complex, it becomes easier to break something without noticing. Testing each part separately will increase the reliability of the software you write.\n\n\n\n\n\n\nTip\n\n\n\nTo test the arguments provided to the functions within your code (for instance their sign or value), avoid @assert (which can be deactivated) and use ArgCheck.jl instead.\nThat is, avoid this:\nfunction mysqrt(x)\n    @assert x &gt;= 0\n    ...\nAnd do this instead:\nusing ArgCheck\nfunction mysqrt(x)\n    @argcheck x &gt;= 0 DomainError\n    ...\nend\n\n\nAt some point, your package may require test-specific dependencies. In essence, you give the test subfolder its own environment and Project.toml file. This often happens when you need to test compatibility with another package, on which you do not depend for the source code itself. Or it may simply be due to testing-specific packages like the ones we will encounter below. For interactive testing work, use TestEnv.jl to activate the full test environment (faster than running ]test repeatedly).\n\n\n\n\n\n\nTip\n\n\n\nThe Julia extension also offers a more advanced own testing framework, which relies on defining “test items” the code. The benefit of this is that the tests will integrate more directly with the VS Code interface and specific subgroups of tests can be run independently, on-demand. See TestItemRunner.jl for more.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you want to have more control over your tests, you can try\n\nReferenceTests.jl to compare function outputs with reference files.\nReTest.jl to define tests next to the source code and control their execution.\nTestSetExtensions.jl to make test set outputs more readable.\nTestReadme.jl to test whatever code samples are in your README.\nReTestItems.jl for an alternative take on VSCode’s test item framework.\n\n\n\n\n23.4.1 Code Coverage\nCode coverage refers to the fraction of lines in your source code that are covered by tests (described in more detail in Section 12.3.2). Codecov is a website that provides easy visualization of this coverage, and many Julia packages use it. It is available as a PkgTemplates.jl plugin. For public GitHub repositories using GitHub Actions, uploads are typically token-less. For private repositories or other CI providers, you’ll need to add the CODECOV_TOKEN secret as documented by Codecov.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Distributing and Sharing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-sharing.html#code-style",
    "href": "julia-sharing.html#code-style",
    "title": "23  Distributing and Sharing Julia Code",
    "section": "23.5 Code Style",
    "text": "23.5 Code Style\nTo make your code easy to read, it is recommended to follow a consistent set of guidelines. The official style guide is very short, so most people use third party style guides like BlueStyle or SciMLStyle.\n\n23.5.1 Formatters\n\n23.5.1.1 JuliaFormatter.jl\nJuliaFormatter.jl is an automated formatter for Julia files which can help you enforce the style guide of your choice. Just add a file .JuliaFormatter.toml at the root of your repository, containing a single line like\nstyle = \"blue\"\nThen, the package directory will be formatted in the BlueStyle whenever you call\nimport JuliaFormatter\n# run from the package root, and\n# formats according to .JuliaFormatter.toml\nJuliaFormatter.format(\".\")  \n\n\n\n\n\n\nNote\n\n\n\nThe default formatter uses JuliaFormatter.jl.\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can format code automatically in GitHub pull requests with the julia-format action, or add the formatting check directly to your test suite.\n\n\n\n\n23.5.1.2 Runic.jl\nRunic.jl is a popular choice as well. Like Python’s popular Black formatter, there is no configuration to the formatting options when using Runic. The benefit is increased consistency and no time wasted debating formatting decisions.\nRunic, requires you to set up Runic using Pkg. The instructions are straightforward and available on the Runic.jl repository.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Distributing and Sharing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-sharing.html#code-quality",
    "href": "julia-sharing.html#code-quality",
    "title": "23  Distributing and Sharing Julia Code",
    "section": "23.6 Code quality",
    "text": "23.6 Code quality\nOf course, there is more to code quality than just formatting. Aqua.jl (Auto QUality Assurance) provides a set of routines that examine other aspects of your package, from ensuring that there are no unused dependencies to catching ambiguous methods statically.\nInclude the following in your tests to have Aqua.jl run various checks each time your tests run:\nusing Aqua, MyAwesomePackage\nAqua.test_all(MyAwesomePackage)\nJET.jl is tool that is similar to a static linter in other languages. This means that it can inspect your code and ‘understand’ it well enough to catch many types of errors before runtime. It does this by running type inference and figuring out how a given type will flow through the call stack of methods.\nYou can either use it in report mode (with a nice VSCode display) or in test mode as follows:\nusing JET, MyAwesomePackage\nJET.report_package(MyAwesomePackage)\nJET.test_package(MyAwesomePackage)\nNote that both Aqua.jl and JET.jl might pick up false positives: refer to their respective documentations for ways to make them less sensitive.\nFinally, ExplicitImports.jl can help you get rid of generic imports to specify where each of the variables in your package comes from. As a project gets more complex, using SomePackage can bring many, sometimes conflicting symbols into your current namespace. ExplicitImports forces you to either qualify the usage (e.g. SomePackage.somefunction(...)) or explicitly opt into importing certain variables.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Distributing and Sharing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-sharing.html#documentation",
    "href": "julia-sharing.html#documentation",
    "title": "23  Distributing and Sharing Julia Code",
    "section": "23.7 Documentation",
    "text": "23.7 Documentation\nRefer to Section 12.4.2 for more detail on documentation and its importance. Here are some additional workflow tips for setting up documentation for your package.\nDocStringExtensions.jl provides a few shortcuts that can speed up docstring creation by taking care of the obvious parts.\nIn addition to docstrings, Documenter.jl allows you to design a website for all of this, based on Markdown files contained in the docs subfolder of your package. Unsurprisingly, its own documentation is excellent and will teach you a lot. To build the documentation locally, just run\njulia&gt; using Pkg\n\njulia&gt; Pkg.activate(\"docs\")\n\njulia&gt; include(\"docs/make.jl\")\nThen, use LiveServer.jl from your package folder to visualize and automatically update the website as the code changes (similar to Revise.jl, but for your docpages instead of your code):\njulia&gt; using LiveServer\n\njulia&gt; servedocs()\nTo host the documentation online easily, just select the Documenter plugin from PkgTemplates.jl during creation. Not only will this fill the docs subfolder with the appropriate starting files: it will also initialize a GitHub Actions workflow to build and deploy your website on GitHub pages. Lastly, in your repository’s Pages settings, configure deployment from the gh-pages branch (root).",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Distributing and Sharing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-sharing.html#literate-programming",
    "href": "julia-sharing.html#literate-programming",
    "title": "23  Distributing and Sharing Julia Code",
    "section": "23.8 Literate programming",
    "text": "23.8 Literate programming\nLiterate programming is so-called for combining written documents with the output of programs (literature + code = literate programming). These tools allow you to interleave code with texts, formulas, images and so on.\nIn addition to the Pluto.jl and Jupyter notebooks, take a look at Literate.jl to enrich your code with comments and translate it to various formats. Books.jl is relevant to draft long documents in a pure Julia way.\nQuarto is an open-source scientific and technical publishing system that supports Python, R and Julia. Quarto can render markdown files (.md), Quarto markdown files (.qmd), and Jupyter Notebooks (.ipynb) into documents (Word, PDF, presentations), web pages, blog posts, books, and more. Additionally, Quarto makes it easy to share or publish rendered content to various online hosts.\nPPTX.jl will create Microsoft PowerPoint files.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Distributing and Sharing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-sharing.html#versions-and-registration",
    "href": "julia-sharing.html#versions-and-registration",
    "title": "23  Distributing and Sharing Julia Code",
    "section": "23.9 Versions and registration",
    "text": "23.9 Versions and registration\n\n23.9.1 Versions and Compatibility\nThe Julia community has adopted semantic versioning, which means every package must have a version, and the version numbering follows strict rules (the concept of versioning was covered in Section 12.6.2).\nTo comply with the versioning requirements in Pkg’s resolver, you need to specify compatibility bounds for your dependencies: this happens in the [compat] section of your Project.toml. To initialize these bounds with current dependency versions, use the ]compat command in the Pkg mode of the REPL, or the package PackageCompatUI.jl.\nOver time, new versions of your dependencies will be released. The CompatHelper.jl GitHub Action will help you monitor upstream Julia dependencies and suggest changes to your Project.toml’s [compat] section accordingly. In addition, Dependabot can monitor the dependencies… of your GitHub actions themselves. Both of these are included in the default PkgTemplates setup.\n\n\n\n\n\n\nTip\n\n\n\nIt may also happen that you incorrectly promise compatibility with an old version of a package and not realize it (since Pkg prefers newer versions within the compatibility bounds, not all combinations get tested). To prevent that, the julia-downgrade-compat GitHub action tests your package with the oldest possible version of every dependency, and verifies that everything still works.\n\n\n\n\n23.9.2 Registration\nIf your package is useful to others in the community, it may be a good idea to register it, that is, make it part of the pool of packages that can be installed with\npkg&gt; add MyAwesomePackage  # made possible by registration\nNote that unregistered packages can also be installed by anyone from the GitHub URL, but this is a less reproducible solution:\npkg&gt; add https://github.com/myuser/MyAwesomePackage  # not ideal\nTo register your package, check out the general registry guidelines. The Registrator.jl bot can help you automate the process. Another handy bot, provided by default with PkgTemplates.jl, is TagBot: it automatically tags new versions of your package following each registry release. If you have performed the necessary SSH configuration, TagBot will also trigger documentation website builds following each release.\n\n23.9.2.1 Local Registry\nFor distributing privately (or publicly if you make the repository public), LocalRegistry.jl provides convenience functions for creating a new registry, adding new packages, and updating versions for the packages. If you want to share packages internally, create and register packages to a repository that’s hosted somewhere you and your team can access. If you wanted to make the repository public, you can publish the registry repository somewhere publicly accessible (such as a public GitHub repository).\nOnce established, other users can add a repository as easily as entering package mode and running registry add. Say that we have already put a registry we called FinancePackages in a repository on the company intranet:\npkg&gt; registry add http://company-intranet.com/git/FinancePackages.git\n\n\n23.9.2.2 Hosted Registries\nAlternatively to a self-hosted local registry, third party services such as JuliaHub provide managed registries well suited for corporate environments.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Distributing and Sharing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-sharing.html#reproducibility",
    "href": "julia-sharing.html#reproducibility",
    "title": "23  Distributing and Sharing Julia Code",
    "section": "23.10 Reproducibility",
    "text": "23.10 Reproducibility\nObtaining consistent and reproducible results is an essential part of model auditing and compliance. One tool to consider is DrWatson.jl. It is a general toolbox for running and re-running models in an orderly fashion.\nSome specific issues come up in attempting to ensure reproducibility:\nA first hurdle is random number generation, which is not guaranteed to remain stable across Julia versions. To ensure that the random streams remain exactly the same, you need to use StableRNGs.jl. The downside to this is that the random number generation will be considerably slower than the usual generator.\nAnother aspect is dataset download and management. The packages DataDeps.jl, DataToolkit.jl and ArtifactUtils.jl can help you bundle non-code elements with your package (some of these rely on artifacts - discussed in Section 12.6.3).\n\n\n\n\n\n\nTipFinancial Modeling Pro Tip\n\n\n\nAlways version-control both Project.toml and Manifest.toml for regulated (auditable) workflows. Instantiating the environment on another machine ensures identical dependency versions, which is crucial for reproducible risk and valuation reports.\njulia&gt; using Pkg\njulia&gt; Pkg.activate(\".\")\njulia&gt; Pkg.instantiate()   # resolves to exact versions recorded in Manifest.toml\nNote that for this to fully work, the replicating machine needs to be the same architecture (e.g. x64), OS (e.g. Windows), and Julia version (e.g. v1.10). If the versions differ, Julia may need to use a different set of dependencies for compatibility reasons. However, it’s still a good practice to store the Manifest.toml for important workflows.\nAnd remember, that with package repositories, you generally do not want to check in the Manifest.toml. Instead, create scripts for the production workflows that do check in the Manifest.toml.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Distributing and Sharing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-sharing.html#interoperability",
    "href": "julia-sharing.html#interoperability",
    "title": "23  Distributing and Sharing Julia Code",
    "section": "23.11 Interoperability",
    "text": "23.11 Interoperability\nTo ensure compatibility with earlier Julia versions, Compat.jl is your best ally.\nMaking packages play nice with one another is a key goal of the Julia ecosystem. Since Julia 1.9, this can be done with package extensions, which override specific behaviors based on the presence of a given package in the environment. For example, if you want to provide pre-configured plotting, but don’t in general need to include a plotting library as part of your package for all users and use cases. PackageExtensionTools.jl eases setting up extensions for your package.\nFurthermore, the Julia ecosystem plays nice with other programming languages too. C and Fortran are natively supported. Python can be easily interfaced with the combination of CondaPkg.jl and PythonCall.jl. Other language compatibility packages can be found in the JuliaInterop organization, like RCall.jl.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Distributing and Sharing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-sharing.html#customization",
    "href": "julia-sharing.html#customization",
    "title": "23  Distributing and Sharing Julia Code",
    "section": "23.12 Customization",
    "text": "23.12 Customization\nPart of interoperability is also flexibility and customization: the Preferences.jl package gives a nice way to specify various options in TOML files. These customizable preferences persist across sessions and provide the preferences at both compile and runtime. For example, say different parts of a company had different preferred data sources but otherwise used the same code. This could be set in a way via Preferences.jl so that each team can share the logic while seamlessly defaulting to different data sources.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Distributing and Sharing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-sharing.html#collaboration",
    "href": "julia-sharing.html#collaboration",
    "title": "23  Distributing and Sharing Julia Code",
    "section": "23.13 Collaboration",
    "text": "23.13 Collaboration\nOnce your package grows big enough, you might need to bring in some help. Working together on a software project has its own set of challenges, which are partially addressed by a good set of ground rules like SciML ColPrac. Of course, collaboration goes both ways: if you find a Julia package you really like, you are more than welcome to contribute as well, for example by opening issues or submitting pull requests.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Distributing and Sharing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-optimizing.html",
    "href": "julia-optimizing.html",
    "title": "24  Optimizing Julia Code",
    "section": "",
    "text": "24.1 Chapter Overview\nThe two fundamental principles for writing fast Julia code:",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimizing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-optimizing.html#chapter-overview",
    "href": "julia-optimizing.html#chapter-overview",
    "title": "24  Optimizing Julia Code",
    "section": "",
    "text": "Ensure that the compiler can infer the type of every variable.\nAvoid unnecessary (heap) allocations.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimizing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-optimizing.html#type-inference",
    "href": "julia-optimizing.html#type-inference",
    "title": "24  Optimizing Julia Code",
    "section": "24.2 Type Inference",
    "text": "24.2 Type Inference\nThe compiler’s job is to optimize and translate Julia code into runnable machine code. If a variable’s type cannot be deduced before the code is run, then the compiler won’t generate efficient code to handle that variable. This phenomenon is called “type instability”. Enabling type inference means making sure that every variable’s type in every function can be deduced from the types of the function inputs alone.\nThat is, the compiler will be able to create more optimized code if it can analyze the function you’ve written and determine what type will be returned. In the following function, the compiler can analyze the expressions and determine with certainty that if mysum is given two Ints as arguments, the return value will also be an Int.\nfunction mysum(a,b)\n    a + b\nend",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimizing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-optimizing.html#avoiding-heap-allocations",
    "href": "julia-optimizing.html#avoiding-heap-allocations",
    "title": "24  Optimizing Julia Code",
    "section": "24.3 Avoiding Heap Allocations",
    "text": "24.3 Avoiding Heap Allocations\nA “heap allocation” (or simply “allocation”) occurs when we create a new variable without knowing how much space it will require (like a Vector with flexible length). This has two implications:\n\nAllocating memory on the heap takes substantially more time than stack allocated memory to be created.\nPeriodically, a garbage collector (GC), needs to run to de-allocate (free up) memory on the heap which is no longer used by the program\n\nExecution of code is stopped while the garbage collector runs, so minimizing its usage is important.\nThe vast majority of performance tips come down to these two fundamental ideas.\nTypically, the most common beginner pitfall is the use of untyped global variables without passing them as arguments. Why is it bad? Because the type of a global variable can change outside of the body of a function, so it causes type instabilities wherever it is used. Those type instabilities in turn lead to more heap allocations.\n\n\n\n\n\n\nTip\n\n\n\nIf you must use globals (e.g., model parameters loaded once), declare them const so their type can’t change and the compiler can specialize code that uses them.\n\n\nMuch more detail on performance considerations is covered in Chapter 10.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimizing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-optimizing.html#sec-measuring-performance",
    "href": "julia-optimizing.html#sec-measuring-performance",
    "title": "24  Optimizing Julia Code",
    "section": "24.4 Measuring performance",
    "text": "24.4 Measuring performance\nThe simplest way to measure how fast a piece of code runs is to use the @time macro, which returns the result of the code and prints the measured runtime and allocations. Because code needs to be compiled before it can be run, you should first run a function without timing it so it can be compiled, and then time it:\n\nsum_abs(vec) = sum(abs(x) for x in vec);\nv = rand(100);\n\nusing BenchmarkTools\n@time sum_abs(v); # Inaccurate, note the &gt;99% compilation time\n@time sum_abs(v); # Accurate\n\n  0.008752 seconds (70.57 k allocations: 3.408 MiB, 99.87% compilation time)\n  0.000001 seconds (1 allocation: 16 bytes)\n\n\nUsing @time is quick but it has flaws, because your function is only measured once. That measurement might have been influenced by other things going on in your computer at the same time. In general, running the same block of code multiple times is a safer measurement method, because it diminishes the probability of only observing an outlier.\n\n24.4.1 BenchmarkTools\nBenchmarkTools.jl is the most popular package for repeated measurements on function executions. Similarly to @time, BenchmarkTools offers @btime which can be used in exactly the same way but will run the code multiple times and provide an average. Additionally, by using $ to interpolate external values, you remove the overhead caused by global variables.\n\nusing BenchmarkTools\n@btime sum_abs(v); # includes global access overhead\n@btime sum_abs($v); # interpolates v, avoiding global overhead\n\n  22.275 ns (1 allocation: 16 bytes)\n  17.368 ns (0 allocations: 0 bytes)\n\n\nIn more complex settings, you might need to construct variables in a setup phase that is run before each sample. This can be useful to generate a new random input every time, instead of always using the same input.\n\nmy_matmul(A, b) = A * b;\n@btime my_matmul(A, b) setup = (\n    A = rand(1000, 1000); # use semi-colons between setup lines\n    b = rand(1000)\n);\n\n  133.500 μs (3 allocations: 8.06 KiB)\n\n\nFor better visualization, the @benchmark macro shows performance histograms:\n\n\n\n\n\n\nNote\n\n\n\nCertain computations may be optimized away by the compiler before the benchmark takes place. If you observe suspiciously fast performance, especially below the nanosecond scale, this is very likely to have happened.\n\n\n\n\n24.4.2 Other tools\nBenchmarkTools.jl works fine for relatively short and simple blocks of code (microbenchmarking). To find bottlenecks in a larger program, you should use a profiler(see next section) or the package TimerOutputs.jl. It allows you to label different sections of your code, then time them and display a table of grouped by label.\nFinally, if you know a loop is slow and you’ll need to wait for it to be done, you can use ProgressMeter.jl or ProgressLogging.jl to track its progress. This will display a progress bar in VS Code or in a notebook, indicating how far along a loop has progressed.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimizing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-optimizing.html#profiling",
    "href": "julia-optimizing.html#profiling",
    "title": "24  Optimizing Julia Code",
    "section": "24.5 Profiling",
    "text": "24.5 Profiling\nProfiling can identify performance bottlenecks at function level, and graphical tools such as ProfileView.jl are the best way to use it.\n\n24.5.1 Sampling\nWhereas a benchmark measures the overall performance of some code, a profiler breaks it down function by function to identify bottlenecks. Sampling-based profilers periodically ask the program which line it is currently executing, and aggregate results by line or by function. Julia offers two kinds: one for runtime (in the module Profile) and one for memory (in the submodule Profile.Allocs).\nThese built-in profilers print textual outputs, but the result of profiling is best visualized as a flame graph. In a flame graph, each horizontal layer corresponds to a specific level in the call stack, and the width of a tile shows how much time was spent in the corresponding function. Here’s an example:\n\n\n\nflamegraph\n\n\n\n\n24.5.2 Visualizing Profile Results\nThe packages ProfileView.jl and PProf.jl both allow users to record and interact with flame graphs. ProfileView.jl is simpler to use, but PProf is more featureful and is based on pprof, an external tool maintained by Google which applies to more than just Julia code. Here we only demonstrate the former:\nusing ProfileView \n@profview do_work(some_input)\n\n\n\n\n\n\nTip\n\n\n\nCalling @profview do_work(some_input) in the integrated Julia REPL will open an interactive flame graph, similar to ProfileView.jl but without requiring a separate package.\nIn VS Code, you can also call (profview?) do_work(some_input) without adding ProfileView.jl; the Julia extension provides this macro.\n\n\nTo integrate profile visualizations into environments like Jupyter and Pluto, use ProfileSVG.jl or ProfileCanvas.jl, whose outputs can be embedded into a notebook.\nNo matter which tool you use, if your code is too fast to collect samples, you may need to run it multiple times in a loop.\n\n\n\n\n\n\nTip\n\n\n\nTo visualize memory allocation profiles, use PProf.jl or VSCode’s @profview_allocs. A known issue with the allocation profiler is that it is not able to determine the type of every object allocated, instead Profile.Allocs.UnknownType is shown instead. Inspecting the call graph can help identify which types are responsible for the allocations.\n\n\n\n\n24.5.3 External profilers\nApart from the built-in Profile standard library, there are a few external profilers that you can use including Intel VTune (in combination with IntelITT.jl), NVIDIA Nsight Systems (in combination with NVTX.jl), and Tracy.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimizing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-optimizing.html#type-stability",
    "href": "julia-optimizing.html#type-stability",
    "title": "24  Optimizing Julia Code",
    "section": "24.6 Type stability",
    "text": "24.6 Type stability\nFor a section of code to be considered type stable, the type inferred by the compiler must be “concrete”, which means that the size of memory that needs to be allocated to store its value is known at compile time. Types declared abstract with abstract type are not concrete and neither are parametric types whose parameters are not specified:\n\n@show isconcretetype(Any)\n@show isconcretetype(AbstractVector)\n@show isconcretetype(Vector) # Shorthand for `Vector{T} where T`\n@show isconcretetype(Vector{Real})\n@show isconcretetype(eltype(Vector{Real}))\n@show isconcretetype(Vector{Int64})\n\nisconcretetype(Any) = false\nisconcretetype(AbstractVector) = false\nisconcretetype(Vector) = false\nisconcretetype(Vector{Real}) = true\nisconcretetype(eltype(Vector{Real})) = false\nisconcretetype(Vector{Int64}) = true\n\n\ntrue\n\n\n\n\n\n\n\n\nNote\n\n\n\nVector{Real} is concrete despite Real being abstract for subtle typing reasons but it will still be slow in practice because the type of its elements is abstract.\n\n\nType-stable code enables static dispatch and aggressive optimizations (e.g., inlining and specialization). Type-unstable code triggers dynamic dispatch at runtime, which prevents many optimizations and often increases allocations.\nType-stability is a contagious thing: if a variable’s type cannot be inferred, then the types of variables that depend on it may not be inferrable either. Most code should be type-stable unless it has a good reason not to be.\n\n24.6.1 Detecting Instabilities\nThe simplest way to detect an instability is with the built-in macro @code_warntype: The output of @code_warntype is difficult to parse, but the key takeaway is the return type of the function’s Body: if it is an abstract type, like Any, something is wrong. In a normal Julia REPL, such cases would show up colored in red as a warning.\n\nusing InteractiveUtils # loaded automatically if using a notebook or REPL\nfunction put_in_vec_and_sum(x)\n    v = []        # Vector{Any} → type-unstable\n    push!(v, x)\n    return sum(v)\nend;\n\n@code_warntype put_in_vec_and_sum(1)\n\n\nMethodInstance for put_in_vec_and_sum(::Int64)\n\n\n  from put_in_vec_and_sum(x) @ Main In[7]:2\n\nArguments\n\n  #self#::Core.Const(Main.put_in_vec_and_sum)\n\n  x::Int64\n\nLocals\n\n  v::Vector{Any}\n\nBody::Any\n\n1 ─\n      (v\n = Base.vect())\n\n│   %2 = Main.push!::Core.Const(push!)\n\n│   %3 = v\n::Vector{Any}\n\n│        (%2)(%3, x)\n\n│   %5 = v::Vector{Any}\n\n│   %6 = Main.sum(%5)::Any\n\n└──      return %6\n\n\n\n\n\n\nUnfortunately, @code_warntype is limited to one function body: calls to other functions are not expanded, which makes deeper type instabilities easy to miss. That is where JET.jl can help: it provides optimization analysis aimed primarily at finding type instabilities. While test integrations are also provided, the interactive entry point of JET is the @report_opt macro.\n\nusing JET\n@report_opt put_in_vec_and_sum(1)\n\n\n═════ 4 possible errors found ═════\n┌ put_in_vec_and_sum(x::Int64) @ Main ./In[7]:5\n│┌ sum(a::Vector{Any}) @ Base ./reducedim.jl:982\n││┌ sum(a::Vector{Any}; dims::Colon, kw::@Kwargs{}) @ Base ./reducedim.jl:982\n│││┌ _sum(a::Vector{Any}, ::Colon) @ Base ./reducedim.jl:986\n││││┌ _sum(a::Vector{Any}, ::Colon; kw::@Kwargs{}) @ Base ./reducedim.jl:986\n│││││┌ _sum(f::typeof(identity), a::Vector{Any}, ::Colon) @ Base ./reducedim.jl:987\n││││││┌ _sum(f::typeof(identity), a::Vector{Any}, ::Colon; kw::@Kwargs{}) @ Base ./reducedim.jl:987\n│││││││┌ mapreduce(f::typeof(identity), op::typeof(Base.add_sum), A::Vector{Any}) @ Base ./reducedim.jl:329\n││││││││┌ mapreduce(f::typeof(identity), op::typeof(Base.add_sum), A::Vector{Any}; dims::Colon, init::Base._InitialValue) @ Base ./reducedim.jl:329\n│││││││││┌ _mapreduce_dim(f::typeof(identity), op::typeof(Base.add_sum), ::Base._InitialValue, A::Vector{Any}, ::Colon) @ Base ./reducedim.jl:337\n││││││││││┌ _mapreduce(f::typeof(identity), op::typeof(Base.add_sum), ::IndexLinear, A::Vector{Any}) @ Base ./reduce.jl:444\n│││││││││││┌ mapreduce_impl(f::typeof(identity), op::typeof(Base.add_sum), A::Vector{Any}, ifirst::Int64, ilast::Int64) @ Base ./reduce.jl:277\n││││││││││││┌ mapreduce_impl(f::typeof(identity), op::typeof(Base.add_sum), A::Vector{…}, ifirst::Int64, ilast::Int64, blksize::Int64) @ Base ./reduce.jl:262\n│││││││││││││ runtime dispatch detected: op::typeof(Base.add_sum)(%91::Any, %110::Any)::Any\n││││││││││││└────────────────────\n││││││││││││┌ mapreduce_impl(f::typeof(identity), op::typeof(Base.add_sum), A::Vector{…}, ifirst::Int64, ilast::Int64, blksize::Int64) @ Base ./reduce.jl:273\n│││││││││││││ runtime dispatch detected: op::typeof(Base.add_sum)(%187::Any, %189::Any)::Any\n││││││││││││└────────────────────\n││││││││││┌ _mapreduce(f::typeof(identity), op::typeof(Base.add_sum), ::IndexLinear, A::Vector{Any}) @ Base ./reduce.jl:437\n│││││││││││ runtime dispatch detected: op::typeof(Base.add_sum)(%97::Any, %115::Any)::Any\n││││││││││└────────────────────\n││││││││││┌ _mapreduce(f::typeof(identity), op::typeof(Base.add_sum), ::IndexLinear, A::Vector{Any}) @ Base ./reduce.jl:440\n│││││││││││ runtime dispatch detected: op::typeof(Base.add_sum)(%118::Any, %139::Any)::Any\n││││││││││└────────────────────\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Julia extension features a static linter, and runtime diagnostics with JET can be automated to run periodically on your codebase and show any problems detected.\n\n\nCthulhu.jl1 exposes the @descend macro which can be used to interactively “step through” lines of the corresponding typed code, and “descend” into a particular line if needed. This is akin to repeatedly calling @code_warntype deeper and deeper into your functions (“slowly succumbing to the madness…” of type instability).\n\n\n24.6.2 Fixing Instabilities\nThe Julia manual has a collection of tips to improve type inference.\n\n\n\n\n\n\nTip\n\n\n\nTo be more forceful about ensuring type stability in your code, one approach is to error whenever a type instability occurs: the macro @stable from DispatchDoctor.jl allows exactly that.\n\n\nWhen working with parametric types, look to avoid usage of generic type parameters (e.g. Array{Any}) whenever possible. For custom types, make use of parametric types to create type-stable abstractions.\nIn the next example, Bond1 stores fields as Any, which forces dynamic dispatch when accessing fields and often leads to allocations. Bond2 fixes the field types to Float64, which is concrete but inflexible. Bond3 is parametric: it’s concrete for each T (e.g., Bond3{Float64}), allowing specialization while remaining flexible.\nstruct Bond1\n    par\n    coupon\nend\n\nstruct Bond2\n    par::Float64\n    coupon::Float64\nend\n\nstruct Bond3{T}\n    par::T\n    coupon::T\nend\n\nb1 = Bond1(100.0, 0.05)            # fields are Any → slower in numeric code\nb2 = Bond2(100.0, 0.05)            # concrete and fast, but fixed to Float64\nb3 = Bond3(100.0, 0.05)            # specializes to Bond3{Float64}",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimizing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-optimizing.html#memory-management",
    "href": "julia-optimizing.html#memory-management",
    "title": "24  Optimizing Julia Code",
    "section": "24.7 Memory management",
    "text": "24.7 Memory management\nAfter ensuring type stability, one should try to reduce the number of heap allocations a program makes. Again, the Julia manual has a series of tricks related to arrays and allocations which you should take a look at. In particular, try to modify existing arrays instead of allocating new objects and try to access arrays in the right order for Julia, i.e. accessing data down columns instead of across rows.\nAlternatively, to ensure that non-allocating functions never regress in future versions of your code, you can write a test set to check allocations by providing the function and a concrete type-signature.\nusing AllocCheck, Test\n@testset \"non-allocating\" begin     \n    @test isempty(AllocCheck.check_allocs(my_func, (Float64, Float64))) \nend",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimizing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-optimizing.html#compilation",
    "href": "julia-optimizing.html#compilation",
    "title": "24  Optimizing Julia Code",
    "section": "24.8 Compilation",
    "text": "24.8 Compilation\nA number of tools allow you to reduce Julia’s latency, also referred to as TTFX (time to first X, where X was historically plotting a graph).\n\n24.8.1 Precompilation\nPrecompileTools.jl reduces the amount of time taken to run functions loaded from a package or local module that you wrote. It allows module authors to specify methods to precompile when a module is loaded for the first time. The methods chosen should represent those which would be used during typical user workflows. These methods then have the same latency as if they had already been run by the end user. This adds upfront pre-compilation time when installing a package version for the first time, but then subsequent uses will be much quicker.\nHere’s an example of precompilation, adapted from the package’s documentation:\nmodule MyPackage\n\nusing PrecompileTools: @compile_workload\n\nstruct MyType\n    x::Int\nend\n\nmyfunction(a::Vector) = a[1].x\n\n@compile_workload begin\n    a = [MyType(1)]\n    myfunction(a)\nend\n\nend\nNote that every method that is called will be compiled, no matter how far down the call stack or which module it comes from. To see if the intended calls were compiled correctly or diagnose other problems related to precompilation, use SnoopCompile.jl. This is especially important for writers of registered Julia packages, as it allows you to diagnose recompilation that happens due to invalidation.\n\n\n24.8.2 Package compilation\nTo reduce the time that packages take to load, you can use PackageCompiler.jl to generate a custom version of Julia, called a sysimage (system image), with its own custom standard library. As packages in the standard library are already compiled, any using or import statement involving them is almost instant.\nOnce PackageCompiler.jl is added to your global environment, activate a local environment for which you want to generate a sysimage, ensure all of the packages you want to compile are in its Project.toml, and run create_sysimage as in the example below. The filetype of sysimage_path differs by operating system: Linux has .so, MacOS has .dylib, and Windows has .dll.\nusing PackageCompiler # installed in global environment\npackages_to_compile = [\"Makie\", \"DifferentialEquations\"]\ncreate_sysimage(packages_to_compile; sysimage_path=\"MySysimage.so\")\nOnce a sysimage is generated, it can be used with the command line flag: julia --sysimage=path/to/sysimage.\n\n\n\n\n\n\nTip\n\n\n\nThe generation and loading of sysimages can be streamlined with VSCode. By default, the command sequence Task: Run Build Task followed by Julia: Build custom sysimage for current environment will compile a sysimage containing all packages in the current environment, but additional details can be specified in a /.vscode/JuliaSysimage.toml file. To automatically detect and use a custom sysimage, set useCustomSysimage to true in the application settings.\n\n\n\n\n24.8.3 Static compilation\nPackageCompiler.jl also facilitates the creation of apps and libraries that can be shared to and run on machines that don’t have Julia installed.\nAt a basic level, all that’s required to turn a Julia module MyModule into an app is a function julia_main()::Cint that returns 0 upon successful completion. Then, with PackageCompiler.jl loaded, run create_app(\"MyModule\", \"MyAppCompiled\"). Command line arguments to the resulting app are assigned to the global variable ARGS::Array{ASCIIString}, the handling of which can be made easier by ArgParse.jl.\nIn Julia, a library is just a sysimage with some extras that enable external programs to interact with it. Any functions in a module marked with Base.@ccallable, and whose type signature involves C-conforming types e.g. Cint, Cstring, and Cvoid, can be compiled into an externally callable library with create_library, similarly to create_app. Unfortunately, the process of compiling and sharing a standalone executable or callable library must take relocatability into account, which is beyond the scope of this blog.\n\n\n\n\n\n\nNote\n\n\n\nAn alternative way to compile a shareable app or library that doesn’t need to compile a sysimage, and therefore results in smaller binaries, is to use StaticCompiler.jl and its sister package StaticTools.jl. The biggest tradeoff of not compiling a sysimage, is that Julia’s garbage collector is no longer included, so all heap allocations must be managed manually, and all code compiled must be type-stable. To get around this limitation, you can use static equivalents of dynamic types, such as a StaticArray (StaticArrays.jl) instead of an Array or a StaticString (StaticTools.jl), use malloc and free from StaticTools.jl directly, or use arena allocators with Bumper.jl. The README of StaticCompiler.jl contains a more detailed guide on how to prepare code to be compiled.\n\n\n\n\n\n\n\n\nNote\n\n\n\nStarting with Julia 1.12, it is anticipated that there will be a new way to compile Julia to small, static binaries with a tool called juliac.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimizing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-optimizing.html#parallelism",
    "href": "julia-optimizing.html#parallelism",
    "title": "24  Optimizing Julia Code",
    "section": "24.9 Parallelism",
    "text": "24.9 Parallelism\nCode can be made to run faster through parallel execution with multithreading (shared-memory parallelism) or multiprocessing / distributed computing. Parallelism was covered in a whole chapter (Chapter 11), but this section provides insight into recommended packages and patterns specific to Julia.\nMany common operations such as maps and reductions can be trivially parallelized through either method by using their respective Julia packages (e.g pmap from Distributed.jl and tmap from OhMyThreads.jl). Multithreading is available on almost all modern hardware, whereas distributed computing is most useful to users of high-performance computing clusters.\n\n24.9.1 Multithreading\nTo use multi-threading, Julia needs to be started with more than one thread. This can be done by either setting the environment variable JULIA_NUM_THREADS to either auto or specify a number like 4. You can also specify how many threads to start julia with if given the -t command line argument (such as running julia -t 4 to start Julia with four threads from the command line). Once Julia is running, you can check if this was successful by calling Threads.nthreads().\n\n\n\n\n\n\nTip\n\n\n\nIn VS Code, the default number of threads can be edited by adding \"julia.NumThreads\": auto, to your settings. This will be applied to the integrated terminal. Setting the threads at the machine level through the environment variables is preferred, since it will apply that setting to more than just your VS Code sessions.\n\n\nWhy doesn’t Julia automatically start with more than one thread? Between “hyper-threading” (synthetic additional thread capacity), multi-core architectures, and the different types of threads it’s actually difficult to predict how many threads will be optimal for a given system. Julia’s current default is to take the more conservative approach and start single-threaded unless otherwise specified. The “auto” option is a best-guess but can, on certain systems and configurations, be very bad for performance. The authors recommend for most common systems to just use “auto”.\n\n\n\n\n\n\nTip\n\n\n\nLinear algebra code calls the low-level libraries BLAS and LAPACK. These libraries manage their own pool of threads, so single-threaded Julia processes can still make use of multiple threads. If you also enable Julia threads, you may oversubscribe cores (Julia threads × BLAS threads), which can hurt performance. Consider limiting BLAS to one thread when using Julia threads.\nIn this case, once LinearAlgebra is loaded, BLAS can be set to use only one thread by calling BLAS.set_num_threads(1). For more information see the docs on multithreading and linear algebra.\n\n\nRegardless of the number of threads, you can parallelize a for loop with the macro Threads.@threads. The macros @spawn and @async function similarly, but require more manual management of tasks and their results. For this reason @threads is recommended for those who do not wish to use third-party packages.\nWhen designing multithreaded code, you should generally try to write to shared memory as rarely as possible. Where it cannot be avoided, you need to be careful to avoid “race conditions”, i.e. situations when competing threads try to write different things to the same memory location. It is usually a good idea to separate memory accesses with loop indices, as in the example below:\nresults = zeros(Int, 4)\nThreads.@threads for i in 1:4\n    results[i] = i^2\nend\nAlmost always, it is not a good idea to use threadid().\nEven if you manage to avoid any race conditions in your multithreaded code, it is very easy to run into subtle performance issues (like false sharing). For these reasons, you might want to consider using a high-level package like OhMyThreads.jl, which provides a user-friendly alternative to Threads and makes managing threads and their memory use much easier. The helpful translation guide demonstrates how Base multi-threading loops can be translated into the OhMyThreads API.\nIf the latency of spinning up new threads becomes a bottleneck, check out Polyester.jl for very lightweight threads that are quicker to start.\nIf you’re on Linux, you should consider using ThreadPinning.jl to pin your Julia threads to CPU cores to obtain stable and optimal performance. The package can also be used to visualize where the Julia threads are running on your system (see threadinfo()).\n\n\n24.9.2 Distributed computing\nJulia’s multiprocessing and distributed relies on the standard library Distributed. The main difference with multi-threading is that data isn’t shared between worker processes. Once Julia is started, processes can be added with addprocs, and their can be queried with nworkers.\nThe macro Distributed.@distributed is a syntactic equivalent for Threads.@threads. Hence, we can use @distributed to parallelize a for loop as before, but we have to additionally deal with sharing and recombining the results array. We can delegate this responsibility to the standard library SharedArrays. However, in order for all workers to know about a function or module, we have to load it @everywhere:\n\nusing Distributed\n\n# Add additional workers then load code on the workers\naddprocs(3)\n@everywhere using SharedArrays\n@everywhere f(x) = 3x^2\n\nresults = SharedArray{Int}(4)\n@sync @distributed for i in 1:4\n    results[i] = f(i)\nend\nresults\n\n4-element SharedVector{Int64}:\n  3\n 12\n 27\n 48\n\n\nNote that @distributed does not force the main process to wait for other workers, so we must use @sync to block execution until all computations are done.\nOne feature @distributed has over @threads is the possibility to specify a reduction function (an associative binary operator) which combines the results of each worker. In this case @sync is implied, as the reduction cannot happen unless all of the workers have finished.\n\nusing Distributed\naddprocs(2)\n\n2-element Vector{Int64}:\n 5\n 6\n\n\n\n@distributed (+) for i in 1:4\n    i^2\nend\n\n30\n\n\nAlternately, the convenience function pmap can be used to easily parallelize a map, both in a distributed and multi-threaded way.\n\nresults = pmap(x -&gt; 3 * x^2, 1:100; distributed=true, batch_size=25, on_error=ex -&gt; 0)\n\n100-element Vector{Int64}:\n     3\n    12\n    27\n    48\n    75\n   108\n   147\n   192\n   243\n   300\n   363\n   432\n   507\n     ⋮\n 23763\n 24300\n 24843\n 25392\n 25947\n 26508\n 27075\n 27648\n 28227\n 28812\n 29403\n 30000\n\n\nFor more functionalities related to higher-order functions, Transducers.jl and Folds.jl are the way to go.\n\n\n\n\n\n\nTip\n\n\n\nMPI.jl implements the Message Passing Interface standard, which is heavily used in high-performance computing beyond Julia. The C library that MPI.jl wraps is highly optimized, so Julia code that needs to be scaled up to a large number of cores, such as an HPC cluster, will typically run faster with MPI than with plain Distributed.\nElemental.jl is a package for distributed dense and sparse linear algebra which wraps the Elemental library written in C++, itself using MPI under the hood.\n\n\n\n\n24.9.3 GPU programming\nGPUs are specialized in executing instructions in parallel over a large number of threads. While they were originally designed for accelerating graphics rendering, more recently they have been used to train and evaluate machine learning models.\nJulia’s GPU ecosystem is managed by the JuliaGPU organization, which provides individual packages for directly working with each GPU vendor’s instruction set. The most popular one is CUDA.jl, which also simplifies installation of CUDA drivers for NVIDIA GPUs. Through KernelAbstractions.jl, you can easily write code that is agnostic to the type of GPU where it will run.\n\n\n24.9.4 SIMD instructions\nIn the Single Instruction, Multiple Data (SIMD) paradigm, several processing units perform the same instruction at the same time, differing only in their inputs. The range of operations that can be parallelized (or “vectorized”) like this is more limited than in the previous sections, and slightly harder to control. Julia can automatically vectorize repeated numerical operations (such as those found in loops) provided a few conditions are met:\n\nReordering operations must not change the result of the computation.\nThere must be no control flow or branches in the core computation.\nAll array accesses must follow some linear pattern.\n\nWhile this may seem straightforward, there are a number of important caveats which prevent code from being vectorized. Performance annotations like @simd or @inbounds help enable vectorization in some cases, as does replacing control flow with ifelse.\nIf this isn’t enough, SIMD.jl allows users to force the use of SIMD instructions and bypass the check for whether this is possible. One particular use-case for this is for vectorizing non-contiguous memory reads and writes through SIMD.vgather and SIMD.vscatter respectively.\n\n\n\n\n\n\nTip\n\n\n\nYou can detect whether the optimizations have occurred by inspecting the output of @code_llvm or @code_native and looking for vector registers (e.g., ymm/xmm/zmm), packed operations, and vectorized loops. Note that the exact things you’re looking for will vary between code and CPU instruction set, an example of what to look for can be seen in this blog post by Kristoffer Carlsson.\n\n\n\n\n24.9.5 Additional Packages\nSome additional packages to be aware of include:\n\nLoopVectorization.jl which can enhance the vectorized loops even further, such as handling the “tail” of a vectorized loops more efficiently than the base compiler. The “tail” refers to situations like where you have a vector width of 8, but don’t have a collection that’s a nice multiple of 8 (say 1001 elements).\nOctavian.jl implements a linear algebra-like library, utilizing parallelism via vectorization to generate efficient code for the system it’s running on.\nTulio.jl is an einsum library, a domain-specific language for tensor (a specific subset of vectors) operations, common in machine learning and linear algebra.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimizing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-optimizing.html#efficient-types",
    "href": "julia-optimizing.html#efficient-types",
    "title": "24  Optimizing Julia Code",
    "section": "24.10 Efficient types",
    "text": "24.10 Efficient types\nUsing an efficient data structure is a tried and true way of improving the performance. While users can write their own efficient implementations through officially documented interfaces, a number of packages containing common use cases are more tightly integrated into the Julia ecosystem.\n\n24.10.1 Static arrays\nUsing StaticArrays.jl, you can construct arrays which contain size information in their type. Through multiple dispatch, statically sized arrays give rise to specialised, efficient methods for certain algorithms like linear algebra. In addition, the SArray, SMatrix and SVector types are immutable, so the array does not need to be garbage collected as it can be stack-allocated. Creating a new SArrays comes at almost no extra cost, compared to directly editing the data of a mutable object. With MArray, MMatrix, and MVector, data remains mutable as in normal arrays.\nTo handle mutable and immutable data structures with the same syntax, you can use Accessors.jl:\n\nusing StaticArrays, Accessors\n\nsx = SA[1, 2, 3] # SA constructs an SArray\n@set sx[1] = 3 # Returns a copy, does not update the variable\n@reset sx[1] = 4 # Replaces the original\n\n3-element SVector{3, Int64} with indices SOneTo(3):\n 4\n 2\n 3\n\n\n\n\n24.10.2 Classic data structures\nAll but the most obscure data structures can be found in the packages from the JuliaCollections organization, especially DataStructures.jl which has all the standards from the computer science courses (stacks, queues, heaps, trees and the like). Iteration and memoization utilities are also provided by packages like IterTools.jl and Memoize.jl.\n\n\n24.10.3 Bits types\nWhen you create custom structs, keeping the fields as simple, concrete types makes it more likely that the compiler will be able to allocate these objects on the stack instead of the heap. An example of this was shown in Section 13.5.3.",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimizing Julia Code</span>"
    ]
  },
  {
    "objectID": "julia-optimizing.html#footnotes",
    "href": "julia-optimizing.html#footnotes",
    "title": "24  Optimizing Julia Code",
    "section": "",
    "text": "So-named for the “slow descent into madness” when descending into functions to follow the Julia compiler’s type inference across many layers of function calls.↩︎",
    "crumbs": [
      "Developing in Julia",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Optimizing Julia Code</span>"
    ]
  },
  {
    "objectID": "applications.html",
    "href": "applications.html",
    "title": "Applied Financial Modeling Techniques",
    "section": "",
    "text": "Here we focus on practical implementation of financial models. This section provides concrete examples and strategies for building effective models across various applications. We’ll discuss model design, optimization, and validation, with an emphasis on real-world usage. The goal is to bridge theory and practice, giving you hands-on knowledge to apply advanced techniques to actual financial challenges.",
    "crumbs": [
      "Applied Financial Modeling Techniques"
    ]
  },
  {
    "objectID": "scenario-generation.html",
    "href": "scenario-generation.html",
    "title": "25  Scenario Generation",
    "section": "",
    "text": "25.1 Chapter Overview\nHow to generate synthetic data for your model using sub-models, with applications to economic scenario generation and portfolio composition.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Scenario Generation</span>"
    ]
  },
  {
    "objectID": "scenario-generation.html#common-use-cases-of-scenario-generators",
    "href": "scenario-generation.html#common-use-cases-of-scenario-generators",
    "title": "25  Scenario Generation",
    "section": "25.2 Common Use Cases of Scenario Generators",
    "text": "25.2 Common Use Cases of Scenario Generators\nScenario generators are widely used in risk management, investment analysis, and regulatory compliance to model potential future outcomes. If the goal is forecasting actual market behavior, real world scenarios (RW) are commonly used. If, on the other hand, pricing financial instruments is needed, risk neutral (RN) scenarios are often used.\n\n25.2.1 Risk management, especially Value at Risk (VaR) & Expected Shortfall (ES).\nRW scenario generators are used to simulate market movements to estimate potential portfolio losses. Basel III regulatory capital requirements have adopted these approaches.\n\n\n25.2.2 Stress Testing & Regulatory Compliance\nRW scenario generators can also be used to generate extreme but plausible market conditions to assess resilience, which is required by central banks and financial regulators (e.g., Federal Reserve and ECB).\n\n\n25.2.3 Portfolio Optimization & Asset Allocation\nRW scenario generators are used to simulate thousands of market conditions to determine optimal portfolio allocations which is commonly used in modern portfolio theory (MPT) and Black-Litterman models.\n\n\n25.2.4 Pension & Insurance Risk Modeling\nRW scenario generators can be used to simulate longevity risk, policyholder behavior, and interest rate movements. They are also used for economic capital estimation under uncertain economic scenarios.\n\n\n25.2.5 Economic & Macro-Financial Forecasting\nCentral banks and institutions (e.g., IMF, World Bank) use RW scenario generators to predict macroeconomic trends.\n\n\n25.2.6 Asset Pricing & Hedging\nRN scenario generators help value options using stochastic models (e.g., Black-Scholes, Heston model). They can help simulate future stock price movements under different volatility conditions. They can also be used for hedging purposes to test how a portfolio performs under different inflation, interest rate, or commodity price scenarios.\n\n\n25.2.7 Fixed Income & Interest Rate Modeling\nYield curve modeling uses RN scenarios to value bonds and interest rate derivatives. Swaps, swaptions, and credit default swaps (CDS) also rely on RN pricing. RN scenario generators can also simulate yield curves for bond and fixed-income pricing. Models like Cox-Ingersoll-Ross (CIR) or Hull-White generate future interest rate paths.\n\n\n25.2.8 Regulatory & Accounting Valuations\nIFRS 13 & fair value accounting uses RN models determine the market-consistent value of liabilities. Solvency II for insurers asks valuation of policyholder guarantees using RN scenarios.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Scenario Generation</span>"
    ]
  },
  {
    "objectID": "scenario-generation.html#common-economic-scenario-generation-approaches",
    "href": "scenario-generation.html#common-economic-scenario-generation-approaches",
    "title": "25  Scenario Generation",
    "section": "25.3 Common Economic Scenario Generation Approaches",
    "text": "25.3 Common Economic Scenario Generation Approaches\nEconomic scenario generation involves the development of plausible future economic scenarios to assess the potential impact on financial portfolios, investments, or decision-making processes. Various approaches are used to generate economic scenarios,such as adapting underlying stochastic differential equations (SDEs) for Monte Carlo scenario generation techniques.\n\n25.3.1 Interest Rate Models\n\n25.3.1.1 Vasicek and Cox Ingersoll Ross (CIR)\nThe Vasicek model is a one-factor model commonly used for simulating interest rate scenarios. It describes the dynamics of short-term interest rates using a stochastic differential equation (SDE). In a Monte Carlo simulation, we can use the Vasicek model to generate multiple interest rate paths. The CIR model is an extension of the Vasicek model with non-constant volatility. It addresses the issue of negative interest rates by ensuring that interest rates remain positive. Vasicek is defined as\n\\[\ndr(t) = \\kappa (\\theta - r(t)) \\, dt + \\sigma \\, dW(t)\n\\]\nwhere\n\n\\(r(t)\\) is the short-term interest rate at time \\(t\\).\n\\(κ\\) is the speed of mean reversion, representing how quickly the interest rate reverts to its long-term mean.\n\\(θ\\) is the long-term mean or equilibrium level of the interest rate.\n\\(σ\\) is the volatility of the interest rate.\n\\(dW(t)\\) is a Wiener process or Brownian motion, representing a random shock.\n\nAnd CIR is defined as\n\\[\ndr(t) = \\kappa (\\theta - r(t)) \\, dt + \\sigma \\sqrt{r(t)} \\, dW(t)\n\\]\nwhere\n\n\\(r(t)\\) is the short-term interest rate at time \\(t\\).\n\\(κ\\) is the speed of mean reversion, representing how quickly the interest rate reverts to its long-term mean.\n\\(θ\\) is the long-term mean or equilibrium level of the interest rate.\n\\(σ\\) is the volatility of the interest rate.\n\\(dW(t)\\) is a Wiener process or Brownian motion, representing a random shock.\n\nThe following code shows a simplified implementation of a CIR model. The specification of \\(dr\\) can be changed to make it a Vasicek model.\n\nusing Random, CairoMakie\n\n# Set seed for reproducibility\nRandom.seed!(1234)\n\n# CIR model parameters\nκ = 0.2       # Speed of mean reversion\nθ = 0.05      # Long-term mean\nσ = 0.1       # Volatility\n\n# Initial short-term interest rate\nr₀ = 0.03\n\n# Number of time steps and simulations\nnum_steps = 252\nnum_simulations = 1_000\n\n# Time increment\nΔt = 1 / 252\n\n# Function to simulate CIR process\nfunction cir_simulation(κ, θ, σ, r₀, Δt, num_steps, num_simulations)\n    interest_rate_paths = zeros(num_steps, num_simulations)\n    for j in 1:num_simulations\n        interest_rate_paths[1, j] = r₀\n        for i in 2:num_steps\n            dW = randn() * sqrt(Δt)\n            # for Vasicek\n            # dr = κ * (θ - interest_rate_paths[i-1, j]) * Δt + σ * dW\n            dr = κ * (θ - interest_rate_paths[i-1, j]) * Δt + σ * sqrt(interest_rate_paths[i-1, j]) * dW\n            interest_rate_paths[i, j] = max(interest_rate_paths[i-1, j] + dr, 0)  # Ensure non-negativity\n        end\n    end\n    return interest_rate_paths\nend\n\n# Run CIR simulation\ncir_paths = cir_simulation(κ, θ, σ, r₀, Δt, num_steps, num_simulations)\n\n# Plot the simulated interest rate paths\nf = Figure()\nAxis(f[1, 1])\nfor i in 1:num_simulations\n    lines!(1:num_steps, cir_paths[:, i])\nend\nf\n\n\n\n\n\n\n25.3.1.2 Hull White\nThe Hull-White model is a one-factor model that extends the Vasicek model by allowing the mean reversion and volatility parameters to be time-dependent. It is commonly used for pricing interest rate derivatives. Brace-Gatarek-Musiela (BGM) Model extends the Hull-White model to incorporate more factors. It is one of the Libor Market Model (LMM) that describes the evolution of forward rates. It allows for the modeling of both the short-rate and the entire yield curve. It is defined as\n\\[\ndr(t) = (\\theta(t) - a r(t)) \\, dt + \\sigma(t) \\, dW(t)\n\\]\nwhere\n\n\\(r(t)\\) is the short-term interest rate at time \\(t\\).\n\\(θ\\) is the long-term mean or equilibrium level of the interest rate.\n\\(a\\) is the speed of mean reversion.\n\\(σ(t)\\) is the time-dependent volatility of the interest rate.\n\\(dW(t)\\) is a Wiener process or Brownian motion, representing a random shock.\n\n\nusing Random, CairoMakie\n\n# Set seed for reproducibility\nRandom.seed!(1234)\n\n# Hull-White model parameters\nα = 0.1       # Mean reversion speed\nσ = 0.02      # Volatility\nr₀ = 0.03     # Initial short-term interest rate\n\n# Number of time steps and simulations\nnum_steps = 252\nnum_simulations = 1_000\n\n# Time increment\nΔt = 1 / 252\n\n# Function to simulate Hull-White process\nfunction hull_white_simulation(α, σ, r₀, Δt, num_steps, num_simulations)\n    interest_rate_paths = zeros(num_steps, num_simulations)\n    for j in 1:num_simulations\n        interest_rate_paths[1, j] = r₀\n        for i in 2:num_steps\n            dW = randn() * sqrt(Δt)\n            dr = α * (σ - interest_rate_paths[i-1, j]) * Δt + σ * dW\n            interest_rate_paths[i, j] = interest_rate_paths[i-1, j] + dr\n        end\n    end\n    return interest_rate_paths\nend\n\n# Run Hull-White simulation\nhull_white_paths = hull_white_simulation(α, σ, r₀, Δt, num_steps, num_simulations)\n\n# Plot the simulated interest rate paths\nf = Figure()\nAxis(f[1, 1])\nfor i in 1:num_simulations\n    lines!(1:num_steps, hull_white_paths[:, i])\nend\nf\n\n\n\n\n\n\n\n25.3.2 Equity Models\n\n25.3.2.1 Geometric Brownian Motion (GBM)\nGBM is a stochastic process commonly used to model the price movement of financial instruments, including stocks. It assumes constant volatility and is characterized by a log-normal distribution. It is defined as\n\\[\ndS(t) = \\mu S(t) \\, dt + \\sigma S(t) \\, dW(t)\n\\]\nwhere\n\n\\(S(t)\\) is the stock price at time \\(t\\).\n\\(μ\\) is the drift coefficient (expected return).\n\\(σ\\) is the volatility coefficient.\n\\(dW(t)\\) is a Wiener process or Brownian motion, representing a random shock.\n\n\nusing Random, CairoMakie\n\n# Set seed for reproducibility\nRandom.seed!(1234)\n\n# GBM parameters\nμ = 0.05       # Drift (expected return)\nσ = 0.2        # Volatility\n\n# Initial stock price\nS₀ = 100\n\n# Number of time steps and simulations\nnum_steps = 252\nnum_simulations = 1_000\n\n# Time increment\nΔt = 1 / 252\n\n# Function to simulate GBM\nfunction gbm_simulation(μ, σ, S₀, Δt, num_steps, num_simulations)\n    stock_price_paths = zeros(num_steps, num_simulations)\n    for j in 1:num_simulations\n        stock_price_paths[1, j] = S₀\n        for i in 2:num_steps\n            dW = randn() * sqrt(Δt)\n            S = stock_price_paths[i-1, j]\n            dS = μ * S * Δt + σ * S * dW\n            stock_price_paths[i, j] = S + dS\n        end\n    end\n    return stock_price_paths\nend\n\n# Run GBM simulation\ngbm_paths = gbm_simulation(μ, σ, S₀, Δt, num_steps, num_simulations)\n\n# Plot the simulated stock price paths\nf = Figure()\nAxis(f[1, 1])\nfor i in 1:num_simulations\n    lines!(1:num_steps, gbm_paths[:, i])\nend\nf\n\n\n\n\n\n\n25.3.2.2 Generalized Autoregressive Conditional Heteroskedasticity (GARCH)\nGARCH models capture time-varying volatility. They are often used in conjunction with other models to forecast volatility. It is defined as\n\\[\n\\sigma^2_t = \\omega + \\alpha_1 r^2_{t-1} + \\beta_1 \\sigma^2_{t-1}\n\\]\n\\[\nr_t = \\varepsilon_t \\sqrt{\\sigma^2_t}\n\\]\n\n\\(σ^2_t\\) is the conditional variance at time \\(t\\)\n\\(r_t\\) is the return at time \\(t\\)\n\\(\\varepsilon_t\\) is a white noise or innovation process\n\\(\\omega\\), \\(\\alpha_1\\), \\(\\beta_1\\) are model parameters\n\n\nusing Random, CairoMakie\n\n# Set seed for reproducibility\nRandom.seed!(1234)\n\n# GARCH(1,1) parameters\nα₀ = 0.01      # Constant term\nα₁ = 0.1       # Coefficient for lagged squared returns\nβ₁ = 0.8       # Coefficient for lagged conditional volatility\n\n# Number of time steps and simulations\nnum_steps = 252\nnum_simulations = 1_000\n\n# Time increment\nΔt = 1 / 252\n\n# Function to simulate GARCH(1,1) volatility\nfunction garch_simulation(α₀, α₁, β₁, num_steps, num_simulations)\n    volatility_paths = zeros(num_steps, num_simulations)\n    for j in 1:num_simulations\n        ε = randn(num_steps)\n        squared_returns = zeros(num_steps)\n        for i in 2:num_steps\n            squared_returns[i] = α₀ + α₁ * ε[i-1]^2 + β₁ * squared_returns[i-1]\n            volatility_paths[i, j] = sqrt(squared_returns[i])\n        end\n    end\n    return volatility_paths\nend\n\n# Run GARCH simulation\ngarch_paths = garch_simulation(α₀, α₁, β₁, num_steps, num_simulations)\n\n# Plot the simulated volatility paths\nf = Figure()\nAxis(f[1, 1])\nfor i in 1:num_simulations\n    lines!(1:num_steps, garch_paths[:, i])\nend\nf\n\n\n\n\n\n\n\n25.3.3 Copulas\nSimulating data using copulas involves generating multivariate samples with specified marginal distributions and a copula structure.\n\nusing Random, CairoMakie, BivariateCopulas\n\n# Set seed for reproducibility\nRandom.seed!(1234)\n\n# Generate a Gaussian copula\ngaussian_copula = Gaussian(0.8)\n\n# Show simulated copula\nf = scatter(rand(gaussian_copula, 10^4))\nf\n\n\n\n\nCopulas can also be used to infer combined distributions from data samples.\n\nusing Copulas, Distributions, Random\n\nX₁ = Gamma(2, 3)\nX₂ = Pareto()\nX₃ = LogNormal(0, 1)\nC = ClaytonCopula(3, 0.7) # A 3-variate Clayton Copula with θ = 0.7\nD = SklarDist(C, (X₁, X₂, X₃)) # The final distribution\n\n# Generate a dataset\nsimu = rand(D, 1000)\n# We may estimate a copula, or get parameters of underlying distributions, using the `fit` function:\nD̂ = fit(SklarDist{ClaytonCopula,Tuple{Gamma,Normal,LogNormal}}, simu)\n\nCopulas.SklarDist{Copulas.ClaytonCopula{3, Float64}, Tuple{Distributions.Gamma{Float64}, Distributions.Normal{Float64}, Distributions.LogNormal{Float64}}}(\nC: Copulas.ClaytonCopula{3, Float64}(\nG: Copulas.ClaytonGenerator{Float64}(0.7255762179151387)\n)\n\nm: (Distributions.Gamma{Float64}(α=1.9509359315325794, θ=3.0668504198367565), Distributions.Normal{Float64}(μ=6.958764796293847, σ=27.415016590130424), Distributions.LogNormal{Float64}(μ=0.01132053842187167, σ=1.0263584835287456))\n)",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Scenario Generation</span>"
    ]
  },
  {
    "objectID": "similarity-calculation.html",
    "href": "similarity-calculation.html",
    "title": "26  Similarity Analysis",
    "section": "",
    "text": "26.1 Chapter Overview\nGiven a set of interest, understanding the relative similarity (or not) of features of interest is useful in classification and data compression techniques.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Similarity Analysis</span>"
    ]
  },
  {
    "objectID": "similarity-calculation.html#the-data",
    "href": "similarity-calculation.html#the-data",
    "title": "26  Similarity Analysis",
    "section": "26.2 The Data",
    "text": "26.2 The Data\nStored data can generally be categorized into two formats: tabular (structured) and non-tabular (unstructured). Structured data format is a structured way of organizing and presenting data in rows and columns, resembling a table. This format is widely used for storing and representing structured datasets, making it easy to read, analyze, and manipulate data. The most common example of structured data is a spreadsheet, where data is organized into rows and columns. Structured data can also be stored in relational databases for easier lookups and matching. On the other hand, unstructured data refers to data that lacks a predefined data model or structure. Unlike structured data, which fits neatly into tables or databases, unstructured data does not have a predefined schema. It can include text documents, images, audio files, video files, social media posts, and more.\nStructured data can be further categorized into numerical and categorical data based on the types of values they represent. The following data tables will be referenced throughout the chapter. Real numerical data can easily be converted or normalized to a series of floating points, and real categorical data to a series of binary literals through one-hot encoding procedures.\n\nsample_csv_data =\n    IOBuffer(\n        raw\"id,sex,benefit_base,education,occupation,issue_age\n         1,M,100000.0,college,1,30.0\n         2,F,200000.0,master,3,20.0\n         3,M,150000.0,high_school,4,40.0\n         4,F,50000.0,college,2,60.0\n         5,M,250000.0,college,1,40.0\n         6,F,200000.0,high_school,2,30.0\"\n    )\n\nIOBuffer(data=UInt8[...], readable=true, writable=false, seekable=true, append=false, size=278, maxsize=Inf, ptr=1, mark=-1)\n\n\n\nusing CSV, DataFrames, TableTransforms\n\ndf = CSV.read(sample_csv_data, DataFrame)\ndf_num = apply(MinMax(), df[:, [:benefit_base, :issue_age]])[1]\n\n6×2 DataFrame\n\n\n\nRow\nbenefit_base\nissue_age\n\n\n\nFloat64\nFloat64\n\n\n\n\n1\n0.25\n0.25\n\n\n2\n0.75\n0.0\n\n\n3\n0.5\n0.5\n\n\n4\n0.0\n1.0\n\n\n5\n1.0\n0.5\n\n\n6\n0.75\n0.25\n\n\n\n\n\n\n\nusing StatsBase\n\narr_cat = hcat(indicatormat(df.sex)', indicatormat(df.education)', indicatormat(df.occupation)')\n\n6×9 Matrix{Bool}:\n 0  1  1  0  0  1  0  0  0\n 1  0  0  0  1  0  0  1  0\n 0  1  0  1  0  0  0  0  1\n 1  0  1  0  0  0  1  0  0\n 0  1  1  0  0  1  0  0  0\n 1  0  0  1  0  0  1  0  0\n\n\nFor unstructured data, due to the nature of their variety, the choice of representation depends on the type of data and the specific task at hand. For text data, a Word2Vec embedding is commonly used, while Convolutional Neural Networks (CNNs) are for image data and wave transforms are for audio data. No matter which transformation is applied, unstructured data can generally be converted to a series of floating points, just like numerical structured data.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Similarity Analysis</span>"
    ]
  },
  {
    "objectID": "similarity-calculation.html#common-similarity-measures",
    "href": "similarity-calculation.html#common-similarity-measures",
    "title": "26  Similarity Analysis",
    "section": "26.3 Common Similarity Measures",
    "text": "26.3 Common Similarity Measures\nThe following measures are commonly used to calculate similarities.\n\n26.3.1 Euclidean Distance (L2 norm)\nEuclidean distance, also known as the L2 norm, is defined as \\[\nd = \\sqrt{\\sum_{i=1}^{n} (w_i - v_i)^2}\n\\] The distance is usually meaningful when applied to numerical data. The following Julia code shows the Euclidean distance for the first two rows in df_num.\n\nusing LinearAlgebra\n\n#d₁₂ = √(∑((Array(df_num[1, :]) .- Array(df_num[2, :])) .* (Array(df_num[1, :]) .- Array(df_num[2, :]))))\nd₁₂ = LinearAlgebra.norm(Array(df_num[1, :]) .- Array(df_num[2, :]))\n\n0.5590169943749475\n\n\n\n\n26.3.2 Manhattan Distance (L1 Norm)\nManhattan distance, also known as the L1 norm, is defined as \\[\nd = \\sum_{i=1}^{n} |w_i - v_i|\n\\] The distance is also usually meaningful when applied to numerical data. The following Julia code shows the Euclidean distance for the first two rows in df_num.\n\nusing LinearAlgebra\n\n#d₁₂ = ∑(abs.(Array(df_num[1, :]) .- Array(df_num[2, :])))\nd₁₂ = LinearAlgebra.norm1(Array(df_num[1, :]) .- Array(df_num[2, :]))\n\n0.75\n\n\n\n\n26.3.3 Cosine Similarity\nCosine similarity is defined as \\[\nd = \\frac{\\sum_{i=1}^{n} w_i \\cdot v_i}{\\sqrt{\\sum_{i=1}^{n} w_i^2} \\cdot \\sqrt{\\sum_{i=1}^{n} v_i^2}}\n\\] The distance would be meaningful when applied to both numerical and categorical data.\nThe following Julia code shows the cosine similarity for the first two rows in df_num.\n\nusing LinearAlgebra\n\nd₁₂ = (Array(df_num[1, :]) ⋅ Array(df_num[2, :])) / norm(df_num[1, :]) / norm(df_num[2, :])\n\n0.7071067811865475\n\n\nThe following Julia code shows the cosine similarity for the first and the third rows in arr_cat.\n\nusing LinearAlgebra\n\nd₁₃ = (arr_cat[1, :] ⋅ arr_cat[3, :]) / norm(arr_cat[1, :]) / norm(arr_cat[3, :])\n\n0.33333333333333337\n\n\nNote how similar the syntax of processing for numerical or categorical data is. Multiple dispatch allows Julia to identify most efficient underlying procedure for different types of data. For categorical data, the \\(dot\\) operation on binary vectors is essentially count of 1’s, while for numerical data it is the \\(dot\\) operation for most numerical processing libraries.\n\n\n26.3.4 Jaccard Similarity\nJaccard similarity is defined as \\[\nd = \\frac{|W \\cap V|}{|W \\cup V|}\n\\] The distance is usually meaningful when applied to categorical data. The following Julia code shows the Jaccard similarity for the first and the third rows in arr_cat.\n\nd₁₃ = (arr_cat[1, :] ⋅ arr_cat[3, :]) / sum(arr_cat[1, :] .| arr_cat[3, :])\n\n0.2\n\n\n\n\n26.3.5 Hamming Distance\nHamming distance is defined as d = Number of positions at which w and v differ. The distance is usually meaningful when applied to categorical data. The following Julia code shows the Hamming distance for the first and the third rows in arr_cat.\n\nd₁₃ = sum(arr_cat[1, :] .⊻ arr_cat[3, :])\n\n4",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Similarity Analysis</span>"
    ]
  },
  {
    "objectID": "similarity-calculation.html#k-nearest-neighbor-knn-clustering",
    "href": "similarity-calculation.html#k-nearest-neighbor-knn-clustering",
    "title": "26  Similarity Analysis",
    "section": "26.4 k-Nearest Neighbor (kNN) Clustering",
    "text": "26.4 k-Nearest Neighbor (kNN) Clustering\nkNN is primarily known as a classification algorithm, but it can also be used for clustering, particularly in the context of density-based clustering. Density-based clustering identifies regions in the data space where the density of data points is higher, and it groups points in these high-density regions. The core idea of kNN clustering is to assign each data point to a cluster based on the density of its neighbors. A data point becomes a core point if it has at least a specified number of neighbors within a certain distance.\n\nusing Random, NearestNeighbors, CairoMakie\n\n# Step 1: Generate synthetic data\nRandom.seed!(1234)  # For reproducibility\ndata = rand(10, 2)  # 10 points with 2 dimensions\nprintln(\"Dataset:\\n\", data)\n\n# Step 2: Create a KD-tree for efficient nearest neighbor search\nkdtree = KDTree(data)\n\n# Step 3: Define a query point (for which we want to find nearest neighbors)\nquery_point = [0.5, 0.5]\n\n# Step 4: Find the nearest neighbors\n# Specify how many neighbors to find\nk = 1\nindices, distances = knn(kdtree, query_point, k)\n\n# Step 5: Display the results\nprintln(\"\\nQuery Point: \", query_point)\nprintln(\"Indices of Nearest Neighbors: \", indices)\nprintln(\"Distances to Nearest Neighbors: \", distances)\n\n# Step 6: Visualize the points and the query\nf = Figure()\naxis = Axis(f[1, 1], title=\"Nearest Neighbors Search\")\nscatter!(data[:, 1], data[:, 2], label=\"Data Points\", color=:blue)\nscatter!([query_point[1]], [query_point[2]], label=\"Query Point\", color=:red, marker=:cross, markersize=10)\n\n# Highlight nearest neighbors\nfor idx in indices\n    lines!([query_point[1], data[idx, 1]], [query_point[2], data[idx, 2]], color=:black)\nend\nf\n\nDataset:\n[0.5798621201341324 0.5667043025437501; 0.4112941179498505 0.5363685687293304; … ; 0.7897644095351307 0.5743234852783174; 0.6960406981439002 0.6776499075995779]\n\nQuery Point: [0.5, 0.5]\nIndices of Nearest Neighbors: [2]\nDistances to Nearest Neighbors: [0.07597457975710152]",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Similarity Analysis</span>"
    ]
  },
  {
    "objectID": "portfolio-optimization.html",
    "href": "portfolio-optimization.html",
    "title": "27  Portfolio Optimization",
    "section": "",
    "text": "27.1 Chapter Overview\nOptimization in a portfolio context with examples of asset selection under different constraints and objectives.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Portfolio Optimization</span>"
    ]
  },
  {
    "objectID": "portfolio-optimization.html#the-data",
    "href": "portfolio-optimization.html#the-data",
    "title": "27  Portfolio Optimization",
    "section": "27.2 The Data",
    "text": "27.2 The Data\n\nμ = [0.1, 0.15, 0.12] # returns\nρ = [0.1 0.05 0.03;\n    0.05 0.12 0.04;\n    0.03 0.04 0.08] # covariances\nnₐ = length(μ) # number of assets\n\n3",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Portfolio Optimization</span>"
    ]
  },
  {
    "objectID": "portfolio-optimization.html#theory",
    "href": "portfolio-optimization.html#theory",
    "title": "27  Portfolio Optimization",
    "section": "27.3 Theory",
    "text": "27.3 Theory\nHarry Markowitz introduced the modern portfolio theory in 1952. The main idea is that investors are pursuing to maximize their expected return of a portfolio given a certain amount of risk. By definition any portfolio yielding a higher return must have higher amount of risk, so there is a trade-off between desired expected returns and allowable risks. The risk versus maximized expected return relationship can be plotted out as a curve, a.k.a. the efficient frontier.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Portfolio Optimization</span>"
    ]
  },
  {
    "objectID": "portfolio-optimization.html#mathematical-tools",
    "href": "portfolio-optimization.html#mathematical-tools",
    "title": "27  Portfolio Optimization",
    "section": "27.4 Mathematical tools",
    "text": "27.4 Mathematical tools\n\n27.4.1 Mean-variance optimization model\nMean-variance optimization is a mathematical framework that seeks to maximize expected returns while minimizing portfolio variance (or standard deviation). It involves calculating the expected return and risk of individual assets and finding the optimal combination of assets to achieve the desired risk-return tradeoff.\n\\[\\begin{align*}\n\\text{minimize} \\quad &{w}^{T}\\Sigma{w}\\\\\n\\text{subject to} \\quad &{R}^{T}\\geq{\\mu}_{\\text{target}}\\\\\n&{1}^{T}{w}={1}\\\\\n&{w}\\geq{0}\n\\end{align*}\\]\n\nusing JuMP, Ipopt\n\n# Create an optimization model\nmodel = Model(optimizer_with_attributes(Ipopt.Optimizer, \"print_level\" =&gt; 0))\n# Set up weights as variables to optimize\n@variable(model, w[1:nₐ] &gt;= zero(0.0))\n# Objective: minimize portfolio variance\n@objective(model, Min, sum(w[i] * ρ[i, j] * w[j] for i in 1:nₐ, j in 1:nₐ))\n# Constraints: Sum of portfolio weights should equal to 1, and all weights should be zero or positive\n@constraint(model, sum(w) == 1)\n# May also add additional constraints\n# target_return = 0.1\n# @constraint(model, dot(μ, w) &gt;= target_return)\n# Solve the optimization problem\noptimize!(model)\n# Print results\n@show \"Optimal Portfolio Weights:\"\nfor i = 1:nₐ\n    @show (\"Asset \", i, \": \", value.(w)[i])\nend\n\n\n******************************************************************************\nThis program contains Ipopt, a library for large-scale nonlinear optimization.\n Ipopt is released as open source code under the Eclipse Public License (EPL).\n         For more information visit https://github.com/coin-or/Ipopt\n******************************************************************************\n\n\"Optimal Portfolio Weights:\" = \"Optimal Portfolio Weights:\"\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 1, \": \", 0.3333333012309821)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 2, \": \", 0.16666675086886984)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 3, \": \", 0.4999999479001481)\n\n\nIn mean-variance portfolio optimization, incorporating a cost of risk-based capital on assets is a practical consideration that reflects the additional capital required to support riskier assets in a portfolio. This approach ensures that the optimization process not only maximizes returns relative to risk but also considers the regulatory or internal cost implications associated with holding riskier assets.\n\\[\\begin{align*}\n\\text{maximize} \\quad &{w}^{T}R_{adj}\\\\\n\\text{subject to} \\quad &{w}^{T}\\Sigma{w}\\le\\sigma^2_{max}\\\\\n&{1}^{T}{w}={1}\\\\\n&{w}\\geq{0}\n\\end{align*}\\]\nwhere \\(R_{adj} = [(\\mu_1 - \\lambda_1), (\\mu_2 - \\lambda_2), ..., (\\mu_N - \\lambda_N)]\\) is the adjusted expected returns.\n\nusing JuMP, Ipopt\n\n# Create an optimization model\nmodel = Model(optimizer_with_attributes(Ipopt.Optimizer, \"print_level\" =&gt; 0))\nr = μ .- [0.01, 0.02, 0.05] # risk adjusted returns\nσ²_max = 0.1 # maximum portfolio variance\n# Set up weights as variables to optimize\n@variable(model, w[1:nₐ] &gt;= zero(0.0))\n# Objective: minimize portfolio variance\n@objective(model, Max, sum(w[i] * r[i] for i in 1:nₐ))\n# Constraints: Sum of portfolio weights should equal to 1, and all weights should be zero or positive\n@constraint(model, sum(w) == 1)\n# Constraints: Sum of allowable portfolio variance is limited\n@constraint(model, sum(w[i] * ρ[i, j] * w[j] for i in 1:nₐ, j in 1:nₐ) &lt;= σ²_max)\n# May also add additional constraints\n# target_return = 0.1\n# @constraint(model, dot(μ, w) &gt;= target_return)\n# Solve the optimization problem\noptimize!(model)\n# Print results\n@show \"Optimal Portfolio Weights:\"\nfor i = 1:nₐ\n    @show (\"Asset \", i, \": \", value.(w)[i])\nend\n\n\"Optimal Portfolio Weights:\" = \"Optimal Portfolio Weights:\"\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 1, \": \", 0.16666454953827514)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 2, \": \", 0.8333339906581219)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 3, \": \", 1.4598036030472386e-6)\n\n\n\n\n27.4.2 Efficient frontier analysis\nThe efficient frontier represents the set of portfolios that offer the highest expected return for a given level of risk or the lowest risk for a given level of return. Efficient frontier analysis involves plotting risk-return combinations for different portfolios and identifying the optimal portfolio on the frontier.\n\nusing JuMP, Ipopt, CairoMakie, LinearAlgebra\n\n# Create an optimization model\nmodel = Model(optimizer_with_attributes(Ipopt.Optimizer, \"print_level\" =&gt; 0))\n# Set up weights as variables to optimize\n@variable(model, w[1:nₐ] &gt;= zero(0.0))\n# Define objective function: minimize portfolio variance\nportfolio_variance = w'ρ * w\n@objective(model, Min, portfolio_variance)\n# Constraints: Sum of portfolio weights should equal to 1, and all weights should be zero or positive\n@constraint(model, sum(w) == 1)\n# Generate a range of target returns\npoints = 100\ntarget_returns = range(minimum(μ), maximum(μ), length=points)\n\nefficient_frontier = []\nfor target_return in target_returns\n    # Add additional constraint for target return\n    @constraint(model, c, dot(μ, w) == target_return)\n    # Solve the problem\n    optimize!(model)\n    # Show solution\n    if termination_status(model) == MOI.LOCALLY_SOLVED\n        push!(efficient_frontier, (sqrt(objective_value(model)), target_return))\n    end\n    unregister(model, :c)\n    delete(model, c)\nend\n# Plot Efficient Frontier\nfig = Figure()\nAxis(fig[1, 1])\nlines!(map(x -&gt; x[1], efficient_frontier), map(x -&gt; x[2], efficient_frontier))\nfig\n\n\n\n\n\n\n27.4.3 Black-Litterman\nThe Black-Litterman model combines the views of investors with market equilibrium assumptions to generate optimal portfolios. It starts with a market equilibrium portfolio and adjusts it based on investor views and confidence levels. The model incorporates subjective opinions while maintaining diversification and risk management principles.\n\\[\\begin{align*}\n\\text{maximize} \\quad & \\mu^T w - \\lambda \\cdot \\frac{1}{2} w^T \\Sigma w \\\\\n\\text{subject to} \\quad & \\sum_{i=1}^{N} w_i = 1 \\\\\n& w_i \\geq 0, \\quad \\forall i\n\\end{align*}\\]\n\nusing JuMP, Ipopt\n\nλ = 2.5 # risk aversion\nrfr = 0.02 # risk free rate\n# Market equilibrium parameters (prior)\nμ_market = [0.08, 0.08, 0.08] # Market equilibrium return\nΣ_market = ρ # Market equilibrium covariance matrix\n# Investor views\nQ = μ # Expected returns on assets according to investor views\nP = [1 0 0; 0 1 0; 0 0 1]     # Pick matrix specifying which assets views are on\nΩ = [0.001^2 0.0 0.0; 0.0 0.002^2 0.0; 0.0 0.0 0.003^2]  # Views uncertainty (covariance matrix)\n\n# Create an optimization model\nmodel = Model(optimizer_with_attributes(Ipopt.Optimizer, \"print_level\" =&gt; 0))\n# Set up weights as variables to optimize\n@variable(model, w[1:nₐ] &gt;= zero(0.0))\n# Black-Litterman expected return adjustment\nΣ_prior_inv = inv(Σ_market)\nτ = 0.05 # Scaling factor\n# Calculate the posterior expected returns\nμ_posterior = Σ_prior_inv * (τ * Σ_market * (Σ_prior_inv + P' * inv(Ω) * P)) \\\n              (τ * Σ_market * (Σ_prior_inv * μ_market + P' * inv(Ω) * Q) + Σ_prior_inv * μ_market)\n# Objective: maximize sharpe ratio\nsr = (w' * μ_posterior - rfr) / (λ / 2 * w' * Σ_market * w)\n@objective(model, Max, sr)\n# Constraints: Sum of portfolio weights should equal to 1, and all weights should be zero or positive\n@constraint(model, sum(w) == 1)\n# Solve the optimization problem\noptimize!(model)\n# Print results\nv = sqrt(value.(w)' * Σ_market * value.(w))\n@show \"Optimal Portfolio Weights, Expected Portfolio Return, Portfolio Volatility:\", v\nfor i = 1:nₐ\n    @show (\"Asset \", i, \": \", value.(w)[i], value.(w)[i] * μ_posterior[i])\nend\n\n(\"Optimal Portfolio Weights, Expected Portfolio Return, Portfolio Volatility:\", v) = (\"Optimal Portfolio Weights, Expected Portfolio Return, Portfolio Volatility:\", 0.2606134101929549)\n(\"Asset \", i, \": \", value.(w)[i], value.(w)[i] * μ_posterior[i]) = (\"Asset \", 1, \": \", 2.0824146208647934e-8, 2.5581726822010987e-10)\n(\"Asset \", i, \": \", value.(w)[i], value.(w)[i] * μ_posterior[i]) = (\"Asset \", 2, \": \", 0.2311617288917621, 0.009281526926739852)\n(\"Asset \", i, \": \", value.(w)[i], value.(w)[i] * μ_posterior[i]) = (\"Asset \", 3, \": \", 0.7688382502840917, 0.03861300028744048)\n\n\n\n\n27.4.4 Risk Parity\nRisk parity is an asset allocation strategy that allocates capital based on risk rather than traditional measures such as market capitalization or asset prices. It aims to balance risk contributions across different assets or asset classes to achieve a more stable portfolio. Risk parity portfolios often include assets with different risk profiles, such as stocks, bonds, and commodities.\n\\[\\begin{align*}\n\\text{minimize} \\quad & \\sum_{i=1}^{N} (w_i \\cdot \\sqrt{\\sigma_i})^2 \\\\\n\\text{subject to} \\quad & \\sum_{i=1}^{N} w_i = 1 \\\\\n& w_i \\geq 0, \\quad \\forall i\n\\end{align*}\\]\n\nusing JuMP, Ipopt\n\n# Create an optimization model\nmodel = Model(optimizer_with_attributes(Ipopt.Optimizer, \"print_level\" =&gt; 0))\n# Set up weights as variables to optimize\n@variable(model, w[1:nₐ] &gt;= zero(0.0))\n# Objective: minimize portfolio variance\nportfolio_variance = w'ρ * w\nmargin = (ρ * w ./ sqrt(portfolio_variance)) .* w\nrisk_contributions = margin ./ sum(margin)\ntarget = repeat([1.0 / nₐ], nₐ)\n@objective(model, Max, sum((risk_contributions .- target) .^ 2))\n# Constraints: Sum of portfolio weights should equal to 1, and all weights should be zero or positive\n@constraint(model, sum(w) == 1)\n# Solve the optimization problem\noptimize!(model)\n# Print results\n@show \"Optimal Portfolio Weights:\"\nfor i = 1:nₐ\n    @show (\"Asset \", i, \": \", value.(w)[i])\nend\n\n\"Optimal Portfolio Weights:\" = \"Optimal Portfolio Weights:\"\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 1, \": \", -6.957484531612737e-9)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 2, \": \", 1.0000000131375544)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 3, \": \", -6.180069741122123e-9)\n\n\n\n\n27.4.5 Sharpe Ratio Maximization\nThe Sharpe ratio measures the risk-adjusted return of a portfolio and is calculated as the ratio of excess return to volatility. Maximizing the Sharpe ratio involves finding the portfolio allocation that offers the highest risk-adjusted return. This approach focuses on achieving the best tradeoff between risk and return.\n\\[\\begin{align*}\n\\text{maximize} \\quad & \\frac{E[R_p] - R_f}{\\sigma_p} \\\\\n\\text{subject to} \\quad & \\sum_{i=1}^{N} w_i = 1 \\\\\n& w_i \\geq 0, \\quad \\forall i\n\\end{align*}\\]\n\nusing JuMP, Ipopt\n\n# Create an optimization model\nmodel = Model(optimizer_with_attributes(Ipopt.Optimizer, \"print_level\" =&gt; 0))\n# Set up weights as variables to optimize\n@variable(model, w[1:nₐ] &gt;= zero(0.0))\n# Objective: minimize portfolio variance\nrfr = 0.05 # risk free rate\n@objective(model, Max, (dot(μ, w) - rfr) / sqrt(sum(w[i] * ρ[i, j] * w[j] for i in 1:nₐ, j in 1:nₐ)))\n# Constraints: Sum of portfolio weights should equal to 1, and all weights should be zero or positive\n@constraint(model, sum(w) == 1)\n# Solve the optimization problem\noptimize!(model)\n# Print results\n@show \"Optimal Portfolio Weights:\"\nfor i = 1:nₐ\n    @show (\"Asset \", i, \": \", value.(w)[i])\nend\n\n\"Optimal Portfolio Weights:\" = \"Optimal Portfolio Weights:\"\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 1, \": \", 0.010841995514843134)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 2, \": \", 0.5352292318109132)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 3, \": \", 0.45392877267424375)\n\n\n\n\n27.4.6 Robust Optimization\nRobust optimization techniques aim to create portfolios that are resilient to uncertainties and fluctuations in market conditions. These techniques consider a range of possible scenarios and optimize portfolios to perform well across different market environments. A robust parameter in robust portfolio optimization is typically chosen to ensure the portfolio’s performance remains stable and satisfactory under different market conditions or variations in input data. Robust optimization may involve incorporating stress tests, scenario analysis, or robust risk measures into the portfolio construction process.\n\\[\\begin{align*}\n\\text{minimize} \\quad & w^T \\Sigma w + \\gamma \\|w - w_0\\|_2^2 \\\\\n\\text{subject to} \\quad & \\sum_{i=1}^{N} w_i = 1 \\\\\n& w_i \\geq 0, \\quad \\forall i \\\\\n& \\|(\\Sigma^{1/2} (w - w_0))\\|_2 \\leq \\epsilon\n\\end{align*}\\]\n\nusing JuMP, Ipopt\n\n# Create an optimization model\nmodel = Model(optimizer_with_attributes(Ipopt.Optimizer, \"print_level\" =&gt; 0))\n# Set up weights as variables to optimize\n@variable(model, w[1:nₐ] &gt;= zero(0.0))\n# Objective: minimize portfolio variance\nε = 0.05  # Uncertainty level\nγ = 0.1  # Robustness parameter\nw₀ = [0.3, 0.4, 0.3] # expected weights\n@objective(model, Min, dot(w, ρ * w) + γ * sum((w[i] - w₀[i])^2 for i in 1:nₐ))\n# Constraints: Sum of portfolio weights should equal to 1, and all weights should be zero or positive\n@constraint(model, sum(w) == 1)\n@constraint(model, sum((ρ[i, j] * (w[i] - w₀[i]) * (w[j] - w₀[j])) for i in 1:nₐ, j in 1:nₐ) &lt;= ε)\n# Solve the optimization problem\noptimize!(model)\n# Print results\n@show \"Optimal Portfolio Weights:\"\nfor i = 1:nₐ\n    @show (\"Asset \", i, \": \", value.(w)[i])\nend\n\n\"Optimal Portfolio Weights:\" = \"Optimal Portfolio Weights:\"\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 1, \": \", 0.31250000098314346)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 2, \": \", 0.31250000376951037)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 3, \": \", 0.37499999524734623)\n\n\n\n\n27.4.7 Asset weights from different methodologies\n\nOptimized asset weights from different methodologies\n\n\nMethodology\nAsset weights\n\n\n\n\nStandard mean variance\n[0.33, 0.17, 0.50]\n\n\n(with RBC costs)\n[0.17, 0.83, 0.00]\n\n\nBlack-Litterman\n[0.00, 0.23, 0.77]\n\n\nRisk parity\n[0.00, 1.00, 0.00]\n\n\nSharpe ratio\n[0.01, 0.54, 0.45]\n\n\nRobust\n[0.31, 0.31, 0.38]\n\n\n\nSeeing from the asset weights from a standard mean variance approach, due to RBC costs asset weights shifted to ones with higher yields. Asset weights from Sharpe ratio approach aligns with the Sharpe ratio for each asset. Those from Robust approach seek results not far away from expected weights under different conditions.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Portfolio Optimization</span>"
    ]
  },
  {
    "objectID": "portfolio-optimization.html#practical-considerations",
    "href": "portfolio-optimization.html#practical-considerations",
    "title": "27  Portfolio Optimization",
    "section": "27.5 Practical considerations",
    "text": "27.5 Practical considerations\n\n27.5.1 Fractional purchases of assets\nIn traditional portfolio optimization, fractional purchases of assets refer to the ability to allocate fractions or percentages of capital to individual assets. However, in certain contexts or practical implementations, fractional purchases may not be allowed or considered.\n\nPractical constraints. Some investment vehicles or platforms may restrict investors from purchasing fractions of shares or assets. For instance, certain mutual funds, exchange-traded funds (ETFs), or other investment products may require whole units of shares to be purchased.\nSimplicity and cost-effectiveness. Handling fractional shares can add complexity and operational costs to portfolio management, especially in terms of transaction fees, administrative overhead, and reconciliation processes.\nMarket liquidity. Some assets may have limited liquidity or trading volumes, making it impractical or difficult to execute fractional purchases without significantly impacting market prices or transaction costs.\nRegulatory considerations. Regulations in certain jurisdictions may impose restrictions on fractional share trading or ownership, potentially limiting the ability to include fractional purchases in portfolio optimization strategies.\n\n\n\n27.5.2 Large number of assets\nIn portfolio optimization, a penalty factor for a large volume of assets typically refers to a mechanism or adjustment applied to the optimization process to mitigate the potential biases or challenges that arise when dealing with a large number of assets. This concept is particularly relevant in the context of mean-variance optimization and other optimization frameworks where computational efficiency and practical portfolio management considerations come into play. Too many assets may have the following issues.\n\nDimensionality. As the number of assets (or dimensions) increases in a portfolio, traditional optimization methods may become computationally intensive or prone to overfitting. This is because the complexity of the optimization problem grows exponentially with the number of assets.\nSparsity and concentration. In practice, not all assets may contribute equally to portfolio performance. Some assets may have negligible impact on the overall portfolio characteristics (such as risk or return) due to low weights or correlations with other assets.\nPenalizing excessive complexity. A penalty factor can be introduced to penalize portfolios that overly diversify or allocate small weights to a large number of assets. This encourages the optimization process to focus on more significant assets or reduce the complexity of the portfolio structure.\n\nThere are various ways to implement a penalty factor for a large volume of assets:\n\nRegularization techniques. Techniques like Lasso (L1 regularization) or Ridge (L2 regularization) regression can penalize small weights or excessive diversification by adding a penalty term to the objective function.\nSubset selection. Methods that explicitly select a subset of assets based on their contribution to portfolio performance, rather than including all assets indiscriminately.\nHeuristic adjustments. Introducing heuristic rules or adjustments based on practical portfolio management principles or empirical observations.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Portfolio Optimization</span>"
    ]
  },
  {
    "objectID": "sensitivity-analysis.html",
    "href": "sensitivity-analysis.html",
    "title": "28  Sensitivity Analysis",
    "section": "",
    "text": "28.1 Chapter Overview\nDifferent approaches to understanding the sensitivity of a model to changes in its inputs: derivatives, finite differences, global sensitivity analysis approaches, and statistical approaches.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sensitivity Analysis</span>"
    ]
  },
  {
    "objectID": "sensitivity-analysis.html#setup",
    "href": "sensitivity-analysis.html#setup",
    "title": "28  Sensitivity Analysis",
    "section": "28.2 Setup",
    "text": "28.2 Setup\nLet’s assume there are certain insurance policies throughout the chapter.\n\nusing Dates\n\n@enum Sex Female = 1 Male = 2\n@enum Risk Standard = 1 Preferred = 2\n\nmutable struct Policy\n    id::Int\n    sex::Sex\n    benefit_base::Float64\n    COLA::Float64\n    mode::Int\n    prem::Float64\n    pp::Int\n    issue_date::Date\n    issue_age::Int\n    risk::Risk\nend",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sensitivity Analysis</span>"
    ]
  },
  {
    "objectID": "sensitivity-analysis.html#the-data",
    "href": "sensitivity-analysis.html#the-data",
    "title": "28  Sensitivity Analysis",
    "section": "28.3 The Data",
    "text": "28.3 The Data\n\nusing MortalityTables\nsample_csv_data =\n    IOBuffer(\n        raw\"id,sex,benefit_base,COLA,mode,prem,pp,issue_date,issue_age,risk\n         1,M,100000.0,0.03,1,1000.0,3,1999-12-05,30,Std\"\n    )\n\nmort = Dict(\n    Male =&gt; MortalityTables.table(988).ultimate,\n    Female =&gt; MortalityTables.table(992).ultimate,\n)\n\nDict{Sex, OffsetArrays.OffsetVector{Float64, Vector{Float64}}} with 2 entries:\n  Male   =&gt; [0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.0225…\n  Female =&gt; [0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.0…\n\n\n\nusing CSV, DataFrames\n\npolicies = let\n\n    # read CSV directly into a dataframe\n    # df = CSV.read(\"sample_inforce.csv\",DataFrame) # use local string for notebook\n    df = CSV.read(sample_csv_data, DataFrame)\n\n    # map over each row and construct an array of Policy objects\n    map(eachrow(df)) do row\n        Policy(\n            row.id,\n            row.sex == \"M\" ? Male : Female,\n            row.benefit_base,\n            row.COLA,\n            row.mode,\n            row.prem,\n            row.pp,\n            row.issue_date,\n            row.issue_age,\n            row.risk == \"Std\" ? Standard : Preferred,\n        )\n    end\n\nend\n\n1-element Vector{Policy}:\n Policy(1, Male, 100000.0, 0.03, 1, 1000.0, 3, Date(\"1999-12-05\"), 30, Standard)\n\n\nGiven a basic insurance product, a pure whole of life (WOL) policy with level benefits and level premiums payable within the first 10 years, the reserve at the end of the \\(y^{th}\\) policy year is defined by\n\\[\nres(y) = \\sum_{t=age+y}^{120} (sur_{t-age-y} * mort_t * B_y * \\sqrt{1 + r}) - (P_y * sur_{t-age-y})\n\\]\nwhere\n\n\\(mort_t\\) is the mortality at age \\(t\\)\n\\(p_y\\) is the survival probability adjusted with COLA, with values of\n\n\\(p_{y-1} = 1\\),\n\\(p_x = p_{x-1} * (1 - mort_{age+y}) / (1 + COLA)\\) for \\(x &gt;= y\\), and\n0 for \\(x &lt; y - 1\\) or \\(age + x &gt;= 120\\), or ultimate age of the current mortality table\n\n\\(B_y\\) is the level benefit throughout the policy\n\\(P_y\\) is the level premium within the first 10 policy years which is 0 for policy years after 10\n\\(r\\) is the level interest rate throughout the policy\n\n\nfunction sur(y::Int, pol::Policy)\n    if y == 0\n        1\n    elseif y &lt; 0 || 120 - y &lt;= pol.issue_age\n        0\n    else\n        sur(y - 1, pol) * (1 - mort[pol.sex][pol.issue_age+y]) / (1 + pol.COLA)\n    end\nend\n\nfunction res(y::Int, pol::Policy)\n    s = 0.0\n    if y &gt;= 1 && y &lt;= 120 - pol.issue_age\n        for t in (pol.issue_age+y):120\n            prem = 0.0\n            if y &lt;= pol.pp\n                prem = pol.prem\n            end\n            s += sur(t - pol.issue_age - y, pol) * mort[pol.sex][t] * pol.benefit_base - prem * sur(t - pol.issue_age - y, pol)\n        end\n    end\n    s\nend\n\nres (generic function with 1 method)",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sensitivity Analysis</span>"
    ]
  },
  {
    "objectID": "sensitivity-analysis.html#common-sensitivity-analysis-methodologies",
    "href": "sensitivity-analysis.html#common-sensitivity-analysis-methodologies",
    "title": "28  Sensitivity Analysis",
    "section": "28.4 Common Sensitivity Analysis Methodologies",
    "text": "28.4 Common Sensitivity Analysis Methodologies\n\n28.4.1 Finite Differences\nDefine a customized finite difference function with respect to the COLA, rippled by a small difference.\n\nfunction res_wrt_r_fd(y::Int, pol::Policy, r::Float64, h=1e-3)\n    p₊, p₋ = deepcopy(pol), deepcopy(pol)\n    p₊.COLA, p₋.COLA = r + h, r - h\n    (res(y, p₋) - res(y, p₊)) / (2res(y, pol))\nend\n\nres_wrt_r_fd(2, policies[1], 0.03) # changes in reserve at year 2 when the interest rate at 3% with a perturbation of 0.1%\n\n0.021366520936389077\n\n\n\n\n28.4.2 Regression Analyses\n\nusing GlobalSensitivity\n\nfunction r1_wrt_r(r)\n    p = deepcopy(policies[1])\n    p.COLA = r[2]\n    p.prem = r[3]\n    res(Int(floor(r[1])), p)\nend\n\n# reserve @ year 1/2, interest rate @ 0.03 ± 0.01, prem @ 1000.0 ± 0.1\nreg_anal = gsa(r1_wrt_r, RegressionGSA(), [[1, 2], [0.029, 0.031], [999.9, 1000.1]], samples=1000)\n@show reg_anal.pearson\n\nreg_anal.pearson = [0.002266826033036182 -0.9999603237826435 -0.003255946696849575]\n\n\n1×3 Matrix{Float64}:\n 0.00226683  -0.99996  -0.00325595\n\n\nThe Pearson Spearman coefficients show the correlation coefficient matrix between inputs and outputs.\n\n\n28.4.3 Sobol Indices\nSobol is a variance-based method, and it decomposes the variance of the output of the model or system into fractions which can be attributed to inputs or sets of inputs. This helps to get not just the individual parameter’s sensitivities, but also gives a way to quantify the affect and sensitivity from the interaction between the parameters.\n\\[\nY = f_0 + \\sum_{i=1}^{d}f_i(X_i) + \\sum_{i&lt;j}^{d}f_{ij}(X_i, X_j) + ... + f_{1,2,...,d}(X_1, X_2, ..., X_d)\n\\]\n\\[\nVar(Y) = \\sum_{i=1}^{d}V_i + \\sum_{i&lt;j}^{d}V_{ij} + ... + V_{1,2,...,d}\n\\]\nThe Sobol Indices are “ordered”, the first order indices given by \\(S_i = \\dfrac{V_i}{Var(Y)}\\), the contribution to the output variance of the main effect of \\(X_i\\). Therefore, it measures the effect of varying \\(X_i\\) alone, but averaged over variations in other input parameters. It is standardized by the total variance to provide a fractional contribution. Higher-order interaction indices \\(S_{ij}, S_{ijk}\\) and so on can be formed by dividing other terms in the variance decomposition by \\(Var(Y)\\).\n\nusing QuasiMonteCarlo, GlobalSensitivity\n\n# reserve @ year 1/2, interest rate @ 0.03 ± 0.01, prem @ 1000.0 ± 0.1\nL, U = QuasiMonteCarlo.generate_design_matrices(1000, [1, 0.029, 999.9], [2, 0.031, 1000.1], SobolSample())\ns = gsa(r1_wrt_r, Sobol(), L, U)\n@show s.S1\n@show s.ST\n\n\n┌ Warning: The `generate_design_matrices(n, d, sampler, R = NoRand(), num_mats)` method does not produces true and independent QMC matrices, see [this doc warning](https://docs.sciml.ai/QuasiMonteCarlo/stable/design_matrix/) for more context. \n│     Prefer using randomization methods such as `R = Shift()`, `R = MatousekScrambling()`, etc., see [documentation](https://docs.sciml.ai/QuasiMonteCarlo/stable/randomization/)\n└ @ QuasiMonteCarlo ~/.julia/packages/QuasiMonteCarlo/KvLfb/src/RandomizedQuasiMonteCarlo/iterators.jl:255\ns.S1 = [0.0, 1.2308143537488936, -0.00010730838653271882]\ns.ST = [0.0, 1.0014202549551285, 5.83151055643934e-6]\n\n\n\n\n3-element Vector{Float64}:\n 0.0\n 1.0014202549551285\n 5.83151055643934e-6\n\n\nThe output shows the first order and total order of variations in different input parameters.\n\n\n28.4.4 Morris Method\nThe Morris method also known as Morris’s OAT method where OAT stands for One At a Time can be described in the following steps:\n\\[\nEE_i = \\frac{f(x_1, x_2, ...x_i + \\Delta, ...x_k) - y}{\\Delta}\n\\]\nWe calculate local sensitivity measures known as “elementary effects”, which are calculated by measuring the perturbation in the output of the model on changing one parameter.\nThese are evaluated at various points in the input chosen such that a wide “spread” of the parameter space is explored and considered in the analysis, to provide an approximate global importance measure. The mean and variance of these elementary effects is computed. A high value of the mean implies that a parameter is important, a high variance implies that its effects are non-linear or the result of interactions with other inputs. This method does not evaluate separately the contribution from the interaction and the contribution of the parameters individually and gives the effects for each parameter which takes into consideration all the interactions and its individual contribution.\n\nusing GlobalSensitivity\n\n# reserve @ year 1/2, interest rate @ 0.03 ± 0.01, prem @ 1000.0 ± 0.1\nm = gsa(r1_wrt_r, Morris(), [[1, 2], [0.029, 0.031], [999.9, 1000.1]])\n@show m.means\n@show m.variances\n\nm.means = [0.0 -718932.6297036522 -17.298407348875823]\nm.variances = [0.0 7.208809723977634e7 0.005810711356193134]\n\n\n1×3 Matrix{Float64}:\n 0.0  7.20881e7  0.00581071\n\n\nFrom the means it can be observed which variables are more important, and the variances imply higher degree of nonlinearity or interactions with other variables.\n\n\n28.4.5 Fourier Amplitude Sensitivity Tests\nFAST offers a robust, especially at low sample size, and computationally efficient procedure to get the first and total order indices as discussed in Sobol. It utilizes monodimensional Fourier decomposition along a curve, exploring the parameter space. The curve is defined by a set of parametric equations,\n\\[\nEE_i = \\frac{f(x_1, x_2, ...x_i + \\Delta, ...x_k) - y}{\\Delta}\n\\]\nwhere \\(s\\) is a scalar variable varying over the range \\(−∞ &lt; s &lt; +∞\\), \\(G_i\\) are transformation functions and \\(w_i, ∀i=1,2,…,N\\) is a set of different (angular) frequencies, to be properly selected, associated with each factor for all \\(N\\) (samples) number of parameter sets.\n\nusing GlobalSensitivity\n\n# reserve @ year 1/2, interest rate @ 0.03 ± 0.01, prem @ 1000.0 ± 0.1\nfast = gsa(r1_wrt_r, eFAST(), [[1, 2], [0.029, 0.031], [999.9, 1000.1]], samples=1000)\n@show fast.S1\n@show fast.ST\n\nfast.S1 = [4.6241081415743115e-12 0.9976898967399184 5.795270630721111e-6]\nfast.ST = [6.602921726051036e-7 0.9999937138229795 0.0023124856279279626]\n\n\n1×3 Matrix{Float64}:\n 6.60292e-7  0.999994  0.00231249\n\n\nThe output shows the first order and total order of variations in different input parameters.\n\n\n28.4.6 Automatic Differentiation\nBy applying the chain rule repeatedly on elementary operations of computer calculations, automatic differentiation can be applied to measure impacts of small differences. More details in Chapter 16 on automatic differentiation.\n\n\n28.4.7 Scenario Analyses\nScenarios can be generated following scenario generation methodologies to evaluate impacts. More details in Chapter 25 on scenario generation.\nWhen scenarios are generated to evaluate sensitivities, one may need to take the following into consideration.\n\nReverse stress testing. Reverse stress testing in scenario analysis involves identifying extreme scenarios that could potentially lead to catastrophic outcomes for a financial institution or a system. Unlike traditional sensitivity testing to simulate the impact of adverse events on the system, reverse stress testing starts with a catastrophic outcome and works backwards to determine the combination of events or circumstances that could lead to such an outcome.\n\nOne typically follows these steps to do reverse stress testing. – Define a critical failure point (e.g., bankruptcy, system outage, regulatory breach). – Analyze the combinations of events or variables that could cause the failure. – Model the path from normal conditions to the adverse outcome.\nPotential benefits that reverse stress testing could bring include: – Focusing on Vulnerabilities: Highlights specific scenarios to avoid at all costs. – Enhancing Resilience: Strengthens systems against extreme risks. – Regulatory Compliance: Often required in highly regulated industries like banking and energy.\n\nStylistic scenarios. Developing stylistic scenarios in scenario analysis involves creating narratives or storylines that describe plausible future states or situations. These scenarios are crafted to capture key uncertainties, trends, and factors that could significantly impact the organization, industry, or environment under study.\nBacktesting against historical data. Backtesting in scenario analysis involves an iterative process of using past data to validate the effectiveness and accuracy of scenarios developed for forecasting future outcomes. Scenarios are first defined and applied on selective historical data, and refined after any discrepancies of scenario outcomes versus historical results are identified.\n\nOne typically follows these steps to do backtesting against historical data. – Define Scenarios: Establish hypothetical scenarios (e.g., market crashes, changes in interest rates, or operational disruptions), and ensure scenarios cover a range of possibilities, such as best-case, base-case, and worst-case scenarios. – Collect Historical Data: Gather relevant historical data for key variables (e.g., stock prices, interest rates, production metrics), and ensure data spans periods where similar events occurred in the past. – Model Scenario Impacts: Use historical data to simulate the impacts of the scenarios on key metrics or performance indicators. – Compare Results: Compare the modeled results of the scenarios with the actual historical outcomes, and assess how well the scenarios predict or explain the observed data. – Adjust and Refine: If the scenarios do not align with historical outcomes, refine the assumptions or parameters in the scenario models, and incorporate lessons learned from historical patterns to improve future scenario analyses.\nSome considerations in incorporating historical data. – Data Quality: Ensure historical data is accurate, complete, and relevant to the scenarios being tested. – Model Limitations: Scenario models are based on assumptions that might not fully capture real-world complexities. – Overfitting: Avoid fine-tuning scenarios to perfectly match historical outcomes, as this reduces their applicability to future events. – Changing Dynamics: Historical events may not fully represent future possibilities due to changes in market conditions, regulations, or technology.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Sensitivity Analysis</span>"
    ]
  },
  {
    "objectID": "autodiff_alm.html",
    "href": "autodiff_alm.html",
    "title": "29  Auto-differentiation and Asset Liability Management (AAD & ALM)",
    "section": "",
    "text": "29.1 Chapter Overview\nAsset liability modeling requires computing derivatives of portfolio values with respect to yield curve changes. Traditional approaches use finite difference methods or analytical approximations, but automatic differentiation (“autodiff” or “AD”) provides exact derivatives with minimal additional computation. This chapter demonstrates how to implement ALM workflows using autodiff in Julia.\nusing FinanceCore              # provides Cashflow object\nusing DifferentiationInterface # autodiff \nimport ForwardDiff             # specific autodiff technique\nusing CairoMakie               # plotting\nusing DataInterpolations       # yield curve interpolation\nusing Transducers              # data aggregation\nusing JuMP, HiGHS              # portfolio optimization\nusing LinearAlgebra            # math\nusing BenchmarkTools           # benchmarking\nusing OhMyThreads              # multi-threading",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Auto-differentiation and Asset Liability Management (AAD & ALM)</span>"
    ]
  },
  {
    "objectID": "autodiff_alm.html#interest-rate-curve-setup",
    "href": "autodiff_alm.html#interest-rate-curve-setup",
    "title": "29  Auto-differentiation and Asset Liability Management (AAD & ALM)",
    "section": "29.2 Interest Rate Curve Setup",
    "text": "29.2 Interest Rate Curve Setup\nWe start by constructing a yield curve using cubic spline interpolation:\nThe curve function creates a discount factor curve from zero rates and time points. This curve will serve as input to our value function, which makes it straightforward to compute sensitivities by differentiating with respect to the rate parameters.\n\n\nzero_rates = [0.01, 0.02, 0.02, 0.03, 0.05, 0.055] #continuous\n\ntimes = [1., 2., 3., 5., 10., 20.]\n\nfunction curve(zero_rates, times)\n    DataInterpolations.CubicSpline([1.0; exp.(-zero_rates .* times)], [0.; times])\nend\n\nc = curve(zero_rates, times)\n\n\nCubicSpline with 7 points\n┌──────┬──────────┐\n│ time │        u │\n├──────┼──────────┤\n│  0.0 │      1.0 │\n│  1.0 │  0.99005 │\n│  2.0 │ 0.960789 │\n│  3.0 │ 0.941765 │\n│  5.0 │ 0.860708 │\n│ 10.0 │ 0.606531 │\n│ 20.0 │ 0.332871 │\n└──────┴──────────┘",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Auto-differentiation and Asset Liability Management (AAD & ALM)</span>"
    ]
  },
  {
    "objectID": "autodiff_alm.html#asset-valuation-framework",
    "href": "autodiff_alm.html#asset-valuation-framework",
    "title": "29  Auto-differentiation and Asset Liability Management (AAD & ALM)",
    "section": "29.3 Asset Valuation Framework",
    "text": "29.3 Asset Valuation Framework\nThe core valuation function operates on any instrument that produces cashflows:\n\nfunction value(curve, asset)\n    cfs = asset(curve)\n    mapreduce(cf -&gt; cf.amount * curve(cf.time), +, cfs)\nend\n\nvalue (generic function with 1 method)\n\n\nThis design separates the valuation logic from the instrument definition. Each asset type implements a callable interface that generates cashflows given a yield curve. Note how the asset itself gets passed the curve (the asset(curve) statement) to determine the cashflows.\nFor fixed bonds, we create a structure that generates periodic coupon payments:\n\nstruct FixedBond{A,B,C}\n    coupon::A\n    tenor::B\n    periodicity::C\nend\nfunction (b::FixedBond)(curve)\n    map(1//b.periodicity:1//b.periodicity:b.tenor) do t\n        Cashflow(b.coupon / b.periodicity + (t == b.tenor ? 1. : 0.), t)\n    end\nend\n\nfunction par_yield(curve, tenor, periodicity)\n    dfs = curve.(1//periodicity:1//periodicity:tenor)\n\n    (1 - last(dfs)) / sum(dfs) * periodicity\nend\n\nThe (b::FixedBond)(curve) function (sometimes called a ‘functor’, since we are using the b object itself as the function invocation) takes the curve and returns an array of Cashflows.\n\n\n\n\n\n\nNote\n\n\n\nCashflow objects are part of the JuliaActuary suite. This allows the cashflows to be tied with the exact timepoint that they occur, rather than needing a bunch of logic to pre-determine a timestep (annual, quarterly, etc.) for which cashflows would get bucketed. This is more efficient in many cases and much simpler code.\n\n\nThe par_yield function computes the coupon rate that prices the bond at par, which we’ll use to construct our asset universe.\nHere’s an example of bond cashflows and valuing that bond using the curve c that we constructed earlier.\n\nFixedBond(0.08, 10, 2)(c)\n\n20-element Vector{Cashflow{Float64, Rational{Int64}}}:\n Cashflow{Float64, Rational{Int64}}(0.04, 1//2)\n Cashflow{Float64, Rational{Int64}}(0.04, 1//1)\n Cashflow{Float64, Rational{Int64}}(0.04, 3//2)\n Cashflow{Float64, Rational{Int64}}(0.04, 2//1)\n Cashflow{Float64, Rational{Int64}}(0.04, 5//2)\n Cashflow{Float64, Rational{Int64}}(0.04, 3//1)\n Cashflow{Float64, Rational{Int64}}(0.04, 7//2)\n Cashflow{Float64, Rational{Int64}}(0.04, 4//1)\n Cashflow{Float64, Rational{Int64}}(0.04, 9//2)\n Cashflow{Float64, Rational{Int64}}(0.04, 5//1)\n Cashflow{Float64, Rational{Int64}}(0.04, 11//2)\n Cashflow{Float64, Rational{Int64}}(0.04, 6//1)\n Cashflow{Float64, Rational{Int64}}(0.04, 13//2)\n Cashflow{Float64, Rational{Int64}}(0.04, 7//1)\n Cashflow{Float64, Rational{Int64}}(0.04, 15//2)\n Cashflow{Float64, Rational{Int64}}(0.04, 8//1)\n Cashflow{Float64, Rational{Int64}}(0.04, 17//2)\n Cashflow{Float64, Rational{Int64}}(0.04, 9//1)\n Cashflow{Float64, Rational{Int64}}(0.04, 19//2)\n Cashflow{Float64, Rational{Int64}}(1.04, 10//1)\n\n\n\nvalue(c, FixedBond(0.09, 10, 2))\n\n1.3526976075662451",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Auto-differentiation and Asset Liability Management (AAD & ALM)</span>"
    ]
  },
  {
    "objectID": "autodiff_alm.html#liability-modeling",
    "href": "autodiff_alm.html#liability-modeling",
    "title": "29  Auto-differentiation and Asset Liability Management (AAD & ALM)",
    "section": "29.4 Liability Modeling",
    "text": "29.4 Liability Modeling\nDeferred annuities require more complex modeling than fixed bonds due to policyholder behavior (optionality). The surrender rate depends on the difference between market rates and the guaranteed rate. The surrender function chosen below is arbitrary, but follows a typical pattern with much higher surrenders if the market rate on competing instruments is higher than what’s currently available. The account value accumulates at the guaranteed rate, and surrenders create negative cashflows representing benefit payments. Lastly, the annuities function is a wrapper function we will use to compute the portfolio value and ALM metrics later.\n\nbegin\n    struct DeferredAnnuity{A,B}\n        tenor::A\n        rate::B\n    end\n\n    function (d::DeferredAnnuity)(curve)\n        av = 1.\n        map(1//12:1//12:d.tenor) do t\n            mkt_rate = -log(curve(d.tenor) / curve(t)) / (d.tenor - t)\n            av *= exp(d.rate / 12)\n            rate_diff = mkt_rate - d.rate\n            sr = t == d.tenor ? 1.0 : surrender_rate(rate_diff) / 12\n            av_surr = av * sr\n            av -= av_surr\n            Cashflow(-av_surr, t)\n\n        end\n    end\n\n    function surrender_rate(rate_diff)\n        1 / (1 + exp(3 - rate_diff * 60))\n    end\n    function annuities(rates, portfolio)\n        times = [1., 2., 3., 5., 10., 20.]\n\n        c = curve(rates, times)\n\n        # threaded map-reduce for more speed\n        OhMyThreads.tmapreduce(+, 1:length(portfolio); ntasks=Threads.nthreads()) do i\n            value(c, portfolio[i])\n        end\n        # mapreduce(l -&gt; value(c,l),+,portfolio)\n    end\n\nend\n\nannuities (generic function with 1 method)\n\n\nHere’s what the surrender rate behavior looks like for different levels of market rates compared to the a 3% crediting rate.\n\nlet\n    cred_rate = 0.03\n    mkt_rates = 0.005:0.001:0.08\n    rate_diff = mkt_rates .- cred_rate\n\n    lines(rate_diff, surrender_rate.(rate_diff),\n        axis=(\n            title=\"Surrender rate by difference to market rate\",\n            xlabel=\"Rate Difference\",\n            ylabel=\"Annual Surrender Rate\"\n        ))\nend\n\n\n\n\nWe model a large portfolio of these annuities with random tenors:\n\nliabilities = map(1:100_000) do i\n    tenor = rand(1:20)\n    DeferredAnnuity(tenor, par_yield(c, tenor, 12))\nend\n\n100000-element Vector{DeferredAnnuity{Int64, Float64}}:\n DeferredAnnuity{Int64, Float64}(19, 0.05183909988436309)\n DeferredAnnuity{Int64, Float64}(20, 0.051933558828553925)\n DeferredAnnuity{Int64, Float64}(14, 0.05108046813021651)\n DeferredAnnuity{Int64, Float64}(13, 0.05055368479631081)\n DeferredAnnuity{Int64, Float64}(15, 0.05141229637424002)\n DeferredAnnuity{Int64, Float64}(20, 0.051933558828553925)\n DeferredAnnuity{Int64, Float64}(11, 0.048613487329580624)\n DeferredAnnuity{Int64, Float64}(5, 0.029452531739357923)\n DeferredAnnuity{Int64, Float64}(19, 0.05183909988436309)\n DeferredAnnuity{Int64, Float64}(3, 0.019937936965500825)\n ⋮\n DeferredAnnuity{Int64, Float64}(5, 0.029452531739357923)\n DeferredAnnuity{Int64, Float64}(7, 0.03847750979086058)\n DeferredAnnuity{Int64, Float64}(20, 0.051933558828553925)\n DeferredAnnuity{Int64, Float64}(8, 0.0419258485355399)\n DeferredAnnuity{Int64, Float64}(18, 0.05177859152668137)\n DeferredAnnuity{Int64, Float64}(20, 0.051933558828553925)\n DeferredAnnuity{Int64, Float64}(16, 0.051607152912174645)\n DeferredAnnuity{Int64, Float64}(15, 0.05141229637424002)\n DeferredAnnuity{Int64, Float64}(14, 0.05108046813021651)\n\n\n\n\n\n\n\n\nNoteConsolidating Cashflows\n\n\n\n\n\nLater on we will generate vectors of vectors of cashflows without any guarantee that the timepoints will line up, making aggregating cashflows by timepoints a non-obvious task. There are many ways to accomplish this, but I like Transducers.\nTransducers are unfamiliar to many people, and don’t let the presence deter you from the main points of this post. The details aren’t central to the point of this blog post so just skip over if confusing.\n\nfunction consolidate(cashflows)\n\n    cashflows |&gt; # take the collection\n    MapCat(identity) |&gt; # flatten it out without changing elements\n    # group by the time, and just keep and sum the amounts \n    GroupBy(x -&gt; x.time, Map(last) ⨟ Map(x -&gt; x.amount), +) |&gt;\n    foldxl(Transducers.right) # perform the aggregation and keep the final grouped result\nend\n\nconsolidate (generic function with 1 method)\n\n\nExample:\n\ncashflow_vectors = [l(c) for l in liabilities]\n\n100000-element Vector{Vector{Cashflow{Float64, Rational{Int64}}}}:\n [Cashflow{Float64, Rational{Int64}}(-0.004808083969809224, 1//12), Cashflow{Float64, Rational{Int64}}(-0.004867578774020018, 1//6), Cashflow{Float64, Rational{Int64}}(-0.004927381341597908, 1//4), Cashflow{Float64, Rational{Int64}}(-0.004987158725210008, 1//3), Cashflow{Float64, Rational{Int64}}(-0.005046558205980105, 5//12), Cashflow{Float64, Rational{Int64}}(-0.005105206960483149, 1//2), Cashflow{Float64, Rational{Int64}}(-0.005162711883061914, 7//12), Cashflow{Float64, Rational{Int64}}(-0.005218659579946738, 2//3), Cashflow{Float64, Rational{Int64}}(-0.0052726165521757825, 3//4), Cashflow{Float64, Rational{Int64}}(-0.0053241295848203765, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.0030295443688923046, 73//4), Cashflow{Float64, Rational{Int64}}(-0.00303493417322771, 55//3), Cashflow{Float64, Rational{Int64}}(-0.0030408607686393274, 221//12), Cashflow{Float64, Rational{Int64}}(-0.0030473319110749316, 37//2), Cashflow{Float64, Rational{Int64}}(-0.003054355701028045, 223//12), Cashflow{Float64, Rational{Int64}}(-0.003061940595233296, 56//3), Cashflow{Float64, Rational{Int64}}(-0.003070095418840538, 75//4), Cashflow{Float64, Rational{Int64}}(-0.0030788293780883934, 113//6), Cashflow{Float64, Rational{Int64}}(-0.003088152073497882, 227//12), Cashflow{Float64, Rational{Int64}}(-0.6829248274912895, 19//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.004783377181503488, 1//12), Cashflow{Float64, Rational{Int64}}(-0.004839628938067403, 1//6), Cashflow{Float64, Rational{Int64}}(-0.004896115454239143, 1//4), Cashflow{Float64, Rational{Int64}}(-0.004952522042686588, 1//3), Cashflow{Float64, Rational{Int64}}(-0.005008516357927531, 5//12), Cashflow{Float64, Rational{Int64}}(-0.005063748136459031, 1//2), Cashflow{Float64, Rational{Int64}}(-0.005117849072112032, 7//12), Cashflow{Float64, Rational{Int64}}(-0.005170432840042458, 2//3), Cashflow{Float64, Rational{Int64}}(-0.005221095283163374, 3//4), Cashflow{Float64, Rational{Int64}}(-0.0052694147752224245, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.0034052147073842754, 77//4), Cashflow{Float64, Rational{Int64}}(-0.0034222845055589825, 58//3), Cashflow{Float64, Rational{Int64}}(-0.0034401086494695596, 233//12), Cashflow{Float64, Rational{Int64}}(-0.003458703030723001, 39//2), Cashflow{Float64, Rational{Int64}}(-0.003478084148257083, 235//12), Cashflow{Float64, Rational{Int64}}(-0.0034982691293209437, 59//3), Cashflow{Float64, Rational{Int64}}(-0.0035192757511736806, 79//4), Cashflow{Float64, Rational{Int64}}(-0.0035411224635144953, 119//6), Cashflow{Float64, Rational{Int64}}(-0.003563828411657379, 239//12), Cashflow{Float64, Rational{Int64}}(-0.697536591529004, 20//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.004989026914463392, 1//12), Cashflow{Float64, Rational{Int64}}(-0.005072503613158184, 1//6), Cashflow{Float64, Rational{Int64}}(-0.005157007583915993, 1//4), Cashflow{Float64, Rational{Int64}}(-0.005242076337285006, 1//3), Cashflow{Float64, Rational{Int64}}(-0.0053272085834838116, 5//12), Cashflow{Float64, Rational{Int64}}(-0.005411862934794599, 1//2), Cashflow{Float64, Rational{Int64}}(-0.0054954569390823806, 7//12), Cashflow{Float64, Rational{Int64}}(-0.00557736649771718, 2//3), Cashflow{Float64, Rational{Int64}}(-0.005656925723839015, 3//4), Cashflow{Float64, Rational{Int64}}(-0.005733427299288154, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.003404660033356735, 53//4), Cashflow{Float64, Rational{Int64}}(-0.0033708218012461827, 40//3), Cashflow{Float64, Rational{Int64}}(-0.003337581040356867, 161//12), Cashflow{Float64, Rational{Int64}}(-0.0033049298363434097, 27//2), Cashflow{Float64, Rational{Int64}}(-0.003272860428437546, 163//12), Cashflow{Float64, Rational{Int64}}(-0.0032413652074448113, 41//3), Cashflow{Float64, Rational{Int64}}(-0.003210436713800933, 55//4), Cashflow{Float64, Rational{Int64}}(-0.003180067635691048, 83//6), Cashflow{Float64, Rational{Int64}}(-0.0031502508072285204, 167//12), Cashflow{Float64, Rational{Int64}}(-0.4672090174521514, 14//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.004988088840310257, 1//12), Cashflow{Float64, Rational{Int64}}(-0.0050771918288985704, 1//6), Cashflow{Float64, Rational{Int64}}(-0.005167564134824113, 1//4), Cashflow{Float64, Rational{Int64}}(-0.005258711609765578, 1//3), Cashflow{Float64, Rational{Int64}}(-0.005350095098399249, 5//12), Cashflow{Float64, Rational{Int64}}(-0.005441128704157761, 1//2), Cashflow{Float64, Rational{Int64}}(-0.0055311784406072055, 7//12), Cashflow{Float64, Rational{Int64}}(-0.005619561337812729, 2//3), Cashflow{Float64, Rational{Int64}}(-0.005705545077075528, 3//4), Cashflow{Float64, Rational{Int64}}(-0.005788348231031512, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.0038503833423463286, 49//4), Cashflow{Float64, Rational{Int64}}(-0.003804934322852083, 37//3), Cashflow{Float64, Rational{Int64}}(-0.0037602800042857328, 149//12), Cashflow{Float64, Rational{Int64}}(-0.0037164078333552157, 25//2), Cashflow{Float64, Rational{Int64}}(-0.0036733054971266055, 151//12), Cashflow{Float64, Rational{Int64}}(-0.003630960918852027, 38//3), Cashflow{Float64, Rational{Int64}}(-0.0035893622538848845, 51//4), Cashflow{Float64, Rational{Int64}}(-0.0035484978856863665, 77//6), Cashflow{Float64, Rational{Int64}}(-0.003508356421914768, 155//12), Cashflow{Float64, Rational{Int64}}(-0.42567536516657395, 13//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.004969120405811204, 1//12), Cashflow{Float64, Rational{Int64}}(-0.005047050365765408, 1//6), Cashflow{Float64, Rational{Int64}}(-0.005125802688680543, 1//4), Cashflow{Float64, Rational{Int64}}(-0.005204945068597846, 1//3), Cashflow{Float64, Rational{Int64}}(-0.005284011648055461, 5//12), Cashflow{Float64, Rational{Int64}}(-0.005362502043055925, 1//2), Cashflow{Float64, Rational{Int64}}(-0.005439880651722219, 7//12), Cashflow{Float64, Rational{Int64}}(-0.005515576287951582, 2//3), Cashflow{Float64, Rational{Int64}}(-0.005588982183192956, 3//4), Cashflow{Float64, Rational{Int64}}(-0.005659456401106093, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.003095135904573605, 57//4), Cashflow{Float64, Rational{Int64}}(-0.0030702310298667437, 43//3), Cashflow{Float64, Rational{Int64}}(-0.003045806631755013, 173//12), Cashflow{Float64, Rational{Int64}}(-0.0030218578210388625, 29//2), Cashflow{Float64, Rational{Int64}}(-0.0029983798227155747, 175//12), Cashflow{Float64, Rational{Int64}}(-0.0029753679754469797, 44//3), Cashflow{Float64, Rational{Int64}}(-0.0029528177310823808, 59//4), Cashflow{Float64, Rational{Int64}}(-0.0029307246542385214, 89//6), Cashflow{Float64, Rational{Int64}}(-0.0029090844219367973, 179//12), Cashflow{Float64, Rational{Int64}}(-0.513456896562433, 15//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.004783377181503488, 1//12), Cashflow{Float64, Rational{Int64}}(-0.004839628938067403, 1//6), Cashflow{Float64, Rational{Int64}}(-0.004896115454239143, 1//4), Cashflow{Float64, Rational{Int64}}(-0.004952522042686588, 1//3), Cashflow{Float64, Rational{Int64}}(-0.005008516357927531, 5//12), Cashflow{Float64, Rational{Int64}}(-0.005063748136459031, 1//2), Cashflow{Float64, Rational{Int64}}(-0.005117849072112032, 7//12), Cashflow{Float64, Rational{Int64}}(-0.005170432840042458, 2//3), Cashflow{Float64, Rational{Int64}}(-0.005221095283163374, 3//4), Cashflow{Float64, Rational{Int64}}(-0.0052694147752224245, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.0034052147073842754, 77//4), Cashflow{Float64, Rational{Int64}}(-0.0034222845055589825, 58//3), Cashflow{Float64, Rational{Int64}}(-0.0034401086494695596, 233//12), Cashflow{Float64, Rational{Int64}}(-0.003458703030723001, 39//2), Cashflow{Float64, Rational{Int64}}(-0.003478084148257083, 235//12), Cashflow{Float64, Rational{Int64}}(-0.0034982691293209437, 59//3), Cashflow{Float64, Rational{Int64}}(-0.0035192757511736806, 79//4), Cashflow{Float64, Rational{Int64}}(-0.0035411224635144953, 119//6), Cashflow{Float64, Rational{Int64}}(-0.003563828411657379, 239//12), Cashflow{Float64, Rational{Int64}}(-0.697536591529004, 20//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.0049038424385172375, 1//12), Cashflow{Float64, Rational{Int64}}(-0.005003342312704534, 1//6), Cashflow{Float64, Rational{Int64}}(-0.005104701202457077, 1//4), Cashflow{Float64, Rational{Int64}}(-0.005207358411037622, 1//3), Cashflow{Float64, Rational{Int64}}(-0.005310692394094387, 5//12), Cashflow{Float64, Rational{Int64}}(-0.005414017645807302, 1//2), Cashflow{Float64, Rational{Int64}}(-0.005516582099269058, 7//12), Cashflow{Float64, Rational{Int64}}(-0.005617565161865263, 2//3), Cashflow{Float64, Rational{Int64}}(-0.005716076516180846, 3//4), Cashflow{Float64, Rational{Int64}}(-0.0058111558260197415, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.005407598148241427, 41//4), Cashflow{Float64, Rational{Int64}}(-0.005320442564745627, 31//3), Cashflow{Float64, Rational{Int64}}(-0.0052350166727639195, 125//12), Cashflow{Float64, Rational{Int64}}(-0.005151284793128043, 21//2), Cashflow{Float64, Rational{Int64}}(-0.005069212027467017, 127//12), Cashflow{Float64, Rational{Int64}}(-0.004988764241340426, 32//3), Cashflow{Float64, Rational{Int64}}(-0.0049099080477190945, 43//4), Cashflow{Float64, Rational{Int64}}(-0.004832610790807698, 65//6), Cashflow{Float64, Rational{Int64}}(-0.004756840530202081, 131//12), Cashflow{Float64, Rational{Int64}}(-0.3673579326275773, 11//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.00419222997633213, 1//12), Cashflow{Float64, Rational{Int64}}(-0.004294485538085034, 1//6), Cashflow{Float64, Rational{Int64}}(-0.00440032720940759, 1//4), Cashflow{Float64, Rational{Int64}}(-0.0045087902482982755, 1//3), Cashflow{Float64, Rational{Int64}}(-0.004618734211714481, 5//12), Cashflow{Float64, Rational{Int64}}(-0.004728822405187647, 1//2), Cashflow{Float64, Rational{Int64}}(-0.004837502211883185, 7//12), Cashflow{Float64, Rational{Int64}}(-0.0049429872815796435, 2//3), Cashflow{Float64, Rational{Int64}}(-0.0050432428255826655, 3//4), Cashflow{Float64, Rational{Int64}}(-0.0051359755663768995, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.01221783636993319, 17//4), Cashflow{Float64, Rational{Int64}}(-0.012348041850912319, 13//3), Cashflow{Float64, Rational{Int64}}(-0.012457251832534334, 53//12), Cashflow{Float64, Rational{Int64}}(-0.012545111548720702, 9//2), Cashflow{Float64, Rational{Int64}}(-0.012611387748361194, 55//12), Cashflow{Float64, Rational{Int64}}(-0.01265596763494828, 14//3), Cashflow{Float64, Rational{Int64}}(-0.012678856503834704, 19//4), Cashflow{Float64, Rational{Int64}}(-0.012680174171184718, 29//6), Cashflow{Float64, Rational{Int64}}(-0.012660150307376739, 59//12), Cashflow{Float64, Rational{Int64}}(-0.6659856353990624, 5//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.004808083969809224, 1//12), Cashflow{Float64, Rational{Int64}}(-0.004867578774020018, 1//6), Cashflow{Float64, Rational{Int64}}(-0.004927381341597908, 1//4), Cashflow{Float64, Rational{Int64}}(-0.004987158725210008, 1//3), Cashflow{Float64, Rational{Int64}}(-0.005046558205980105, 5//12), Cashflow{Float64, Rational{Int64}}(-0.005105206960483149, 1//2), Cashflow{Float64, Rational{Int64}}(-0.005162711883061914, 7//12), Cashflow{Float64, Rational{Int64}}(-0.005218659579946738, 2//3), Cashflow{Float64, Rational{Int64}}(-0.0052726165521757825, 3//4), Cashflow{Float64, Rational{Int64}}(-0.0053241295848203765, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.0030295443688923046, 73//4), Cashflow{Float64, Rational{Int64}}(-0.00303493417322771, 55//3), Cashflow{Float64, Rational{Int64}}(-0.0030408607686393274, 221//12), Cashflow{Float64, Rational{Int64}}(-0.0030473319110749316, 37//2), Cashflow{Float64, Rational{Int64}}(-0.003054355701028045, 223//12), Cashflow{Float64, Rational{Int64}}(-0.003061940595233296, 56//3), Cashflow{Float64, Rational{Int64}}(-0.003070095418840538, 75//4), Cashflow{Float64, Rational{Int64}}(-0.0030788293780883934, 113//6), Cashflow{Float64, Rational{Int64}}(-0.003088152073497882, 227//12), Cashflow{Float64, Rational{Int64}}(-0.6829248274912895, 19//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.0040787064420970826, 1//12), Cashflow{Float64, Rational{Int64}}(-0.004181837084799977, 1//6), Cashflow{Float64, Rational{Int64}}(-0.004290237133783712, 1//4), Cashflow{Float64, Rational{Int64}}(-0.004402449495381768, 1//3), Cashflow{Float64, Rational{Int64}}(-0.00451663974771565, 5//12), Cashflow{Float64, Rational{Int64}}(-0.004630522083421415, 1//2), Cashflow{Float64, Rational{Int64}}(-0.004741279733252552, 7//12), Cashflow{Float64, Rational{Int64}}(-0.00484548256916171, 2//3), Cashflow{Float64, Rational{Int64}}(-0.004939006489146975, 3//4), Cashflow{Float64, Rational{Int64}}(-0.005016961898414784, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.0034881369196172805, 9//4), Cashflow{Float64, Rational{Int64}}(-0.0034750286149361565, 7//3), Cashflow{Float64, Rational{Int64}}(-0.0034894594281216834, 29//12), Cashflow{Float64, Rational{Int64}}(-0.0035317716401467763, 5//2), Cashflow{Float64, Rational{Int64}}(-0.0036029399996526067, 31//12), Cashflow{Float64, Rational{Int64}}(-0.0037046064641791895, 8//3), Cashflow{Float64, Rational{Int64}}(-0.0038391371219646596, 11//4), Cashflow{Float64, Rational{Int64}}(-0.00400970300555729, 17//6), Cashflow{Float64, Rational{Int64}}(-0.004220386926584579, 35//12), Cashflow{Float64, Rational{Int64}}(-0.9066566084455148, 3//1)]\n ⋮\n [Cashflow{Float64, Rational{Int64}}(-0.00419222997633213, 1//12), Cashflow{Float64, Rational{Int64}}(-0.004294485538085034, 1//6), Cashflow{Float64, Rational{Int64}}(-0.00440032720940759, 1//4), Cashflow{Float64, Rational{Int64}}(-0.0045087902482982755, 1//3), Cashflow{Float64, Rational{Int64}}(-0.004618734211714481, 5//12), Cashflow{Float64, Rational{Int64}}(-0.004728822405187647, 1//2), Cashflow{Float64, Rational{Int64}}(-0.004837502211883185, 7//12), Cashflow{Float64, Rational{Int64}}(-0.0049429872815796435, 2//3), Cashflow{Float64, Rational{Int64}}(-0.0050432428255826655, 3//4), Cashflow{Float64, Rational{Int64}}(-0.0051359755663768995, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.01221783636993319, 17//4), Cashflow{Float64, Rational{Int64}}(-0.012348041850912319, 13//3), Cashflow{Float64, Rational{Int64}}(-0.012457251832534334, 53//12), Cashflow{Float64, Rational{Int64}}(-0.012545111548720702, 9//2), Cashflow{Float64, Rational{Int64}}(-0.012611387748361194, 55//12), Cashflow{Float64, Rational{Int64}}(-0.01265596763494828, 14//3), Cashflow{Float64, Rational{Int64}}(-0.012678856503834704, 19//4), Cashflow{Float64, Rational{Int64}}(-0.012680174171184718, 29//6), Cashflow{Float64, Rational{Int64}}(-0.012660150307376739, 59//12), Cashflow{Float64, Rational{Int64}}(-0.6659856353990624, 5//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.004424258662627023, 1//12), Cashflow{Float64, Rational{Int64}}(-0.004531287849794747, 1//6), Cashflow{Float64, Rational{Int64}}(-0.004641425855601325, 1//4), Cashflow{Float64, Rational{Int64}}(-0.004753930805967418, 1//3), Cashflow{Float64, Rational{Int64}}(-0.004867946524016453, 5//12), Cashflow{Float64, Rational{Int64}}(-0.004982492484160319, 1//2), Cashflow{Float64, Rational{Int64}}(-0.005096454586812677, 7//12), Cashflow{Float64, Rational{Int64}}(-0.005208577175287752, 2//3), Cashflow{Float64, Rational{Int64}}(-0.005317456788965952, 3//4), Cashflow{Float64, Rational{Int64}}(-0.005421538222340484, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.010779718099495577, 25//4), Cashflow{Float64, Rational{Int64}}(-0.010707757626314915, 19//3), Cashflow{Float64, Rational{Int64}}(-0.01063260474415043, 77//12), Cashflow{Float64, Rational{Int64}}(-0.010554332167721845, 13//2), Cashflow{Float64, Rational{Int64}}(-0.01047301604582292, 79//12), Cashflow{Float64, Rational{Int64}}(-0.010388735819327524, 20//3), Cashflow{Float64, Rational{Int64}}(-0.010301574071366447, 27//4), Cashflow{Float64, Rational{Int64}}(-0.010211616370121152, 41//6), Cashflow{Float64, Rational{Int64}}(-0.010118951104718079, 83//12), Cashflow{Float64, Rational{Int64}}(-0.48575081864113745, 7//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.004783377181503488, 1//12), Cashflow{Float64, Rational{Int64}}(-0.004839628938067403, 1//6), Cashflow{Float64, Rational{Int64}}(-0.004896115454239143, 1//4), Cashflow{Float64, Rational{Int64}}(-0.004952522042686588, 1//3), Cashflow{Float64, Rational{Int64}}(-0.005008516357927531, 5//12), Cashflow{Float64, Rational{Int64}}(-0.005063748136459031, 1//2), Cashflow{Float64, Rational{Int64}}(-0.005117849072112032, 7//12), Cashflow{Float64, Rational{Int64}}(-0.005170432840042458, 2//3), Cashflow{Float64, Rational{Int64}}(-0.005221095283163374, 3//4), Cashflow{Float64, Rational{Int64}}(-0.0052694147752224245, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.0034052147073842754, 77//4), Cashflow{Float64, Rational{Int64}}(-0.0034222845055589825, 58//3), Cashflow{Float64, Rational{Int64}}(-0.0034401086494695596, 233//12), Cashflow{Float64, Rational{Int64}}(-0.003458703030723001, 39//2), Cashflow{Float64, Rational{Int64}}(-0.003478084148257083, 235//12), Cashflow{Float64, Rational{Int64}}(-0.0034982691293209437, 59//3), Cashflow{Float64, Rational{Int64}}(-0.0035192757511736806, 79//4), Cashflow{Float64, Rational{Int64}}(-0.0035411224635144953, 119//6), Cashflow{Float64, Rational{Int64}}(-0.003563828411657379, 239//12), Cashflow{Float64, Rational{Int64}}(-0.697536591529004, 20//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.004557385177676505, 1//12), Cashflow{Float64, Rational{Int64}}(-0.004664387545950275, 1//6), Cashflow{Float64, Rational{Int64}}(-0.0047742008193853965, 1//4), Cashflow{Float64, Rational{Int64}}(-0.00488614452367044, 1//3), Cashflow{Float64, Rational{Int64}}(-0.004999441950790049, 5//12), Cashflow{Float64, Rational{Int64}}(-0.0051132127602090665, 1//2), Cashflow{Float64, Rational{Int64}}(-0.005226466320959016, 7//12), Cashflow{Float64, Rational{Int64}}(-0.005338096093052123, 2//3), Cashflow{Float64, Rational{Int64}}(-0.005446875388809937, 3//4), Cashflow{Float64, Rational{Int64}}(-0.005551454897149994, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.009612003204621645, 29//4), Cashflow{Float64, Rational{Int64}}(-0.009517691790623088, 22//3), Cashflow{Float64, Rational{Int64}}(-0.009421239317538287, 89//12), Cashflow{Float64, Rational{Int64}}(-0.009322735198487444, 15//2), Cashflow{Float64, Rational{Int64}}(-0.009222270238825802, 91//12), Cashflow{Float64, Rational{Int64}}(-0.009119936465989032, 23//3), Cashflow{Float64, Rational{Int64}}(-0.009015826957992887, 31//4), Cashflow{Float64, Rational{Int64}}(-0.008910035671122777, 47//6), Cashflow{Float64, Rational{Int64}}(-0.008802657267349105, 95//12), Cashflow{Float64, Rational{Int64}}(-0.4224849041397684, 8//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.00484668200548985, 1//12), Cashflow{Float64, Rational{Int64}}(-0.0049100274641540355, 1//6), Cashflow{Float64, Rational{Int64}}(-0.004973771861878092, 1//4), Cashflow{Float64, Rational{Int64}}(-0.005037561154641696, 1//3), Cashflow{Float64, Rational{Int64}}(-0.005101018961600112, 5//12), Cashflow{Float64, Rational{Int64}}(-0.00516374613478892, 1//2), Cashflow{Float64, Rational{Int64}}(-0.005225320508363633, 7//12), Cashflow{Float64, Rational{Int64}}(-0.00528529684781004, 2//3), Cashflow{Float64, Rational{Int64}}(-0.005343207020245248, 3//4), Cashflow{Float64, Rational{Int64}}(-0.005398560407581078, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.0028498899525714987, 69//4), Cashflow{Float64, Rational{Int64}}(-0.0028466477109700184, 52//3), Cashflow{Float64, Rational{Int64}}(-0.0028438381970336854, 209//12), Cashflow{Float64, Rational{Int64}}(-0.0028414643370289795, 35//2), Cashflow{Float64, Rational{Int64}}(-0.0028395292581497736, 211//12), Cashflow{Float64, Rational{Int64}}(-0.0028380362943343424, 53//3), Cashflow{Float64, Rational{Int64}}(-0.002836988992328804, 71//4), Cashflow{Float64, Rational{Int64}}(-0.002836391118006782, 107//6), Cashflow{Float64, Rational{Int64}}(-0.0028362466629578338, 215//12), Cashflow{Float64, Rational{Int64}}(-0.6510276535254121, 18//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.004783377181503488, 1//12), Cashflow{Float64, Rational{Int64}}(-0.004839628938067403, 1//6), Cashflow{Float64, Rational{Int64}}(-0.004896115454239143, 1//4), Cashflow{Float64, Rational{Int64}}(-0.004952522042686588, 1//3), Cashflow{Float64, Rational{Int64}}(-0.005008516357927531, 5//12), Cashflow{Float64, Rational{Int64}}(-0.005063748136459031, 1//2), Cashflow{Float64, Rational{Int64}}(-0.005117849072112032, 7//12), Cashflow{Float64, Rational{Int64}}(-0.005170432840042458, 2//3), Cashflow{Float64, Rational{Int64}}(-0.005221095283163374, 3//4), Cashflow{Float64, Rational{Int64}}(-0.0052694147752224245, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.0034052147073842754, 77//4), Cashflow{Float64, Rational{Int64}}(-0.0034222845055589825, 58//3), Cashflow{Float64, Rational{Int64}}(-0.0034401086494695596, 233//12), Cashflow{Float64, Rational{Int64}}(-0.003458703030723001, 39//2), Cashflow{Float64, Rational{Int64}}(-0.003478084148257083, 235//12), Cashflow{Float64, Rational{Int64}}(-0.0034982691293209437, 59//3), Cashflow{Float64, Rational{Int64}}(-0.0035192757511736806, 79//4), Cashflow{Float64, Rational{Int64}}(-0.0035411224635144953, 119//6), Cashflow{Float64, Rational{Int64}}(-0.003563828411657379, 239//12), Cashflow{Float64, Rational{Int64}}(-0.697536591529004, 20//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.004934341012856126, 1//12), Cashflow{Float64, Rational{Int64}}(-0.005006985379965156, 1//6), Cashflow{Float64, Rational{Int64}}(-0.005080281729186395, 1//4), Cashflow{Float64, Rational{Int64}}(-0.0051538260967086925, 1//3), Cashflow{Float64, Rational{Int64}}(-0.00522718538625404, 5//12), Cashflow{Float64, Rational{Int64}}(-0.00529989663220021, 1//2), Cashflow{Float64, Rational{Int64}}(-0.005371466505717817, 7//12), Cashflow{Float64, Rational{Int64}}(-0.005441371096268787, 2//3), Cashflow{Float64, Rational{Int64}}(-0.0055090560020900166, 3//4), Cashflow{Float64, Rational{Int64}}(-0.005573936764447176, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.0029003255089834427, 61//4), Cashflow{Float64, Rational{Int64}}(-0.002882910770566691, 46//3), Cashflow{Float64, Rational{Int64}}(-0.0028659134998877214, 185//12), Cashflow{Float64, Rational{Int64}}(-0.0028493311657012427, 31//2), Cashflow{Float64, Rational{Int64}}(-0.0028331613441325196, 187//12), Cashflow{Float64, Rational{Int64}}(-0.002817401719508703, 47//3), Cashflow{Float64, Rational{Int64}}(-0.0028020500852618748, 63//4), Cashflow{Float64, Rational{Int64}}(-0.002787104344907078, 95//6), Cashflow{Float64, Rational{Int64}}(-0.0027725625130955237, 191//12), Cashflow{Float64, Rational{Int64}}(-0.5617474374389465, 16//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.004969120405811204, 1//12), Cashflow{Float64, Rational{Int64}}(-0.005047050365765408, 1//6), Cashflow{Float64, Rational{Int64}}(-0.005125802688680543, 1//4), Cashflow{Float64, Rational{Int64}}(-0.005204945068597846, 1//3), Cashflow{Float64, Rational{Int64}}(-0.005284011648055461, 5//12), Cashflow{Float64, Rational{Int64}}(-0.005362502043055925, 1//2), Cashflow{Float64, Rational{Int64}}(-0.005439880651722219, 7//12), Cashflow{Float64, Rational{Int64}}(-0.005515576287951582, 2//3), Cashflow{Float64, Rational{Int64}}(-0.005588982183192956, 3//4), Cashflow{Float64, Rational{Int64}}(-0.005659456401106093, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.003095135904573605, 57//4), Cashflow{Float64, Rational{Int64}}(-0.0030702310298667437, 43//3), Cashflow{Float64, Rational{Int64}}(-0.003045806631755013, 173//12), Cashflow{Float64, Rational{Int64}}(-0.0030218578210388625, 29//2), Cashflow{Float64, Rational{Int64}}(-0.0029983798227155747, 175//12), Cashflow{Float64, Rational{Int64}}(-0.0029753679754469797, 44//3), Cashflow{Float64, Rational{Int64}}(-0.0029528177310823808, 59//4), Cashflow{Float64, Rational{Int64}}(-0.0029307246542385214, 89//6), Cashflow{Float64, Rational{Int64}}(-0.0029090844219367973, 179//12), Cashflow{Float64, Rational{Int64}}(-0.513456896562433, 15//1)]\n [Cashflow{Float64, Rational{Int64}}(-0.004989026914463392, 1//12), Cashflow{Float64, Rational{Int64}}(-0.005072503613158184, 1//6), Cashflow{Float64, Rational{Int64}}(-0.005157007583915993, 1//4), Cashflow{Float64, Rational{Int64}}(-0.005242076337285006, 1//3), Cashflow{Float64, Rational{Int64}}(-0.0053272085834838116, 5//12), Cashflow{Float64, Rational{Int64}}(-0.005411862934794599, 1//2), Cashflow{Float64, Rational{Int64}}(-0.0054954569390823806, 7//12), Cashflow{Float64, Rational{Int64}}(-0.00557736649771718, 2//3), Cashflow{Float64, Rational{Int64}}(-0.005656925723839015, 3//4), Cashflow{Float64, Rational{Int64}}(-0.005733427299288154, 5//6)  …  Cashflow{Float64, Rational{Int64}}(-0.003404660033356735, 53//4), Cashflow{Float64, Rational{Int64}}(-0.0033708218012461827, 40//3), Cashflow{Float64, Rational{Int64}}(-0.003337581040356867, 161//12), Cashflow{Float64, Rational{Int64}}(-0.0033049298363434097, 27//2), Cashflow{Float64, Rational{Int64}}(-0.003272860428437546, 163//12), Cashflow{Float64, Rational{Int64}}(-0.0032413652074448113, 41//3), Cashflow{Float64, Rational{Int64}}(-0.003210436713800933, 55//4), Cashflow{Float64, Rational{Int64}}(-0.003180067635691048, 83//6), Cashflow{Float64, Rational{Int64}}(-0.0031502508072285204, 167//12), Cashflow{Float64, Rational{Int64}}(-0.4672090174521514, 14//1)]\n\n\nAnd running consolidate groups the cashflows into timepoint =&gt; amount pairs.\n\nconsolidate(cashflow_vectors)\n\nTransducers.GroupByViewDict{Rational{Int64},Float64,…}(...):\n  20//3   =&gt; -578.141\n  125//12 =&gt; -242.513\n  29//4   =&gt; -500.104\n  229//12 =&gt; -16.8259\n  9//4    =&gt; -571.587\n  71//4   =&gt; -45.4666\n  10//3   =&gt; -713.616\n  109//6  =&gt; -31.4885\n  95//12  =&gt; -466.29\n  19//6   =&gt; -689.977\n  43//6   =&gt; -503.978\n  175//12 =&gt; -95.5289\n  143//12 =&gt; -179.928\n  12      =&gt; -2104.79\n  5//3    =&gt; -565.613\n  19//4   =&gt; -749.539\n  199//12 =&gt; -59.7767\n  187//12 =&gt; -76.1478\n  13//4   =&gt; -702.062\n  ⋮       =&gt; ⋮\n\n\n\n\n\nHere’s a visualization of the liability cashflows, showing that when the interest rates are bumped up slightly, that there is more surrenders that occur earlier on (so there’s fewer policies around at the time of each maturity). Negative cashflows are outflows:\n\nlet\n    d = consolidate([p(c) for p in liabilities])\n    ks = collect(keys(d)) |&gt; sort!\n    vs = [d[k] for k in ks]\n\n    c2 = curve(zero_rates .+ 0.005, times)\n    d2 = consolidate([p(c2) for p in liabilities])\n    ks2 = collect(keys(d2)) |&gt; sort!\n    vs2 = [d2[k] for k in ks2]\n\n    f = Figure(size = (900, 600))\n    ax = Axis(f[1, 1], \n        xlabel = \"Time (Years)\",\n        ylabel = \"Cashflow Amount (cumulative)\",\n        title = \"Cumulative Liability Cashflows: Base vs +50bp Rate Shock\",\n    )\n\n    lines!(ax, ks, cumsum(vs), label = \"Base Scenario\")\n\n    lines!(ax, ks2, cumsum(vs2), label = \"+50bp Rate Shock\")\n\n    axislegend(ax, position = :rb)\n\n    f\nend\n\n\n\n\nIn the upwards shaped yield curve, without a surrender charge or market value adjustment, many mid-to-late-duration policyholders elect to surrender instead of hold to maturity.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Auto-differentiation and Asset Liability Management (AAD & ALM)</span>"
    ]
  },
  {
    "objectID": "autodiff_alm.html#computing-derivatives-with-autodiff",
    "href": "autodiff_alm.html#computing-derivatives-with-autodiff",
    "title": "29  Auto-differentiation and Asset Liability Management (AAD & ALM)",
    "section": "29.5 Computing Derivatives with Autodiff",
    "text": "29.5 Computing Derivatives with Autodiff\nRather than approximating derivatives through finite differences, autodiff computes exact values, gradients, and Hessians. The concepts and background are covered in Chapter 16.\nThe value_gradient_and_hessian function returns the present value, key rate durations (gradient), and convexities (Hessian diagonal) for the entire liability portfolio. We compute similar derivatives for each potential asset.\n\nvgh_liab = let\n    value_gradient_and_hessian(z -&gt; annuities(z, liabilities), AutoForwardDiff(), zero_rates)\nend\n\n(-102338.03168024882, [11473.162480401212, 29674.265475181794, 29760.303258614313, 256235.3773731404, 124309.28378885919, 6639.18472412876], [-165677.5776306093 194733.2820872364 … 10917.847428856832 -42666.210707907914; 194733.2820872362 -1.6718628083482927e6 … 2.472861164370926e6 1.2759102344684233e6; … ; 10917.847428856741 2.4728611643709284e6 … -3.712481627843568e7 -2.766946612898521e6; -42666.21070790795 1.275910234468423e6 … -2.7669466128985276e6 -2.1589110524045557e7])\n\n\n\n29.5.1 Gradients and Hessians in ALM\nLet’s dive into the results here a little bit.\nThe first element of vgh_liab is the value of the liability portfolio using the yield curve constructed earlier:\n\nvgh_liab[1]\n\n-102338.03168024882\n\n\nThe second element of vgh_liab is the partial derivative with respect to each of the inputs (here, just the zero_rates rates that dictate the curve). The sum of the partials is the effective duration of the liabilities.\n\n@show sum(vgh_liab[2])\nvgh_liab[2]\n\nsum(vgh_liab[2]) = 458091.57710032567\n\n\n6-element Vector{Float64}:\n  11473.162480401212\n  29674.265475181794\n  29760.303258614313\n 256235.3773731404\n 124309.28378885919\n   6639.18472412876\n\n\nThis is the sensitivity relative to a full unit change in rates (e.g. 1.0). So if we wanted to estimate the dollar impact of a 50bps change, we would take 0.005 times the gradient/hessian. Also note these are ‘dollar durations’ but we could divide by the price to get effective or percentage durations:\n\n-sum(vgh_liab[2]) / vgh_liab[1]\n\n4.476259407955147\n\n\nAdditionally, note that this is the dynamic duration of the liabilities, not the static duration which ignores the effect of the interest-sensitive behavior of the liabilities.\n\nlet \n    dynamic(zero_rates) = value(curve(zero_rates,times),liabilities[1])\n    cfs = liabilities[1](c)\n    static(zero_rates) = let\n        c = curve(zero_rates,times)\n        # note that `cfs` are defined outside of the function, so \n        # will not change as the curve is sensitized\n        mapreduce(cf -&gt; c(cf.time) * cf.amount,+,cfs)\n    end\n\n    @show gradient(dynamic,AutoForwardDiff(),zero_rates) |&gt; sum\n    @show gradient(static,AutoForwardDiff(),zero_rates) |&gt; sum\nend\n\ngradient(dynamic, AutoForwardDiff(), zero_rates) |&gt; sum = 7.049841100296051\ngradient(static, AutoForwardDiff(), zero_rates) |&gt; sum = 9.768741903898462\n\n\n9.768741903898462\n\n\nDue to the steepness of the surrender function, the policy exiting sooner, on average, results in a higher change in value than if the policy was not sensitive to the change in rates. The increase in value from earlier cashflows outweighs the greater discount rate.\nThe third element of vgh_liab is the Hessian matrix, containing all second partial derivatives with respect to the yield curve inputs:\n\nvgh_liab[3]\n\n6×6 Matrix{Float64}:\n     -1.65678e5   1.94733e5  -2.20931e5  …  10917.8        -42666.2\n      1.94733e5  -1.67186e6   2.87573e6         2.47286e6       1.27591e6\n     -2.20931e5   2.87573e6  -9.91864e6        -4.18996e6      -2.77722e6\n      6.55604e5  -3.32804e6   1.27248e7         1.885e7         8.15171e6\n  10917.8         2.47286e6  -4.18996e6        -3.71248e7      -2.76695e6\n -42666.2         1.27591e6  -2.77722e6  …     -2.76695e6      -2.15891e7\n\n\nThis matrix captures the convexity characteristics of the liability portfolio. The diagonal elements represent “key rate convexities”—how much the duration at each key rate changes as that specific rate moves:\n\n@show diag(vgh_liab[3])\n@show sum(diag(vgh_liab[3]))  # Total dollar convexity\n\ndiag(vgh_liab[3]) = [-165677.5776306093, -1.6718628083482927e6, -9.918640741888657e6, -2.8696069550070945e7, -3.712481627843568e7, -2.1589110524045557e7]\nsum(diag(vgh_liab[3])) = -9.916617748041974e7\n\n\n-9.916617748041974e7\n\n\nLike duration, we can convert dollar convexity to percentage convexity by dividing by the portfolio value:\n\nsum(diag(vgh_liab[3])) / vgh_liab[1]\n\n969.0061050837932\n\n\nThe off-diagonal elements show cross-convexities—how the sensitivity to one key rate changes when a different key rate moves. For most portfolios, these cross-terms are smaller than the diagonal terms but can be significant for complex instruments.\nThis convexity measurement is also dynamic, capturing how the surrender behavior changes the second-order interest rate sensitivity:\n\nlet \n    dynamic(zero_rates) = value(curve(zero_rates,times),liabilities[1])\n    cfs = liabilities[1](c)\n    static(zero_rates) = let\n        c = curve(zero_rates,times)\n        mapreduce(cf -&gt; c(cf.time) * cf.amount,+,cfs)\n    end\n\n    @show hessian(dynamic,AutoForwardDiff(),zero_rates) |&gt; diag |&gt; sum\n    @show hessian(static,AutoForwardDiff(),zero_rates) |&gt; diag |&gt; sum\nend\n\n(hessian(dynamic, AutoForwardDiff(), zero_rates) |&gt; diag) |&gt; sum = -1689.6549256406988\n(hessian(static, AutoForwardDiff(), zero_rates) |&gt; diag) |&gt; sum = -137.6180536078304\n\n\n-137.6180536078304\n\n\nThe dynamic convexity differs from static convexity because the surrender function creates path-dependent behavior. As rates change, not only do the discount factors change, but the timing and magnitude of cashflows shift as well. This interaction between discount rate changes and cashflow timing changes produces the additional convexity captured in the dynamic measurement. Note how the convexity is larger in the dynamic case.\nFor ALM purposes, this convexity information helps quantify how well a duration-matched hedge will perform under large rate movements.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Auto-differentiation and Asset Liability Management (AAD & ALM)</span>"
    ]
  },
  {
    "objectID": "autodiff_alm.html#optimizing-an-asset-portfolio",
    "href": "autodiff_alm.html#optimizing-an-asset-portfolio",
    "title": "29  Auto-differentiation and Asset Liability Management (AAD & ALM)",
    "section": "29.6 Optimizing an Asset Portfolio",
    "text": "29.6 Optimizing an Asset Portfolio\n\n29.6.1 Define Asset Universe\nWe will create a set of par bonds and select a portfolio of assets that matches the liabilities, subject to duration and KRD constraints:\n\nasset_universe = [\n    FixedBond(par_yield(c,t,4),t,4)\n    for t in 1:20\n]\n\n20-element Vector{FixedBond{Float64, Int64, Int64}}:\n FixedBond{Float64, Int64, Int64}(0.009998004795647176, 1, 4)\n FixedBond{Float64, Int64, Int64}(0.019932158064569137, 2, 4)\n FixedBond{Float64, Int64, Int64}(0.01997170973543043, 3, 4)\n FixedBond{Float64, Int64, Int64}(0.02388292451655035, 4, 4)\n FixedBond{Float64, Int64, Int64}(0.02952635925170046, 5, 4)\n FixedBond{Float64, Int64, Int64}(0.03446708593197626, 6, 4)\n FixedBond{Float64, Int64, Int64}(0.038602669873830896, 7, 4)\n FixedBond{Float64, Int64, Int64}(0.04207416430705521, 8, 4)\n FixedBond{Float64, Int64, Int64}(0.04493178862970366, 9, 4)\n FixedBond{Float64, Int64, Int64}(0.04717539935714873, 10, 4)\n FixedBond{Float64, Int64, Int64}(0.04881213298628277, 11, 4)\n FixedBond{Float64, Int64, Int64}(0.04996932107971216, 12, 4)\n FixedBond{Float64, Int64, Int64}(0.050768208246499, 13, 4)\n FixedBond{Float64, Int64, Int64}(0.051299382922034086, 14, 4)\n FixedBond{Float64, Int64, Int64}(0.051633982591327676, 15, 4)\n FixedBond{Float64, Int64, Int64}(0.051830458100536235, 16, 4)\n FixedBond{Float64, Int64, Int64}(0.051938805267308895, 17, 4)\n FixedBond{Float64, Int64, Int64}(0.05200329735010632, 18, 4)\n FixedBond{Float64, Int64, Int64}(0.0520643078824987, 19, 4)\n FixedBond{Float64, Int64, Int64}(0.052159576852118396, 20, 4)\n\n\nAnd we capture the measures for each of the available assets for the portfolio selection:\n\nvgh_assets= [value_gradient_and_hessian(x-&gt;value(curve(x,times),a),AutoForwardDiff(), zero_rates) for a in asset_universe]\n\n20-element Vector{Tuple{Float64, Vector{Float64}, Matrix{Float64}}}:\n (0.9999999999999999, [-0.9976397847116376, 0.001889581158300676, -0.0005911493114439049, 7.570359994056739e-5, -6.627009344962502e-6, 4.849306454692366e-7], [0.9976397847116376 0.0 … 0.0 0.0; 0.0 -0.003779162316601352 … 0.0 0.0; … ; 0.0 0.0 … 6.627009344962502e-5 0.0; 0.0 0.0 … 0.0 -9.698612909384728e-6])\n (1.0, [-0.023388491193219983, -1.9440744792877025, 0.002357046582052256, -0.00030184744874882244, 2.6423391558419248e-5, -1.9335286336438875e-6], [0.023388491193219983 0.0 … 0.0 0.0; 0.0 3.888148958575405 … 0.0 0.0; … ; 0.0 0.0 … -0.00026423391558419245 0.0; 0.0 0.0 … 0.0 3.867057267287775e-5])\n (0.9999999999999999, [-0.021831553704588873, -0.04038472750022178, -2.8579627416823383, 0.0013610088406224487, -0.00011914120745198929, 8.718144131725877e-6], [0.021831553704588873 0.0 … 0.0 0.0; 0.0 0.08076945500044357 … 0.0 0.0; … ; 0.0 0.0 … 0.0011914120745198928 0.0; 0.0 0.0 … 0.0 -0.0001743628826345176])\n (1.0, [-0.12086175598272814, 0.6873410858480586, -2.580077722810009, -1.9182858491821426, 0.12039440671851541, -0.00880984684370108], [0.12086175598272814 0.0 … 0.0 0.0; 0.0 -1.3746821716961173 … 0.0 0.0; … ; 0.0 0.0 … -1.2039440671851547 0.0; 0.0 0.0 … 0.0 0.1761969368740217])\n (0.9999999999999999, [-0.03587182758225417, -0.03178847835869419, -0.1623244800675958, -4.434521498439157, 0.004426971666740553, -0.00032394314178212753], [0.03587182758225417 0.0 … 0.0 0.0; 0.0 0.06357695671738837 … 0.0 0.0; … ; 0.0 0.0 … -0.04426971666740108 0.0; 0.0 0.0 … 0.0 0.006478862835641435])\n (1.0, [0.030718124690857655, -0.6006841432615017, 1.4677638724025694, -5.637585375627018, -0.7347382844153851, 0.05016595986898516], [-0.030718124690857655 0.0 … 0.0 0.0; 0.0 1.2013682865230033 … 0.0 0.0; … ; 0.0 0.0 … 7.347382844153855 0.0; 0.0 0.0 … 0.0 -1.003319197379703])\n (1.0000000000000002, [0.04918723421089006, -0.7875296020508475, 1.9813722618339165, -5.475677497528941, -2.004491225500428, 0.11757953091355329], [-0.04918723421089006 0.0 … 0.0 0.0; 0.0 1.575059204101695 … 0.0 0.0; … ; 0.0 0.0 … 20.044912255004277 0.0; 0.0 0.0 … 0.0 -2.3515906182710657])\n (1.0, [0.033435294105167324, -0.7017190688236229, 1.6989628644341668, -4.366584268206587, -3.585974828657458, 0.16298608630113406], [-0.033435294105167324 0.0 … 0.0 0.0; 0.0 1.4034381376472458 … 0.0 0.0; … ; 0.0 0.0 … 35.859748286574586 0.0; 0.0 0.0 … 0.0 -3.259721726022681])\n (1.0, [-0.002431911053734727, -0.453291445963555, 0.943680562936937, -2.736926924650531, -5.249469475354395, 0.14536286030892792], [0.002431911053734727 0.0 … 0.0 0.0; 0.0 0.90658289192711 … 0.0 0.0; … ; 0.0 0.0 … 52.49469475354395 0.0; 0.0 0.0 … 0.0 -2.907257206178559])\n (1.0, [-0.043736737346666915, -0.1561960575179688, 0.05060552918717243, -1.0331954789936535, -6.749501521945598, 0.021211022722198228], [0.043736737346666915 0.0 … 0.0 0.0; 0.0 0.3123921150359376 … 0.0 0.0; … ; 0.0 0.0 … 67.49501521945596 0.0; 0.0 0.0 … 0.0 -0.42422045444396633])\n (1.0, [-0.07783333114293098, 0.09131520428519538, -0.691404940606247, 0.358435555117127, -7.871988981058315, -0.24640615516201977], [0.07783333114293098 0.0 … 0.0 0.0; 0.0 -0.18263040857039076 … 0.0 0.0; … ; 0.0 0.0 … 78.71988981058314 0.0; 0.0 0.0 … 0.0 4.928123103240393])\n (0.9999999999999998, [-0.10282370981723697, 0.27316914850365254, -1.236189221863321, 1.3810544521498946, -8.590793469976155, -0.6595197816774591], [0.10282370981723697 0.0 … 0.0 0.0; 0.0 -0.5463382970073051 … 0.0 0.0; … ; 0.0 0.0 … 85.90793469976155 0.0; 0.0 0.0 … 0.0 13.190395633549183])\n (1.0, [-0.11949556676658321, 0.39420696816817774, -1.5990337099647314, 2.061593933011007, -8.929700855238488, -1.2105657614617258], [0.11949556676658321 0.0 … 0.0 0.0; 0.0 -0.7884139363363555 … 0.0 0.0; … ; 0.0 0.0 … 89.29700855238488 0.0; 0.0 0.0 … 0.0 24.21131522923451])\n (1.0, [-0.12861217374499395, 0.45940241183877495, -1.795349002854901, 2.4278333854267613, -8.91526236179936, -1.8910239053244644], [0.12861217374499395 0.0 … 0.0 0.0; 0.0 -0.9188048236775499 … 0.0 0.0; … ; 0.0 0.0 … 89.15262361799358 0.0; 0.0 0.0 … 0.0 37.82047810648928])\n (1.0000000000000002, [-0.13095883074250508, 0.47410464862949026, -1.8414811453964168, 2.509733511219679, -8.577552928649126, -2.6913836723507663], [0.13095883074250508 0.0 … 0.0 0.0; 0.0 -0.9482092972589805 … 0.0 0.0; … ; 0.0 0.0 … 85.77552928649126 0.0; 0.0 0.0 … 0.0 53.82767344701532])\n (1.0000000000000002, [-0.12736808399407992, 0.4441629661639336, -1.755136918280806, 2.3401183445834253, -7.95046134051263, -3.601221533618457], [0.12736808399407992 0.0 … 0.0 0.0; 0.0 -0.8883259323278672 … 0.0 0.0; … ; 0.0 0.0 … 79.50461340512629 0.0; 0.0 0.0 … 0.0 72.02443067236912])\n (1.0, [-0.11873204577234525, 0.3759779476315208, -1.5555709896240153, 1.9549504787382388, -7.071672780590235, -4.609356718883685], [0.11873204577234525 0.0 … 0.0 0.0; 0.0 -0.7519558952630416 … 0.0 0.0; … ; 0.0 0.0 … 70.71672780590235 0.0; 0.0 0.0 … 0.0 92.18713437767369])\n (1.0000000000000002, [-0.10600658486565401, 0.2765052903118995, -1.263620816446502, 1.3933438321898368, -5.982443557400675, -5.704063303455876], [0.10600658486565401 0.0 … 0.0 0.0; 0.0 -0.553010580623799 … 0.0 0.0; … ; 0.0 0.0 … 59.824435574006756 0.0; 0.0 0.0 … 0.0 114.0812660691175])\n (0.9999999999999998, [-0.09021040840946391, 0.15322952181447289, -0.901645146872998, 0.6974094494321246, -4.727239626091724, -6.873321888975546], [0.09021040840946391 0.0 … 0.0 0.0; 0.0 -0.30645904362894577 … 0.0 0.0; … ; 0.0 0.0 … 47.27239626091725 0.0; 0.0 0.0 … 0.0 137.46643777951095])\n (1.0, [-0.07242116850655933, 0.014120467762457808, -0.49340697871628564, -0.08799425539267751, -3.353297361284719, -8.105097006013095], [0.07242116850655933 0.0 … 0.0 0.0; 0.0 -0.028240935524915616 … 0.0 0.0; … ; 0.0 0.0 … 33.53297361284719 0.0; 0.0 0.0 … 0.0 162.1019401202619])\n\n\n\n\n29.6.2 Optimization Routine\nThis optimization function uses functionality from JuMP, a robust optimization library in Julia.\nWith derivatives available, we can optimize the asset portfolio to match liability characteristics.The optimization maximizes asset yield while constraining the difference between asset and liability key rate durations. This ensures that small yield curve movements don’t create large changes in surplus.\n\nfunction optimize_portfolio(assets, vgh_assets, liabs, vgh_liabs, constraints)\n    n = length(assets)\n\n    # Create model\n    model = Model(HiGHS.Optimizer)\n    set_silent(model)  # Suppress solver output\n\n    # Decision variables: weight vector w\n    @variable(model, w[1:n])\n\n    @constraint(model, w .&gt;= 0)  # Long-only constraint\n    # Budget/asset value constraint\n    budget_sum = sum(w .* [a[1] for a in vgh_assets]) + vgh_liabs[1]\n    @constraint(model, budget_sum &lt;= 1e2)\n    @constraint(model, budget_sum &gt;= -1e2)\n\n    # Objective: Maximize total yield\n    @objective(model, Max, sum(w[i] * assets[i].coupon for i in 1:n))\n\n    # Gradient component (krd) constraints\n    for j in 1:length(vgh_liabs[2])\n        gradient_sum = sum(w[i] * vgh_assets[i][2][j] for i in 1:n) - sum(vgh_liabs[2][j])\n\n        @constraint(model, gradient_sum &gt;= constraints[:krd][:lower])\n        @constraint(model, gradient_sum &lt;= constraints[:krd][:upper])\n    end\n\n    # total duration constraint \n    duration_gap = sum(w[i] * sum(vgh_assets[i][2]) for i in 1:n) + sum(vgh_liabs[2])\n    @constraint(model, duration_gap &lt;= constraints[:krd][:upper])\n    @constraint(model, duration_gap &gt;= constraints[:krd][:lower])\n\n    # Solve\n    optimize!(model)\n\n    # Return results\n    if termination_status(model) == MOI.OPTIMAL\n        return (\n            status=:optimal,\n            weights=JuMP.value.(w),\n            objective_value=objective_value(model),\n        )\n    else\n        return (status=termination_status(model), weights=nothing)\n    end\nend\n\n\n# Define gradient constraints\nconstraints = Dict(\n    :krd =&gt; Dict(:lower =&gt; -0.35e6, :upper =&gt; 0.35e6),\n    :duration =&gt; Dict(:lower =&gt; -0.05e6, :upper =&gt; 0.05e6)\n)\n\n# Optimize\nresult = optimize_portfolio(asset_universe, vgh_assets, liabilities, vgh_liab, constraints)\n\n(status = :optimal, weights = [-0.0, -0.0, 7094.533974533662, 39882.20531208219, -0.0, -0.0, -0.0, -0.0, -0.0, 13106.76798691019, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0, 42354.52440672279], objective_value = 3921.7047571609205)\n\n\n\n\n29.6.3 Results\nThe optimization produces asset weights that hedge the liability portfolio. We can visualize both the resulting cashflow patterns and the key rate duration matching:\n\n\nlet\n    d = consolidate([p(c) for p in liabilities])\n    ks = collect(keys(d)) |&gt; sort!\n    vs = -cumsum([d[k] for k in ks])\n\n    f = Figure(size = (900, 600))\n    ax = Axis(f[1, 1], \n        xlabel = \"Time (Years)\",\n        ylabel = \"Cashflow Amount (cumulative)\",\n        title = \"Cumulative Asset vs Liability Cashflows\",\n    )\n\n    lines!(ax, ks, vs, label = \"Liabilities\")\n\n\n    asset_cfs = map(1:length(asset_universe)) do i\n        cfs =\n            result.weights[i] * asset_universe[i](c)\n    end\n\n    d = consolidate(asset_cfs)\n    ks2 = collect(keys(d)) |&gt; sort!\n    vs2 = cumsum([d[k] for k in ks2])\n    lines!(ax, ks2, vs2, label = \"Assets\")\n\n    axislegend(ax, position = :rb)\n\n    f\nend    \n\n\n\n\n\n\n\nlet\n    asset_krds = sum(getindex.(vgh_assets,2) .* result.weights)\n    liab_krds = -vgh_liab[2]\n\n    f = Figure(size = (800, 500))\n    ax = Axis(f[1, 1], \n        xlabel = \"Tenor (Years)\",\n        ylabel = \"Key Rate Dollar Duration\",\n        title = \"Asset vs Liability Key Rate Dollar Duration Profile\",\n    )\n    \n    scatter!(ax, times, asset_krds, label = \"Optimized Assets\")\n    \n    scatter!(ax, times, liab_krds, label = \"Liabilities\")\n    \n    axislegend(ax, position = :rt)\n    \n    f\nend\n\n\n\n\nThe first plot shows the distribution of asset cashflows over time. The second compares the key rate duration profiles of the optimized asset portfolio and the liability portfolio, demonstrating how well the hedge performs across different points on the yield curve.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Auto-differentiation and Asset Liability Management (AAD & ALM)</span>"
    ]
  },
  {
    "objectID": "autodiff_alm.html#computational-benefits",
    "href": "autodiff_alm.html#computational-benefits",
    "title": "29  Auto-differentiation and Asset Liability Management (AAD & ALM)",
    "section": "29.7 Computational Benefits",
    "text": "29.7 Computational Benefits\nAutodiff provides several advantages over traditional finite difference approaches:\n\nExact derivatives rather than approximations\nSingle function evaluation computes value and all derivatives\nNo tuning of step sizes or dealing with numerical artifacts\nScales efficiently to high-dimensional parameter spaces\n\nFor ALM applications, this means more accurate risk measurement and the ability to optimize portfolios with complex constraints that would be computationally expensive using traditional methods.\nHere, we value 100,000 interest-sensitive policies with a monthly timestep for up to 20 years and compute 1st and 2nd order partial sensitives extremely quickly:\n\n@btime value_gradient_and_hessian(z -&gt; annuities(z, liabilities), AutoForwardDiff(), zero_rates)\n\n  5.281 s (127541709 allocations: 36.60 GiB)\n\n\n(-102338.03168024882, [11473.162480401212, 29674.265475181794, 29760.303258614313, 256235.3773731404, 124309.28378885919, 6639.18472412876], [-165677.5776306093 194733.2820872364 … 10917.847428856832 -42666.210707907914; 194733.2820872362 -1.6718628083482927e6 … 2.472861164370926e6 1.2759102344684233e6; … ; 10917.847428856741 2.4728611643709284e6 … -3.712481627843568e7 -2.766946612898521e6; -42666.21070790795 1.275910234468423e6 … -2.7669466128985276e6 -2.1589110524045557e7])\n\n\nHowever, there’s still some performance left on the table! the (d::DeferredAnnuity)(curve) function defined above is not type stable. In the appendix to this post, we’ll cover a way to improve the performance even more.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Auto-differentiation and Asset Liability Management (AAD & ALM)</span>"
    ]
  },
  {
    "objectID": "autodiff_alm.html#conclusion",
    "href": "autodiff_alm.html#conclusion",
    "title": "29  Auto-differentiation and Asset Liability Management (AAD & ALM)",
    "section": "29.8 Conclusion",
    "text": "29.8 Conclusion\nThe Julia ecosystem supports this workflow through packages like DifferentiationInterface for autodiff, JuMP for optimization, and FinanceCore for financial mathematics. This combination enables sophisticated ALM implementations that are both mathematically precise and computationally efficient.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Auto-differentiation and Asset Liability Management (AAD & ALM)</span>"
    ]
  },
  {
    "objectID": "autodiff_alm.html#appendix-even-more-performance-advanced",
    "href": "autodiff_alm.html#appendix-even-more-performance-advanced",
    "title": "29  Auto-differentiation and Asset Liability Management (AAD & ALM)",
    "section": "29.9 Appendix: Even more performance (Advanced)",
    "text": "29.9 Appendix: Even more performance (Advanced)\nJulia is fastest when all functions are type stable (i.e. the return type can be inferred at compile time). Looking back at the function defined above, the issue is that the av function is defined outside of the scope used within the map block. This means that the compiler can’t be sure that av won’t be modified while being used within the map. Therefore, av get’s ‘boxed’ and held as an Any type. This type uncertainty propagates to the value returned from the (d::DeferredAnnuity)(curve) function:\nfunction (d::DeferredAnnuity)(curve)\n    av = 1. \n    map(1//12:1//12:d.tenor) do t\n        mkt_rate = -log(curve(d.tenor) / curve(t)) / (d.tenor - t)\n        av *= exp(d.rate / 12)\n        rate_diff = mkt_rate - d.rate\n        sr = t == d.tenor ? 1.0 : surrender_rate(rate_diff) / 12\n        av_surr = av * sr\n        av -= av_surr\n        Cashflow(-av_surr, t)\n    end\nend\nAn alterative would be to write a for loop and initialize an array to hold the cashflows. The challenge with that is to concretely define the output type of the resulting array. Particularly when combine with AD, the types within the program are no longer basic floats and integers, as we have dual numbers and more complex types running through our functions.\nTo maintain most of the simplicity, an alternative approach1 is to use small, immutable containers from MicroCollections.jl and combine them with BangBang.jl. Then, instead of using map we will write a regular loop. The macro @unroll is defined to unroll the first N iterations of the loop. This means that the macro transforms the source code to explicitly write out the first two loops. An example of this might be as follows where two iterations of the loop are unrolled.\nfunction basic_loop()\n    out = []\n    for i ∈ 1:10\n        push!(out,i)\n    end\n    out\nend\n\nfunction partially_unrolled_loop()\n    out = []\n    push!(out,1)\n    push!(out,2) # two steps unrolled\n    for i ∈ 3:10\n        push!(out,i)\n    end\n    out\nend\nHere’s the macro that does this (expand to see the full definition):\n\n\n\n\n\n\nNote@unroll macro\n\n\n\n\n\n\n\"\"\"\n    @unroll N for_loop\n\nUnroll the first `N` iterations of a for loop, with remaining iterations handled by a regular loop.\n\nThis macro takes a for loop and explicitly expands the first `N` iterations, which can improve \nperformance and type stability, particularly when building collections where the first few \niterations determine the container's type.\n\n# Arguments\n- `N::Int`: Number of loop iterations to unroll (must be a compile-time constant)\n- `for_loop`: A standard for loop expression\n\n\"\"\"\nmacro unroll(N::Int, loop)\n    Base.isexpr(loop, :for) || error(\"only works on for loops\")\n    Base.isexpr(loop.args[1], :(=)) || error(\"This loop pattern isn't supported\")\n    val, itr = esc.(loop.args[1].args)\n    body = esc(loop.args[2])\n    @gensym loopend\n    label = :(@label $loopend)\n    goto = :(@goto $loopend)\n    out = Expr(:block, :(itr = $itr), :(next = iterate(itr)))\n    unrolled = map(1:N) do _\n        quote\n            isnothing(next) && @goto loopend\n            $val, state = next\n            $body\n            next = iterate(itr, state)\n        end\n    end\n    append!(out.args, unrolled)\n    remainder = quote\n        while !isnothing(next)\n            $val, state = next\n            $body\n            next = iterate(itr, state)\n        end\n        @label loopend\n    end\n    push!(out.args, remainder)\n    out\nend\n\nMain.Notebook.@unroll\n\n\n\n\n\nThen, we re-write and redefine (d::DeferredAnnuity)(curve) to utilize this technique.\n\nusing BangBang, MicroCollections\n\nfunction (d::DeferredAnnuity)(curve)\n    times = 1//12:1//12:d.tenor\n    out = UndefVector{Union{}}(length(times)) # 1\n    av = 1.0\n    @unroll 2 for (i, t) ∈ enumerate(times) # 2\n        mkt_rate = -log(curve(d.tenor) / curve(t)) / (d.tenor - t)\n        av *= exp(d.rate / 12)\n        rate_diff = mkt_rate - d.rate\n        sr = t == d.tenor ? 1.0 : surrender_rate(rate_diff) / 12\n        av_surr = av * sr\n        av -= av_surr\n        cf = Cashflow(-av_surr, t)\n        out = setindex!!(out, cf, i) # 3\n    end\n    out\nend;\n\n\nWe tell the out vector how many elements to expect\nWe unroll two iterations of the loop so that the compiler can use the calculated result to determine the type of the output container.\nWe use setindex!! from BangBang to efficiently update the output vector and its type.\n\nUsing this technique, we can see that we achieve a significant speedup (less than half the runtime) from the earlier version due to improving the type stability of the code:\n\n@btime value_gradient_and_hessian(z -&gt; annuities(z, liabilities), AutoForwardDiff(), zero_rates)\n\n  1.921 s (600199 allocations: 5.94 GiB)\n\n\n(-102338.03168024882, [11473.162480401212, 29674.265475181794, 29760.303258614313, 256235.3773731404, 124309.28378885919, 6639.18472412876], [-165677.5776306093 194733.2820872364 … 10917.847428856832 -42666.210707907914; 194733.2820872362 -1.6718628083482927e6 … 2.472861164370926e6 1.2759102344684233e6; … ; 10917.847428856741 2.4728611643709284e6 … -3.712481627843568e7 -2.766946612898521e6; -42666.21070790795 1.275910234468423e6 … -2.7669466128985276e6 -2.1589110524045557e7])",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Auto-differentiation and Asset Liability Management (AAD & ALM)</span>"
    ]
  },
  {
    "objectID": "autodiff_alm.html#footnotes",
    "href": "autodiff_alm.html#footnotes",
    "title": "29  Auto-differentiation and Asset Liability Management (AAD & ALM)",
    "section": "",
    "text": "With thanks to the helpful persons on the Julia Zulip and in particular Mason Protter for this approach.↩︎",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Auto-differentiation and Asset Liability Management (AAD & ALM)</span>"
    ]
  },
  {
    "objectID": "stochastic-mortality.html",
    "href": "stochastic-mortality.html",
    "title": "30  Stochastic Mortality Projections",
    "section": "",
    "text": "30.1 Chapter Overview\nA term life insurance policy is used to illustrate: selecting key model features, design tradeoffs between a few different approaches, and a discussion of the performance impacts of the different approaches to parallelism.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Stochastic Mortality Projections</span>"
    ]
  },
  {
    "objectID": "stochastic-mortality.html#introduction",
    "href": "stochastic-mortality.html#introduction",
    "title": "30  Stochastic Mortality Projections",
    "section": "30.2 Introduction",
    "text": "30.2 Introduction\nMonte Carlo simulation is common in risk and valuation contexts. This worked example will create a population of term life insurance policies and simulate the associated claims stochastically. For this chapter, the focus is not so much on the outcomes of the model, but instead how and why the model was chosen to be setup in the way that it is.\nThe general structure of the example is:\n\nDefine the datatypes and sample data\nDefine the core logic that governs the projected outcomes for the modeled policies\nEvaluate a few ways to structure the simulation, including:\nallocating and non-allocating approaches\nsingle threaded and mutli-threaded approaches\n\nAs will be shown, the number of simulations able to be completed on modern CPU hardware is really remarkable!\n\nusing CSV, DataFrames\nusing MortalityTables, FinanceCore\nusing Dates\nusing ThreadsX\nusing BenchmarkTools\nusing Random\nusing CairoMakie",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Stochastic Mortality Projections</span>"
    ]
  },
  {
    "objectID": "stochastic-mortality.html#data-and-types",
    "href": "stochastic-mortality.html#data-and-types",
    "title": "30  Stochastic Mortality Projections",
    "section": "30.3 Data and Types",
    "text": "30.3 Data and Types\n\n30.3.1 @enums and the Policy Type\nOur core unit of simulation with be a single life insurance Policy. Important characteristics include: the age a policy was issued at, the sex of the insured, and risk class to determine the assumed mortality rate. To make the example more realistic and demonstrate how it might look for a real block of inforce policies, additional fields have been included such as ID (not really used, but a common identifier) and COLA which is a cost-of-living-adjustment, used to modify the policy benefit through time. To be clear: the Policy type has more fields than will actually be used in the calculations, with the purpose to show how typical fields used in practice might be defined.\nBefore we define the core Policy type, there’s a couple of types we might consider defining that would subsequently get used by Policy: types representing sex and risk class. A typical approach might be to simply define associated types, like this:\nabstract type Sex end\n\nstruct Male &lt;: Sex end\nstruct Female &lt;: Sex end\nThen, if we were to include a sex field in Policy, we could write it like this:\nstruct Policy\n    # ... \n    sex::Sex\n    # ...\nend\nThis would be a totally valid and logical approach! However, high performance is a top priority for this simulation, and therefore this approach would sacrifice being able to keep Policy data on the stack instead of the heap. This is because Sex is of unknown concrete type, which could be as simple as our definition above (with no fields in Male and Female) or someone could add a new Alien subtype of Sex with a number of associated data fields! This is why Julia cannot assume that subtypes of Sex will always be a simple Singletons with no associated data.\nInstead, we can utilize Enums, which are a sort of lightweight type where the only thing that matters with it is distinguishing between categories. Enums in Base Julia are basically a set of constants grouped together that reference an associated integer.\n\n@enum Sex Female = 1 Male = 2\n@enum Risk Standard = 1 Preferred = 2\n@enum PaymentMode Annual = 1 Quarterly = 4 Monthly = 12\n\nEnums are convenient because it lets us use human-meaningful names for integer-based categories. Julia will also keep track of valid options: we cannot now use anything other than Female or Male where we have said a Sex must be specified.\n\n\n\n\n\n\nNote\n\n\n\nThere exist Julia packages which are more powerful versions of Enums, essentially leaning into the ability to use the type system instead of just nice names for categorical variables.\n\n\nMoving on to the definition of the policy itself, here’s what that looks like. Note that every field has a type annotation associated with it.\n\nstruct Policy\n    id::Int\n    sex::Sex\n    benefit_base::Float64\n    COLA::Float64\n    mode::PaymentMode\n    issue_date::Date\n    issue_age::Int\n    risk::Risk\nend\n\nThe benefit of the way we have defined it here, using simple bits-types for each field is that our new composite Policy type is also a bitstype:\n\nlet\n    p = Policy(1, Male, 1e6, 0.02, Annual, today(), 50, Standard)\n\n    isbits(p)\nend\n\ntrue\n\n\n\n\n\n\n\n\nNote\n\n\n\nType annotations are optional, but providing them is able to coerce the values to be all plain bits (i.e. simple, non-referenced values like arrays are) when the type is constructed. We could have defined Policy without the types specified:\nstruct Policy\n    id\n    sex\n    ...\n    risk\nend\nLeaving out the annotations here forces Julia to assume that Any type could be used for the given field. Having the field be of type Any makes the instantiated struct data be stored in the heap, since Julia can’t know the size of Policy in bits in advance.\nWe would also find that the un-annotated type is about 50 times slower than the one with annotation due to the need to utilize runtime lookup and reference memory on the heap instead of the stack.\n\n\n\n\n30.3.2 The Data\nTo partially illustrate a common workflow, we’ll pretend that the data we are interested in comes from a CSV file, which will be defined inline using an IOBuffer so that the structure of the source data is clear to the reader. Only two policies will be listed for brevity, but we will duplicate them for simulation purposes later on.\n\nsample_csv_data =\n    IOBuffer(\n        raw\"id,sex,benefit_base,COLA,mode,issue_date,issue_age,risk\n         1,M,100000.0,0.03,12,1999-12-05,30,Std\n         2,F,200000.0,0.03,12,1999-12-05,30,Pref\"\n    )\n\nIOBuffer(data=UInt8[...], readable=true, writable=false, seekable=true, append=false, size=152, maxsize=Inf, ptr=1, mark=-1)\n\n\nWe will not load the sample data using a common pattern:\n\nLoad the source file into a DataFrame\nmap over each row of the dataframe, and return an instantiated Policy object\nWithin the map, apply basic data parsing and translation logic as needed.\n\n\npolicies = let\n\n    # read CSV directly into a dataframe\n1    df = CSV.read(sample_csv_data, DataFrame)\n\n    # map over each row and construct an array of Policy objects\n    map(eachrow(df)) do row\n        Policy(\n            row.id,\n            row.sex == \"M\" ? Male : Female,\n            row.benefit_base,\n            row.COLA,\n            PaymentMode(row.mode),\n            row.issue_date,\n            row.issue_age,\n            row.risk == \"Std\" ? Standard : Preferred,\n        )\n    end\n\n\nend\n\n\n1\n\nCSV.read(\"sample_inforce.csv\",DataFrame) could be used if the data really was in a CSV file named sample_inforce.csv instead of our demonstration IOBuffer.\n\n\n\n\n2-element Vector{Policy}:\n Policy(1, Male, 100000.0, 0.03, Monthly, Date(\"1999-12-05\"), 30, Standard)\n Policy(2, Female, 200000.0, 0.03, Monthly, Date(\"1999-12-05\"), 30, Preferred)",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Stochastic Mortality Projections</span>"
    ]
  },
  {
    "objectID": "stochastic-mortality.html#model-assumptions",
    "href": "stochastic-mortality.html#model-assumptions",
    "title": "30  Stochastic Mortality Projections",
    "section": "30.4 Model Assumptions",
    "text": "30.4 Model Assumptions\n\n30.4.1 Mortality Assumption\nMortalityTables.jl provides common life insurance industry tables, and we will use two tables: one each for male and female policies respectively.\n\nmort = Dict(\n1    Male =&gt; MortalityTables.table(988).ultimate,\n    Female =&gt; MortalityTables.table(992).ultimate,\n)\n\nfunction mortality(pol::Policy, params)\n    return params.mortality[pol.sex]\nend\n\n\n1\n\nultimate refers to not differentiating the mortality by a ‘select’ underwriting period, which is common but unnecessary for the demonstration in this chapter.\n\n\n\n\nmortality (generic function with 1 method)",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Stochastic Mortality Projections</span>"
    ]
  },
  {
    "objectID": "stochastic-mortality.html#model-structure-and-mechanics",
    "href": "stochastic-mortality.html#model-structure-and-mechanics",
    "title": "30  Stochastic Mortality Projections",
    "section": "30.5 Model Structure and Mechanics",
    "text": "30.5 Model Structure and Mechanics\n\n30.5.1 Core Model Behavior\nThe overall flow of the model loop will be as follows:\n\nDetermine some initialized values for each policy at the start of the projection.\nStep through annual timesteps and simulate whether a death has occurred.\n\nIf a death has occurred, log the benefit paid out.\nIf a death has not occurred keep track of the remaining lives inforce and increment policy values.\n\n\nThe code is shown first and then discussion will follow it:\n\nfunction pol_project!(out, policy, params)\n    # some starting values for the given policy\n    dur = length(policy.issue_date:Year(1):params.val_date) + 1\n    start_age = policy.issue_age + dur - 1\n    COLA_factor = (1 + policy.COLA)\n    cur_benefit = policy.benefit_base * COLA_factor^(dur - 1)\n\n    # get the right mortality vector\n    qs = mortality(policy, params)\n\n    # grab the current thread's id to write to results container \n    # without conflicting with other threads\n    tid = Threads.threadid()\n\n    ω = lastindex(qs)\n\n1    @inbounds for t in 1:min(params.proj_length, ω - start_age)\n\n        q = qs[start_age+t] # get current mortality\n\n        if (rand() &lt; q)\n            # if dead then just return and don't increment the results anymore\n2            out.benefits[t, tid] += cur_benefit\n            return\n        else\n            # pay benefit, add a life to the output count, and increment the benefit for next year\n            out.lives[t, tid] += 1\n            cur_benefit *= COLA_factor\n        end\n    end\nend\n\n\n1\n\ninbounds turns off bounds-checking, which makes hot loops faster but first write loop without it to ensure you don’t create an error (will crash if you have the error without bounds checking)\n\n2\n\nNote that the loop is iterating down a column (i.e. across rows) for efficiency (since Julia is column-major).\n\n\n\n\npol_project! (generic function with 1 method)\n\n\n\n\n30.5.2 Inputs and Outputs\n\n30.5.2.1 Inputs\nThe general approach for non-allocating model runs is to provide a previously instantiated container for the function to write the results to. Here, the incoming argument out(put) will be a named tuple with associated matrices as the lives and benefits fields. We know how many periods the model will simulate for and can therefore size the array appropriately at creation.\nOther inputs include: params which defines some global-like parameters and policy which is a single Policy object.\n\n\n\n\n\n\nNote\n\n\n\nNote that the unit of the core model logic is a single policy. This simplifies the logic and reduces the chance for error due to needing to code for entire arrays of policies at a single time (as would be the case for array oriented programming style, as described in Section 6.5).\n\n\n\n\n\n30.5.3 Threading\nThe simulations is using a threaded parallelism approach where it could be operating on any of the computer’s available threads. Multi-processor (multi-machine) or GPU-based computation would require some modifications see (Chapter 11). For the scale and complexity of this example, thread-based parallelism on a single CPU is all one should need for compute.\nThe threads are handled by distributed the work across threads (this is done by ThreadsX in the foreach loop below), but we need to write the appropriate place in the matrix so that threads do not compete for the same column in the output data. Therefore, when we create the lives and benefits matrices we need to have them sized so that the number of rows is the number of projection periods and the number of columns is the number of threads.\n\n\n30.5.4 Simulation Control\nParameters for our projection:\n\nparams = (\n    val_date=Date(2021, 12, 31),\n    proj_length=100,\n    mortality=mort,\n)\n\n(val_date = Date(\"2021-12-31\"), proj_length = 100, mortality = Dict{Sex, OffsetArrays.OffsetVector{Float64, Vector{Float64}}}(Male =&gt; [0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571  …  0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], Female =&gt; [0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745  …  0.376246, 0.386015, 0.393507, 0.398308, 0.4, 0.4, 0.4, 0.4, 0.4, 1.0]))\n\n\nHaving defined the model behavior at the unit of the policy above, we now need to define how the model should iterate over the entire population of interest. Given a vector of Policys in the policies argument, the project function will:\n\nCreate output containers, keeping in mind the projection length and number of threads being used.\nLoop over each policy, letting ThreadsX.foreach distribute the work across different threads.\nSum up the results across threads via reduce.\n\n\nfunction project(policies, params)\n    threads = Threads.nthreads()\n    benefits = zeros(params.proj_length, threads)\n    lives = zeros(Int, params.proj_length, threads)\n    out = (; benefits, lives)\n    ThreadsX.foreach(policies) do pol\n        pol_project!(out, pol, params)\n    end\n1    map(x -&gt; vec(reduce(+, x, dims=2)), out)\nend\n\n\n1\n\nvec turns the result into a 1D vector instead of a 1D matrix for later convenience.\n\n\n\n\nproject (generic function with 1 method)",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Stochastic Mortality Projections</span>"
    ]
  },
  {
    "objectID": "stochastic-mortality.html#running-the-projection",
    "href": "stochastic-mortality.html#running-the-projection",
    "title": "30  Stochastic Mortality Projections",
    "section": "30.6 Running the projection",
    "text": "30.6 Running the projection\nExample of a single projection:\n\nproject(repeat(policies, 100_000), params) # &lt;!&gt;\n\n(benefits = [1.3150006923636582e9, 1.3629884483817015e9, 1.3948748567356443e9, 1.499693567449984e9, 1.5360213473517604e9, 1.674305473104532e9, 1.7184075669820323e9, 1.7706879727328618e9, 1.8503094635752912e9, 1.931569575039408e9  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [194930, 189828, 184800, 179608, 174494, 169153, 163882, 158607, 153256, 147870  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\nrepeat creates a vector that repeats the two demonstration policies many times.\n\n\n30.6.1 Stochastic Projection\nThis defines a loop to calculate the results n times (this is only running the two policies in the sample data n times). This is emulating running our population of policies through \\(n\\) stochastic scenarios, similar to what might be done for a risk or pricing exercise.\n\nfunction stochastic_proj(policies, params, n)\n\n    ThreadsX.map(1:n) do i\n        project(policies, params)\n    end\nend\n\nstochastic_proj (generic function with 1 method)\n\n\n\n30.6.1.1 Demonstration\nWe’ll simulate the two policies’ outcomes 1,000 times and visualize the resulting distribution of claims value:\n\nstoch = stochastic_proj(policies, params, 1000)\n\n1000-element Vector{@NamedTuple{benefits::Vector{Float64}, lives::Vector{Int64}}}:\n (benefits = [0.0, 406558.8212920808, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 250008.03453253524, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [0.0, 0.0, 0.0, 0.0, 0.0, 457585.5351474526, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 242726.24711896625, 500016.0690650705, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 1, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n ⋮\n (benefits = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 242726.24711896625, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [0.0, 0.0, 0.0, 215659.12675438428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 235656.5506009381, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [0.0, 0.0, 0.0, 0.0, 0.0, 228792.7675737263, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [197358.6511126606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [0.0, 0.0, 0.0, 0.0, 222128.90055701582, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [394717.3022253212, 0.0, 0.0, 0.0, 0.0, 228792.7675737263, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\nlet\n    v = [pv(0.03, s.benefits) for s in stoch]\n    hist(v,\n        bins=15,\n        axis=(\n            xlabel=\"Present Value of Benefits\",\n            ylabel=\"Number of scenarios\"\n        )\n    )\nend",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Stochastic Mortality Projections</span>"
    ]
  },
  {
    "objectID": "stochastic-mortality.html#benchmarking",
    "href": "stochastic-mortality.html#benchmarking",
    "title": "30  Stochastic Mortality Projections",
    "section": "30.7 Benchmarking",
    "text": "30.7 Benchmarking\nUsing a 2024 Macbook Air M3 laptop, about 45 million policies able to be stochastically projected per second:\n\npolicies_to_benchmark = 4_500_000\n# adjust the `repeat` depending on how many policies are already in the array\n# to match the target number for the benchmark\nn = policies_to_benchmark ÷ length(policies)\n\n@benchmark project(p, r) setup = (p = repeat($policies, $n); r = $params)\n\n\nBenchmarkTools.Trial: 61 samples with 1 evaluation per sample.\n Range (min … max):  63.185 ms … 69.875 ms  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     63.477 ms              ┊ GC (median):    0.00%\n Time  (mean ± σ):   64.550 ms ±  2.124 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%\n\n  ▅█▃▃                                                         \n  ████▁▆▃▁▁▁▃▃▁▁▃▁▁▁▃▁▁▃▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▃▁▁▁▃▁▃▁▄▅ ▁\n  63.2 ms         Histogram: frequency by time        69.3 ms &lt;\n\n Memory estimate: 69.09 KiB, allocs estimate: 504.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Stochastic Mortality Projections</span>"
    ]
  },
  {
    "objectID": "stochastic-mortality.html#conclusion",
    "href": "stochastic-mortality.html#conclusion",
    "title": "30  Stochastic Mortality Projections",
    "section": "30.8 Conclusion",
    "text": "30.8 Conclusion\nThis example has worked through a recommended pattern of setting up and running a stochastic simulation using a threaded approach to parallelism. The results show that quite a bit of simulation power is available using even consumer laptop hardware!",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Stochastic Mortality Projections</span>"
    ]
  },
  {
    "objectID": "bayesian-mortality.html",
    "href": "bayesian-mortality.html",
    "title": "31  Bayesian Mortality Modeling",
    "section": "",
    "text": "31.1 Chapter Overview\nAn example of using a Bayesian MCMC approach to fitting a mortality curve to sample data, with multi-level models and censored data.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Bayesian Mortality Modeling</span>"
    ]
  },
  {
    "objectID": "bayesian-mortality.html#generating-fake-data",
    "href": "bayesian-mortality.html#generating-fake-data",
    "title": "31  Bayesian Mortality Modeling",
    "section": "31.2 Generating fake data",
    "text": "31.2 Generating fake data\nThe problem of interest is to look at mortality rates, which are given in terms of exposures (whether or not a life experienced a death in a given year).\nWe’ll grab some example rates from an insurance table, which has a “selection” component: When someone enters observation, say at age 50, their mortality is path dependent (so for someone who started being observed at 50 will have a different risk/mortality rate at age 55 than someone who started being observed at 45).\nAddtionally, there may be additional groups of interest, such as:\n\nhigh/medium/low risk classification\nsex\ngroup (e.g. company, data source, etc.)\ntype of insurance product offered\n\nThe example data will start with only the risk classification above.\n\nusing MortalityTables\nusing Turing\nusing DataFramesMeta\nusing MCMCChains\nusing LinearAlgebra\nusing CairoMakie\nusing StatsBase\n\n\nn = 10_000\ninforce = map(1:n) do i\n    (\n        issue_age=rand(30:70),\n        risk_level=rand(1:3),\n    )\n\nend\n\n10000-element Vector{@NamedTuple{issue_age::Int64, risk_level::Int64}}:\n (issue_age = 31, risk_level = 3)\n (issue_age = 68, risk_level = 1)\n (issue_age = 39, risk_level = 2)\n (issue_age = 37, risk_level = 3)\n (issue_age = 53, risk_level = 2)\n (issue_age = 30, risk_level = 2)\n (issue_age = 33, risk_level = 2)\n (issue_age = 70, risk_level = 1)\n (issue_age = 39, risk_level = 1)\n (issue_age = 44, risk_level = 2)\n ⋮\n (issue_age = 45, risk_level = 1)\n (issue_age = 67, risk_level = 2)\n (issue_age = 65, risk_level = 2)\n (issue_age = 53, risk_level = 1)\n (issue_age = 41, risk_level = 3)\n (issue_age = 57, risk_level = 2)\n (issue_age = 46, risk_level = 1)\n (issue_age = 60, risk_level = 1)\n (issue_age = 53, risk_level = 1)\n\n\n\nbase_table = MortalityTables.table(\"2001 VBT Residual Standard Select and Ultimate - Male Nonsmoker, ANB\")\n\nfunction tabular_mortality(params, issue_age, att_age, risk_level)\n    q = params.ultimate[att_age]\n    if risk_level == 1\n        q *= 0.7\n    elseif risk_level == 2\n        q = q\n    else\n        q *= 1.5\n    end\nend\n\ntabular_mortality (generic function with 1 method)\n\n\n\nfunction model_outcomes(inforce, assumption, assumption_params; n_years=5)\n\n    outcomes = map(inforce) do pol\n        alive = 1\n        sim = map(1:n_years) do t\n            att_age = pol.issue_age + t - 1\n            q = assumption(\n                assumption_params,\n                pol.issue_age,\n                att_age,\n                pol.risk_level\n            )\n            if rand() &lt; q\n                out = (att_age=att_age, exposures=alive, death=1)\n                alive = 0\n                out\n            else\n                (att_age=att_age, exposures=alive, death=0)\n            end\n        end\n        filter!(x -&gt; x.exposures == 1, sim)\n\n    end\n\n\n    df = DataFrame(inforce)\n\n    df.outcomes = outcomes\n    df = flatten(df, :outcomes)\n\n    df.att_age = [x.att_age for x in df.outcomes]\n    df.death = [x.death for x in df.outcomes]\n    df.exposures = [x.exposures for x in df.outcomes]\n    select!(df, Not(:outcomes))\n\n\nend\n\nexposures = model_outcomes(inforce, tabular_mortality, base_table)\ndata = combine(groupby(exposures, [:issue_age, :att_age])) do subdf\n    (exposures=nrow(subdf),\n        deaths=sum(subdf.death),\n        fraction=sum(subdf.death) / nrow(subdf))\nend\n\n\ndata2 = combine(groupby(exposures, [:issue_age, :att_age, :risk_level])) do subdf\n    (exposures=nrow(subdf),\n        deaths=sum(subdf.death),\n        fraction=sum(subdf.death) / nrow(subdf))\nend\n\n615×6 DataFrame590 rows omitted\n\n\n\nRow\nissue_age\natt_age\nrisk_level\nexposures\ndeaths\nfraction\n\n\n\nInt64\nInt64\nInt64\nInt64\nInt64\nFloat64\n\n\n\n\n1\n30\n30\n1\n75\n0\n0.0\n\n\n2\n30\n30\n2\n82\n0\n0.0\n\n\n3\n30\n30\n3\n76\n0\n0.0\n\n\n4\n30\n31\n1\n75\n0\n0.0\n\n\n5\n30\n31\n2\n82\n0\n0.0\n\n\n6\n30\n31\n3\n76\n0\n0.0\n\n\n7\n30\n32\n1\n75\n0\n0.0\n\n\n8\n30\n32\n2\n82\n0\n0.0\n\n\n9\n30\n32\n3\n76\n0\n0.0\n\n\n10\n30\n33\n1\n75\n0\n0.0\n\n\n11\n30\n33\n2\n82\n0\n0.0\n\n\n12\n30\n33\n3\n76\n0\n0.0\n\n\n13\n30\n34\n1\n75\n0\n0.0\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n604\n70\n71\n1\n103\n1\n0.00970874\n\n\n605\n70\n71\n2\n83\n5\n0.060241\n\n\n606\n70\n71\n3\n86\n3\n0.0348837\n\n\n607\n70\n72\n1\n102\n3\n0.0294118\n\n\n608\n70\n72\n2\n78\n2\n0.025641\n\n\n609\n70\n72\n3\n83\n5\n0.060241\n\n\n610\n70\n73\n1\n99\n3\n0.030303\n\n\n611\n70\n73\n2\n76\n1\n0.0131579\n\n\n612\n70\n73\n3\n78\n4\n0.0512821\n\n\n613\n70\n74\n1\n96\n2\n0.0208333\n\n\n614\n70\n74\n2\n75\n3\n0.04\n\n\n615\n70\n74\n3\n74\n5\n0.0675676",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Bayesian Mortality Modeling</span>"
    ]
  },
  {
    "objectID": "bayesian-mortality.html#a-single-binomial-parameter-model",
    "href": "bayesian-mortality.html#a-single-binomial-parameter-model",
    "title": "31  Bayesian Mortality Modeling",
    "section": "31.3 1: A single binomial parameter model",
    "text": "31.3 1: A single binomial parameter model\nEstimate \\(q\\), the average mortality rate, not accounting for any variation within the population/sample. Our model is defines as:\n\\[\nq ~ Beta(1,1)\np(\\text{death}) ~ \\text{Binomial}(q)\n\\]\n\n@model function mortality(data, deaths)\n    q ~ Beta(1, 1)\n    for i = 1:nrow(data)\n        deaths[i] ~ Binomial(data.exposures[i], q)\n    end\nend\n\nm1 = mortality(data, data.deaths)\n\n\nDynamicPPL.Model{typeof(mortality), (:data, :deaths), (), (), Tuple{DataFrame, Vector{Int64}}, Tuple{}, DynamicPPL.DefaultContext}(mortality, (data = 205×5 DataFrame\n Row │ issue_age  att_age  exposures  deaths  fraction   \n     │ Int64      Int64    Int64      Int64   Float64    \n─────┼───────────────────────────────────────────────────\n   1 │        30       30        233       0  0.0\n   2 │        30       31        233       0  0.0\n   3 │        30       32        233       0  0.0\n   4 │        30       33        233       0  0.0\n   5 │        30       34        233       0  0.0\n   6 │        31       31        218       1  0.00458716\n   7 │        31       32        217       0  0.0\n   8 │        31       33        217       0  0.0\n  ⋮  │     ⋮         ⋮         ⋮        ⋮         ⋮\n 199 │        69       72        247       9  0.0364372\n 200 │        69       73        238      10  0.0420168\n 201 │        70       70        279       7  0.0250896\n 202 │        70       71        272       9  0.0330882\n 203 │        70       72        263      10  0.0380228\n 204 │        70       73        253       8  0.0316206\n 205 │        70       74        245      10  0.0408163\n                                         190 rows omitted, deaths = [0, 0, 0, 0, 0, 1, 0, 0, 2, 0  …  4, 8, 6, 9, 10, 7, 9, 10, 8, 10]), NamedTuple(), DynamicPPL.DefaultContext())\n\n\n\n\n31.3.1 Sampling from the posterior\nWe use a No-U-Turn-Sampler (NUTS) technique to sample multiple chains at once:\n\nnum_chains = 4\nchain = sample(m1, NUTS(), MCMCThreads(), 400, num_chains)\n\n\nChains MCMC chain (400×13×4 Array{Float64, 3}):\n\nIterations        = 201:1:600\nNumber of chains  = 4\nSamples per chain = 400\nWall duration     = 2.08 seconds\nCompute duration  = 8.21 seconds\nparameters        = q\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n\n           q    0.0087    0.0004    0.0000   739.2302   984.6658    1.0036     ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           q    0.0079    0.0084    0.0087    0.0090    0.0095\n\n\n\n\nHere, we have asked for the outcomes to be modeled via a single parameter for the population. We see that the posterior distribution of \\(q\\) is very close to the overall population mortality rate:\n\n# Posterior mean of q should be close to the pooled fraction\nsum(data.deaths) / sum(data.exposures)\n\n0.008680449675753694\n\n\nHowever, We can see that the sampling of possible posterior parameters doesn’t really fit the data very well since our model was so simplified. The lines represent the posterior binomial probability.\nThis is saying that for the observed data, if there really is just a single probability p that governs the true process that came up with the data, there’s a pretty narrow range of values it could possibly be:\n\nlet\n    data_weight = sqrt.(data.exposures) / 2\n    f = Figure(title=\"Parametric Bayesian Mortality\"\n    )\n    ax = Axis(f[1, 1],\n        xlabel=\"age\",\n        ylabel=\"mortality rate\",\n        limits=(nothing, nothing, -0.01, 0.10),\n    )\n    scatter!(ax,\n        data.att_age,\n        data.fraction,\n        markersize=data_weight,\n        color=(:blue, 0.5),\n        label=\"Experience data point (size indicates relative exposure quantity)\",)\n\n    # show n samples from the posterior plotted on the graph\n    n = 300\n    ages = sort!(unique(data.att_age))\n\n    q_posterior = sample(chain, n)[:q]\n\n\n    for i in 1:n\n\n        hlines!(ax, [q_posterior[i]], color=(:grey, 0.1))\n    end\n\n    sim05 = Float64[]\n    sim95 = Float64[]\n    for r in eachrow(data)\n        outcomes = map(1:n) do i\n            rand(Binomial(r.exposures, q_posterior[i]), 500)\n        end\n        push!(sim05, quantile(Iterators.flatten(outcomes), 0.05) / r.exposures)\n        push!(sim95, quantile(Iterators.flatten(outcomes), 0.95) / r.exposures)\n\n\n    end\n\n\n\n    f\nend\n\n\n\n\n\nlet\n    n = 300\n    q_posterior = sample(chain, n)[:q]\n\n\nend\n\n2-dimensional AxisArray{Float64,2,...} with axes:\n    :iter, 1:300\n    :chain, 1:1\nAnd data, a 300×1 Matrix{Float64}:\n 0.008619788009461815\n 0.008501715283944257\n 0.00863829916911474\n 0.008534309043623008\n 0.008548886636930566\n 0.009015823843725004\n 0.008631078079973296\n 0.008959770158964118\n 0.008581889416265311\n 0.008166368819955657\n ⋮\n 0.008745406235479064\n 0.008363059920630897\n 0.008036400737086091\n 0.008804744442405244\n 0.008939660609940472\n 0.008725058484678791\n 0.008599700311582394\n 0.008158724288367893\n 0.008700386033144165",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Bayesian Mortality Modeling</span>"
    ]
  },
  {
    "objectID": "bayesian-mortality.html#parametric-model",
    "href": "bayesian-mortality.html#parametric-model",
    "title": "31  Bayesian Mortality Modeling",
    "section": "31.4 2. Parametric model",
    "text": "31.4 2. Parametric model\nIn this example, we utilize a MakehamBeard parameterization because it’s already very similar in form to a logistic function. This is important because our desired output is a probability (ie the probability of a death at a given age), so the value must be constrained to be in the interval between zero and one.\nThe prior values for a,b,c, and k are chosen to constrain the hazard (mortality) rate to be between zero and one.\nThis isn’t an ideal parameterization (e.g. we aren’t including information about the select underwriting period), but is an example of utilizing Bayesian techniques on life experience data. ”\n\n@model function mortality2(data, deaths)\n    a ~ Exponential(0.1)\n    b ~ Exponential(0.1)\n    c = 0.0\n    k ~ truncated(Exponential(1), 1, Inf)\n\n    # use the variables to create a parametric mortality model\n    m = MortalityTables.MakehamBeard(; a, b, c, k)\n\n    # loop through the rows of the dataframe to let Turing observe the data \n    # and how consistent the parameters are with the data\n    for i = 1:nrow(data)\n        age = data.att_age[i]\n        q = MortalityTables.hazard(m, age)\n        deaths[i] ~ Binomial(data.exposures[i], q)\n    end\nend\n\nmortality2 (generic function with 2 methods)\n\n\nWe combine the model with the data and sample from the posterior using a similar call as before:\n\nm2 = mortality2(data, data.deaths)\n\nchain2 = sample(m2, NUTS(), MCMCThreads(), 400, num_chains)\n\n\nChains MCMC chain (400×15×4 Array{Float64, 3}):\n\nIterations        = 201:1:600\nNumber of chains  = 4\nSamples per chain = 400\nWall duration     = 5.9 seconds\nCompute duration  = 22.22 seconds\nparameters        = a, b, k\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n\n           a    0.0001    0.0000    0.0000   584.7235   672.9357    1.0042     ⋯\n           b    0.0874    0.0054    0.0002   581.3264   681.4963    1.0045     ⋯\n           k    1.9436    0.8760    0.0288   656.5508   499.7290    1.0011     ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           a    0.0000    0.0000    0.0001    0.0001    0.0001\n           b    0.0774    0.0837    0.0873    0.0910    0.0986\n           k    1.0311    1.2973    1.6984    2.3343    4.2571\n\n\n\n\n\n31.4.1 Plotting samples from the posterior\nWe can see that the sampling of possible posterior parameters fits the data well:\n\nlet\n    data_weight = sqrt.(data.exposures) / 2\n\n    p = scatter(\n        data.att_age,\n        data.fraction,\n        markersize=data_weight,\n        alpha=0.5,\n        label=\"Experience data point (size indicates relative exposure quantity)\",\n        axis=(\n            xlabel=\"age\",\n            limits=(nothing, nothing, -0.01, 0.10),\n            ylabel=\"mortality rate\",\n            title=\"Parametric Bayesian Mortality\"\n        )\n    )\n\n\n    # show n samples from the posterior plotted on the graph\n    n = 300\n    ages = sort!(unique(data.att_age))\n\n    for i in 1:n\n        s = sample(chain2, 1)\n        a = only(s[:a])\n        b = only(s[:b])\n        k = only(s[:k])\n        c = 0\n        m = MortalityTables.MakehamBeard(; a, b, c, k)\n        lines!(ages, age -&gt; MortalityTables.hazard(m, age), alpha=0.1, label=\"\")\n    end\n    p\nend\n\n\n\n\n\nlet\n    data_weight = sqrt.(data.exposures) / 2\n    f = Figure(title=\"Parametric Bayesian Mortality\"\n    )\n    ax = Axis(f[1, 1],\n        xlabel=\"age\",\n        ylabel=\"mortality rate\",\n        limits=(nothing, nothing, -0.01, 0.10),\n    )\n    scatter!(ax,\n        data.att_age,\n        data.fraction,\n        markersize=data_weight,\n        color=(:blue, 0.5),\n        label=\"Experience data point (size indicates relative exposure quantity)\",)\n\n    # show n samples from the posterior plotted on the graph\n    n = 300\n    ages = sort!(unique(data.att_age))\n\n    for i in 1:n\n        s = sample(chain2, 1)\n        a = only(s[:a])\n        b = only(s[:b])\n        k = only(s[:k])\n        c = 0\n        m = MortalityTables.MakehamBeard(; a, b, c, k)\n        qs = MortalityTables.hazard.(m, ages)\n        lines!(ax, ages, qs, color=(:grey, 0.1))\n    end\n    f\nend\n\n\n\n\nRecall that the lines are not plotting the possible outcomes of the claims rates, but the mean claims rate for the given age.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Bayesian Mortality Modeling</span>"
    ]
  },
  {
    "objectID": "bayesian-mortality.html#multi-level-model",
    "href": "bayesian-mortality.html#multi-level-model",
    "title": "31  Bayesian Mortality Modeling",
    "section": "31.5 3. Multi-level model",
    "text": "31.5 3. Multi-level model\nThis model extends the prior to create a multi-level model. Each risk class (risk_level) gets its own \\(a\\) paramater in the MakhamBeard model. The prior for \\(a_i\\) is determined by the hyper-parameter \\(\\bar{a}\\).\n\n@model function mortality3(data, deaths)\n    risk_levels = length(levels(data.risk_level))\n    b ~ Exponential(0.1)\n    ā ~ Exponential(0.1)\n    a ~ filldist(Exponential(ā), risk_levels)\n    c = 0\n    k ~ truncated(Exponential(1), 1, Inf)\n\n    # use the variables to create a parametric mortality model\n\n    # loop through the rows of the dataframe to let Turing observe the data \n    # and how consistent the parameters are with the data\n    for i = 1:nrow(data)\n        risk = data.risk_level[i]\n\n        m = MortalityTables.MakehamBeard(; a=a[risk], b, c, k)\n        age = data.att_age[i]\n        q = MortalityTables.hazard(m, age)\n        deaths[i] ~ Binomial(data.exposures[i], q)\n    end\nend\n\nm3 = mortality3(data2, data2.deaths)\n\nchain3 = sample(m3, NUTS(), 1000)\n\nsummarize(chain3)\n\n\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n\n           b    0.0884    0.0056    0.0004   236.3095   352.3040    1.0079     ⋯\n           ā    0.0002    0.0003    0.0000   306.6128   326.3798    1.0002     ⋯\n        a[1]    0.0000    0.0000    0.0000   248.0581   352.1054    1.0084     ⋯\n        a[2]    0.0001    0.0000    0.0000   241.2689   365.3431    1.0090     ⋯\n        a[3]    0.0001    0.0000    0.0000   250.2072   346.8012    1.0068     ⋯\n           k    2.0106    1.1428    0.0451   440.6973   254.5145    1.0044     ⋯\n                                                                1 column omitted\n\n\n\n\n\nlet data = data2\n\n    data_weight = sqrt.(data.exposures)\n    color_i = data.risk_level\n    cm = CairoMakie.Makie.wong_colors()\n\n    p, ax, _ = scatter(\n        data.att_age,\n        data.fraction,\n        markersize=data_weight,\n        alpha=0.5,\n        color=[(CairoMakie.Makie.wong_colors()[c], 0.7) for c in color_i],\n        colormap=CairoMakie.Makie.wong_colors(),\n        label=\"Experience data point (size indicates relative exposure quantity)\",\n        axis=(\n            xlabel=\"age\",\n            limits=(nothing, nothing, -0.01, 0.10),\n            ylabel=\"mortality rate\",\n            title=\"Parametric Bayesian Mortality\"\n        )\n    )\n\n\n    # show n samples from the posterior plotted on the graph\n    n = 100\n\n    ages = sort!(unique(data.att_age))\n    for r in 1:3\n        for i in 1:n\n            s = sample(chain3, 1)\n            a = only(s[Symbol(\"a[$r]\")])\n            b = only(s[:b])\n            k = only(s[:k])\n            c = 0\n            m = MortalityTables.MakehamBeard(; a, b, c, k)\n            lines!(ages, age -&gt; MortalityTables.hazard(m, age), label=\"risk level $r\", alpha=0.2, color=(CairoMakie.Makie.wong_colors()[r], 0.2))\n        end\n    end\n    axislegend(ax, merge=true)\n    p\nend\n\n\n\n\nAgain, the lines are not plotting the possible outcomes of the claims rates, but the mean claims rate for the given age and risk class.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Bayesian Mortality Modeling</span>"
    ]
  },
  {
    "objectID": "bayesian-mortality.html#handling-non-unit-exposures",
    "href": "bayesian-mortality.html#handling-non-unit-exposures",
    "title": "31  Bayesian Mortality Modeling",
    "section": "31.6 Handling non-unit exposures",
    "text": "31.6 Handling non-unit exposures\nThe key is to use the Poisson distribution, which is a continuous approximation to the Binomial distribution:\n\n@model function mortality4(data, deaths)\n    risk_levels = length(levels(data.risk_level))\n    b ~ Exponential(0.1)\n    ā ~ Exponential(0.1)\n    a ~ filldist(Exponential(ā), risk_levels)\n    c ~ Beta(4, 18)\n    k ~ truncated(Exponential(1), 1, Inf)\n\n    # use the variables to create a parametric mortality model\n\n    # loop through the rows of the dataframe to let Turing observe the data \n    # and how consistent the parameters are with the data\n    for i = 1:nrow(data)\n        risk = data.risk_level[i]\n\n        m = MortalityTables.MakehamBeard(; a=a[risk], b, c, k)\n        age = data.att_age[i]\n        q = MortalityTables.hazard(m, age)\n        deaths[i] ~ Poisson(data.exposures[i] * q)\n    end\nend\n\nm4 = mortality4(data2, data2.deaths)\n\nchain4 = sample(m4, NUTS(), 1000)\n\n\nChains MCMC chain (1000×19×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 27.11 seconds\nCompute duration  = 27.11 seconds\nparameters        = b, ā, a[1], a[2], a[3], c, k\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n\n           b    0.1032    0.0087    0.0007   162.3063   194.7900    1.0339     ⋯\n           ā    0.0001    0.0001    0.0000   211.9405   286.5972    1.0300     ⋯\n        a[1]    0.0000    0.0000    0.0000   162.8706   172.3253    1.0326     ⋯\n        a[2]    0.0000    0.0000    0.0000   150.3623   222.1351    1.0341     ⋯\n        a[3]    0.0000    0.0000    0.0000   156.1219   135.7489    1.0368     ⋯\n           c    0.0011    0.0004    0.0000   243.3639   499.4269    1.0118     ⋯\n           k    2.0483    1.0739    0.0432   364.4628   237.0317    1.0007     ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           b    0.0875    0.0968    0.1027    0.1090    0.1211\n           ā    0.0000    0.0000    0.0000    0.0001    0.0004\n        a[1]    0.0000    0.0000    0.0000    0.0000    0.0000\n        a[2]    0.0000    0.0000    0.0000    0.0000    0.0000\n        a[3]    0.0000    0.0000    0.0000    0.0000    0.0001\n           c    0.0004    0.0008    0.0010    0.0013    0.0019\n           k    1.0197    1.3139    1.7338    2.4389    4.6872\n\n\n\n\n\nrisk_factors4 = [mean(chain4[Symbol(\"a[$f]\")]) for f in 1:3]\n\nrisk_factors4 ./ risk_factors4[2]\n\nlet data = data2\n\n    data_weight = sqrt.(data.exposures) / 2\n    color_i = data.risk_level\n\n    p, ax, _ = scatter(\n        data.att_age,\n        data.fraction,\n        markersize=data_weight,\n        alpha=0.5,\n        color=color_i,\n        label=\"Experience data point (size indicates relative exposure quantity)\",\n        axis=(xlabel=\"age\",\n            limits=(nothing, nothing, -0.01, 0.10),\n            ylabel=\"mortality rate\",\n            title=\"Parametric Bayesian Mortality\"\n        )\n    )\n\n\n    # show n samples from the posterior plotted on the graph\n    n = 100\n\n    ages = sort!(unique(data.att_age))\n    for r in 1:3\n        for i in 1:n\n            s = sample(chain4, 1)\n            a = only(s[Symbol(\"a[$r]\")])\n            b = only(s[:b])\n            k = only(s[:k])\n            c = 0\n            m = MortalityTables.MakehamBeard(; a, b, c, k)\n            lines!(ages, age -&gt; MortalityTables.hazard(m, age), label=\"risk level $r\", alpha=0.2, color=(CairoMakie.Makie.wong_colors()[r], 0.2))\n        end\n    end\n    axislegend(ax, merge=true)\n    p\nend",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Bayesian Mortality Modeling</span>"
    ]
  },
  {
    "objectID": "bayesian-mortality.html#model-predictions",
    "href": "bayesian-mortality.html#model-predictions",
    "title": "31  Bayesian Mortality Modeling",
    "section": "31.7 Model Predictions",
    "text": "31.7 Model Predictions\nWe can generate predictive estimates by passing a vector of missing in place of the outcome variables and then calling predict.\nWe get a table of values where each row is the the prediction implied by the corresponding chain sample, and the columns are the predicted value for each of the outcomes in our original dataset.\n\npreds = predict(mortality4(data2, fill(missing, length(data2.deaths))), chain4)\n\n\nChains MCMC chain (1000×615×1 Array{Float64, 3}):\n\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nparameters        = deaths[1], deaths[2], deaths[3], deaths[4], deaths[5], deaths[6], deaths[7], deaths[8], deaths[9], deaths[10], deaths[11], deaths[12], deaths[13], deaths[14], deaths[15], deaths[16], deaths[17], deaths[18], deaths[19], deaths[20], deaths[21], deaths[22], deaths[23], deaths[24], deaths[25], deaths[26], deaths[27], deaths[28], deaths[29], deaths[30], deaths[31], deaths[32], deaths[33], deaths[34], deaths[35], deaths[36], deaths[37], deaths[38], deaths[39], deaths[40], deaths[41], deaths[42], deaths[43], deaths[44], deaths[45], deaths[46], deaths[47], deaths[48], deaths[49], deaths[50], deaths[51], deaths[52], deaths[53], deaths[54], deaths[55], deaths[56], deaths[57], deaths[58], deaths[59], deaths[60], deaths[61], deaths[62], deaths[63], deaths[64], deaths[65], deaths[66], deaths[67], deaths[68], deaths[69], deaths[70], deaths[71], deaths[72], deaths[73], deaths[74], deaths[75], deaths[76], deaths[77], deaths[78], deaths[79], deaths[80], deaths[81], deaths[82], deaths[83], deaths[84], deaths[85], deaths[86], deaths[87], deaths[88], deaths[89], deaths[90], deaths[91], deaths[92], deaths[93], deaths[94], deaths[95], deaths[96], deaths[97], deaths[98], deaths[99], deaths[100], deaths[101], deaths[102], deaths[103], deaths[104], deaths[105], deaths[106], deaths[107], deaths[108], deaths[109], deaths[110], deaths[111], deaths[112], deaths[113], deaths[114], deaths[115], deaths[116], deaths[117], deaths[118], deaths[119], deaths[120], deaths[121], deaths[122], deaths[123], deaths[124], deaths[125], deaths[126], deaths[127], deaths[128], deaths[129], deaths[130], deaths[131], deaths[132], deaths[133], deaths[134], deaths[135], deaths[136], deaths[137], deaths[138], deaths[139], deaths[140], deaths[141], deaths[142], deaths[143], deaths[144], deaths[145], deaths[146], deaths[147], deaths[148], deaths[149], deaths[150], deaths[151], deaths[152], deaths[153], deaths[154], deaths[155], deaths[156], deaths[157], deaths[158], deaths[159], deaths[160], deaths[161], deaths[162], deaths[163], deaths[164], deaths[165], deaths[166], deaths[167], deaths[168], deaths[169], deaths[170], deaths[171], deaths[172], deaths[173], deaths[174], deaths[175], deaths[176], deaths[177], deaths[178], deaths[179], deaths[180], deaths[181], deaths[182], deaths[183], deaths[184], deaths[185], deaths[186], deaths[187], deaths[188], deaths[189], deaths[190], deaths[191], deaths[192], deaths[193], deaths[194], deaths[195], deaths[196], deaths[197], deaths[198], deaths[199], deaths[200], deaths[201], deaths[202], deaths[203], deaths[204], deaths[205], deaths[206], deaths[207], deaths[208], deaths[209], deaths[210], deaths[211], deaths[212], deaths[213], deaths[214], deaths[215], deaths[216], deaths[217], deaths[218], deaths[219], deaths[220], deaths[221], deaths[222], deaths[223], deaths[224], deaths[225], deaths[226], deaths[227], deaths[228], deaths[229], deaths[230], deaths[231], deaths[232], deaths[233], deaths[234], deaths[235], deaths[236], deaths[237], deaths[238], deaths[239], deaths[240], deaths[241], deaths[242], deaths[243], deaths[244], deaths[245], deaths[246], deaths[247], deaths[248], deaths[249], deaths[250], deaths[251], deaths[252], deaths[253], deaths[254], deaths[255], deaths[256], deaths[257], deaths[258], deaths[259], deaths[260], deaths[261], deaths[262], deaths[263], deaths[264], deaths[265], deaths[266], deaths[267], deaths[268], deaths[269], deaths[270], deaths[271], deaths[272], deaths[273], deaths[274], deaths[275], deaths[276], deaths[277], deaths[278], deaths[279], deaths[280], deaths[281], deaths[282], deaths[283], deaths[284], deaths[285], deaths[286], deaths[287], deaths[288], deaths[289], deaths[290], deaths[291], deaths[292], deaths[293], deaths[294], deaths[295], deaths[296], deaths[297], deaths[298], deaths[299], deaths[300], deaths[301], deaths[302], deaths[303], deaths[304], deaths[305], deaths[306], deaths[307], deaths[308], deaths[309], deaths[310], deaths[311], deaths[312], deaths[313], deaths[314], deaths[315], deaths[316], deaths[317], deaths[318], deaths[319], deaths[320], deaths[321], deaths[322], deaths[323], deaths[324], deaths[325], deaths[326], deaths[327], deaths[328], deaths[329], deaths[330], deaths[331], deaths[332], deaths[333], deaths[334], deaths[335], deaths[336], deaths[337], deaths[338], deaths[339], deaths[340], deaths[341], deaths[342], deaths[343], deaths[344], deaths[345], deaths[346], deaths[347], deaths[348], deaths[349], deaths[350], deaths[351], deaths[352], deaths[353], deaths[354], deaths[355], deaths[356], deaths[357], deaths[358], deaths[359], deaths[360], deaths[361], deaths[362], deaths[363], deaths[364], deaths[365], deaths[366], deaths[367], deaths[368], deaths[369], deaths[370], deaths[371], deaths[372], deaths[373], deaths[374], deaths[375], deaths[376], deaths[377], deaths[378], deaths[379], deaths[380], deaths[381], deaths[382], deaths[383], deaths[384], deaths[385], deaths[386], deaths[387], deaths[388], deaths[389], deaths[390], deaths[391], deaths[392], deaths[393], deaths[394], deaths[395], deaths[396], deaths[397], deaths[398], deaths[399], deaths[400], deaths[401], deaths[402], deaths[403], deaths[404], deaths[405], deaths[406], deaths[407], deaths[408], deaths[409], deaths[410], deaths[411], deaths[412], deaths[413], deaths[414], deaths[415], deaths[416], deaths[417], deaths[418], deaths[419], deaths[420], deaths[421], deaths[422], deaths[423], deaths[424], deaths[425], deaths[426], deaths[427], deaths[428], deaths[429], deaths[430], deaths[431], deaths[432], deaths[433], deaths[434], deaths[435], deaths[436], deaths[437], deaths[438], deaths[439], deaths[440], deaths[441], deaths[442], deaths[443], deaths[444], deaths[445], deaths[446], deaths[447], deaths[448], deaths[449], deaths[450], deaths[451], deaths[452], deaths[453], deaths[454], deaths[455], deaths[456], deaths[457], deaths[458], deaths[459], deaths[460], deaths[461], deaths[462], deaths[463], deaths[464], deaths[465], deaths[466], deaths[467], deaths[468], deaths[469], deaths[470], deaths[471], deaths[472], deaths[473], deaths[474], deaths[475], deaths[476], deaths[477], deaths[478], deaths[479], deaths[480], deaths[481], deaths[482], deaths[483], deaths[484], deaths[485], deaths[486], deaths[487], deaths[488], deaths[489], deaths[490], deaths[491], deaths[492], deaths[493], deaths[494], deaths[495], deaths[496], deaths[497], deaths[498], deaths[499], deaths[500], deaths[501], deaths[502], deaths[503], deaths[504], deaths[505], deaths[506], deaths[507], deaths[508], deaths[509], deaths[510], deaths[511], deaths[512], deaths[513], deaths[514], deaths[515], deaths[516], deaths[517], deaths[518], deaths[519], deaths[520], deaths[521], deaths[522], deaths[523], deaths[524], deaths[525], deaths[526], deaths[527], deaths[528], deaths[529], deaths[530], deaths[531], deaths[532], deaths[533], deaths[534], deaths[535], deaths[536], deaths[537], deaths[538], deaths[539], deaths[540], deaths[541], deaths[542], deaths[543], deaths[544], deaths[545], deaths[546], deaths[547], deaths[548], deaths[549], deaths[550], deaths[551], deaths[552], deaths[553], deaths[554], deaths[555], deaths[556], deaths[557], deaths[558], deaths[559], deaths[560], deaths[561], deaths[562], deaths[563], deaths[564], deaths[565], deaths[566], deaths[567], deaths[568], deaths[569], deaths[570], deaths[571], deaths[572], deaths[573], deaths[574], deaths[575], deaths[576], deaths[577], deaths[578], deaths[579], deaths[580], deaths[581], deaths[582], deaths[583], deaths[584], deaths[585], deaths[586], deaths[587], deaths[588], deaths[589], deaths[590], deaths[591], deaths[592], deaths[593], deaths[594], deaths[595], deaths[596], deaths[597], deaths[598], deaths[599], deaths[600], deaths[601], deaths[602], deaths[603], deaths[604], deaths[605], deaths[606], deaths[607], deaths[608], deaths[609], deaths[610], deaths[611], deaths[612], deaths[613], deaths[614], deaths[615]\ninternals         = \n\nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ⋯\n      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ⋯\n\n   deaths[1]    0.0860    0.2978    0.0090   1082.3866   1010.5515    1.0004   ⋯\n   deaths[2]    0.1370    0.3747    0.0122    942.2927    941.8609    0.9991   ⋯\n   deaths[3]    0.1360    0.3487    0.0112    971.8526   1004.0241    0.9998   ⋯\n   deaths[4]    0.0960    0.3080    0.0097   1021.0017    682.2254    1.0001   ⋯\n   deaths[5]    0.1250    0.3457    0.0115    907.9073    892.3067    0.9994   ⋯\n   deaths[6]    0.1310    0.3660    0.0121    920.3683    933.4012    1.0041   ⋯\n   deaths[7]    0.0990    0.3087    0.0111    761.7172    790.5959    0.9990   ⋯\n   deaths[8]    0.1380    0.3675    0.0117    983.3500    952.1043    1.0001   ⋯\n   deaths[9]    0.1410    0.3785    0.0117   1045.1133   1025.2438    0.9990   ⋯\n  deaths[10]    0.0960    0.3208    0.0110    841.1203    831.2332    0.9996   ⋯\n  deaths[11]    0.1370    0.3555    0.0118    909.0094    919.7658    1.0034   ⋯\n  deaths[12]    0.1190    0.3507    0.0112    987.3264    990.3357    0.9999   ⋯\n  deaths[13]    0.1170    0.3397    0.0109    946.5897    907.6851    1.0007   ⋯\n  deaths[14]    0.1340    0.3823    0.0117   1079.0924    835.7793    1.0031   ⋯\n  deaths[15]    0.1520    0.3963    0.0124   1031.8385    948.5814    0.9998   ⋯\n  deaths[16]    0.0810    0.2802    0.0083   1137.7476   1008.0890    0.9991   ⋯\n  deaths[17]    0.1310    0.3577    0.0122    837.0730    849.3045    1.0001   ⋯\n      ⋮           ⋮         ⋮         ⋮          ⋮           ⋮          ⋮      ⋱\n                                                   1 column and 598 rows omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n   deaths[1]    0.0000    0.0000    0.0000    0.0000    1.0000\n   deaths[2]    0.0000    0.0000    0.0000    0.0000    1.0000\n   deaths[3]    0.0000    0.0000    0.0000    0.0000    1.0000\n   deaths[4]    0.0000    0.0000    0.0000    0.0000    1.0000\n   deaths[5]    0.0000    0.0000    0.0000    0.0000    1.0000\n   deaths[6]    0.0000    0.0000    0.0000    0.0000    1.0000\n   deaths[7]    0.0000    0.0000    0.0000    0.0000    1.0000\n   deaths[8]    0.0000    0.0000    0.0000    0.0000    1.0000\n   deaths[9]    0.0000    0.0000    0.0000    0.0000    1.0000\n  deaths[10]    0.0000    0.0000    0.0000    0.0000    1.0000\n  deaths[11]    0.0000    0.0000    0.0000    0.0000    1.0000\n  deaths[12]    0.0000    0.0000    0.0000    0.0000    1.0000\n  deaths[13]    0.0000    0.0000    0.0000    0.0000    1.0000\n  deaths[14]    0.0000    0.0000    0.0000    0.0000    1.0000\n  deaths[15]    0.0000    0.0000    0.0000    0.0000    1.0000\n  deaths[16]    0.0000    0.0000    0.0000    0.0000    1.0000\n  deaths[17]    0.0000    0.0000    0.0000    0.0000    1.0000\n      ⋮           ⋮         ⋮         ⋮         ⋮         ⋮\n                                                598 rows omitted",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Bayesian Mortality Modeling</span>"
    ]
  },
  {
    "objectID": "other-techniques.html",
    "href": "other-techniques.html",
    "title": "32  More Useful Techniques",
    "section": "",
    "text": "32.1 Chapter Overview\nOther useful techniques are surveyed, such as: memoization to avoid repeated computations, pseudo–Monte Carlo, creating a model office, and tips on modeling a complete balance sheet. Also covered are elements of practical review such as static and dynamic validations, implied rate analysis, and explanatory vs. predictive modeling considerations.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>More Useful Techniques</span>"
    ]
  },
  {
    "objectID": "other-techniques.html#conceptual-techniques",
    "href": "other-techniques.html#conceptual-techniques",
    "title": "32  More Useful Techniques",
    "section": "32.2 Conceptual Techniques",
    "text": "32.2 Conceptual Techniques\n\n32.2.1 Taking Things to the Extreme\nConsider what happens if something is taken to an extreme. For example, what happens in the model if we input negative rates? Where should negative rates be allowed and can the model handle them?\n\n\n32.2.2 Range Bounding\nSometimes you just need to know that an outcome is within a certain range - if you can develop a “high” and “low” estimate by making assumptions that you know are outside of feasible ranges, then you can determine whether something is reasonable or within tolerances.\nTo take an example from the pages of interview questions: say you need to determine if a mortgaged property’s value is greater than the amount of the outstanding loan (say $100,000). You don’t have an appraisal, but know that it’s in reasonable condition and that (1) a comparable house with many more issues sold for $100 per square foot. You also don’t know the square footage of the house, but know from the number of rooms and layout that it must be at least 1000 square feet. Therefore you know that the value should at least be greater than:\n\\[\n\\frac{\\$100}{\\text{sq. ft}} \\times 1000 \\text{sq. ft} = \\$100,000\n\\]\nWe’d then conclude that the value of the house very likely exceeds the outstanding balance of the loan and resolves our query without complex modeling or expensive appraisals.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>More Useful Techniques</span>"
    ]
  },
  {
    "objectID": "other-techniques.html#modeling-techniques",
    "href": "other-techniques.html#modeling-techniques",
    "title": "32  More Useful Techniques",
    "section": "32.3 Modeling Techniques",
    "text": "32.3 Modeling Techniques\n\n32.3.1 Serialization\nSerialization is the process of converting a data structure or object state into a format that can be easily stored or transmitted, allowing it to be reconstructed later.\nIn most finance workflows, the slowest parts are not the regressions themselves—they are the data prep, calibration, and scenario generation steps that lead up to them. If you are pricing thousands of scenarios, rolling a model office forward month-by-month, or recalibrating prepayment and default curves, rerunning those steps on every iteration wastes time and money and complicates audits. Serialization lets you checkpoint those expensive steps and ship lightweight “artifacts” between notebooks, jobs, and environments.\n\n\n\n\n\n\nTip\n\n\n\nWhy serialize? - Speed and cost: Avoid recomputing expensive steps (e.g., yield curve calibration, Monte Carlo paths) between runs. - Reproducibility and audit: Persist a snapshot of the “model office” (data, parameters, code version, random seeds) so results can be reproduced for validation and regulators. - Deployment: Move model artifacts between environments in a controlled, versioned way.\n\n\nWhat to use when:\n\n\n\n\n\n\n\n\n\nFormat/Tool\nPros\nCons\nTypical use\n\n\n\n\nSerialization stdlib (.jls)\nFast, no extra dependency, preserves Julia types\nNot stable across Julia versions; Julia-only\nShort-lived caches, memoization artifacts\n\n\nJLD2 (.jld2)\nPortable binary, stores multiple named arrays/structs, widely used\nExtra dependency; still Julia-focused\nPersisting model states and results across sessions/machines\n\n\nArrow/Parquet\nLanguage-agnostic, columnar, efficient for large tables\nHeavier dependency; not for arbitrary Julia structs\nLarge tabular market/position data for interop\n\n\nCSV/JSON/TOML\nHuman-readable, easy diffing/versioning\nLarger files, slower, lossy for binary data\nConfigs, small tables, metadata sidecars\n\n\n\n\n32.3.1.1 Serialization Principles\nDesign principles:\n\nMinimality: Save just enough to reproduce downstream results (parameters, seeds, small derived tables), not entire raw datasets unless necessary.\nDeterminism: Include the random seed and any non-default options so recomputation is bit-for-bit identical when needed.\nPortability: Prefer concrete, serializable types (structs, arrays, Dict) and stable formats when artifacts will live across Julia versions or be shared with others.\nTraceability: Attach metadata (model version, code commit, created_at, inputs’ file hashes) so an auditor or colleague can answer “what produced this file?” a year later.\n\nWhat to serialize vs. recompute:\nSerialize: fitted parameters, calibrated curves, scenario indexes, precomputed shocks, and intermediate aggregates that are expensive but compact. - Recompute: anything cheap, or large raw inputs you can reload from a columnar format (Arrow/Parquet). - Reference big inputs by path and hash in the artifact’s metadata rather than embedding them.\nOperational guidance:\n\nVersion your artifacts: embed minimal metadata (e.g., julia_version, created_at, model_version) and, if possible, a git commit hash for traceability.\nKeep configs human-readable: store run configuration in JSON/TOML and reference it from binary artifacts.\nSeparate data from models: store large tabular datasets in Arrow/Parquet; store small model objects/results in JLD2/Serialization.\nSensitive data: never serialize secrets. Encrypt at rest if files contain PII; control access with OS permissions.\nInterop: do not deserialize untrusted files. Prefer Arrow/Parquet/CSV for sharing with non-Julia systems.\n\n\n\n32.3.1.2 Example: Snapshot a “Model Office” State\nCapture parameters, fitted coefficients, seeds, and minimal metadata. Prefer concrete, serializable structs and plain arrays to keep files portable.\n\nusing Dates, Serialization\n\nstruct ModelState\n    θ::Vector{Float64}        # fitted parameters (example)\n    seed::Int64              # RNG seed used for the run\n    timestamp::DateTime       # when the snapshot was created\n    note::String              # short description\nend\n\n# Atomic write to avoid half-written files\nfunction atomic_serialize(path::AbstractString, obj)\n    dir = dirname(path)\n    mkpath(dir)\n    tmp = tempname(dir)\n    serialize(tmp, obj)\n    mv(tmp, path; force=true)\n    return path\nend\n\n# Example: save/load a state\nθ = [1.0, 2.0]                      # pretend these were estimated\nstate = ModelState(θ, 42, now(), \"OLS on 2025-08-11\")\n\npath = joinpath(\"artifacts\", \"model_state.jls\")\natomic_serialize(path, state)\n\nrestored = deserialize(path)\n\nModelState([1.0, 2.0], 42, DateTime(\"2025-08-11T15:46:54.755\"), \"OLS on 2025-08-11\")\n\n\nTip: Keep snapshot files small and focused. Store big inputs (e.g., loan-level data) separately in efficient tabular formats and reference them via metadata (e.g., file hashes/paths) in the snapshot.\n\n\n32.3.1.3 Example: Cross-session persistence with JLD2\n\nJLD2 stores multiple named variables in one file and is less brittle across Julia versions than raw Serialization. Good default for sharing artifacts with colleagues.\n\n\nusing JLD2, Random, LinearAlgebra, Dates\n\nX = hcat(ones(100), rand(100))\ny = X * [1.0, 2.0] .+ 0.1 .* randn(100)\nθ = X \\ y\n\nmeta = (\n    julia_version=string(VERSION),\n    created_at=string(Dates.now()),\n    description=\"OLS fit for prepayment speed model (toy example)\",\n)\n\nmkpath(\"artifacts\")\nfile = joinpath(\"artifacts\", \"ols_artifact_v2.jld2\")\njldsave(file; θ, meta, X_size=size(X))\n\n# Load (returns a tuple in the same order)\nθ2, meta2, Xsz2 = JLD2.load(file, \"θ\", \"meta\", \"X_size\")\n\n@assert Xsz2 == (100, 2)\n@assert length(θ2) == 2\n\n\n\n32.3.1.4 Example: Disk-Backed Memoization (Cache Expensive Results)\n\nCache outputs keyed by inputs to avoid re-running slow steps (e.g., pricing a large scenario set). Include a label and serialize atomically.\n\n\nusing SHA, Serialization\n\n# Build a stable cache key from a label and arguments\nfunction cachekey(label::AbstractString, args...; kwargs...)\n    io = IOBuffer()\n    print(io, label, '|', args, '|', kwargs)\n    return bytes2hex(sha1(take!(io)))\nend\n\nfunction memoize_to_disk(f; label::AbstractString=\"f\", cache_dir::AbstractString=\"cache\")\n    mkpath(cache_dir)\n    return function (args...; kwargs...)\n        key = cachekey(label, args...; kwargs...)\n        path = joinpath(cache_dir, string(key, \".jls\"))\n        if isfile(path)\n            return deserialize(path)\n        else\n            res = f(args...; kwargs...)\n            # Atomic write\n            tmp = tempname(cache_dir)\n            serialize(tmp, res)\n            mv(tmp, path; force=true)\n            return res\n        end\n    end\nend\n\n# Example: cache an OLS fit (stand-in for a slow calibration)\nols = (X, y) -&gt; X \\ y\nols_cached = memoize_to_disk(ols; label=\"ols_v1\")\n\n# First call computes and caches; second call loads from disk\nθa = ols_cached([ones(3) [1.0, 2.0, 3.0]], [1.0, 3.0, 5.0])\nθb = ols_cached([ones(3) [1.0, 2.0, 3.0]], [1.0, 3.0, 5.0])\n@assert θa == θb\n\n\n\n\n\n\n\nTipFinancial Modeling Pro Tip\n\n\n\n\nFor recurring production runs, use a directory convention like artifacts/YYYY-MM-DD/ with consistent filenames, and clean caches on a schedule to control disk use.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>More Useful Techniques</span>"
    ]
  },
  {
    "objectID": "other-techniques.html#model-validation",
    "href": "other-techniques.html#model-validation",
    "title": "32  More Useful Techniques",
    "section": "32.4 Model Validation",
    "text": "32.4 Model Validation\n\n32.4.1 Static and dynamic validation\nStatic validation typically involves splitting the dataset into training and testing sets, where the testing set is held out and not used during model training. The model is trained on the training set and then evaluated on the held-out testing set to assess its performance. This approach helps to measure how well the model generalizes to unseen data.\nThe following example shows how to do a static validation in Julia.\n\nusing Random, Statistics, LinearAlgebra\n\n# Synthetic time-indexed data\nT = 200\nx = rand(T)\ny = 1.0 .+ 2.0 .* x .+ 0.1 .* randn(T)  # Vector response\n\n# Chronological holdout (static validation)\ncut = 150\nXtrain = hcat(ones(cut), x[1:cut])\nytrain = y[1:cut]\nXtest = hcat(ones(T - cut), x[cut+1:end])\nytest = y[cut+1:end]\n\n# OLS fit\nθ = Xtrain \\ ytrain  # Vector length 2\n\n# Predictions on the holdout\nŷ = Xtest * θ\n\n# Metrics\nmse = mean((ŷ .- ytest) .^ 2)\nmae = mean(abs.(ŷ .- ytest))\n\nprintln(\"Static validation (chronological holdout):\")\nprintln(\"Mean Squared Error (MSE): \", mse)\nprintln(\"Mean Absolute Error (MAE): \", mae)\n\nStatic validation (chronological holdout):\nMean Squared Error (MSE): 0.008635168139088349\nMean Absolute Error (MAE): 0.07685118593200958\n\n\nThe following example shows how to do a dynamic validation in Julia.\n\nusing Random, Statistics, LinearAlgebra\n\n# Reproducibility\nRandom.seed!(42)\n\n# Simulate a simple linear data-generating process\nT = 200\nx = rand(T)\ny = 1.0 .+ 2.0 .* x .+ 0.1 .* randn(T)  # y is a Vector (not an n×1 matrix)\n\n# Walk-forward expanding-window validation: 1-step-ahead forecasts\ninitial_window = 60\nsqerrs = Float64[]\nabserrs = Float64[]\n\nfor t in (initial_window+1):T\n    Xtr = hcat(ones(t - 1), x[1:t-1])\n    ytr = y[1:t-1]\n\n    θ = Xtr \\ ytr  # OLS on past data only\n\n    # 1-step-ahead prediction at time t\n    ŷt = [1.0, x[t]]' * θ\n    e = ŷt - y[t]\n\n    push!(sqerrs, e^2)\n    push!(abserrs, abs(e))\nend\n\nprintln(\"Dynamic validation (walk-forward expanding window):\")\nprintln(\"Mean Squared Error (MSE): \", mean(sqerrs))\nprintln(\"Mean Absolute Error (MAE): \", mean(abserrs))\n\nDynamic validation (walk-forward expanding window):\nMean Squared Error (MSE): 0.012102241884186706\nMean Absolute Error (MAE): 0.08733803592034403\n\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes static and dynamic validation of a financial model can refer to the following analysis:\n\nStatic validation: whether the model reproduces time zero prices/balances from the model.\nDynamic validation: whether the model reproduces flows (e.g. cashflows, settlements) that are in-trend for historical data.\n\n\n\n\n\n32.4.2 Implied rate analysis\nImplied rates are rates that are derived from the prices of financial instruments, such as bonds or options. For example, in the context of bonds, the implied rate is the interest rate that equates the present value of future cash flows from the bond (coupons and principal) to its current market price.\n\nusing Zygote\n\n# Define the bond cash flows and prices\ncash_flows = [100, 100, 100, 100, 1000]  # Coupons and principal\nprices = [950, 960, 1010, 1020, 1050]  # Market prices\n\n# Define a function to calculate the present value of cash flows given a rate\nfunction present_value(rate, cash_flows)\n    pv = 0\n    for (i, cf) in enumerate(cash_flows)\n        pv += cf / (1 + rate)^i\n    end\n    return pv\nend\n\n# Define a function to calculate the implied rate using bisection method\nfunction implied_rate(cash_flows, price)\n    f(rate) = present_value(rate, cash_flows) - price\n    return rootassign(f, 0.0, 1.0)\nend\nfunction rootassign(f, l, u)\n    # Define an initial value\n    x = 0.05\n    # tolerance of difference in value\n    tol = 1e-6\n    # maximum number of iteration of the algorithm\n    max_iter = 100\n    iter = 0\n    while abs(f(x)) &gt; tol && iter &lt; max_iter\n        x -= f(x) / gradient(f, x)[1]\n        iter += 1\n    end\n    if iter &lt; max_iter && l &lt; x &lt; u\n        return x\n    else\n        return -1.0\n    end\nend\n\n# Calculate implied rates for each bond\nimplied_rates = [implied_rate(cash_flows, price) for price in prices]\n# Print the results\nfor (i, rate) in enumerate(implied_rates)\n    println(\"Implied rate for bond $i: $rate\")\nend\n\nImplied rate for bond 1: 0.09658339166435045\nImplied rate for bond 2: 0.09380219311021369\nImplied rate for bond 3: 0.08046244727376842\nImplied rate for bond 4: 0.0779014164014789\nImplied rate for bond 5: 0.07041724037694008\n\n\n\n\n\n\n\n\nTip\n\n\n\nJuliaActuary’s FinanceCore.jl provides a fast, robust irr function. More related utilities (e.g. present value) are found in ActuaryUtilities.jl.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>More Useful Techniques</span>"
    ]
  },
  {
    "objectID": "other-techniques.html#sec-model-assessment",
    "href": "other-techniques.html#sec-model-assessment",
    "title": "32  More Useful Techniques",
    "section": "32.5 Predictive vs. Explanatory Model Assessments",
    "text": "32.5 Predictive vs. Explanatory Model Assessments\nModel assessment should be driven by the model’s purpose. A predictive model is judged by how well it forecasts targets under realistic deployment conditions. An explanatory (or structural) model is judged by how well its parameters are identified, interpretable, and stable under interventions—so that counterfactuals are credible.\nPredictive assessment - Define the forecast target and loss explicitly (point, quantile, probability, or full distribution). - Point forecasts (levels/returns): prioritize scale-aware losses such as RMSE and MAE. \\[\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{T}\\sum_{t=1}^{T}(\\hat{y}_t - y_t)^2}, \\quad \\mathrm{MAE} = \\frac{1}{T}\\sum_{t=1}^{T}|\\hat{y}_t - y_t|\n\\] Avoid MAPE when values can be near zero; consider symmetric MAPE variants if needed. - Quantile forecasts (e.g., VaR at level \\(\\tau\\)): use pinball (quantile) loss. \\[\n  L_\\tau(\\hat{q}_t, y_t) = \\left(\\tau - \\mathbf{1}\\{y_t &lt; \\hat{q}_t\\}\\right)(y_t - \\hat{q}_t)\n\\] - Probabilistic forecasts (default probabilities, loss distributions): use log score (negative log-likelihood), Brier score for binary events, CRPS for full distributions; evaluate calibration (reliability curves, PIT uniformity) and sharpness (narrowness of distributions). - Interval forecasts: evaluate coverage vs nominal and average interval width (or Winkler score). - Classification tasks (e.g., downgrade prediction): use AUROC/PR, calibration error, expected utility with cost-sensitive thresholds. - Validation design: for time series, use walk-forward (rolling-origin) evaluation with realistic feature availability; guard against leakage; include transaction costs/slippage where applicable.\nExplanatory assessment - Parameter interpretability: signs, magnitudes, and units align with theory and domain knowledge; elasticities and risk premia within plausible ranges. - Identification: demonstrate that parameters are uniquely recoverable from the data/design (e.g., instrument relevance/exogeneity for IV; rank conditions for GMM); report overidentification tests where applicable (e.g., Hansen’s J). - Stability/invariance: test parameter constancy across regimes (structural break tests such as Chow; rolling/CUSUM diagnostics); assess sensitivity to alternative samples/specifications. - Counterfactual validity: show that the model’s structural relationships remain invariant under contemplated interventions (policy changes, shocks); evaluate out-of-sample counterfactual predictions when historical policy variation exists. - Moment fit for structural models: report distance between empirical and model-implied moments; assess which moments are well matched and which are not. - Sensitivity/uncertainty analysis: local and global parameter sensitivity (e.g., Sobol indices), posterior uncertainty where Bayesian, and scenario robustness for key assumptions.\nSummary mapping\n\n\n\n\n\n\n\n\n\nGoal\nPrimary metrics/loss\nValidation design\nFinance examples\n\n\n\n\nPredict point\nRMSE, MAE, MAPE (with caution)\nWalk-forward CV; leakage checks\nForecast next-month returns, prepayment speeds\n\n\nPredict quantile\nPinball loss at τ\nRolling quantile backtests\nVaR at 99%\n\n\nPredict probability/distribution\nLog score, Brier, CRPS; calibration and sharpness\nTime-stamped splits; reliability/PIT checks\nDefault probability, loss distribution for stress\n\n\nClassification\nAUROC/PR, calibration error, expected utility\nCost-sensitive thresholds; class imbalance handling\nDowngrade/watchlist prediction\n\n\nExplanatory (structural)\nIdentification tests, parameter plausibility, moment fit\nStability/structural-break tests; sensitivity analysis\nTerm-structure model, demand/supply elasticities\n\n\nCounterfactual\nInvariance under intervention; policy simulation accuracy\nNatural experiments; out-of-sample policy periods\nImpact of capital requirement change on lending\n\n\n\n\n\n\n\n\n\nTipFinance Modeling Pro-tip\n\n\n\nAlign the loss you optimize in estimation with the metric you report in evaluation. If your risk committee cares about 99% tail losses, train and evaluate on quantile/tail losses, not just RMSE.\n\n\n\n32.5.1 Counterfactuals\nA counterfactual asks: “What would the outcome have been if we had made a different decision or if a policy/input had taken a different value?” It is a “what if” exercise where we deliberately change one input and keep the model’s underlying relationships the same.\nThe key idea is to separate two questions: - What we typically observe when an input is high or low (patterns in past data). - What would have happened if we had actively set that input to a new level (a deliberate change).\nTo run a counterfactual analysis in practice: 1. Pick the specific change (e.g., raise a capital ratio from 8% to 10%; lower mortgage rates by 100 bps; cap deposit betas at 0.30). 2. Hold the model’s mechanism the same (the estimated relationships and parameters do not change because of the thought experiment). 3. Recompute the outcomes under the new input and compare to the original (baseline) outcomes. 4. Report the difference as the counterfactual effect, along with uncertainty or sensitivity to key assumptions.\nWhen to trust a counterfactual: - Parameters are interpretable and align with finance intuition (signs, magnitudes, units). - Relationships are reasonably stable across time and regimes. - Results are robust to plausible alternative specifications and samples.\nFinance examples - Capital policy: “If the Tier 1 capital requirement were 10% instead of 8%, how would loan growth and ROE have changed?” - Mortgages: “If mortgage rates were 100 bps lower last quarter, holding credit standards fixed, what would prepayment speeds have been?” - ALM: “If deposit betas were capped at 0.30, how would net interest margin and the liquidity coverage ratio have evolved?”\nTip: Be explicit about what you changed, what you held fixed, and why the model’s relationships should remain valid under the proposed change.\nChange Log - Rewrote the counterfactual definition in plain language without causal notation or equations. - Added a simple, repeatable checklist for conducting counterfactual analyses. - Included concise finance-focused examples consistent with the chapter’s tone and audience.\nRecommendations - When you first introduce counterfactuals in later chapters, show a small worked example: baseline vs counterfactual run, side-by-side, with a sentence on what was held fixed and why.",
    "crumbs": [
      "Applied Financial Modeling Techniques",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>More Useful Techniques</span>"
    ]
  },
  {
    "objectID": "ecosystem.html",
    "href": "ecosystem.html",
    "title": "33  The Julia Ecosystem Today",
    "section": "",
    "text": "33.1 Chapter Overview\nA tour of relevant available packages as of 2023.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>The Julia Ecosystem Today</span>"
    ]
  },
  {
    "objectID": "ecosystem.html#the-julia-ecosystem",
    "href": "ecosystem.html#the-julia-ecosystem",
    "title": "33  The Julia Ecosystem Today",
    "section": "33.2 The Julia Ecosystem",
    "text": "33.2 The Julia Ecosystem\nThe Julia ecosystem favors composability and interoperability, enabled by multiple dispatch. In other words, because it’s easy to automatically specialize functionality based on the type of data being used, there’s much less need to bundle a lot of features within a single package.\nAs you’ll see, Julia packages tend to be less vertically integrated because it’s easier to pass data around. Counterexamples of this in Python and R:\n\nNumpy-compatible packages that are designed to work with a subset of numerically fast libraries in Python\nspecial functions in Pandas to read CSV, JSON, database connections, etc.\nThe Tidyverse in R has a tightly coupled set of packages that works well together but has limitations with some other R packages\n\nJulia is not perfect in this regard, but it’s neat to see how frequently things just work. It’s not magic, but because of Julia features outside the scope of this article it’s easy for package developers (and you!) to do this.\nJulia also has language-level support for documentation, so packages can follow a consistent style of help-text and have the docs be auto-generated into web pages available locally or online.\nThe following highlighted packages were chosen for their relevance to typical actuarial work, with a bias towards those used regularly by the authors. This is a small sampling of the over 6000 registered Julia Packages[^2]\n\n33.2.1 Data\nJulia offers a rich data ecosystem with a multitude of available packages. Perhaps at the center of the data ecosystem are CSV.jl and DataFrames.jl. CSV.jl is for reading and writing files text files (namely CSVs) and offers top-class read and write performance. DataFrames.jl is a mature package for working with dataframes, comparable to Pandas or dplyr.\nOther notable packages include ODBC.jl, which lets you connect to any database (given you have the right drivers installed), and Arrow.jl which implements the Apache Arrow standard in Julia.\nWorth mentioning also is Dates, a built-in package making date manipulation straightforward and robust.\nCheck out JuliaData org for more packages and information.\n\n\n33.2.2 Plotting\nPlots.jl is a meta-package providing an interface to consistently work with several plotting backends, depending if you are trying to emphasize interactivity on the web or print-quality output. You can very easily add animations or change almost any feature of a plot.\nStatsPlots.jl extends Plots.jl with a focus on data visualization and compatibility with dataframes.\nMakie.jl supports GPU-accelerated plotting and can create very rich, beautiful visualizations, but its main downside is that it has not yet been optimized to minimize the time-to-first-plot.\n\n\n33.2.3 Statistics\nJulia has first-class support for missing values, which follows the rules of three-valued logic so other packages don’t need to do anything special to incorporate missing values.\nStatsBase.jl and Distributions.jl are essentials for a range of statistics functions and probability distributions respectively.\nOthers include:\n\nTuring.jl, a probabilistic programming (Bayesian statistics) library, which is outstanding in its combination of clear model syntax with performance.\nGLM.jl for any type of linear modeling (mimicking R’s glm functionality).\nLsqFit.jl for fitting data to non-linear models.\nMultivariateStats.jl for multivariate statistics, such as PCA.\n\nYou can find more packages and learn about them here.\n\n\n33.2.4 Machine Learning\nFlux, Gen, Knet, and MLJ are all very popular machine learning libraries. There are also packages for PyTorch, Tensorflow, and SciKitML available. One advantage for users is that the Julia packages are written in Julia, so it can be easier to adapt or see what’s going on in the entire stack. In contrast to this design, PyTorch and Tensorflow are built primarily with C++.\nAnother advantage is that the Julia libraries can use automatic differentiation to optimize on a wider range of data and functions than those built into libraries in other languages.\n\n\n33.2.5 Differentiable Programming\nSensitivity testing is very common in actuarial workflows: essentially, it’s understanding the change in one variable in relation to another. In other words, the derivative!\nJulia has unique capabilities where almost across the entire language and ecosystem, you can take the derivative of entire functions or scripts. For example, the following is real Julia code to automatically calculate the sensitivity of the ending account value with respect to the inputs:\njulia&gt; using Zygote\n\njulia&gt; function policy_av(pol)\n    COIs = [0.00319, 0.00345, 0.0038, 0.00419, 0.0047, 0.00532]\n    av = 0.0\n    for (i,coi) in enumerate(COIs)\n        av += av * pol.credit_rate\n        av += pol.annual_premium\n        av -= pol.face * coi\n    end\n    return av                # return the final account value\nend\n\njulia&gt; pol = (annual_premium = 1000, face = 100_000, credit_rate = 0.05);\n\njulia&gt; policy_av(pol)        # the ending account value\n4048.08\n\njulia&gt; policy_av'(pol)       # the derivative of the account value with respect to the inputs\n(annual_premium = 6.802, face = -0.0275, credit_rate = 10972.52)\nWhen executing the code above, Julia isn’t just adding a small amount and calculating the finite difference. Differentiation is applied to entire programs through extensive use of basic derivatives and the chain rule. Automatic differentiation, has uses in optimization, machine learning, sensitivity testing, and risk analysis. You can read more about Julia’s autodiff ecosystem here.\n\n\n33.2.6 Utilities\nThere are also a lot of quality-of-life packages, like Revise.jl which lets you edit code on the fly without needing to re-run entire scripts.\nBenchmarkTools.jl makes it incredibly easy to benchmark your code - simply add @benchmark in front of what you want to test, and you will be presented with detailed statistics. For example:\njulia&gt; using ActuaryUtilities, BenchmarkTools\n\njulia&gt; @benchmark present_value(0.05,[10,10,10])\n\nBenchmarkTools.Trial: 10000 samples with 994 evaluations.\n Range (min … max):  33.492 ns … 829.015 ns  ┊ GC (min … max): 0.00% … 95.40%\n Time  (median):     34.708 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   36.599 ns ±  33.686 ns  ┊ GC (mean ± σ):  4.40% ±  4.55%\n\n  ▁▃▆▆▆██▇▄▃▂         ▁                                        ▂\n  █████████████▆▆▇█▇████▇██▇█▇█▇▇▆▆▅▅▅▅▅▄▅▄▄▅▅▅▅▄▄▁▅▄▄▅▄▄▅▅▆▅▆ █\n  33.5 ns       Histogram: log(frequency) by time      45.6 ns &lt;\n\n Memory estimate: 112 bytes, allocs estimate: 1.\nTest is a built-in package for performing testsets, while Documenter.jl will build high-quality documentation based on your inline documentation.\nClipData.jl lets you copy and paste from spreadsheets to Julia sessions.\n\n\n33.2.7 Other packages\nJulia is a general-purpose language, so you will find packages for web development, graphics, game development, audio production, and much more. You can explore packages (and their dependencies) at https://juliahub.com/.\n\n\n33.2.8 Actuarial packages\nSaving the best for last, the next article in the series will dive deeper into actuarial packages, such as those published by JuliaActuary for easy mortality table manipulation, common actuarial functions, financial math, and experience analysis.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>The Julia Ecosystem Today</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Balbás, Alejandro, José Garrido, and Silvia Mayoral. 2009.\n“Properties of Distortion Risk Measures.” Methodology\nand Computing in Applied Probability 11 (3): 385–99. https://doi.org/10.1007/s11009-008-9089-z.\n\n\nHardy, Mary R. 2006. “An Introduction to Risk Measures for\nActuarial Applications.” SOA Syllabus Study Note 19.\n\n\nHeer, Jeffrey, Michael Bostock, and Vadim Ogievetsky. 2010. “A\nTour Through the Visualization Zoo.” Communications of the\nACM. https://homes.cs.washington.edu/~jheer/files/zoo/.\n\n\nLeemis, Lawrence M, and Jacquelyn T McQueston. 2008. “Univariate\nDistribution Relationships.” The American Statistician\n62 (1): 45–53. https://doi.org/10.1198/000313008x270448.\n\n\nLewis, N D. 2013. 100 Statistical Tests. Createspace.\n\n\nMatthews, Dylan. 2014. “The Ice Bucket Challenge, and Why We Give\nto Charity.” 2014. https://web.archive.org/web/20140823152725/https://www.vox.com/2014/8/20/6040435/als-ice-bucket-challenge-and-why-we-give-to-charity-donate.\n\n\nMurphy, Hannah, and Cristina Criddle. 2024. “Meta AI Chief Says\nLarge Language Models Will Not Reach Human Intelligence.” https://www.ft.com/content/23fab126-f1d3-4add-a457-207a25730ad9.\n\n\nSchwarz, C. J. 2016. “A Short Tour of Bad Graphs.” Online.\nhttp://www.stat.sfu.ca/~cschwarz/posters/1999/absenteeism.pdf.\n\n\nStrommen, Stephen J. 2022. “Low Interest Rates and the NAIC\nEconomic Scenario Generator Project.” Risk &\nRewards. https://www.soa.org/sections/investment/investment-newsletter/2022/june/rr-2022-06-strommen/.\n\n\nTufte, Edward. 2001. The Visual Display of Quantitative\nInformation. 2nd ed. Graphics Press.\n\n\nWang, Shaun S. 2002. “A Universal Framework for Pricing Financial\nand Insurance Risks.” ASTIN Bulletin 32 (2): 213–34. https://doi.org/10.2143/AST.32.2.1027.",
    "crumbs": [
      "Appendices",
      "References"
    ]
  }
]