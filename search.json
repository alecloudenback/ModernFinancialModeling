[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Thinking for Actuaries and Financial Professionals",
    "section": "",
    "text": "Preface\nThis book is intended to enable practitioners and advanced students of financial disciplines to utilize the tools, language, and ideas of computational and related sciences in their own work.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "cover_draft.html",
    "href": "cover_draft.html",
    "title": "1  Draft of Cover",
    "section": "",
    "text": "Draft of Cover",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Draft of Cover</span>"
    ]
  },
  {
    "objectID": "why-program.html",
    "href": "why-program.html",
    "title": "2  Why Program?",
    "section": "",
    "text": "2.1 In this Chapter\nWe motivate why a financial professional should adopt programming skills which will improve their own capabilities and enjoyment of the discipline, whilst allowing themselves to better themselves and the industry we work in.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-program.html#the-long-view",
    "href": "why-program.html#the-long-view",
    "title": "2  Why Program?",
    "section": "2.2 The Long View",
    "text": "2.2 The Long View\nIt might be odd to say that technology and its use in insurance is on a one-hundred-year cycle, but that seems to be the case.\n130 years ago, actuaries crowded into a room at a meeting of the Actuarial Society of America to watch a demonstration that would revolutionize the industry: Herman Hollerith’s tabulating punch card machine1.\nFor the next half-century, the increasing automation — from tabulating machines to early-adopting mainframes and computers — was a critical competitive differentiator. Companies like Prudential, MetLife, and others partnered with technology companies in the development of hardware and software2.\nThe dramatic embodiment of this information-driven cycle was portrayed in the infamous Billion Dollar Bubble movie, which showcased the power and abstraction of the computer to commit millions of dollars of fraud by creating and maintaining fake insurance policies.\nThe movie also starts to hint at the oscillation away from the technological-competitive focus of insurance companies. I argue that the focus on technology was lost over the last 50 years with the rise of Wall Street finance, investment-oriented life insurance, industry consolidation, and the explosion of financial structuring like derivatives, reserve financing, or other advanced forms of reinsurance.\nValue-add came from the C-Suite, not from the underlying business processes, operations, and analysis. The result is, e.g., ever-more complicated reinsurance treaties layered into mainframes and admin systems older than most of the actuaries interfacing with them.\nThe pace of strategic value-add isn’t slowing, though it must stretch further (in complexity and risk) to find comparable opportunities as the past. Having more agile, data-oriented operations enables companies to be able to react to and implement those opportunities. Technological value-add can improve a company’s bottom line through lower expenses and higher top-line growth, but often with a more favorable risk profile than some of the “strategic” opportunities.\nToday, there is a trend reverting back to technological value-creation and is evident across many traditional sectors. Tesla claims that it’s a technology company; Amazon is the #1 product retailer because of its vehement focus on internal information sharing3; Airlines are so dependent on their systems that the skies become quieter on the rare occasion that their computers give way.\nWhy is it, that companies that are so involved in things (cars, shopping) and physical services (flights) are so much more focused on improving their technological operations than insurance companies whose very focus is ‘information-based’? The market has rewarded those who have prioritized their internal technological solutions.\nCommoditized investing services and low yield environments have reduced insurance companies’ comparative advantage to “manage money”. Yield compression and the explosion of consumer-oriented investment services means a more competitive focus on the ability to manage the entire policy lifecycle efficiently (digitally), perform more real-time analysis of experience and risk management, and handle the growing product and regulatory complexity.\nThese are problems that have technological solutions and are waiting for insurance company adoption.\nCompanies that treat data like coordinates on a grid (spreadsheets) will get left behind. Two main hurdles have prevented technology companies from breaking into insurance:\n\nHigh regulatory barriers to entry, and\nDifficulty in selling complex insurance products without traditional distribution.\n\nOnce those two walls are breached, traditional insurance companies without a strong technology core will struggle to keep up. The key to thriving is not just adding “developers” to an organization; it’s going to be getting domain experts like actuaries to be an integral part of the technology transformation.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-program.html#whats-coding-got-to-do-with-this",
    "href": "why-program.html#whats-coding-got-to-do-with-this",
    "title": "2  Why Program?",
    "section": "2.3 What’s coding got to do with this?",
    "text": "2.3 What’s coding got to do with this?\nEverything. Programming is the optimal way to interact between the computer and actuary — and importantly between computer and computer. Programming is the actionable expression of ideas, math, analysis, and information. Think of programming as the 21st-century leap in the actuary’s toolkit, just as spreadsheets were in the preceding 40 years. Versus a spreadsheet-oriented workflow:\n\nMore natural automation of, and between processes\nBetter reproducibility\nScaling to fit any size dataset and workload\nStatistics and machine learning capabilities\nAdvanced visualizations to garner new views into your data\n\nThis list isn’t comprehensive and some benefits are subtle — when you are code-oriented instead of spreadsheet-oriented, you tend to want to structure your data in a portable and shareable way. For example, relying more on data warehouses instead of email attachments. This, in turn, enables data discovery and insights that otherwise wouldn’t be there. Investing in a code-oriented workflow is playing the long-game.\nThe actuary of the future needs to have coding as one of their core skills. Already today, the advances of business processes, insurance products, and financial ingenuity are written with lines of code — not spreadsheets. Not being able to code necessarily means that you are following what others are doing today.\nIt’s commonly accepted now that to gather insights from your data, you need to know how to code. Similar to your data, your business architecture, modeling needs, and product peculiarities are often better suited to customized solutions. Why stop at data science when learning how to solve problems with a computer?",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-program.html#the-10x-actuary",
    "href": "why-program.html#the-10x-actuary",
    "title": "2  Why Program?",
    "section": "2.4 The 10x Actuary",
    "text": "2.4 The 10x Actuary\nAs we swing back to a technological focus, we do not leave the finance-driven complexity behind. The increasingly complex business needs will highlight a large productivity difference between an actuary who can code and one who can’t — simply because the former can react, create, synthesize, and model faster than the latter. From the efficiency of transforming administration extracts, summarizing and aggregating valuation output, to analyzing claims data in ways that spreadsheets simply can’t handle, you can become a “10x Actuary”4.\nFlipping switches in a graphical user interface versus being able to build models is the difference between having a surface-level familiarity and having full command over the analysis and the concepts involved — with the flexibility to do what your software can’t.\nYour current software might be able to perform the first layer of analysis but be at a loss when you want to visualize, perform sensitivity analysis, statistics, stochastic analysis, or process automation. Things that, when done programmatically, are often just a few lines of additional code.\nDo I advocate dropping the license for your software vendor? No, not yet anyway. But the ability to supplement and break out of the modeling box has been an increasingly important part of most actuaries’ work.\nAdditionally, code-based solutions can leverage the entire-technology sector’s progress to solve problems that are hard otherwise: scalability, data workflows, integration across functional areas, version control and versioning, model change governance, reproducibility, and more.\n30-40 years ago, there were no vendor-supplied modeling solutions and so you had no choice but to build models internally. This shifted with the advent of vendor-supplied modeling solutions. Today, it’s never been better for companies to leverage open source to support their custom modeling, risk analysis/monitoring, and reporting workflows.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-program.html#risk-governance",
    "href": "why-program.html#risk-governance",
    "title": "2  Why Program?",
    "section": "2.5 Risk Governance",
    "text": "2.5 Risk Governance\nCode-based workflows are highly conducive to risk governance frameworks as well. If a modern software project has all of the following benefits, then why not a modern insurance product and associated processes?\n\nAccess control and approval processes\nVersion control, version management, and reproducibility\nContinuous testing and validation of results\nOpen and transparent design\nMinimization of manual overrides, intervention, and opportunity for user error\nAutomated trending analysis, system metrics, and summary statistics\nContinuously updated, integrated, and self-generating documentation\nIntegration with other business processes through a formal boundary (e.g. via an API)\nTools to manage collaboration in parallel and in sequence",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-program.html#managing-and-leading-the-transformation",
    "href": "why-program.html#managing-and-leading-the-transformation",
    "title": "2  Why Program?",
    "section": "2.6 Managing and Leading the Transformation",
    "text": "2.6 Managing and Leading the Transformation\nThe ability to understand the concepts, capabilities, challenges, and lingo is not a dichotomy, it’s a spectrum. Most actuaries, even at fairly high levels, are still often involved in analytical work. Still above that, it’s difficult to lead something that you don’t understand.\nConversely, the skill and practice of coding enhances managerial capabilities. When you are really skilled at pulling apart a problem or process into its constituent parts and designing optimal solutions; that’s a core attribute of leadership: having the vision of where the organization should be instead of thinking about where it is now.\nNor is the skillset described here limiting in any other aspect of career development any more than mathematical ability, project collaboration, or financial acumen — just to name a few.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-program.html#outlook",
    "href": "why-program.html#outlook",
    "title": "2  Why Program?",
    "section": "2.7 Outlook",
    "text": "2.7 Outlook\nIt will increasingly be essential for companies to modernize to remain competitive. That modernization isn’t built with big black-box software packages; it will be with domain experts who can translate the expertise into new forms of analysis - doing it faster and more robustly than the competition.\nSpaceX doesn’t just hire rocket scientists - they hire rocket scientists who code.\nBe an actuary who codes.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-program.html#footnotes",
    "href": "why-program.html#footnotes",
    "title": "2  Why Program?",
    "section": "",
    "text": "Co-evolution of Information Processing Technology and Use: Interaction Between the Life Insurance and Tabulating Industries↩︎\nFrom Tabulators to Early Computers in the U.S. Life Insurance Industry↩︎\nHave you had your Bezos moment? What you can learn from Amazon↩︎\nThe 10x [Rockstar] developer is NOT a myth↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Why Program?</span>"
    ]
  },
  {
    "objectID": "why-julia.html",
    "href": "why-julia.html",
    "title": "3  Why use Julia?",
    "section": "",
    "text": "3.1 Expressiveness and Syntax\nExpressiveness is the manner in which and scope of ideas and concepts that can be represented in a programming language. Syntax refers to how the code looks on the screen and its readability.\nIn a language with high expressiveness and pleasant syntax, you:\nExpressiveness can be hard to explain, but perhaps two short examples will illustrate.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "why-julia.html#expressiveness-and-syntax",
    "href": "why-julia.html#expressiveness-and-syntax",
    "title": "3  Why use Julia?",
    "section": "",
    "text": "Go from idea in your head to final product faster.\nEncapsulate concepts naturally and write concise functions.\nCompose functions and data naturally.\nFocus on the end-goal instead of fighting the tools.\n\n\n\n3.1.1 Example: Retention Analysis\nThis is a really simple example relating Cessions, Policys, and Lives to do simple retention analysis.\nFirst, let’s define our data:\n\n# Define our data structures\nstruct Life\n  policies\nend\n\nstruct Policy\n  face\n  cessions\n end\n\nstruct Cession\n  ceded\nend\nNow to calculate amounts retained. First, let’s say what retention means for a Policy:\n# define retention\nfunction retained(pol::Policy)\n  pol.face - sum(cession.ceded for cession in pol.cessions)\nend\nAnd then what retention means for a Life:\nfunction retained(l::Life)\n  sum(retained(policy) for policy in life.policies)\nend\nIt’s almost exactly how you’d specify it English. No joins, no boilerplate, no fiddling with complicated syntax. You can express ideas and concepts the way that you think of them, not, for example, as a series of dataframe joins or as row/column coordinates on a spreadsheet.\nWe defined retained and adapted it to mean related, but different things depending on the specific context. That is, we didn’t have to define retained_life(...) and retained_pol(...) because Julia can be dispatch based on what you give it. This is, as some would call it, unreasonably effective.\nLet’s use the above code in practice then.\nThe julia&gt; syntax indicates that we’ve moved into Julia’s interactive mode (REPL mode):\n# create two policies with two and one cessions respectively\njulia&gt; pol_1 = Policy( 1000, [ Cession(100), Cession(500)] )\njulia&gt; pol_2 = Policy( 2500, [ Cession(1000) ] )\n\n# create a life, which has the two policies\njulia&gt; life = Life([pol_1, pol_2])\njulia&gt; retained(pol_1)\n400\njulia&gt; retained(life)\n1900\nAnd for the last trick, something called “broadcasting”, which automatically vectorizes any function you write, no need to write loops or create if statements to handle a single vs repeated case:\njulia&gt; retained.(life.policies) # retained amount for each policy\n[400 ,  1500]\n\n\n3.1.2 Example: Random Sampling\nAs another motivating example showcasing multiple dispatch, here’s random sampling in Julia, R, and Python.\nWe generate 100:\n\nUniform random numbers\nStandard normal random numbers\nBernoulli random number\nRandom samples with a given set\n\n\n\n\n\nTable 3.1: A comparison of random outcome generation in Julia, R, and Python.\n\n\n\n\n\n\n\n\n\n\nJulia\nR\nPython\n\n\n\n\nusing Distributions\n\nrand(100)\nrand(Normal(), 100)\nrand(Bernoulli(0.5), 100)\nrand([\"Preferred\",\"Standard\"], 100)\nrunif(100)\nrnorm(100)\nrbern(100, 0.5)\nsample(c(\"Preferred\",\"Standard\"),\n100, replace=TRUE)\n\nimport scipy.stats as sps\nimport numpy as np\n\n\nsps.uniform.rvs(size=100)\nsps.norm.rvs(size=100)\nsps.bernoulli.rvs(p=0.5,size=100)\nnp.random.choice([\"Preferred\",\"Standard\"],\nsize=100)\n\n\n\n\n\n\n\nBy understanding the different types of things passed to rand(), it maintains the same syntax across a variety of different scenarios. We could define rand(Cession) and have it generate a random Cession like we used above.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "why-julia.html#the-speed",
    "href": "why-julia.html#the-speed",
    "title": "3  Why use Julia?",
    "section": "3.2 The Speed",
    "text": "3.2 The Speed\nAs the journal Nature said, “Come for the Syntax, Stay for the Speed”.\nRecall the Solvency II compliance which ran 1000x faster than the prior vendor solution mentioned earlier: what does it mean to be 1000x faster at something? It’s the difference between something taking 10 seconds instead of 3 hours — or 1 hour instead of 42 days.\nWhat analysis would you like to do if it took less time? A stochastic analysis of life-level claims? Machine learning with your experience data? Daily valuation instead of quarterly?\nSpeaking from experience, speed is not just great for production time improvements. During development, it’s really helpful too. When building something, I can see that I messed something up in a couple of seconds instead of 20 minutes. The build, test, fix, iteration cycle goes faster this way.\nAdmittedly, most workflows don’t see a 1000x speedup, but 10x to 1000x is a very common range of speed differences vs R or Python or MATLAB.\nSometimes you will see less of a speed difference; R and Python have already circumvented this and written much core code in low-level languages. This is an example of what’s called the “two-language” problem where the language productive to write in isn’t very fast. For example, more than half of R packages use C/C++/Fortran and core packages in Python like Pandas, PyTorch, NumPy, SciPy, etc. do this too.\nWithin the bounds of the optimized R/Python libraries, you can leverage this work. Extending it can be difficult: what if you have a custom retention management system running on millions of policies every night?\nJulia packages you are using are almost always written in pure Julia: you can see what’s going on, learn from them, or even contribute a package of your own!",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "why-julia.html#more-of-julias-benefits",
    "href": "why-julia.html#more-of-julias-benefits",
    "title": "3  Why use Julia?",
    "section": "3.3 More of Julia’s benefits",
    "text": "3.3 More of Julia’s benefits\nJulia is easy to write, learn, and be productive in:\n\nIt’s free and open-source\n\nVery permissive licenses, facilitating the use in commercial environments (same with most packages)\n\nLarge and growing set of available packages\nWrite how you like because it’s multi-paradigm: vectorizable (R), object-oriented (Python), functional (Lisp), or detail-oriented (C)\nBuilt-in package manager, documentation, and testing-library\nJupyter Notebook support (it’s in the name! Julia-Python-R)\nMany small, nice things that add up:\n\nUnicode characters like α or β\nNice display of arrays\nSimple anonymous function syntax\nWide range of text editor support\nFirst-class support for missing values across the entire language\nLiterate programming support (like R-Markdown)\n\nBuilt-in Dates package that makes working with dates pleasant\nAbility to directly call and use R and Python code/packages with the PyCall and RCall packages\nError messages are helpful and tell you what line the error came from, not just the type of error\nDebugger functionality so you can step through your code line by line\n\nFor power-users, advanced features are easily accessible: parallel programming, broadcasting, types, interfaces, metaprogramming, and more.\nThese are some of the things that make Julia one of the world’s most loved languages on the StackOverflow Developer Survey.\nFor those who are enterprise-minded: in addition to the liberal licensing mentioned above, there are professional products from organizations like Julia Computing that provide hands-on support, training, IT governance solutions, behind-the-firewall package management, and deployment/scaling assistance.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "why-julia.html#the-tradeoff",
    "href": "why-julia.html#the-tradeoff",
    "title": "3  Why use Julia?",
    "section": "3.4 The Tradeoff",
    "text": "3.4 The Tradeoff\nJulia is fast because it’s compiled, unlike R and Python where (loosely speaking) the computer just reads one line at a time. Julia compiles code “just-in-time”: right before you use a function for the first time, it will take a moment to pre-process the code section for the machine. Subsequent calls don’t need to be re-compiled and are very fast.\nA hypothetical example: running 10,000 stochastic projections where Julia needs to precompile but then runs each 10x faster:\n\nJulia runs in 2 minutes: the first projection takes 1 second to compile and run, but each 9,999 remaining projections only take 10ms.\nPython runs in 17 minutes: 100ms of a second for each computation.\n\nTypically, the compilation is very fast (milliseconds), but in the most complicated cases it can be several seconds. One of these is the “time-to-first-plot” issue because it’s the most common one users encounter: super-flexible plotting libraries have a lot of things to pre-compile. So in the case of plotting, it can take several seconds to display the first plot after starting Julia, but then it’s remarkably quick and easy to create an animation of your model results. The time-to-first plot is a solvable problem that’s receiving a lot of attention from the core developers and will get better with future Julia releases.\nFor users working with a lot of data or complex calculations (like actuaries!), the runtime speedup is worth a few seconds at the start.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "why-julia.html#package-ecosystem",
    "href": "why-julia.html#package-ecosystem",
    "title": "3  Why use Julia?",
    "section": "3.5 Package Ecosystem",
    "text": "3.5 Package Ecosystem\nUsing packages as dependencies in your project is assisted by Julia’ bundled package manager.\nFor each project, you can track the exact set of dependencies and replicate the code/process on another machine or another time. In R or Python, dependency management is notoriously difficult and it’s one of the things that the Julia creators wanted to fix from the start.\nPackages can be one of the thousands of publicly available, or private packages hosted internally behind a firewall.\nAnother powerful aspect of the package ecosystem is that due to the language design, packages can be combined/extended in ways that are difficult for other common languages. This means that Julia packages often interop without any additional coordination.\nFor example, packages that operate on data tables work without issue together in Julia. In R/Python, many features tend to come bundled in a giant singular package like Python’s Pandas which has Input/Output, Date manipulation, plotting, resampling, and more. There’s a new Consortium for Python Data API Standards which seeks to harmonize the different packages in Python to make them more consistent (R’s Tidyverse plays a similar role in coordinating their subset of the package ecosystem).\nIn Julia, packages tend to be more plug-and-play. For example, every time you want to load a CSV you might not want to transform the data into a dataframe (maybe you want a matrix or a plot instead). To load data into a dataframe, in Julia the practice is to use both the CSV and DataFrames packages, which help separate concerns. Some users may prefer the Python/R approach of less modular but more all-inclusive packages.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "why-julia.html#conclusion",
    "href": "why-julia.html#conclusion",
    "title": "3  Why use Julia?",
    "section": "3.6 Conclusion",
    "text": "3.6 Conclusion\nLooking at other great tools like R and Python, it can be difficult to summarize a single reason to motivate a switch to Julia, but hopefully this article piqued an interest to try it for your next project.\nThat said, Julia shouldn’t be the only tool in your tool-kit. SQL will remain an important way to interact with databases. R and Python aren’t going anywhere in the short term and will always offer a different perspective on things!\nIn an earlier article, I talked about becoming a 10x Actuary which meant being proficient in the language of computers so that you could build and implement great things. In a large way, the choice of tools and paradigms shape your focus. Productivity is one aspect, expressiveness is another, speed one more. There are many reasons to think about what tools you use and trying out different ones is probably the best way to find what works best for you.\nIt is said that you cannot fully conceptualize something unless your language has a word for it. Similar to spoken language, you may find that breaking out of spreadsheet coordinates (and even a dataframe-centric view of the world) reveals different questions to ask and enables innovated ways to solve problems. In this way, you reward your intellect while building more meaningful and relevant models and analysis.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "why-julia.html#footnotes",
    "href": "why-julia.html#footnotes",
    "title": "3  Why use Julia?",
    "section": "",
    "text": "Python first appeared in 1990. R is an implementation of S, which was created in 1976, though depending on when you want to place the start of an independent R project varies (1993, 1995, and 2000 are alternate dates). The history of these languages is long and substantial changes have occurred since these dates.↩︎\nAviva Case Study↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Why use Julia?</span>"
    ]
  },
  {
    "objectID": "elements-of-financial-modeling.html",
    "href": "elements-of-financial-modeling.html",
    "title": "4  Elements of Financial Modeling",
    "section": "",
    "text": "4.1 In this Chapter\nWe explain what constitutes a financial model and what are common uses of a model. We explain what makes an adept practitioner.",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Elements of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "elements-of-financial-modeling.html#what-is-a-model",
    "href": "elements-of-financial-modeling.html#what-is-a-model",
    "title": "4  Elements of Financial Modeling",
    "section": "4.2 What is a model?",
    "text": "4.2 What is a model?\nA model represents aspects of the world around us distilled down into simpler, more tractable components. It is impossible to fully capture the everything that may affect the objects of our interest. We may build models for a variety of reasons, as listed in Table 4.1.\n\n\n\nTable 4.1: The REDCAPE model use framework, from “The Model Thinker” by Scott Page.\n\n\n\n\n\n\n\n\n\nUse\nDescription\n\n\n\n\nReason\nTo identify conditions and deduce logical implications.\n\n\nExplain\nTo provide (testable) explanations for empirical phenomena.\n\n\nDesign\nTo choose features of institutions, policies, and rules.\n\n\nCommunicate\nTo relate knowledge and understandings.\n\n\nAct\nTo guide policy choices and strategic actions.\n\n\nPredict\nTo make numerical and categorical predictions of future and unknown phenomena.\n\n\nExplore\nTo investigate possibilities and hypotheticals.\n\n\n\n\n\n\nFor example, say we want to simulate the returns for the stocks in our retirement portfolio. It would be impossible to try to build a model which would capture all of the individual people working jobs and making decisions, weather events that damage property, political machinations, etc. Instead, we try to capture certain fundamental characteristics. For example, it is common to model equity returns as cumulative pluses and minuses from random movements where those movements have certain theoretical or historical characteristics.\nWhether we are using this model of equity returns to estimate available retirement income or replicate an exotic option price, a key aspect of the model is the assumptions used therein. For the retirement income scenario we might assume a healthy eight percent return on stocks and conclude that such a return will be sufficient to retire at age 53. Alternatively, we may assume that future returns will follow a stochastic path with a certain distribution of volatility and drift. These two assumption sets will produce output - results from our model that must be inpsected, questioned, and understood in the context of the “small world” of the model’s mechanistic workings. Lastly, to be effective practitioners we must be able to contextualize the “small world” results withing the “large world” that exists around us.\n\n4.2.1 “Small world” vs “Large world”\nWhat do we mean by “small world” vs “large world”? Say that our model is one that discounts a fixed set of future cashflows using the US Treasury rate curve. If I run my model using current rates today, and then re-run my model tomorrow with the same future cashlows and the present value of those cashflows has increased by 5% I may ask why the result has changed so much in such a short period of time! In the “small”, mechanistic world of the model I may be able to see that the rates I used to discount the cashflows with have fallen substantially. The “small world” answer is that the inputs have changed which produced a mechanical change in the output. The “large world” answer may be that the Federal Reserve lowered the Federal Funds Rate to prevent the economy from entering a deflationary recession. Of course, we can’t completely explain the relation between our model and the real world (otherwise we could capture that relationship in our model!). An effective practitioner will always try to look up from the immediate work and take stock of how the world at large is or is not reflected in the model.",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Elements of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "elements-of-financial-modeling.html#what-is-a-financial-model",
    "href": "elements-of-financial-modeling.html#what-is-a-financial-model",
    "title": "4  Elements of Financial Modeling",
    "section": "4.3 What is a Financial Model?",
    "text": "4.3 What is a Financial Model?\nFinancial models are those used extensively to ascertain better understanding of complex contracts, perform scenario analysis, and inform market participants’ decisions related to perceived value (and therefore price). It can’t be quantified directly, but it is likely not an exaggeration that many billions of dollars is transacted each day as a result of decisions made from the output of financial models.\nMost financial models can be characterized with a focus on the first or both of:\n\nAttempting to project pattern of cashflows or obligations at future timepoints\nReducing the projected obligations into a current value\n\nExamples of this:\n\nProjecting a retiree’s savings through time (1), and determining how much the should be saving today for their retirement goal (2)\nProjecting the obligation of an exotic option across different potential paths (1), and determining the premium for that option (2)\n\nModels are sometimes taken a step further, such as transforming the underlying economic view into an accounting or regulatory view (such as representing associated debits and credits, capital requirements, or associated intangible, capitalized balances).\nWe should also distinguish a financial model from a purely statistical model, where the often the inputs and output data are known and the intention is to estimate relationships between variables (example: linear regressions). That said, a financial model may have statistical components and many aspects of modeling is shared between the two kinds.",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Elements of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "elements-of-financial-modeling.html#the-process-of-building-a-financial-model",
    "href": "elements-of-financial-modeling.html#the-process-of-building-a-financial-model",
    "title": "4  Elements of Financial Modeling",
    "section": "4.4 The Process of Building a Financial Model",
    "text": "4.4 The Process of Building a Financial Model\n\n\n\n\n\n\nTODO: Describe model building process and make associated diagram",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Elements of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "elements-of-financial-modeling.html#sec-predictive-vs-explanatory",
    "href": "elements-of-financial-modeling.html#sec-predictive-vs-explanatory",
    "title": "4  Elements of Financial Modeling",
    "section": "4.5 Predictive versus Explanatory Models",
    "text": "4.5 Predictive versus Explanatory Models\nGiven a set of inputs, our model will generate an output and we are generally interested in its accuracy. The model need not have a realistic mechanism for how the world works. That is, we may primarily be interested in accurately calculating an output value without the model having any scientific, explanatory power of how different parts of the real-world system interact.\n\n4.5.1 A Historical Example\nConsider the classic underdog story where Copernicus overthrew the status quo when he proposed (correctly) that the earth orbited the sun instead of the other way around1.\nThe existing Ptolemic model used a geocentric view of the solar system in which the planets and sun orbited the Earth in perfect circles with an epicycle used to explain retrograde motion (as see in Figure 4.1). Retrograde motion is the term used to describe the apparent, temporarily reversed motion of a planet as viewed from Earth when the Earth is overtaking the other planet in orbit around the sun. This was accurate enough to match the obersvational data that described the position of the planets in the sky.\n\n\n\n\n\n\nFigure 4.1: In the Ptolemic solar model, the retrograde motion of the planets was explained by adding an epicycle to the circular orbit around the earth.\n\n\n\nFamously, Copernicus came along and said that the sun, not the Earth, should be at the center (a heliocentric model). Earth revolves around the sun! Today, we know this to be a much better description of reality than one in which the Earth arrogantly sits at the center of the universe. However the model was actually slightly less accurate in predicting the apparent position of the planets (to the limits of observational precision at the time)! Why would this be?\nFirst, the Copernican proposal still used perfectly circular orbits with an epicycle adjustment, which we know today to be inaccurate (in favor of an elliptical orbit consistent with the theory of gravity). Despite being more scientifically correct, it was still not the complete picture.\nSecond, the geocentric model was already very accurate because it was essentially a Taylor-series approximation which described to sufficient observational accuracy the apparent position of the planet relative to the Earth. The heliocentric model was effectively a re-parameterization of the orbital approximation.\nThird, we have considered a limited criteria for which we are evaluating the model for accuracy, namely apparent position of the planets. It’s not until we contemplate other observational data that the Copernican model would demonstrate greater modeling accuracy: apparent brightness of the planets as they undergo retrograde motion and angular relationship of the planets to the sun.\nFor modelers today, this demonstrates a few things to keep in mind:\n\nPredictive models need not have a scientific, causal structure to make accurate predictions.\nIt is difficult to capture the complete scientific inter-relationships of a system and much care and thought needs to be given in what aspects are included in our model.\nWe should look at, or seek out, additional data that is related to our model because we may accurately fit (or overfit) to one outcome while achieving an increasingly poor fit to other related variables.\n\nStriving to better understand the world is a good thing to do but trying to include more components into the model is not always going to help achieve our goals.\n\n\n4.5.2 Examples in the Financial Context\n\n4.5.2.1 Home Prices\nAmerican home prices which have a strong degree of seasonality and have the strongest prices around April of each year. We may find that including a simple oscillating term in our model captures the variability in prices better than if we tried to imperfectly capture the true market dynamics of home sales: supply and demand curves varying by personal (job bonus payment timing, school calendars), local (new homes built, company relocation), and national (monetary policy, tax incentives for home-ownership). In other words, one could likely predict a stable pattern like this with a model that contains a simple sinusoidal periodic component. One could likely spend months trying to build a more scientific model and not achieve as good of fit, even though the latter tries to be more conceptually accurate.\n:::  :::\n\n\n4.5.2.2 Replicating Portfolio\nAnother example in the financial modeling realm: in attempting to value a portfolio of insurance contracts a replicating portfolio of hypothetical assets will sometimes be constructed2. The point of this is to create a basket of assets that can be more quickly (minutes to hours) valued in response to changing market conditions than it would take to run the actuarial model (hours to days). This is an example where the basket of assets has no ability to explain why the projected cashflows are what they are - but retains strong predictive accuracy.",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Elements of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "elements-of-financial-modeling.html#what-makes-a-good-model",
    "href": "elements-of-financial-modeling.html#what-makes-a-good-model",
    "title": "4  Elements of Financial Modeling",
    "section": "4.6 What makes a good model?",
    "text": "4.6 What makes a good model?\nThe answer is: it depends.\n\n4.6.1 Achieving original purpose\nA model is built for a specific set of reasons and therefore we must evaluate a model in terms of achieving that goal. We should not critique a model if we want to use it outside of what it was inteded to do. This includes: contents of output and required level of accuracy.\nA model may have been created to for scenario analysis to value all assets in a portfolio to within half a percent of a more accurate, but much more computationally expensive model. If we try to add a never-before-seen asset class or use the model to order trades we may be extending the design scope of the original model.\n\n\n4.6.2 Usability\nHow easy is it for someone to use? Does it require pages and pages of documentation, weeks of specialized training and an on-call help desk? All else equal, it is an indicator of how usable the model is by the amount of support and training. However, one may sometimes wish to create a highly capable, complex model which is known to require a high amount of experience and expertise. An analogy here might be the cockpit of a small Cessna aircraft versus a fighter jet: the former is a lot simpler and takes less training to master but is also more limited.\nFigure 4.2 illustrates this concept and shows that if your goal is very high capability that you may need to expect to develop training materials and support the more complex model. On this view, a better model is one that is able to have a shorter amount of time and experience to acheive the same level of capability.\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/iRM0c/src/scenes.jl:220\n\n\n\n\n\n\n\n\nFigure 4.2: Tradeoff between complexity and capability\n\n\n\n\n\n\n\n4.6.3 Performance\nFinancial models are generally not used for their awe-inspiring beauty - users are results oriented and the faster a model returns the requested results, the better. Aside from direct computational costs such as server runtime, a shorter model runtime means that one can iterate faster, test new ideas on the fly, and stay focused on the problem at hand.\nMany readers may be familiar with the cadence of (1) try running model overnight, (2) see results failed in the morning, (3) spend day developing, (4) repeat step 1. It is preferred if this cycle can be measured in minutes instead of hours or days.\nOf course, requirements must be considered here too: needs for high frequency trading, daily portfolio rebalancing, and quarterly valuations are different when it comes to performance.\n\n\n4.6.4 Separation of Model Logic and Data\nWhen business logic is embedded within data, or data inputs are spread out across multiple locations it’s tough to keep track of things. Using a spreadsheet as an example, often times it’s incredibly difficult to ascertain a model’s operation if inputs are spread out across locations on many tabs. Or if related calculations are performed in multiple locations, or if it’s not clear where the line is drawn between calculations performed in the worksheets or in macros.\n\n\n4.6.5 Abstraction of Modeled Systems\nAt different times we are interested in different ladder of abstraction: sometimes we are interested in the small details, but other times we are interested in understanding the behavior of systems at a higher level.\nSay we are an insurance company with a portfolio of fixed income assets supporting long term insurance liabilities. We might delineate different levels of abstraction like so:\n\n\n\n\n\nThink about moving up and down a ladder of abstraction when analyzing a problem.\n\n\n\n\n\nTable 4.2: An example of the different levels of abstraction when thinking about modeling an insurance company’s assets and liabilites.\n\n\n\n\n\n\nItem\n\n\n\n\nMore Abstract\nSensitivity of an entire company’s solvency position\n\n\n\nSensitivity of a portfolio of assets\n\n\n\nBehavior over time of an individual contract\n\n\nMore granular\nMechanics of an individual bond or insurance policy\n\n\n\n\n\n\nAt different times, we are often interested in different aspects of a problem. In general, you start to be able to obtain more insights and a greater understanding of the system when you move up the ladder of abstraction.\nIn fact, a lot of designing a model is essentially trying to figure out where to put the right abstractions. What is the right level of detail to model this in and what is the right level of detail to expose to other systems?\nLet us also distinguish between vertical abstraction, as described above, and horizontal abstraction which will refer to encapsulating different properties, or mechanics of components of model that effectively exist on the same level of vertical abstraction. For example, both asset and liability mechanics sit at the most granular level in Table 4.2, But it may make sense in our model to separate the data and behavior from each other. If we were to do that, that would be an example of creating horizontal abstraction in service of our overall modeling goals.",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Elements of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "elements-of-financial-modeling.html#what-makes-a-good-modeler",
    "href": "elements-of-financial-modeling.html#what-makes-a-good-modeler",
    "title": "4  Elements of Financial Modeling",
    "section": "4.7 What makes a good modeler?",
    "text": "4.7 What makes a good modeler?\nA model is nothing without it’s operator, and a skilled practitioner is worth their weight in gold. What elements separate a good modeler from a mediocre modeler?\n\n4.7.1 Domain Expertise\nAn expert who knows enough about all of the domains that are applicable is crucial. Imagine if someone said let’s emulate an architect by having a construction worker and an artist work together. It’s all too common for business to attempt to pair a business expert with an information technologist in the same way.\nUnfortunately, this means that there’s generally no easy way out of learning enough about finance, actuarial science, computers, and/or programming in order to be an effective modeler.\nAlso, a word of warning for the financial analysts out there: the computer scientists may find it easier to learn applied financial modeling than the other way around since the tools, techniques, and language of problem solving is already more a more general and flexible skill-set. There’s more technologists starting banks than there are financiers starting technology companies.\n\n\n4.7.2 Model Theory\nIf it is granted that financial modeling must involve, as the essential part, a building up of modeler’s knowledge, the next issue is to characterize that knowledge more explicitly. The modeler’s knowledge should be regarded as a theory, in the sense of Ryle’s3 “Concept of the Mind.” Very briefly: a person who has or possesses a theory in this sense knows how to do certain things and in addition can support the actual doing with explanations, justifications, and answers to queries, about the model and it’s results4.\nA financial model is rarely left in a final state. Regulatory changes, additional mechanics, sensitivity testing, market dynamics, new products, and new systems to interact with force a model to undergo change and development through its entire life. And like a living thing, it must have nurturing caregivers. This metaphor sounds extended, but Naur’s point is that unless the model also lives in the heads of it’s developers then it cannot successfully be maintained through time:\n\nThe conclusion seems inescapable that at least with certain kinds of large programs, the continued adaption, modification, and correction of errors in them, is essentially dependent on a certain kind of knowledge possessed by a group of programmers who are closely and continuously connected with them.\n\nAssume that we need to adapt the model to fit a new product. One possessing a high degree of model theory includes:\n\nthe ability to describe the trade-offs between alternate approaches that would accomplish the desired change\nrelate the proposed change to the design of the current system and any challenges that will arise as a result of prior design decisions\nprovide a quantitative estimation for the impact the change will have: runtime, risk metrics, valuation changes, etc.\nAnalogize how the system works to themselves or to others\nDescribe key limitations that the model has and where it is most divorced from the reality it seeks to represent.\n\nAbstractions and analogies of the system are a critical aspect of model theory, as the human mind cannot retain perfectly precise detail about how the system works in each sub-component. The ability to, at some times, collapse and compartmentalize parts of the model to limit the mental overload while at others recall important implementation details requires training - and is enhanced by learning concepts like those which will be covered in this book.\nAn example of how the right abstractions (and language describing those abstractions) can be helpful in simplifying the mental load:\nInstead of:\nThe valuation process starts by reading an extract into three tabs of the spreadsheet. A macro loops through the list of policies on the first tab and in column C it gives the name of the applicable statutory valuation ruleset. The ruleset is defined as the combination of (1) the logic in the macro in the “Valuation” VBA module with, (2) the underlying rate tables from the tabs named XXX to ZZZ, along with (3) the additional policy level detail on the second tab. The valuation projection is then run with the current policy values taken from the third tab of the spreadsheet and the resulting reserve (equal to the actuarial present value of claims) is saved and recorded in column J of the first tab. Finally, a pivot table is used to sum up the reserves by different groups.\nWe could instead design the process so that the following could be said instead:\nPolicy extracts are parsed into a Policy datatype which contains a subtype ValuationKind indicating the applicable statutory ruleset to apply. From there, we map the valuation function over the set of Policys and perform an additive reduce to determine the total reserve.\nThere are terminologies and concepts in the second example which we will develop over the course of this section of the book - we don’t want to dwell on the details bright now. However, we do want to emphasize that the process itself being able to condensed down to descriptions that are much more meaningful to the understanding of the model is a key differentiator for a code-based model instead of spreadsheets. It is not exaggerating that we could develop a handful of compartmentalized logics such that our primary valuation process described above could look like this in real code:\npolicies = parse(Policy,CSV.File(\"extract.csv\")) \nreserve = mapreduce(+,value,policies)\nWe’ve abstracted the mechanistic workings of the model into concise and meaningful symbols that not only perform the desired calculations but also make it obvious to an informed but unfamiliar reader what it’s doing.\nparse , mapreduce, + , value , Policy are all imbued with meaning - the first three would be understood by any computer scientist by the nature of their training (and is training that this book covers). The last two are unique to our model and have “real world” meaning that our domain expert modeler would understand which analogizes very directly to the way we would suggest implementing the details of value or Policy. The benefit of this, again, is to provide tools and concepts which let us more easily develop model theory.\n\n\n4.7.3 Curiosity\nA model never answers all of the questions and many times find itself overdrawn: sometimes more questions arise then answers provided. It is our experience that you modeler who continues to pursue questions that arise as a result of the analysis and in particular possesses an Insatiable itch for resolving apparent contradictions in model conclusions. That is, if an incomplete understanding or an incorrect model allows one to arrive at contradictory conclusions it’s suggest that a deeper understanding or model revision is required.\n\n\n4.7.4 Rigor\nWhen developing a model it’s important to ensure that assumptions and parameters are very clear, the methodology is in line with establish theory, inappropriate thought has been given to how the model will be used. Additionally one should be mindful of standards of practice. For example, professional actuarial societies have a long list of Actuarial Standards of Practice (“ASOPs”), some of which apply to modeling and the use of data that models ultimately rely on.\n\n\n4.7.5 Clarity\nA rigorous understanding of the fundamentals is important as it is all too easy to let imprecise communication and terminology interfere with the task at hand. Many terms in finance are overloaded with multiple meanings depending on the context such as the speakers background or company norms. When there is a term that is prone to misunderstanding because of its multiple overloaded meanings, a practitioner should take care to use that term And convey which definition is intended either explicitly or through the appropriate context clues.\n\n\n4.7.6 Humble\nIrreducible & epistemic/reducible uncertainty…\n\n\n4.7.7 Architecture\nAny sufficiently complex project benefits from architectural thinking\n\n\n4.7.8 Planning\nWhen tackling a large problem, it helps\n\n\n4.7.9 Toolset\nAn experience professional is aware of a number of approaches that can be used in solving a problem. From heuristics that are able to be calculated on a napkin to complex economic models, the ability to draw on a wide tool set allows a practitioner to find the right solution for a given problem. Further, it is the intention of this book to enumerate a number of additional approaches that may prove useful in practice.",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Elements of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "elements-of-financial-modeling.html#footnotes",
    "href": "elements-of-financial-modeling.html#footnotes",
    "title": "4  Elements of Financial Modeling",
    "section": "",
    "text": "Prof. Richard Fitzpatrick has excellent coverage of the associated mathematics and implications in “A Modern Almagest”: https://farside.ph.utexas.edu/books/Syntaxis/Almagest/Almagest.html↩︎\nSee, e.g., SOA Investment Symposium March 2010. Replicating Portfolios in the Insurance Industry (Curt Burmeister Mike Dorsel Patricia Matson)↩︎\nRyle, G. The Concept of Mind. Harmondsworth, England, Penguin, 1963, first published 1949. Applying “Theory Building”↩︎\nThe idea of “model theory” is adapted from Peter Naur’s 1985 essay, “Programming as Theory Building”. Indeed, this whole paragraph is only a slightly modified version of Naur’s description of theory in the programming context.↩︎",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Elements of Financial Modeling</span>"
    ]
  },
  {
    "objectID": "foundations-of-programming.html",
    "href": "foundations-of-programming.html",
    "title": "5  Elements of Programming",
    "section": "",
    "text": "5.1 In this section\nStart building up computer science concepts by introducing tangible programming essentials. Data types, variables, control flow, functions, and scope are introduced.",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "foundations-of-programming.html#in-this-section",
    "href": "foundations-of-programming.html#in-this-section",
    "title": "5  Elements of Programming",
    "section": "",
    "text": "On Your First Readthrough\n\n\n\nThis chapter is intended to be an introductory reference for most of the basic building blocks which we will build abstractions on top of in chapters that follow. We want this chapter to essentially be an easy and mildly opinionated stepping-stone on your journey.\nAt some point, you will likely find yourself seeking more precise or thorough documentation and will begin directly searching or reading the documentation of a language or library itself. However, it may be intimidating or frustrating reading reference documentation due to the density and terminology - let this chapter (and book writ large) be a bridge for you.\nIf reading this book in a linear fashion and new to programming, we suggest skipping the following sections and returning when encountering the concept or term later in the book:\n\nSection 5.5.4 through Section 5.5.9 which covers advanced and custom data types\nAfter Section 5.6.3 which deals with advanced function usage and program organization via scope",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "foundations-of-programming.html#computer-science-programming-and-coding",
    "href": "foundations-of-programming.html#computer-science-programming-and-coding",
    "title": "5  Elements of Programming",
    "section": "5.2 Computer Science, Programming, and Coding",
    "text": "5.2 Computer Science, Programming, and Coding\nComputer Science is the study of computing and information. As a science, it is distinct from programming languages which are merely coarse implementations of specific computer science concepts1. Programming (or “coding”) is the art and science of writing code in programming languages to have the computer perform desired tasks. While this may sound mechanistic, programming truly is one of the highest forms of abstract thinking and the design space of potential solutions is so large and potentially complex that much art and experience is needed to create a well-made program.\nThe language of computer science also provides a lexicon so that financial practitioners can discuss model architecture and problem characteristics. Having the language to describe a concept will also help see aspects of the problem in new ways, opening one up to more innovative solutions.\nIn the context of this financial modeling that we do, we can consider a financial model to be a type of computer program. It takes as input abstract information (data), performs calculations (an algorithm), and returns new data as an output. In this context, we generally do not need to consider many things that a software engineer may contemplate such as a graphical user interface, networking, or access restrictions. But there are many similarities: a good financial modeler must understand data types, algorithms, and some hardware details.\nWe will build up the concepts over this and the following chapter:\n\nThis chapter will provide a survey of important concepts in computer science that will prove useful for our financial modeling. First, we will talk about data types, boolean logic, and basic expressions. We’ll build on those to discuss algorithms (functions) which perform useful work and use control flow and recursion.\nIn the following chapters about abstraction, we will step back and discuss higher level concepts: the “schools of thought” around organizing the relationship between data and functions (functional versus object-oriented programming), design patterns, computational complexity, and compilation.\n\n\n\n\n\n\n\nTip\n\n\n\nThere will be brief references to hardware considerations for completeness, but hardware knowledge is not necessary to understand most programming languages (including Julia). It’s impossible to completely avoid talking about hardware when you care about the performance of your code, so feel free to gloss over the reference to hardware details on the first read and come back later after Chapter 9.\n\n\nIt’s highly recommended that you follow along and have a Julia session open (e.g. a REPL or a notebook) when first going through this chapter. See Chapter 27 if you haven’t gotten that set up yet. Follow along with the examples as we go.\n\n\n\n\n\n\nTip\n\n\n\nYou can get some help in the REPL by typing a ? followed by the symbol you want help with, for example:\n help?&gt; sum\nsearch: sum sum! summary cumsum cumsum! ...\n\n  sum(f, itr; [init])\n\n\n  Sum the results of calling function f on each element of itr.\n\n... More text truncated...\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThis introductory chapter is intended to provide a survey of the important concepts and building blocks, not to be a complete reference. For full details on available functions, more complete definitions, and a more complete tour of all language features, see the Manual at docs.julialang.org.",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "foundations-of-programming.html#expressions-and-control-flow",
    "href": "foundations-of-programming.html#expressions-and-control-flow",
    "title": "5  Elements of Programming",
    "section": "5.3 Expressions and Control Flow",
    "text": "5.3 Expressions and Control Flow\n\n5.3.1 Assignment and Variables\nOne of the first things it will be convenient to understand is the concept of variables. In virtually every programming language, we can assign values to make our program more organized and meaningful to the human reader. In the following example, we assign values to intermediate symbols to benefit us humans as we convert (silly!) American distance units:\n\nfeet_per_yard = 3\nyards_per_mile = 1760\n\nfeet = 3000\nmiles = feet / feet_per_yard / yards_per_mile\n\n0.5681818181818182\n\n\nBeyond readability, variables are a form of abstraction which allows us to think beyond specific instances of data and numbers to a more general representation. For example, the last line in the prior code example is a very generic computation of a unit conversion relationship and feet could be any number and the expression remains a valid calculation.\nWe will dive a little bit deeper into variables and assignment in Section 5.4.3.",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "foundations-of-programming.html#expressions",
    "href": "foundations-of-programming.html#expressions",
    "title": "5  Elements of Programming",
    "section": "5.4 Expressions",
    "text": "5.4 Expressions\nHaving already seen some more illustrative examples above, we can zoom in onto smaller pieces called expressions which are effectively the basic block of code that gets evaluated. Here is an expression that adds two integers together that evaluate to a new integer (3 in this case):\n\n1 + 2\n\n3\n\n\nA bigger program is built up of many of these smaller bits of code.\n\n5.4.1 Compound Expression\nThere’s two kinds of blocks where we can ensure that sub-expressions get evaluated in order and return the last expression as the overall return value: begin and let blocks.\n\nc = begin\n    a = 3\n    b = 4\n    a + b\nend\n\na, b, c\n\n(3, 4, 7)\n\n\nThe variables inside the begin block are evaluated in the same scope as c and therefore have the assigned values when we call a and b in the last line. Contrast that with the let block below, where d and e are not available when we try to get the value of f. This is because let creates a new inner scope that’s not available in f’s scope. More on scope later in the chapter.\n\nf = let\n    d = 1\n    e = 2\n    d + e\nend\nf\n\n3\n\n\n\nd\n\nLoadError: UndefVarError: `d` not defined\nUndefVarError: `d` not defined\n\n\n\n\n5.4.2 Conditional Expressions\nConditionals are expressions that evaluate to a boolean true or false. This is the beginning of really being able to assemble complex logic to perform useful work. Here are a handful expressions that would evaluate to true:\n1 &gt; 0\n1 == 1 # check for equality\nFloat64 isa Rational\n(5 &gt; 0) & (-1 &lt; 2) # \"and\" expression\n(5 &gt; 0) | (-1 &gt; 2) # \"or\" expression\n1 != 2\n\n\n\n\n\n\nNote\n\n\n\nIn Julia, the booleans have an integer equality: true is equal to 1 (true == 1) and false is equal to 0 (false == 0). However:\n\ntrue != 5. Only 1 is equal to true (in some languages, any non-zero number is “truthy”).\ntrue is not egal to 1 (egal is defined later in this chapter).\n\n\n\nConditionals can be used to assemble different logical paths for the program to follow and the general pattern is an if block:\nif condition\n    # do one thing\nelseif condition\n    # do something else\nelse\n    # do something if none of the \n    # other conditions are met\nend\nA complete example:\n\nfunction buy_or_sell(my_value, market_price)\n    if my_value &gt; market_price\n        \"buy more\"\n    elseif my_value &lt; market_price\n        \"sell\"\n    else\n        \"hold\"\n    end\nend\n\nbuy_or_sell(10, 15), buy_or_sell(15, 10), buy_or_sell(10, 10)\n\n(\"sell\", \"buy more\", \"hold\")\n\n\n\n5.4.2.1 Equality\nThe “Ship of Theseus2” problem is an example of how equality can be philosophically complex concept. In computer science we have the advantage that while we may not be able to resolve what’s the “right” type of equality, we can be more precise about it.\nHere is an example for which we can see the difference between two types of equality:\n\nEgal equality is when a program could not distinguish between two objects at all\nEqual equality is when the values of two objects are the same\n\nIf two things are egal, then they are also equal.\nIn the following example, s and t are equal but not egal:\n\ns = [1, 2, 3]\nt = [1, 2, 3]\ns == t, s === t\n\n(true, false)\n\n\nOne way to think about this is that while the values are equal, there is a way that one of the arrays could be made not equal to the other:\n\nt[2] = 5\nt\n\n3-element Vector{Int64}:\n 1\n 5\n 3\n\n\nNow t is no longer equal to s:\n\ns == t\n\nfalse\n\n\nThe reason this happens is that arrays are containers that can have their contents modified. Even though they originally had the same values, s and t are different containers, and it just so happened that the values they contained started out the same.\nSome data can’t be modified, including some kinds of collections. Immutable types like the following tuple, with the same stored values, are egal because there is no way for us to make them different:\n\n(2, 4) === (2, 4)\n\ntrue\n\n\nUsing this terminology, we could now interpret the “Ship of Theseus” as that his ship is “equal” but not “egal”.\n\n\n\n5.4.3 Assignment and Variables\nWhen we say x = 2 we are assigning the integer value of 2 to the variable x. This is an expression that lets us bind a something to a variable so that it can be referenced more concisely or in different parts of our code. When we re-assign the variable we are not mutating the value: x = 3 does not change the 2.\nWhen we have a mutable object (e.g. an Array or a mutable struct), we can mutate the value inside the referenced container. For example:\n\n1x = [1, 2, 3]\n2x[1] = 5\nx\n\n\n1\n\nx refers to the array which currently contains the elements 1, 2, and 3.\n\n2\n\nWe re-assign the first element of the array to be the value 5 instead of 1\n\n\n\n\n3-element Vector{Int64}:\n 5\n 2\n 3\n\n\nIn the above example, x has not been reassigned. It is possible for two variables to refer to the same object:\n\nx = [1, 2, 3]\n1y = x\nx[1] = 6\ny\n\n\n1\n\ny refers to the same underlying array as x\n\n\n\n\n3-element Vector{Int64}:\n 6\n 2\n 3\n\n\nNote that variables can be re-assigned unless they are marked as const:\nconst PHI =  π * 2\n\n1\n\nCapitalizing constant variables is a convention in Julia.\n\n\nIf we tried to re-assign PHI, we would get an error.\n\n\n\n\n\n\nWarning\n\n\n\nNote that if we declare a const variable that refers to a mutable container like an array, the container can still be mutated. It’s the reference to the container that remains constant, not necessarily the elements within the container.\n\n\n\n\n5.4.4 Loops\nLoops are ways for the program to move through a program and repeat expressions while we want it to. There are two primary loops: for and while.\nfor loops are loops that iterate over a defined range or set of values. Let’s assume that we have the array v = [6,7,8]. Here are multiple examples of using a for loop in order to print each value to output (println):\n# use fixed indices\nfor i in 1:3\n    println(v[i])\nend\n# use indices the of the array\nfor i in eachindex(v)\n    println(v[i])\nend\n# use the elements of the array\nfor x in v\n    println(x)\nend\n# use the elements of the array\nfor x ∈ v          # ∈ is typed \\in&lt;tab&gt;\n    println(x)\nend\nwhile loops will run repeatedly until an expression is false. Here’s some examples of printing each value of v again:\n# index the array\ni = 1\nwhile i &lt;= length(v) \n    println(v[i])\n1    global i += 1\nend\n\n1\n\nglobal is used to increment i by 1. i is defined outside the scope of the while loop (see Section 5.7).\n\n\n# index the array\ni = 1\nwhile true\n    println(v[i])\n    if i &gt;= length(v)\n1        break\n    end\n    global i += 1 \nend\n\n1\n\nbreak is used to terminate the loop manually, since the condition that follows the while will never be false.\n\n\n\n\n5.4.5 Performance of loops\nLoops are highly performant in Julia and often the fastest way to accomplish things. Those coming from Python or R may have developed a habit to avoid writing loops. Fear the for loop not!",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "foundations-of-programming.html#data-types",
    "href": "foundations-of-programming.html#data-types",
    "title": "5  Elements of Programming",
    "section": "5.5 Data Types",
    "text": "5.5 Data Types\nData types are a way of categorizing information by intrinsic characteristics. We instinctively know that 13.24 is different than \"this set of words\" and types are how we will formalize this distinction. This is a key conceptual point, and mathematically it’s like we have different sets of objects to perform specialized operations on. Beyond this set-like abstraction is implementation details related to computer hardware. You probably know that computers only natively “speak” in binary zeros and ones. Data types are a primary way that a computer can understand if it should interpret 01000010 as B or as 663.\nEach 0 or 1 within a computer is called a bit and eight bits in a row form a byte (such as 01000010). This is where we get terms like “gigabytes” or “kilobits per second” as a measure of the quantity or rate of bits something can handle4.\n\n5.5.1 Numbers\nNumbers are usually grouped into two categories: integers and floating-point5 numbers. Integers are like the mathematical set of integers while floating-point is a way of representing decimal numbers. Both have some limitations since computers can only natively represent a finite set of numbers due to the hardware (more on this in Chapter 9). Here are three integers that are input into the REPL (Read-Eval-Print-Loop)6 and the result is printed below the input:\n\n2\n\n2\n\n\n\n423\n\n423\n\n\n\n1929234\n\n1929234\n\n\nAnd three floating-point numbers:\n\n0.2\n\n0.2\n\n\n\n-23.3421\n\n-23.3421\n\n\n\n14e3      # the same as 14,000.0\n\n14000.0\n\n\nOn most systems, 0.2 will be interpreted as a 64-bit floating point type called Float64 in Julia since most architectures these days are 64-bit7, while on a 32-bit system 0.2 would be interpreted as a Float32. Given that there are a finite amount of bits attempting to represent a continuous, infinite set of numbers means that some numbers are not able to be represented with perfect precision. For example, if we ask for 0.2, the closest representations in 64 and 32 bit are:\n\n0.20000000298023223876953125 in 32-bit\n0.200000000000000011102230246251565404236316680908203125 in 64-bit\n\nThis leads to special considerations that computers take when performing calculations on floating point maths, some of which will be covered in more detail in Chapter 9. For now, just note that floating point numbers have limited precision and even if we input 0.2, your computations will use the above decimal representations even if it will print out a number with fewer digits shown:\n\n1x = 0.2\n\n2big(x)\n\n\n1\n\nHere, we assign the value 0.2 to a variable x. More on variables/assignments in Section 5.4.3.\n\n2\n\nbig(x) is a arbitrary precision floating point number and by default prints the full precision that was embedded in our variable x, which was originally Float64.\n\n\n\n\n0.200000000000000011102230246251565404236316680908203125\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote the difference in what printed between the last example and when we input 0.2 earlier in the chapter. The former had the same (not-exactly equal to \\(0.2\\)) value, but it printed an abbreviated set of digits as a nicety for the user, who usally doesn’t want to look at floating point numbers with their full machine precision. The system has the full precision (0.20...3125) but is truncating the ouput.\nIn the last example, we’ve converted the normal Float64 to a BigFloat which will not truncate the output when printing.\n\n\nIntegers are similarly represented as 32 or 64 bits (with Int32 and Int64) and are limited to exact precision:\n\n-32,767 to 32,767 for Int32\n-2,147,483,647 to 2,147,483,647 for Int64\n\nAdditional range in the positive direction if one chooses to use “unsigned”, non-negative numbers (UInt32 and UInt64). Unlike floating point numbers, the integers have a type Int which will use the system bit architecture by default (that is, Int(30) will create a 64 bit integer on 64-bit systems and 32-bit on 32-bit systems).\n\n\n\n\n\n\nFinancial Modeling Pro-tip\n\n\n\nExcel’s numeric storage and routine is complex and not quite the same as most programming languages, which follow the Institute of Electrical and Electronics Engineer’s standards (such as the IEEE 754 standard for double precision floating point numbers). Excel uses IEEE for the computations but results (and therefore the cells that comprise many calculations interim values) are stored with 15 significant digits of information. In some ways this is the worst of both worlds: having the sometimes unusual (but well-defined) behavior of floating point arithmetic and having additional modifications to various steps of a calculation. In general, you can assume that the programming language result (following the IEEE 754 standard) is a better result because there are aspects to the IEEE 754 defines techniques to minimize issues that arise in floating point math. Some of the issues (round-off or truncation) can be amplified instead of minimized with Excel.\nIn practice, this means that it can be difficult to exactly replicate a calculation in Excel in a progamming language and vice-versa. It’s best to try to validate a programming model versus Excel model using very small unit calculations (e.g. a single step or iteration of a routine) instead of an all-in result. You may need to define some tolerance threshold for comparison of a value that is the result of a long chain of calculation.\n\n\n\n\n5.5.2 Type Hierarchy\nWe can describe a hierarchy of types. Both Float64 and Int64 are examples of Real numbers (here, Real is an abstract Julia type which corresponds to the mathematical set of real numbers commonly denoted with \\(\\mathbb{R}\\) ). Both Float64 and Int32 are Real numbers, so why not just define all numbers as a Real type? Because for performant calculations, the computer must know in advance how many bits each number is represented with.\nFigure 5.1 shows the type hierarchy for most built-in Julia number types.\n\n\n\n\n\n\ngraph TD\n    Number --&gt; Real\n    Number --&gt; Complex\n\n    Real --&gt; Integer\n    Real --&gt; AbstractFloat\n    Real --&gt; Rational\n    Real --&gt; Irrational\n\n    Integer --&gt; Signed\n    Integer --&gt; Unsigned\n\n    Signed --&gt; Int8\n    Signed --&gt; Int16\n    Signed --&gt; Int32\n    Signed --&gt; Int64\n    Signed --&gt; Int128\n    Signed --&gt; BigInt\n\n    Unsigned --&gt; UInt8\n    Unsigned --&gt; UInt16\n    Unsigned --&gt; UInt32\n    Unsigned --&gt; UInt64\n    Unsigned --&gt; UInt128\n\n    AbstractFloat --&gt; Float16\n    AbstractFloat --&gt; Float32\n    AbstractFloat --&gt; Float64\n    AbstractFloat --&gt; BigFloat\n\n\n\n\nFigure 5.1: Numeric Type Hierarchy in Julia. Leafs of the tree are concrete types.\n\n\n\n\n\nThe integer and floating point types described in the prior section are known as concrete types because there are no possible sub types (child types). Further, a concrete type can be a bit type if the data type will always have the same number of bits in memory: a Float32 will always be 32 bits in memory, for example. Contrast this with strings (described below) which can contain an arbitrary number of characters.\n\n\n5.5.3 Collections\nCollections are types that are really useful for storing data which contains many elements. This section describes some of the most common and useful types of containers.\n\n5.5.3.1 Arrays\nArrays are the most common way to represent a collection of similar data. For example, we can represent a set of integers as follows:\n\n[1, 10, 300]\n\n3-element Vector{Int64}:\n   1\n  10\n 300\n\n\nAnd a floating point array:\n\n[0.2, 1.3, 300.0]\n\n3-element Vector{Float64}:\n   0.2\n   1.3\n 300.0\n\n\nNote the above two arrays are different types of arrays. The first is Vector{Int64} and the second is Vector{Float64}. These are arrays of concrete types and so Julia will know that each element of an array is the same amount of bits which will enable more efficient computations. With the following set of mixed numbers, Julia will promote the integers to floating point since the integers can be accurately represented8 in floating point.\n\n[1, 1.3, 300.0, 21]\n\n4-element Vector{Float64}:\n   1.0\n   1.3\n 300.0\n  21.0\n\n\nHowever, if we explicitly ask Julia to use a Real-typed array, the type is now Vector{Real}. Recall that Real is an abstract type. Having heterogeneous types within the array is conceptually fine, but in practice limits performance. Again, this will be covered in more detail in Chapter 9.\nIn Julia, arrays can be multi-dimensional. Here are are two three-dimensional arrays with length three in each dimension:\n\nrand(3, 3, 3)\n\n3×3×3 Array{Float64, 3}:\n[:, :, 1] =\n 0.173719  0.977853  0.827777\n 0.611725  0.517142  0.706056\n 0.449393  0.225043  0.00283585\n\n[:, :, 2] =\n 0.207057  0.136713  0.929456\n 0.427211  0.438434  0.87896\n 0.318335  0.21055   0.965546\n\n[:, :, 3] =\n 0.965014  0.492456  0.411422\n 0.845258  0.363532  0.469196\n 0.585662  0.104668  0.0098619\n\n\n\n[x + y + z for x in 1:3, y in 11:13, z in 21:23]\n\n3×3×3 Array{Int64, 3}:\n[:, :, 1] =\n 33  34  35\n 34  35  36\n 35  36  37\n\n[:, :, 2] =\n 34  35  36\n 35  36  37\n 36  37  38\n\n[:, :, 3] =\n 35  36  37\n 36  37  38\n 37  38  39\n\n\nThe above example demonstrates array comprehension syntax which is a convenient way to create arrays in Julia.\nA two-dimensional array has the rows by semi-colons (;):\n\nx = [1 2 3; 4 5 6]\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn Julia, a Vector{Float64} is simply a one-dimensional array of floating points and a Matrix{Float64} is a two-dimensional array. More precisely, they are type aliases of the more generic Array{Float64,1} and Array{Float64,2} names. Arrays with three or more dimensions don’t have a type alias pre-defined.\n\n\n\n\n5.5.3.2 Array indexing\nArray elements are accessed with the integer position, starting at 1 for the first element9 10:\n\nv = [10, 20, 30, 40, 50]\nv[2]\n\n20\n\n\nWe can also access a subset of the vector’s contents by passing a range:\n\nv[2:4]\n\n3-element Vector{Int64}:\n 20\n 30\n 40\n\n\nAnd we can generically reference the array’s contents, such as:\n\nv[begin+1:end-1]\n\n3-element Vector{Int64}:\n 20\n 30\n 40\n\n\nWe can assign values into the array as well, as well as combine arrays and push new elements to the end:\n\nv[2] = -1\npush!(v, 5)\nvcat(v, [1, 2, 3])\n\n9-element Vector{Int64}:\n 10\n -1\n 30\n 40\n 50\n  5\n  1\n  2\n  3\n\n\n\n\n5.5.3.3 Array Alignment\nWhen you have an MxN matrix (M rows, N columns), a choice must be made as to which elements are next to each other in memory. Typical math convention and fundamental computer linear algebra libraries (dating back decades!) are column major and Julia follows that legacy. Column major means that elements going down the rows of a column are stored next to each other in memory. This is important to know so that (1) you remember that vectors are treated like a column vector when working with arrays (that is: a N element 1D vector is like a Nx1 matrix), and (2) when iterating through an array, it will be faster for the computer to access elements next to each other column-wise. A 10x10 matrix is actually stored in memory as 100 elements coming in order, one after another in single file.\nThis 3x4 matrix is stored with the elements of columns next to each other, which we can see with vec:\n\nmat = [1 2 3; 4 5 6; 7 8 9]\n\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\n\n\nvec(mat)\n\n9-element Vector{Int64}:\n 1\n 4\n 7\n 2\n 5\n 8\n 3\n 6\n 9\n\n\n\n\n5.5.3.4 Ranges\nA range is a representation of a range of numbers. We actually used them above to index into arrays. They are expressed as start:stop\nWe don’t have to actually store all of these numbers on the computer somewhere as in an Array. Instead, this is an object that represents the ordered set of numbers. So for example, we can sum up 1 through the number of atoms on the earth instantaneously:\n\nsum(1:big(100_000_000_000_000_000_000_000_000_000_000_000_000_000_000_000_000))\n\n5000000000000000000000000000000000000000000000000050000000000000000000000000000000000000000000000000\n\n\nThis is possible due to two things:\n\nnot needing to actually store that many numbers in memory, and\nJulia being smart enough to apply the triangular number formula11 when sum is given a consecutive range.\n\nThere are more general ways to construct ranges:\nStep by another number instead of the default 1:\n\n1:2:7\n\n1:2:7\n\n\nSpecify the number of values within the range, inclusive of the first number12:\n\nrange(0, 10, 21)\n\n0.0:0.5:10.0\n\n\n\n\n5.5.3.5 Characters, Strings, and Symbols\nCharacters are represented in most programming languages as letters within quotation marks. In Julia, individual characters are represented using single quotes:\n\n'a'\n\n'a': ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\n\n\nLetters and other characters present more difficulties than numbers to represent within a computer (think of how many languages and alphabets exist!), and it essentially only works because the world at large has agreed to a given representation. Originally ASCII (American Standard Code for Information Interchange) was used to represent just 95 of the most common English characters (“a” through “z”, zero through nine, etc.). Now, UTF (Unicode Transformation Format) can encode more than a million characters and symbols from many human languages.\nStrings are a collection13 of characters, and can be created in Julia with double quotes:\n\n\"hello world\"\n\n\"hello world\"\n\n\nIt’s easy to ascertain how ‘normal’ characters can be inserted into a string, but what about things like new lines or tabs? They are represented by their own characters but are normally not printed in computer output. However, those otherwise invisible characters do exist. For example, here we will use a string literal (indicated by the \"\"\" ) to tell Julia to interpret the string as given, including the invisible new line created by hitting return on the keyboard between the two words:\n\n\"\"\"\nhello\nworld\n\"\"\"\n\n\"hello\\nworld\\n\"\n\n\nThe output above shows the \\n character contained within the string.\nSymbols are a way of representing an identifier which cannot be seen as a collection of individual characters. :helloworld is distinct from \"helloworld\" - you can kind of think of the former as an un-executed bit of code - if we were to execute it (with eval(:helloworld)), we would get an error UndefVarError: `a` not defined . Symbols can look like strings but do not behave like them. For now, it is best to not worry about symbols but it is an important aspect of Julia which allows the language to represent aspects of itself as data. This allows for powerful self-reference and self-modification of code but this is a more advanced topic generally out of scope of this book.\n\n\n5.5.3.6 Tuples\nTuples are a set of values that belong together and are denoted by a values inside parenthesis and separated by a comma. An example might be x-y coordinates in 2 dimensional space:\n\nx = 3\ny = 4\np1 = (x, y)\n\n(3, 4)\n\n\nTuple’s values can be accessed like arrays:\n\np1[1]\n\n3\n\n\nTuples fill a middle ground between scalar types and arrays in more ways that one:\n\nTuples have no problem having heterogeneous types in the different slots.\nTuples are immutable, meaning that you cannot overwrite the value in memory (an error will be thrown if we try to do p[1] = 5).\nIt’s generally expected that within an array, you would be able to apply the same operation to all the elements (e.g. square each element) or do something like sum all of the elements together which isn’t generally case for a tuple.\nTuples are generally stack allocated instead of being heap allocated like arrays14, meaning that a lot of times they can be faster than arrays.\n\n\n5.5.3.6.1 Named Tuples\nNamed tuples provide a way to give each field within the tuple a specific name. For example, our x-y coordinate example above could become:\n\np2 = (x=3, y=4)\n\n(x = 3, y = 4)\n\n\nThe benefit is that we can give more meaning to each field and access the values in a nicer way. Previously, we used location[1] to access the x-value, but with the new definition we can access it by name:\n\np2.x\n\n3\n\n\n\n\n\n5.5.3.7 Dictionaries\nDictionaries are a container which relates a key to an associated value. Kind of like how arrays relate an index to a value, but the difference is that a dictionary is (1) un-ordered and (2) the key doesn’t have to be an integer.\nHere’s an example which relates a name to an age:\n\nd = Dict(\n    \"Joelle\" =&gt; 10,\n    \"Monica\" =&gt; 84,\n    \"Zaylee\" =&gt; 39,\n)\n\nDict{String, Int64} with 3 entries:\n  \"Monica\" =&gt; 84\n  \"Zaylee\" =&gt; 39\n  \"Joelle\" =&gt; 10\n\n\nThen we can look up an age given a name:\n\nd[\"Zaylee\"]\n\n39\n\n\nDictionaries are super flexible data strucures and can be used in many situations.\n\n\n\n5.5.4 Parametric Types\nWe just saw how tuples can contain heterogeneous types of data inside a common container. Let’s look at this a little bit closer by looking at the full type:\n\ntypeof(p1)\n\nTuple{Int64, Int64}\n\n\nlocation is a Tuple{Int64,Int64} type, which means that its first and second elements are both Int64. Contrast this with:\n\ntypeof((\"hello\", 1.0))\n\nTuple{String, Float64}\n\n\nThese tuples are both of the form Tuple{T,U} where T and U are both types. Why does this matter? We and the compiler can distinguish between a Tuple{Int64,Int64} and a Tuple{String,Float64} which allows us to reason about things (“I can add the first element of tuple together only if both are numbers”) and the compiler to optimize (sometimes it can know exactly how many bits in memory a tuple of a certain kind will need and be more efficient about memory use). Further, we will see how this can become a powerful force in writing appropriately abstracted code and more logically organize our entire program when we encounter “multiple dispatch” later on.\n\n\n5.5.5 Types for things not there\nnothing represents that there’s nothing to be returned - for example if there’s no solution to an optimization problem or if a function just doesn’t have any value to return (such as in the case with input/output like println).\nmissing is to represent something should be there but it’s not, as is all too common in real-world data. Julia natively supports missing and three-value logic, which an an extension of the two-value boolean (true/false) logic, to handle missing logical values:\n\n\n\nTable 5.1: Three value logic with true, missing, and false.\n\n\n\n\n\n\n\n(a) Not logic\n\n\n\n\n\nNOT (!)\nValue\n\n\n\n\ntrue\nfalse\n\n\nmissing\nmissing\n\n\nfalse\ntrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) And logic\n\n\n\n\n\nAND (&)\ntrue\nmissing\nfalse\n\n\n\n\ntrue\ntrue\nmissing\nfalse\n\n\nmissing\nmissing\nmissing\nfalse\n\n\nfalse\nfalse\nfalse\nfalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Or Logic\n\n\n\n\n\nOR (|)\ntrue\nmissing\nfalse\n\n\n\n\ntrue\ntrue\ntrue\ntrue\n\n\nmissing\ntrue\nmissing\nmissing\n\n\nfalse\ntrue\nmissing\nfalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nMissing and Nothing are the types while missing and nothing are the values here15. This is analogous to Float64 being a type and 2.0 being a value.\n\n\n\n\n5.5.6 Union Types\nWhen two types may arise in a context, union types are a way to represent that. For example, if we have a data feed and we know that it will produce either a Float64 or a Missing type then we can say that the value for this is Union{Float64,Missing}. This is much better for the compiler (and our performance!) than saying that the type of this is Any.\n\n\n5.5.7 Creating User Defined Types\nWe’ve talked about some built-in types but so much additional capabilities come from being able to define our own types. For example, taking the x-y-coordinate example from above, we could do the following instead of defining a tuple:\n\nstruct BasicPoint\n    x::Int64\n    y::Int64\nend\n\np3 = BasicPoint(3, 4)\n\nBasicPoint(3, 4)\n\n\nBasicPoint is a composite type because it is composed of elements of other types. Fields are accessed the same way as named tuples:\n\n1p3.x, p3.y\n\n\n1\n\nNote that here, Julia will return a tuple instead of a single value due to the comma separated expressions.\n\n\n\n\n(3, 4)\n\n\nstructs in Julia are immutable like tuples above.\nBut wait, didn’t tuples let us mix types too via parametric types? Yes, and we can do the same with our type!\n\nstruct Point{T}\n    x::T\n    y::T\nend\n\nLine 1 The {T} after the type’s name allows for different Points to be created depending on what the type of the underlying x and y is.\nHere’s two new points which now have different types:\n\np4 = Point(1, 4)\np5 = Point(2.0, 3.0)\n\np4, p5\n\n(Point{Int64}(1, 4), Point{Float64}(2.0, 3.0))\n\n\nNote that the types are not equal because they have different type parameters!\n\ntypeof(p4), typeof(p5), typeof(p4) == typeof(p5)\n\n(Point{Int64}, Point{Float64}, false)\n\n\nBut both are now subtypes of PPoint2D. The expression X isa Y is true when X is a (sub)type of Y:\n\np4 isa Point, p5 isa Point\n\n(true, true)\n\n\nNote though, that the x and y are both of the same type in each PPoint2D that we created. If instead we wanted to allow the coordinates to be of different types, then we could have defined PPoint2D as follows:\nstruct Point{T,U}\n    x::T\n    y::U\nend\n\n\n\n\n\n\nNote\n\n\n\nCan we define the structs above without indicating a (parametric) type? Yes!\nstruct Point\n    x # no type here!\n    y # no type declared here either!\nend\nBut! x and y will both be allowed to be Any, which is the fallback type where Julia says that it doesn’t know any more about the type until runtime (the time at which our program encounters the data when running). This means that the compiler (and us!) can’t reason about or optimize the code as effectively as when the types are explicit or parametric. This is an example of how Julia can provide a nice learning curve - don’t worry about the types until you start to get more sophisticated about the program design or need to extract more performance from the code.\n\n\nThe above structs that we have defined are examples of concrete types types which hold data. Abstract types don’t directly hold data themselves but are used to define a hierarchy of types which we will later exploit (Chapter 8) to implement custom behavior depending on what type our data is.\nHere’s an example of (1) defining a set of related types that sits above our Point2D:\n\nabstract type Coordinate end\nabstract type CartesianCoordinate &lt;: Coordinate end\nabstract type PolarCoordinate &lt;: Coordinate end\n\nstruct Point2D{T} &lt;: CartesianCoordinate\n    x::T\n    y::T\nend\n\nstruct Point3D{T} &lt;: CartesianCoordinate\n    x::T\n    y::T\n    z::T\nend\n\nstruct Polar2D{T} &lt;: PolarCoordinate\n    r::T\n    θ::T\nend\n\n\n\n\n\n\n\nUnicode Characters\n\n\n\nJulia has wonderful Unicode support, meaning that it’s not a problem to include characters like θ. The character can be typed in Julia editors by entering \\theta and then pressing the TAB key on the keyboard.\nUnicode is helpful for following conventions that you may be used to in math. For example, the math formula \\(\\text{circumference}(r) = 2 \\times r \\times \\pi\\) can be written in Julia with circumference(r) = 2 * r * π.\nThe name for the characters follows the same for LaTeX, so you can search the internet for,e.g. “theta LaTeX” to find the appropriate name. Furhter, you can use the REPL help mode to find out how to enter a character if you can copy and paste it from somewhere:\nhelp?&gt; θ\n\"θ\" can be typed by \\theta&lt;tab&gt;\n\n\n\n\n5.5.8 Mutable structs\nIt is possible to define structs where the data can be modified - such a data field is said to be mutable because it can be changed or mutated. Here’s an example of what it would look like if we made Point2D mutable:\nmutable struct Point2D{T}\n    x::T\n    y::T\nend\nYou may find that this more naturally represents what you are trying to do. However, recall that an advantage of an immutable datatype is that costly memory doesn’t necessarily have to be allocated for it. So you may think that you’re being more efficient by re-using the same object… but it may not actually be faster. Again, more will be revealed in Chapter 9.\n\n\n\n\n\n\nFinancial Modeling Pro-tip\n\n\n\nGenerally you should default to using immutable types and consider only moving to mutable types in specific circumstances. You’ll see some examples in the applications later in the book.\n\n\n\n\n5.5.9 Constructors\nConstructors are functions that return a data type (functions will be covered more generally later in the chapter). When we declare a struct, an implicit function is defined that takes a tuple of arguments and returns the data type that was declared. In the following example, after we define MyType the struct, Julia creates a function (also called MyType) which takes two arguments and will return the datatype MyType:\n\nstruct MyDate\n    year::Int\n    month::Int\n    day::Int\nend\n\nmethods(MyDate)\n\n# 2 methods for type constructor: MyDate(year::Int64, month::Int64, day::Int64) in Main at In[54]:2  MyDate(year, month, day) in Main at In[54]:2 \n\n\nImplicit constructors are nice in that you don’t have to define a default method and the language does it for you. Sometimes there’s reasons to want to control how an object is created, either for convenience or to enforce certain restrictions.\nWe can use an inner constructor (i.e. inside the struct block) to enforce restrictions:\n\nstruct MyDate\n    year::Int\n    month::Int\n    day::Int\n\n    function MyDate(y,m,d)\n        if ~(m in 1:12)\n            error(\"month is not between 1 and 12\")\n        else if ~(d in 1:31)\n            error(\"day is not between 1 and 31\")\n        else\n            return new(y,m,d)\n        end\n\n    end\n                \nend\nAnd outer constructors are simply functions defined that have the same name as the data type , but are not defined inside the struct block. Extending the MyDate example, say we want to provide a default constructor for if no day is given such that the date returns the 1st of the month:\nfunction MyDate(y,m)\n    return MyDate(y,m,1)\nend",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "foundations-of-programming.html#functions",
    "href": "foundations-of-programming.html#functions",
    "title": "5  Elements of Programming",
    "section": "5.6 Functions",
    "text": "5.6 Functions\nFunctions are a set of expressions that take inputs and return specified outputs.\n\n5.6.1 Special Operators\nOperators are the glue of expressions which combine values. We’ve already seen quite a few, but let’s develop a little bit of terminology for these functions.\nUnary operators are operators which only take a single argument. Examples include the ! which negates a boolean value or - which negates a number:\n\n!true, -5\n\n(false, -5)\n\n\nBinary operators take two arguments and are some of the most common functions we encounter, such as + or - or &gt;:\n\n1 + 2, 1 - 2, 1 &gt; 2\n\n(3, -1, false)\n\n\nThe above unary and binary operators are special kinds of functions which don’t require the use of parenthesis. However, they can be written with parathesis for greater clarity:\n\n!(true), -(5), +(1, 2), -(1, 2)\n\n(false, -5, 3, -1)\n\n\nIn Julia, we distinguish between functions which define behavior that maps a set of inputs to outputs. But a single function can adapt its behavior to the arguments themselves. We have just seen the function - be used in two different ways: negation and subtraction depending on whether it had one or two arguments given to it. In this way there is a conceptual hierarchy of functions that complements the hierarchy we have discussed in relation to types:\n\n- is the overall function\n-(x) is a unary function which negates its values, -(x,y) subtracts y from x\nSpecific methods are then created for each combination of concrete types: -(x::Float64) is a different method than -(x::Int)\n\nMethods are specific compiled versions of the function for specific types. This is important because at a hardware level, operations for different types (e.g. integers versus floating point) differ considerably. By optimizing for the specific types Julia is able to achieve nearly ideal performance without the same sacrifices of other dynamic languages. We will develop more with respect to methods when we talk about dispatch in Chapter 8.\n\n\n5.6.2 Defining Functions\nFuncitons more generally are defined like so:\nfunction functionname(arguments)\n    # ... code that does things\nend\nHere’s an example which returns the difference between the highest and lowest values in a collection:\nfunction value_range(collection)\n\n    hi = maximum(collection)\n    lo = minimum(collection)\n1    return hi - lo\nend\n\n1\n\nreturn is optional but recommended to convey to readers of the program where you expect your function to terminate and return a value.\n\n\n\n\n5.6.3 Defining Methods on Types\nHere’s another example of a funciton which calculates the distance between a point and the origin:\n\n1function distance(point)\n2    return sqrt(point.x^2 + point.y^2)\nend\n\n\n1\n\nA function block is declared with the name distance which takes a single argument called point\n\n2\n\nWe compute the distance formula for a point with x and y coordinates. The return value make explicit what value the function will output.\n\n\n\n\ndistance (generic function with 1 method)\n\n\n\n\n\n\n\n\nNote\n\n\n\nAn alternate, simpler function syntax for distance would be:\ndistance(point) = sqrt(point.x^2 + point.y^2)\n\n\nHowever, we might at this point note a flaw in our function’s definition if we think about the various Coordinates we defined earlier: our definition would currently only work for Point2D. For example, if we try a Point3D we will get the wrong answer:\n\ndistance(Point3D(1, 1, 1,))\n\n1.4142135623730951\n\n\nThe above value should be \\(\\sqrt(3)\\), or approximately \\(1.73205\\).\nWhat we need to do is define a refined distance for each type, which we’ll call dist to distinguish from the earlier definition.\n\n\"\"\"\n    dist(point)\n\nThe euclidean distance of a point from the origin.\n\"\"\"\ndist(p::Point2D) = sqrt(p.x^2 + p.y^2)\ndist(p::Point3D) = sqrt(p.x^2 + p.y^2 + p.z^2)\ndist(p::Polar2D) = p.r\n\ndist (generic function with 3 methods)\n\n\nNow our result will be correct:\n\ndist(Point3D(1, 1, 1,))\n\n1.7320508075688772\n\n\nThis is referred to dispatching on the argument types. Julia will look up to find the most specific method of a function for the given argument types, and falling back to a generic implementation if one is defined.\nIn Chapter 8 we will see how dispatch (single and multiple) can provide very nice abstractions to simplify the design of a model.\n\n\n\n\n\n\nDocstrings (Documentation Strings)\n\n\n\nNotice the strings preceding the defintion of dist. In Julia, putting a string (\"...\") or string literal (\"\"\"...\"\"\") right above the definition will allow Julia to recognize the string as documentation and provided it to the user in help mode (Section 27.4.1) and/or have a documentation tool create a webpage or PDF documentation resource.\n\n\n\n\n\n\n\n\nDefining Methods for Parametric Types\n\n\n\nWe learned that Float64 &lt;: Real in the type hierarchy. However, note that Tuple{Float64} is not a sub-type of Tuple{Real}. This is called being invariant in type theory… but for our purposes this just practically means that when we define a method we need to specify that we want it to apply to all subtypes.\nFor example, myfunction(x::Tuple{Real}) would not be called if x was a Tuple{Float64} because it’s not a sub-type of Tuple{Real}. To act the way we want, would define the method with the signature of myfunction(Tuple{&lt;:Real}) or myfunction{Tuple{T}} where {T&lt;:Real}.\n\n\n\n\n5.6.4 Keyword Arguments\nKeyword arguments are arguments that are passed to a function but do not use position to pass data to functions but instead used named arguments. In the following example, filepath is a positional argument while the two arguments after the semicolon (;) are keyword arguments.\nfunction read_data(filepath; normalize_names, has_header_row)\n    # ... function would be defined here\nend\nThe function would need to be called and have the two keyword arguments specified:\nread_data(\"results.csv\"; normalizenames=true, hasheaderrow=false)\n\n\n5.6.5 Default Arguments\nWe are able to define default arguments for both positional and keyword arguments via an assignment expression in the function signature. For example, we can make it so that the user need not specify all the options for each call. Modifying the prior example so that typical CSVs work with less customization from the user:\nfunction read_data(filepath;\n    normalizenames = true,\n    hasheader = false\n)\nThis is a simplified example, but if you look at the documentation for most data import packages you’ll see a lot of functionality defined via keyword arguments which have sensible defaults so that most of the the time you need not worry about modifying them.\n\n\n5.6.6 Anonymous Functions\nAnonymous functions are functions that have no name and are used in contexts where the name does not matter. The syntax is x -&gt; ...expression with x.... As an example, say that we want to create a vector from another where each element is squared. map applies a function to each member of a given collection:\n\nv = [4, 1, 5]\n1map(x -&gt; x^2, v)\n\n\n1\n\nThe x -&gt; x^2 is the anonymous function in this example.\n\n\n\n\n3-element Vector{Int64}:\n 16\n  1\n 25\n\n\nThey are often used when constructing something from another value, or defining a function within optimization or solving routines.\n\n\n5.6.7 First Class Nature\nFunctions in many languages including Julia are first class which means that functions can be assigned and moved around like data variables.\nIn this example, we have a general approach to calculate the error of a modeled result compared to a known truth. In this context, there are different ways to measure error of the modeled result and we can simplify the implementation of loss by keeping the different kinds of error defined separately. Then, we can assign a function to a variable and use it as an argument to another function:\n\nfunction square_error(guess, correct)\n    (correct - guess)^2\nend\n\nfunction abs_error(guess, correct)\n    abs(correct - guess)\nend\n\n# obs meaning \"observations\"\nfunction loss(modeled_obs,\n    actual_obs,\n1    loss_function\n)\n    sum(\n        loss_function.(modeled_obs, actual_obs)\n    )\nend\n\n2let\n3    a = loss([1, 5, 11], [1, 4, 9], square_error)\n    b = loss([1, 5, 11], [1, 4, 9], abs_error)\n    a, b\nend\n\n\n1\n\nloss_function is a variable that will refer to a function instead of data.\n\n2\n\nUsing a let block here is good practice to not have temporary variables a and b scattered around our workspace.\n\n3\n\nUsing a function as an argument to another function is an example of functions being treated as “first class”.\n\n\n\n\n(5, 3)\n\n\n\n\n5.6.8 Broadcasting\nLooking at the prior definition of dist, what if we wanted to compute the squared distance from the origin for a set of points? If those points are stored in an array, we can broadcast functions to all members of a collection at the same time. This is accomplished using the dot-syntax as follows:\n\npoints = [Point2D(1, 2), Point2D(3, 4), Point2D(6, 7)]\ndist.(points) .^ 2\n\n3-element Vector{Float64}:\n  5.000000000000001\n 25.0\n 85.0\n\n\nLet’s unpack that a bit more:\n\nThe . in dist.(points) tells Julia to apply the function dist to each element in points.\nThe . in .^ tells Julia to square each values as well\n\nWhy broadcasting is useful:\n\nWithout needing any redefinition of functions we were able to transform the function dist and exponentiation (^) to work on a collection of data. This means that we can keep our code simpler and easier to reason about (operating on individual things is easier than adding logic to handle collections of things).\nWhen multiple broadcasted operations are joined together, Julia can fuse the operations so that each operation is performed at the same time instead of each step sequentially. That is, if the operation were not fused, the computer would first calculate dist for each point, and then apply the square on the collection of distances. When it’s fused, the operations can happen at the same time without creating an interim set of values.\n\n\n\n\n\n\n\nNote\n\n\n\nFor readers coming from numpy-flavored Python or R, broadcasting is a way that can feel familiar to the array-oriented behavior of those two languages. Once you feel comfortable with Julia in general, you may find yourself relaxing and relying less on array-oriented design and instead picking whichever iteration paradigm feels most natural for the problem at hand: loops or broadcasting over arrays.\n\n\n\n5.6.8.1 Broadcasting Rules\nWhat happens if one of the collections is not the same size as the others? When broadcasting, singleton dimensions (i.e. the 1 in 1xN, “1-by-N”, dimensions) will be expanded automatically when it makes sense. For example, if you have a single element and a one dimensional array, the single element will be expanded in the function call without using any additional memory (if that dimension matches one of the dimensions of the other array).\nThe rules with an MxN and a PxQ array:\n\neither (M and P) or (N and Q) need to be the same, and\none of the non-matching dimensions needs to be 1\n\nSome examples might clarify. This 1x1 element is being combined with a 4x1, so there is a compatible dimension (N and Q match, M is 1):\n\n2 .^ [0, 1, 2, 3]\n\n4-element Vector{Int64}:\n 1\n 2\n 4\n 8\n\n\nHere, this 1x3 works with the 2x3 (N and Q match, M is 1)\n\n[1 2 3] .+ [1 2 3; 4 5 6]\n\n2×3 Matrix{Int64}:\n 2  4  6\n 5  7  9\n\n\nThis 3x1 isn’t compatible with this 2x3 array (neither M and P nor N and Q match)\n\n[1, 2, 3] .+ [1 2 3; 4 5 6]\n\nLoadError: DimensionMismatch: arrays could not be broadcast to a common size; got a dimension with lengths 3 and 2\nDimensionMismatch: arrays could not be broadcast to a common size; got a dimension with lengths 3 and 2\n\nStacktrace:\n [1] _bcs1\n   @ ./broadcast.jl:555 [inlined]\n [2] _bcs\n   @ ./broadcast.jl:549 [inlined]\n [3] broadcast_shape\n   @ ./broadcast.jl:543 [inlined]\n [4] combine_axes\n   @ ./broadcast.jl:524 [inlined]\n [5] instantiate\n   @ ./broadcast.jl:306 [inlined]\n [6] materialize(bc::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2}, Nothing, typeof(+), Tuple{Vector{Int64}, Matrix{Int64}}})\n   @ Base.Broadcast ./broadcast.jl:903\n [7] top-level scope\n   @ In[67]:1\n\n\nThis 2x4 isn’t compatible with the 2x3 (M and P match, but N nor Q is 1):\n\n[1 2; 3 4] .+ [1 2 3; 4 5 6]\n\nLoadError: DimensionMismatch: arrays could not be broadcast to a common size; got a dimension with lengths 2 and 3\nDimensionMismatch: arrays could not be broadcast to a common size; got a dimension with lengths 2 and 3\n\nStacktrace:\n [1] _bcs1\n   @ ./broadcast.jl:555 [inlined]\n [2] _bcs (repeats 2 times)\n   @ ./broadcast.jl:549 [inlined]\n [3] broadcast_shape\n   @ ./broadcast.jl:543 [inlined]\n [4] combine_axes\n   @ ./broadcast.jl:524 [inlined]\n [5] instantiate\n   @ ./broadcast.jl:306 [inlined]\n [6] materialize(bc::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{2}, Nothing, typeof(+), Tuple{Matrix{Int64}, Matrix{Int64}}})\n   @ Base.Broadcast ./broadcast.jl:903\n [7] top-level scope\n   @ In[68]:1\n\n\n\n\n5.6.8.2 Not Broadcasting\nWhat if you do not want the array to be used element-wise when broadcasting? Then you can wrap the array in a Ref, which is used in broadcasting to make the array be treated like a scalar. In the example below, in(needle,haystack) searches a collection (haystack) for an item (needle) and returns true or false if the item is in the collection:\n\nin(4, [1 2 3; 4 5 6])\n\ntrue\n\n\nWhat if we had an array of things (“needles”) that we wanted to search for? By default, broadcasting would effectively split the array up into collections of individual elements to search:\n\nin.([1, 9], [1 2 3; 4 5 6])\n\n2×3 BitMatrix:\n 1  0  0\n 0  0  0\n\n\nEffectively, the result above is the result of this broadcasted result:\nin(1, [1,2,3]) # the first row of the above result\nin(9, [4,5,6])\nIf we were expecting Julia to return [1,0] (that the first needle is in the haystack but the second needle is not), then we need to tell Julia not to broadcast along the second array with Ref:\n\nin.([1, 9], Ref([1 2 3; 4 5 6]))\n\n2-element BitVector:\n 1\n 0\n\n\n\n\n\n5.6.9 Passing by Sharing\nWe often want to share data between scopes, such as between modules or by passing something into a function’s scope. Arguments to a function in Julia are passed-by-sharing which means that an outside variable can be mutated from within a function. We can modify the array in the outer scope (scope discussed later in this chapter) from within the function. In this example, we modify the array that is assigned to v by doubling each element:\n\nv = [1, 2, 3]\n\nfunction double!(v)\n    for i in eachindex(v)\n        v[1] = 2 * v[i]\n    end\nend\n\ndouble!(v)\n\nv\n\n3-element Vector{Int64}:\n 6\n 2\n 3\n\n\n\n\n\n\n\n\nTip\n\n\n\nConvention in Julia is that a function that modifies it’s arguments has a ! in it’s name and we follow this convention in double! above. Another example would be the built-in function sort! which will sort an array in-place without allocating a new array to store the sorted values.\n\n\nWe won’t discuss all potential ways that programming languages can behave in this regard, but an alternative that one may have seen before (e.g. in Matlab) is pass-by-value where a modification to an argument only modifies the value within the scope. Here’s how to replicate that in Julia by copying the value before handing it to a function. This time, v is not modified because we only passed a copy of the array and not the array itself:\n\nv = [1, 2, 3]\ndouble!(copy(v))\nv\n\n3-element Vector{Int64}:\n 1\n 2\n 3",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "foundations-of-programming.html#sec-scope",
    "href": "foundations-of-programming.html#sec-scope",
    "title": "5  Elements of Programming",
    "section": "5.7 Scope",
    "text": "5.7 Scope\nIn projects of even modest complexity, it can be challenging to come up with unique identifiers for different functions or variables. Scope refers to the bounds for which an identifier is available. We will often talk about the local scope that’s inside some expression that creates a narrowly defined scope (such as a function or let or module block) or the global scope which is the top level scope that contains everything else inside of it. Here are a few examples to demonstrate scope.\n\n1i = 1\n2let\n3    j = 3\n    i + j\nend\n\n\n1\n\ni is defined in the global scope and would be available to other inner scopes.\n\n2\n\nThe let ... end block creates a local scope which inherits the defined global scope definitions.\n\n3\n\nj is only defined in the local scope created by the let block.\n\n\n\n\n4\n\n\nIn fact, if we try to use j outside of the scope defined above we will get an error:\n\nj\n\nLoadError: UndefVarError: `j` not defined\nUndefVarError: `j` not defined\n\n\n\n\n\n\n\n\nTip\n\n\n\nlet blocks are a great way to organize your code in bite-sized chunks or to be able to re-use common variable names without worrying about conflict. Here’s an example of using let blocks to:\n\nPerform intermediate calculations without fear of returning a partially modified variable\nRe-use common variable names\n\n\nbonds = let\n    df = CSV.read(\"bonds.csv\", DataFrame)\n    df.issuer = lookup_issuer(df.CUSIP)\n    df\nend\n\nmortgages = let\n    df = CSV.read(\"bonds.csv\", DataFrame)\n    df.issuer = lookup_issuer(df.CUSIP)\n    df\nend\nIf we were running this interactively (e.g. step-by step in VS Code, the REPL, or notebooks) then these two code blocks will run completely and will run separately. The short, descriptive name df is reused, but there’s no chance of conflict. We also can’t easily run the block of code (let ... end) and get a partially evaluated result (e.g. getting the dataframe before it has been appropriately modified to add the issuer column).\n\n\nHere is an example with functions:\n\nx = 2\nbase = 10\n1foo() = base^x\n2foo(x) = base^x\n3foo(x, base) = base^x\n\nfoo(), foo(4), foo(4, 4)\n\n\n1\n\nBoth base and x are inherited from the global scope.\n\n2\n\nx is based on the local scope from the function’s arguments and base is inherited from the global scope.\n\n3\n\nBoth base and x are defined in the local scope via the function’s arguments.\n\n\n\n\n(100, 10000, 256)\n\n\nIn Julia, it’s always best to explicitly pass arguments to functions rather than relying on them coming from an inherited scope. This is more straight-forward and easier to reason about and it also allows Julia to optimize the function to run faster because all relevant variables coming from outside the function are defined at the function’s entry point (the arguments).\n\n5.7.1 Modules and Namespaces\nModules are ways to encapsulate related functionality together. Another benefit is that the variables inside the module don’t “pollute” the namespace of your current scope. Here’s an example:\n\n1module Shape\n\nstruct Triangle{T}\n    base::T\n    height::T\nend\n\n2function area(t::Triangle)\n    return 1 / 2 * t.base * t.height\nend\nend\n\n3t = Shape.Triangle(4, 2)\n4area = Shape.area(t)\n\n\n1\n\nmodule defines an encapsulated block of code which is anchored to the namespace Shape\n\n2\n\nHere, area a function defined within the Shape module.\n\n3\n\nOutside of Shape module, we can access the definitions inside via the Module.identifier syntax.\n\n4\n\nHere, area is a variable in our global scope that does not conflict with the area defined within the Shape module. If Shape.area were not within a module then when we said area = ... we would have reassigned area to no longer refer to the function and instead would refer to the area of our triangle.\n\n\n\n\n4.0\n\n\n\n\n\n\n\n\nNote\n\n\n\nSummarizing related terminology:\n\nA module is a block of code such as module MySimulation ... end\nA package is a module that has a specific set of files and associated metadata. Essentially, it’s a module with a Project.toml file that has a name and unique identifier listed, and a file in a src/ directory called MySimulation.jl\n\nLibrary is just another name for a package, and the most common context this comes up is when talking about the packages that are bundled with Julia itself called the standard library (stdlib).",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "foundations-of-programming.html#footnotes",
    "href": "foundations-of-programming.html#footnotes",
    "title": "5  Elements of Programming",
    "section": "",
    "text": "Said differently, computer science may contemplate ideas and abstractions more generally than a specific implementation, as in mathematics where a theorem may be proved (\\(a^2 + b^2 = c^2\\)) without resorting to specific numeric examples (\\(3^2 + 4^2 = 5^2\\)).↩︎\nThe Ship of Theseus problem specifically refers to a legendary ancient Greek ship, owned by the hero Theseus. The paradox arises from the scenario where, over time, each wooden part of the ship is replaced with identical materials, leading to the question of whether the fully restored ship is still the same ship as the original. The Ship of Theseus problem is a thought experiment in philosophy that explores the nature of identity and change. It questions whether an object that has had all of its components replaced remains fundamentally the same object.↩︎\nThis binary representations correspond to B and 66 with the ASCII character set and 8-bit integer encodings respectely, discussed later in this chapter.↩︎\nSome distinctions you may encounter: in short-form, “kb” means kilobits while the upper-case “B” in “kB” means kilobytes. Also confusingly, sometimes the “k” can be binary or decimal - because computers speak in binary, a binary “k” means 1024 (equal to 2^10) instead of the usual decimal 1000. In most computer contexts, the binary (multiples of 1024) is more common.↩︎\nThe term floating point refers to the fact that the number’s radix (decimal) point can “float” between the significant digits of the number.↩︎\nThat is, it reads the code input from the user, evaluates what code was given to it, prints the result of the input to the screen, and loops through the process again.↩︎\nThis means that their central processing units (CPUs) use instructions that are 64 bits long.↩︎\nAccurate only to a limited precision, as described in Section 5.5.1.↩︎\nWhether an index starts at 1 or 0 is sometimes debated. Zero-based indexing is natural in the context of low-level programming which deal with bits and positional offsets in computer memory. For higher level programming one-based indexing is more natural: in a set of data stored in an array, it is much more natural to reference the first (through \\(n^{th}\\)) datum instead of the zeroth (through \\((n-1)^{th}\\) datum.↩︎\nArrays in Julia can actually be indexed with an arbitrary starting point: see the package OffsetArrays.jl↩︎\nThe triangular numbers (sum of integers from \\(1\\) to \\(n\\)) are:\\[\nT_n = \\sum_{k=1}^n k = 1 + 2 + \\cdots + n\n    = \\frac{n^2 + n}{2} = \\frac{n(n+1)}{2}\n    = \\left(\\frac{n+1}{2}\\right)\n\\]↩︎\nWhether the last number is in the resulting range depends on if the step evenly divides the end of the range.↩︎\nUnder the hood, strings are essentially a vector of characters but there are complexities with character encoding that don’t allow a lossless conversion to individual characters of uniform bit length. This is for historical compatibility reasons and to avoid making most documents’ file sizes larger than it needs to be.↩︎\nWhat this means will be explained in Chapter 9 .↩︎\nMissing and Nothing are instances of singleton type, which means that there is only a single value that either type can take on.↩︎",
    "crumbs": [
      "Conceptual Foundations: Modeling and Programming",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Elements of Programming</span>"
    ]
  },
  {
    "objectID": "first-abstractions.html",
    "href": "first-abstractions.html",
    "title": "6  Functional Abstractions",
    "section": "",
    "text": "6.1 In this section\nDemonstrate different approaches to a problem which gradually introduce more re-usable or general techniques. These techniques will allow for constructing sophisticated models while maintaining consistency and simplicity. Imperative programming, functional programming, and recursion.",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functional Abstractions</span>"
    ]
  },
  {
    "objectID": "first-abstractions.html#introduction",
    "href": "first-abstractions.html#introduction",
    "title": "6  Functional Abstractions",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nThis chapter will center around a simple task: calculate the present value of a portfolio of a single fixed, risk-free, coupon-paying bonds under two different interest rate environments. The focus will be on describing different approaches to this problem, not be adding complexity to the problem (e.g. no getting into credit spreads, settlement timing, etc.).\n\ncf_bond = [10, 10, 10, 10, 110];\n1rate = [0.05, 0.06, 0.05, 0.04, 0.05];\n\n\n1\n\nThe rates given are the one year rate for time zero, time one, etc.\n\n\n\n\nThe other bond and set of rates is described later in the chapter.\nMathematically, the problem is to determine the \\(\\text{Present Value}\\), where:\n\\[\n\\text{Present Value} = \\sum{\\text{Cashflow}_t \\times \\text{Discount Factor}_t}\n\\]\nWhere\n\\[\n\\text{Discount Factor}_t = \\prod^t{\\frac{1}{1+\\text{Discount Rate}_i}}\n\\]\nWe will repeatedly solve the same problem before extending it to more examples. It may feel repetitive but the focus here is not the problem, but rather the variations between the different approaches.",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functional Abstractions</span>"
    ]
  },
  {
    "objectID": "first-abstractions.html#imperative-style",
    "href": "first-abstractions.html#imperative-style",
    "title": "6  Functional Abstractions",
    "section": "6.3 Imperative Style",
    "text": "6.3 Imperative Style\nOne of the most familiar styles of programming is called imperative (or procedural), where we provide step-by-step commands are provided to the computer. The programmer defines the data involved and how that data moves through the program one step at a time. It commonly uses loops to perform tasks repeatedly or across a set of data. The program’s state (assigment and logic of the program’s variables) is defined and managed by the programmer explicitly.\nHere’s an imperative style of calculating the present value of the bond.\n\nlet\n1\n    pv = 0.0\n    discount = 1.0\n\n2    for i in 1:length(cf_bond)\n        discount = discount / (1 + rate[i])\n3        pv = pv + discount * cf_bond[i]\n    end\n    pv\nend\n\n\n1\n\nDeclare variables to keep track of the discount rate and running total for value\n\n2\n\nLoop the length of the cashflow vector.\n\n3\n\nAt each step of the loop, look up (via index i) update the discount factor to account for the prevailing rate and add the discounted cashflow to the running total present value.\n\n\n\n\n121.48888490821489\n\n\nThis style is simple, digestible, and clear. If we were performing the calculation by hand, it would likely follow a pattern very similar to this. Look up the first cashflow and discount rate, compute a discount factor, and subtotal the value. Repeat for the next set of values.\n\n6.3.1 Iterators\nNote that in the prior code example we defined an index variable i and had to manually define the range over which it would operate (1 through the length of the bond’s cashflow vector). A couple of reasons this could be sub-optimal:\n\nWe are making the assumption that the indices of the vectors start with one, when in reality Julia arrays can be defined to start at 0 or another arbitrary index.\nWe manually perform the lookup of the values within each iteration.\n\nWe can solve the first one (partially) by letting Julia return an iterable set of values corresponding to the indices of the cf_bond vector. Tihs is an example of an iterator which is an object upon which we can repeatedly ask for the next value until it tells us to stop.\nBy using eachindex we can get the indices of the vector since Julia already knows what they are:\n\neachindex(cf_bond)\n\nBase.OneTo(5)\n\n\n\n\n\n\n\n\nLazy Programming\n\n\n\nThe result, Base.OneTo(5) is a lazy object which represents a collection that does not get fully instantiated until asked to (which may not actually be necessary). Many (most?) iterators are lazy but we can interact with them without fully instantiating the data that they represent. For example, we could find the largest index:\n\nmaximum(eachindex(cf_bond))\n\n5\n\n\nThe point is if we have an object that represents a set, we need not actually enumerate each element of the set to interact with it.\nWe can fully instantiate an iterator with collect\n\ncollect(eachindex(cf_bond))\n\n5-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n\n\nLaziness is generally a good thing in programming because sometimes it can be computationally or memory expensive to fully instantiate the collection of interest.\n\n\nAnd when used in context:\n\nlet\n    pv = 0.0\n    discount = 1.0\n\n    for i in eachindex(cf_bond)\n        discount = discount / (1 + rate[i])\n        pv = pv + discount * cf_bond[i]\n    end\n    pv\nend\n\n121.48888490821489\n\n\nHere Julia gave us the index associated with the bond cashflows, but we are still looking up the values (why not just ask for the values instead of their index?) as well as assuming that the indices are the same for the discount rates.\nWe can get the value and the associated index with enumerate:\n\ncollect(enumerate(cf_bond))\n\n5-element Vector{Tuple{Int64, Int64}}:\n (1, 10)\n (2, 10)\n (3, 10)\n (4, 10)\n (5, 110)\n\n\nThis would allow us to skip the step of needing to look up the bond’s cashflows. However, we can go even further by just asking for value associated with both collections. With zip (named because it’s sort of like zipping up two collections together), we get an iterator that provides the values of the underlying collections:\n\ncollect(zip(cf_bond, rate))\n\n5-element Vector{Tuple{Int64, Float64}}:\n (10, 0.05)\n (10, 0.06)\n (10, 0.05)\n (10, 0.04)\n (110, 0.05)\n\n\nThis provides the simplest implementation of the imperative approaches:\n\nlet\n    pv = 0.0\n    discount = 1.0\n\n    for (cf, r) in zip(cf_bond, rate)\n        discount = discount / (1 + r)\n        pv = pv + discount * cf\n    end\n    pv\nend\n\n121.48888490821489\n\n\nThe primary downsides to this approach are:\n\nNeeding to keep track of state is fine in simple cases, but can quickly become difficult to reason about and error prone as the number and complexity of variables grows.\nProgram flow is explicitly stated, leaving fewer places that the compiler can automatically optimize or parallelize.\n\nNote that it’s when state is mutable that a program tends to be more complex.",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functional Abstractions</span>"
    ]
  },
  {
    "objectID": "first-abstractions.html#functional-techniques-and-terminology",
    "href": "first-abstractions.html#functional-techniques-and-terminology",
    "title": "6  Functional Abstractions",
    "section": "6.4 Functional Techniques and Terminology",
    "text": "6.4 Functional Techniques and Terminology\nFunctional programming is a paradigm which attempts to minimize state via composing functions together.\nTable 6.1 introduces a set of core functional methods to familiarize yourself with. Note that anonymous functions (#sec-anonymous-functions) are used frequently to define intermediary steps.\n\n\n\n\nTable 6.1: Important Functional Methods.\n\n\n\n\n\n\n\n\n\n\nFunction\nDesciption\nExample\n\n\n\n\nmap(f,v)\nApply function f to each element of the collection v.\nmap(\n    x-&gt;x^2,\n    [1,3,5]\n) # [1,9,25]\n\n\nreduce(op,v)\nApply binary op to pairs of values, reducing the dimension of the collection v.\n\nHas a couple of important, optional keyword arguments to note (which also apply to other variants of reduce below):\n\ninit defines the identity element (e.g. the initial value of + and * is 0 and 1 respectively)\ndims defines which dimension to reduce across (if the dimension of v is more than one).\n\nreduce(\n    *,\n    [1,3,5]\n) # 15\n\n\nmapreduce(op,f,v)\nMaps f over collection v and returns a reduced result using op.\nmapreduce(\n    *,\n    x-&gt;x^2,\n    [1,3,5]\n) # 35\n\n\nfoldl(op,v)foldr(op,v)\nLike reduce, but applies op from left to right (foldl) or right to left (foldr). Also has mapfoldl and mapfoldr versions.\nfoldl(\n    *,\n    [1,3,5]\n) # 15\n\n\naccumulate(op,v)\nApply op along v , creating a vector with the cumulative result.\naccumulate(\n    +,\n    [1,3,5]\n) # [1, 4, 9]\n\n\nfilter(f,v)\nApply f along v and return a copy of v with elements where f is true\nfilter(\n    &gt;=(3),\n    [1,3,5]\n) # [3, 5]\n\n\n\n\n\n\n\nThis paradigm is very powerful in a few ways:\n\nIt provides a language for talking about what a computation is doing. Instead of “looping over a collection called portfolio and calling a value function” we can more concisely refer to this as mapreduce(valuem,portfolio).\nOften times you are forced to think about the design of the program more deeply, recognizing the core calculations and data used within the model.\nThe compiler is free to apply more optimizations. For example, with reduce, the compiler could drive the calculation in any order since the operation is associative.\nThe lack of mutable state\n\nLet’s build a version of the present value calculation using the functional building blocks described above.\n\n6.4.1 map\nmap is so named for the mathematical concept of mapping an input to an output. Here, it’s effectively the same thing. We take a collection and use the given function to calculate an output. The size of the output equals the size of the input.\nFirst, to show how we could calculate the discount factor we will use map to compute the one-period discount factors:\n\nmap(x -&gt; 1 / (1 + x), rate)\n\n5-element Vector{Float64}:\n 0.9523809523809523\n 0.9433962264150942\n 0.9523809523809523\n 0.9615384615384615\n 0.9523809523809523\n\n\nmap transforms the rate collection by applying the anonomyous function x -&gt; 1 / (1 + x), which is the single period discount factor. This operation is conveyed visually in Figure 6.1.\n\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\nInput Array (rate)\n\n\ncluster_1\n\nMap Function\n\n\ncluster_2\n\nOutput Array\n\n\n\nrate1\n\n0.05\n\n\n\nmap1\n\n1 / (1+x)\n\n\n\nrate1-&gt;map1\n\n\n\n\n\nrate2\n\n0.06\n\n\n\nmap2\n\n1 / (1+x)\n\n\n\nrate2-&gt;map2\n\n\n\n\n\nrate3\n\n0.05\n\n\n\nmap3\n\n1 / (1+x)\n\n\n\nrate3-&gt;map3\n\n\n\n\n\nrate4\n\n0.04\n\n\n\nmap4\n\n1 / (1+x)\n\n\n\nrate4-&gt;map4\n\n\n\n\n\nrate5\n\n0.05\n\n\n\nmap5\n\n1 / (1+x)\n\n\n\nrate5-&gt;map5\n\n\n\n\n\noutput1\n\n0.9524\n\n\n\nmap1-&gt;output1\n\n\n\n\n\noutput2\n\n0.9434\n\n\n\nmap2-&gt;output2\n\n\n\n\n\noutput3\n\n0.9524\n\n\n\nmap3-&gt;output3\n\n\n\n\n\noutput4\n\n0.9615\n\n\n\nmap4-&gt;output4\n\n\n\n\n\noutput5\n\n0.9524\n\n\n\nmap5-&gt;output5\n\n\n\n\n\n\n\n\nFigure 6.1: A diagram showing that map creates a new collection mirroring the old one, after applying the given function to each element in the original collection.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nmap is an absolute workhorse of a function and the authors recommend using it liberally within your code. We find ourselves using map frequently, usually avoiding defining an explicit loop (unless we are modifying some existing collection).\nAn anti-pattern where map would likely be a better tool than a loop often looks like this:\noutput = []\nfor x in collection\n    result = # ... do stuff ...\n    push!(output,result)\nend\noutput\nInstead, map simplifies this to:\nmap(collection) do x\n    # ... do stuff\nend\nNot only does this have the advantage of being clearer, more concise, and less work… it also let’s Julia manage the output type of your computation so you don’t have to worry about the type of output.\n\n\n\n\n6.4.2 accumulate\naccumulate takes an operation and a collection and returns a collection where each element is the cumulative result of applying the operation from the first element to the current one. For example, to calculate the cumulative product of the one-period discount factors:\n\naccumulate(*, map(x -&gt; 1 / (1 + x), rate))\n\n5-element Vector{Float64}:\n 0.9523809523809523\n 0.898472596585804\n 0.8556881872245752\n 0.822777103100553\n 0.7835972410481457\n\n\nThis results in a vector of the cumulative discount factors for each point in time corresponding to the given cashflows.\n\n\n\n\n\n\n\n\nG\n\n\ncluster_3\n\n\n\ncluster_2\n\nOutput Array\n\n\ncluster_0\n\nInput Array (discount factors)\n\n\ncluster_1\n\nAccumulate Function (*)\n\n\n\ndf1\n\n0.9524\n\n\n\nacc1\n\ndf1 * init\n\n\n\ndf1-&gt;acc1\n\n\n\n\n\ndf2\n\n0.9434\n\n\n\nacc2\n\ndf2 * output_1\n\n\n\ndf2-&gt;acc2\n\n\n\n\n\ndf3\n\n0.9524\n\n\n\nacc3\n\ndf3 * output_2\n\n\n\ndf3-&gt;acc3\n\n\n\n\n\ndf4\n\n0.9615\n\n\n\nacc4\n\ndf4 * output_3\n\n\n\ndf4-&gt;acc4\n\n\n\n\n\ndf5\n\n0.9524\n\n\n\nacc5\n\ndf5 * output_4\n\n\n\ndf5-&gt;acc5\n\n\n\n\n\noutput1\n\n0.9524\n\n\n\nacc1-&gt;output1\n\n\n\n\n\noutput2\n\n0.8985\n\n\n\nacc2-&gt;output2\n\n\n\n\n\noutput3\n\n0.8561\n\n\n\nacc3-&gt;output3\n\n\n\n\n\noutput4\n\n0.8230\n\n\n\nacc4-&gt;output4\n\n\n\n\n\noutput5\n\n0.7831\n\n\n\nacc5-&gt;output5\n\n\n\n\n\noutput0\n\ninit=1.0\n\n\n\noutput0-&gt;acc1\n\n\n\n\n\noutput1-&gt;acc2\n\n\n\n\n\noutput2-&gt;acc3\n\n\n\n\n\noutput3-&gt;acc4\n\n\n\n\n\noutput4-&gt;acc5\n\n\n\n\n\n\n\n\nFigure 6.2: A diagram showing that accumulate creates a new collection where each element is the cumulative result of applying the given operation to all previous elements.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor accumulate and reduce, an important value is the init (an optional keyword argument), which is the initial value to start the accumulation or reduction. By default, for common operations this identity element is predefined. For example, for + the identity is 0 while for * it is 1. The identity element \\(e\\) is the one where for a given binary operation \\(\\bigodot\\), that \\(x \\bigodot e = x\\).\nAnother example beyond addition and subtraction is string concatenation. In Julia, two strings are concatenated with * (like in mathematics, \\(a * b\\) is also written as \\(ab\\)). The identity element for strings where the binary operation \\(\\bigodot = *\\) is \"\". For example:\n\naccumulate(*, [\"a\", \"b\", \"c\"], init=\"\")\n\n3-element Vector{String}:\n \"a\"\n \"ab\"\n \"abc\"\n\n\nThis is a taste of a branch of mathematics known as Category Theory, a very rich subject but largely beyond the immediate scope of this book. The category theoretical term for sets of things that work with the binary operator and identity elements as described above is a monoid. You may ignore this fact.\n\n\n\n\n6.4.3 reduce\nreduce takes an operation and a collection and applies the operation repeatedly to pairs of elements until there is only a single value left.\nFor example, we start with the calculation of the vector of discounted cashflows\n\ndfs = accumulate(*, map(x -&gt; 1 / (1 + x), rate))\ndiscounted_cfs = map(*, cf_bond, dfs)\n\n5-element Vector{Float64}:\n  9.523809523809524\n  8.98472596585804\n  8.556881872245752\n  8.22777103100553\n 86.19569651529602\n\n\nThen we can sum them with reduce:\n\nreduce(+, discounted_cfs)\n\n121.48888490821487\n\n\n\n\n\n\n\n\n\n\nG\n\n\ncluster_0\n\nInput Array (discounted cashflows)\n\n\ncluster_1\n\nReduce Function (+)\n\n\ncluster_2\n\nOutput Value\n\n\n\ncf1\n\n9.52\n\n\n\nred0\n\ninit=0.0 + cf1\n\n\n\ncf1-&gt;red0\n\n\n\n\n\ncf2\n\n8.98\n\n\n\nred1\n\nred0 + cf2\n\n\n\ncf2-&gt;red1\n\n\n\n\n\ncf3\n\n8.56\n\n\n\nred2\n\nred1 + cf3\n\n\n\ncf3-&gt;red2\n\n\n\n\n\ncf4\n\n8.23\n\n\n\nred3\n\nred2 + cf4\n\n\n\ncf4-&gt;red3\n\n\n\n\n\ncf5\n\n86.20\n\n\n\nred4\n\nred3 + cf5\n\n\n\ncf5-&gt;red4\n\n\n\n\n\nred0-&gt;red1\n\n\n\n\n\nred1-&gt;red2\n\n\n\n\n\nred2-&gt;red3\n\n\n\n\n\nred3-&gt;red4\n\n\n\n\n\noutput\n\n121.49\n\n\n\nred4-&gt;output\n\n\n\n\n\n\n\n\nFigure 6.3: A diagram showing how reduce applies the given operation to pairs of elements, ultimately reducing the collection to a single value.\n\n\n\n\n\n\n\n6.4.4 mapreduce\nWe can combine map, accumulate and reduce to concisely calculate the present value in a functional style:\n\ndfs = accumulate(*, map(x -&gt; 1 / (1 + x), rate))\nmapreduce(*, +, cf_bond, dfs)\n\n121.48888490821487\n\n\nThis calculates the discount factors, applies them to the cashflows with map, and sums the result with a reduction.\n\n\n\n\n\n\nTip\n\n\n\nAt the risk of sounding obvious, an easy way to make the program more “functional” is to simply use more functions. Do this one thing and it will improve the model’s organization, maintainability, and reduce bugs!\nTake the example from earlier:\npv = 0.0\ndiscount = 1.0\n\nfor (cf, r) in zip(cf_bond, rate)\n    discount = discount / (1 + r)\n    pv = pv + discount * cf\nend\npv\nWe can easy turn this code into a function so that it can operate on data beyond the single pair of cf_bond and rate previously defined:\nfunction pv(rates,cashflows)\n    pv = 0.0\n    discount = 1.0\n\n1    for (cf, r) in zip(rates, cashflows)\n        discount = discount / (1 + r)\n        pv = pv + discount * cf\n    end\n    pv\nend\n\n1\n\nHere, cf_bond and rate would refer to whatever was passed as arguments to the function instead of any globally defined values.\n\n\nNow we could use this definition of pv on other instances of rates and cashflows.\n\n\n\n\n6.4.5 filter\nfilter does what you might think - filter a collection based on some criterion that can be determined as true or false.\nFor example filtering out even numbers using the isodd function:\n\nfilter(isodd, 1:6)\n\n3-element Vector{Int64}:\n 1\n 3\n 5\n\n\nOr filtering out things that don’t match a criteria:\n\nfilter(x -&gt; ~(x == 5), 1:6)\n\n5-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 6\n\n\nWhile we didn’t need filter to calculate a bond’s present value in the example above, one can imagine how you may want to filter dates that a bond might pay a cashflow, say last day of a quarter:\n\nusing Dates\nlet d = Date(2024, 01, 01)\n    filter(d -&gt; lastdayofquarter(d) == d, d:Day(1):lastdayofyear(d))\nend\n\n4-element Vector{Date}:\n 2024-03-31\n 2024-06-30\n 2024-09-30\n 2024-12-31\n\n\n\n\n6.4.6 More Tips on Functional Styles\n\n6.4.6.1 do Syntax for Function Arguments\nIn more complex situations such as with multiple collections or multi-line logic, there is a clearer syntax that is often used. do is a reserved keyword in Julia that creates an anonymous function and passes its arguments to a function like map. For example, this (terrible) code which decides if a number is prime. The anonymous function requires a begin block since the logic of the function is extended into multiple lines.\nmap(x -&gt; begin\n  if x == 1\n  \"prime\"\n  elseif x == 2\n  \"not prime\"\n  elseif x == 3\n  \"prime\"\n  elseif x &gt; 4\n  \"probably not prime\"\n  end\nend, \n[1, 2, 3, 10]\n)\nThis can be written more cleanly with the do syntax:\nmap([1, 2, 3, 10]) do x\nif x == 1\n\"prime\"\nelseif x == 2\n\"not prime\"\nelseif x == 3\n\"prime\"\nelseif x &gt; 4\n\"probably not prime\"\nend)\n\n\n6.4.6.2 Multiple Collections\nmap and the other functional operators discussed in this section can take multiple arguments. This is convienient if you have multiple arguments to a function:\n\ndiscounts = [0.9, 0.81, 0.73]\ncashflows = [10, 10, 10]\n\nmap((d, c) -&gt; d * c, discounts, cashflows)\n\n3-element Vector{Float64}:\n 9.0\n 8.100000000000001\n 7.3\n\n\nOr an example with the do syntax:\n\nmap(discounts, cashflows) do d, c\n    d * c\nend\n\n3-element Vector{Float64}:\n 9.0\n 8.100000000000001\n 7.3\n\n\n\n\n6.4.6.3 Mixing Funcitonal And Imperative Styles\nOne of the best things about Julia is how natural it can be to mix the different styles Sometimes the best is the mix of both styles and that’s one of the benefits of Julia: use the style that’s most natural to the problem.\n\n\n\n\n\n\nNote\n\n\n\nLisp (“list processing”) is another, much older language than Julia (created in the 1950s!). One of it’s claims to fame is how flexible and powerful the tools are within the language to build upon. There’s a couple aspects of this curse that we wish to describe because we can learn from it while Julia is still a relatively young language.\nPart of the “curse” is that: because there’s so much freedom in what can be expressed in the language, there’s not an obvious “best” way of doing things. This can lead to decision paralysis where you are trying to over-analyze what’s the best way to write part of your code. Our advice: don’t worry about it! A working implemntation of something is better than an over-optimized idea.\nThe other part of the “curse” is that because is that it’s relatively easy to implement so many things from the building blocks that Julia provides and compose them together to do what you want. This has a downside becuase the general approach to packages is smaller, standalone pieces that you call as needed. For example, consider Python’s Pandas library, upon which Python’s data science community was built. It came bundled with a CSV reader, Excel reader, Database reader, DataFrame type, visualization library, and statistical functions. In Julia, each of those are separate packages that specialize for the respective topics. This is advantageous in that they can progress independently from one another, you don’t have to include functionality that you don’t need, and you can mix and match libraries depending on your preference.",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functional Abstractions</span>"
    ]
  },
  {
    "objectID": "first-abstractions.html#array-oriented-styles",
    "href": "first-abstractions.html#array-oriented-styles",
    "title": "6  Functional Abstractions",
    "section": "6.5 Array-Oriented Styles",
    "text": "6.5 Array-Oriented Styles\nArray-oriented programming is one that is practiced in two main contexts:\n\nGPU programming\nPython numerical computing\n\nThe former because GPUs want large blocks of similar data to operate in parallel. The latter is because native Python is too slow for many modeling problems so libraries like NumPy,SciPy,and tensor libraries define C++ (or similar) libraries for users to call out to.\nArray-oriented programming is not always natural for financial and actuarial applications. Differences in behavior or timing of underling cashflows can make a set of otherwise similar products difficult to capture in nicely gridded arrays. Nonetheless, certain applications (scenario generation, some valuation routines) fit very naturally into this paradigm. Furthermore, for those that work well it’s often a great way to extract additional performance due to the parallelization offered via CPU or GPU array programming.\nTable 6.2 shows the bond present value example in this style.\n\n\n\nTable 6.2: Julia’s broadcasting makes for an array-oriented style, similar to the approach that would be used with Python’s NumPy.\n\n\n\n\n\n\n\n\n\nJulia\nPython (NumPy)\n\n\n\n\ncf_bond = [10, 10, 10, 10, 110]\nrate = [0.05, 0.06, 0.05, 0.04, 0.05]\n\ndiscount_factors = cumprod(1 ./ (1 .+ rate))\nsum(cf_bond .* discount_factors)\nimport numpy as np\n\ncf_bond = np.array([10, 10, 10, 10, 110])\nrate = np.array([0.05, 0.06, 0.05, 0.04, 0.05])\n\ndiscount_factors = np.cumprod(1 / (1 + rate))\nresult = np.sum(cf_bond * discount_factors)\n\n\n\n\n\n\nThe downsides to this style are:\n\nSometimes it is unnatural because of non-uniformity of the data we are working with. For example if the length of the cashflows were shorter than the discount rates, we would have to perform intermediate steps to shorten or lengthen arrays in order to get them to be the same size.\nA good bit of runtime performance is lost because the computer needs to allocate and fill many intermediate arrays (note how in Table 6.2, the discount_factors needs to instantiate an entirely new vector even though it’s only temporarily used). See more on allocations in Chapter 9.",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functional Abstractions</span>"
    ]
  },
  {
    "objectID": "first-abstractions.html#recursion",
    "href": "first-abstractions.html#recursion",
    "title": "6  Functional Abstractions",
    "section": "6.6 Recursion",
    "text": "6.6 Recursion\nThe Fibbonaci sequence is a classic example of a recursive algorithm:\n\\[F(n) = \\begin{cases}\n0, & \\text{if } n = 0\\\\\n1, & \\text{if } n = 1\\\\\nF(n-1) + F(n-2), & \\text{if } n &gt; 1\n\\end{cases}\\]\nIn code, this translates into a function definition that refers to itself:\nfunction fibonacci(n)\n    if n == 0\n        return 0\n    elseif n == 1\n        return 1\n    else\n        return fibonacci(n-1) + fibonacci(n-2)\n    end\nend\nOne could imagine a possible pattern where the value of a stream of cashflows is defined as the sum of the value of the discounted next period plus the cashflows that occur this period.\n\nfunction pv_recursive(rate,cashflows,accumulated_value=0.0)\n  if isempty(cashflows)\n    return accumulated_value\n  else\n    v = (accumulated_value + cashflows[end]) / (1 + rate)\n    return pv_recursive(rate,cashflows[begin:end-1], v)\n  end\nend\n\npv_recursive (generic function with 2 methods)\n\n\nAnd an example of its use:\n\npv_recursive(0.05,[10,10,10])\n\n27.232480293704782\n\n\nGenerally, the recursive pattern includes defining a ‘base case’ where you stop the recursive behavior and return the result that has been accumulated up to that point.",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Functional Abstractions</span>"
    ]
  },
  {
    "objectID": "type-abstractions.html",
    "href": "type-abstractions.html",
    "title": "7  Data and Types",
    "section": "",
    "text": "7.1 In this section\nThe powerful benefits that using assigning types to data has within the model’s system, some examples and relating some aspects of object oriented design.",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data and Types</span>"
    ]
  },
  {
    "objectID": "type-abstractions.html#using-types-to-value-a-portfolio",
    "href": "type-abstractions.html#using-types-to-value-a-portfolio",
    "title": "7  Data and Types",
    "section": "7.2 Using Types to Value a Portfolio",
    "text": "7.2 Using Types to Value a Portfolio\nWe will assemble a set of interfaces that let’s us value a portfolio of assets. Using the constructs introduced in the prior chapter, we can describe this as additively reducing over the value-mapped collection of assets in the portfolio. Or more concisely:\nmapreduce(value,+,portfolio)\nIn portfolio, the assets may be heterogeneous so we will need to define what the valuation semantics are for the different kinds of assets. To get to our end goal, we will need to:\n\nDefine the different kinds of assets within our portfolio\nHow the assets are to be valued.",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data and Types</span>"
    ]
  },
  {
    "objectID": "type-abstractions.html#benefits-of-using-types",
    "href": "type-abstractions.html#benefits-of-using-types",
    "title": "7  Data and Types",
    "section": "7.3 Benefits of Using Types",
    "text": "7.3 Benefits of Using Types\nDefining types allows us to do several things:\n\nSeparate concerns. For examaple, deciding how to value an option need not know how we value a bond. The code and associated logic is kept distinct which is easier to reason about and to test.\nRe-use code. When a set of types within a hiearchy all share the same logic, then we can define the method at the highest relevant level and avoid writing the method for each possible type. In our simple example we won’t get as much benefit here since the hiearchy is simple and the set of types small.\nDispatch on type. By defining types for our assets, we can use multiple dispatch to define specialized behavior for each type. This allows us to write generic code that works with any asset type, and the Julia compiler will automatically select the appropriate method based on the type of the asset at runtime. This is a powerful feature that enables extensibility and modularity in our code.\nImprove readability and clarity. By defining types for our assets, we make our code more expressive and self-documenting. The types provide a clear indication of what kind of data we are working with, making it easier for other developers (or ourselves in the future) to understand and maintain the codebase.w\nEnable type safety. By specifying the expected types for function arguments and return values, we can catch type-related errors at compile time rather than at runtime. This helps prevent bugs and makes our code more robust.\n\nWith these benefits in mind, let’s start by defining the types for our assets. We’ll create an abstract type called Asset that will serve as the parent type for all our asset types. If you haven’t read it already, Section 5.5.7 is a good reference for details on types at the language level (this section is focused on organizaiton and building up the abstracted valuation process).",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data and Types</span>"
    ]
  },
  {
    "objectID": "type-abstractions.html#defining-types-for-portfolio-valuation",
    "href": "type-abstractions.html#defining-types-for-portfolio-valuation",
    "title": "7  Data and Types",
    "section": "7.4 Defining Types for Portfolio Valuation",
    "text": "7.4 Defining Types for Portfolio Valuation\nWe will define five types of assets in this simplified universe:\n\nCash\nRisk Free Bonds (coupon and zero-coupon varieties)\nEuropean Puts and Calls on Equities\n\nTo do the valuation of these, we need some economic parameters as well: risk free rates and option implied volatilites, which we will pass via named tuples. In a more robust model it would be wise to use types to differentiate between different kinds of economic assumption sets but we will limit the scope here such that\nHere’s the outline of what follows to get an understanding of types, type hierarchy, and multiple dispatch.\n\nDefine the Cash and Bond types.\nDefine the most basic economic parameter set.\nDefine the value functions for Cash and Bonds.\n\n\n## Data type definitions\n1abstract type AbstractAsset end\n\nstruct Cash &lt;: AbstractAsset\n    balance::Float64\nend\n\nabstract type AbstractBond &lt;: AbstractAsset end\n\nstruct CouponBond &lt;: AbstractBond\n    par::Float64\n    coupon::Float64\n    tenor::Int\nend\n\nstruct ZeroCouponBond &lt;: AbstractBond\n    par::Float64\n    tenor::Int\nend\n\n\n1\n\nGeneral convention is to name abstract types beginning with Abstract...\n\n\n\n\nNow to define the economic parameters:\n\nstruct EconomicAssumptions{T}\n  riskfree::T\nend\n\nThis is a paramatetric type because later on we will vary what objects we use for riskfree. For now, we will use simple scalar values, like in this potential scenario:\n::: {#a955a3e9 .cell execution_count=3} {.julia .cell-code}  econ_baseline= EconomicAssumptions(0.05)\n::: {.cell-output .cell-output-display execution_count=4} EconomicAssumptions{Float64}(0.05) ::: :::\nNow on to defining the valuation for Cash and AbstractBonds. Cash is always equal to it’s balance:\n\nvalue(asset::Cash, ea::EconomicAssumptions) = asset.balance\n\nvalue (generic function with 1 method)\n\n\nRisk free bonds are the discounted present value of the riskless cashflows. We first define a method that generically operates on any fixed bond, all that’s left to do is for different types of bonds to define how much cashflow occurs at the given point in time by defining cashflow for the associated type.\n\n2function value(asset::AbstractBond, r::Float64)\n    discount_factor = 1.0\n    value = 0.0\n    for t in 1:asset.tenor\n1        discount_factor /= (1 + r)\n        value += discount_factor * cashflow(asset, t)\n    end\n    return value\nend\n\nfunction cashflow(bond::CouponBond, time)\n    if time == bond.tenor\n        (1 + bond.coupon) * bond.par\n    else\n        bond.coupon * bond.par\n    end\nend\n\n3function value(bond::ZeroCouponBond, r::Float64)\n    return bond.par / (1 + r)^bond.tenor\nend\n\n\n1\n\nx /= y, x += y, etc. are shorthand ways to write x = x / y or x = x + y\n\n2\n\nvalue is defined for AbstractBonds in general…\n\n3\n\n… and then more specifically for ZeroCouponBonds. This will be explained when discussing “dispatch” below.\n\n\n\n\nvalue (generic function with 3 methods)\n\n\n\n7.4.1 (Multiple) Dispatch\nWhen a function is called, the computer has to decide which method to use. In the example above, when we want to value a ZeroCouponBond, does the value(asset::AbstractBond, r) or value(bond::ZeroCouponBond, r) version get used? Dispatch is the process of determining the right method to use and the rule is that the most specific defined method gets used. In this case, that means that even though our ZeroCouponBond is an AbstractBond, the routine that will used is the most specific value(bond::ZeroCouponBond, r).\nAlready, this is a powerful tool to simplify our code. Imagine the alternative of a long chain of conditional statements trying to find the right logic to use:\n# don't do this!\nfunction value(asset,r)\n    if asset.type == \"ZeroCouponBond\"\n        # special code for Zero coupon bonds\n        # ...\n    elseif asset.type == \"ParBond\"\n        # special code for Par bonds\n        # ...\n    elseif asset.type == \"AmortizingBond\"\n        # special code for Amortizing Bonds\n        # ...\n    else\n        # here define the generic AbstractBond logic\n    end\nend\nA more general concept is that of multiple dispatch, where the types of all arguments are used to determine which method to use. This is a very general paradigm, and in many ways is more extensible than traditional object oriented approaches, (more on that in the next section).\nIn our definition of value above, we used a simple scalar interest rate to determine the rate to discount the cash flows. What if instead of a scalar interest rate value we wanted to instead pass an object that represented a term structure of interest rates? Note how in the definition of value for ZeroCouponBond, we have defined a more specific signature: both the first and second arguments are specific, concrete types. When we call value(ZeroCouponBond(100.0,3),0.05), we avoid the loop that’s defined in the generic case and jump immediate to a more efficient definition of its value. This is dispatching on the combination of types and picking the most relevant (specific) version for what has been passed to it.\nDespite the definitions above, the following will error because we haven’t defined a method for value which takes as it’s second argument a type of EconomicAssumptions:\n#| error: true\nvalue(ZeroCouponBond(100.0,5),econ_baseline)\nLet’s fix that. Here we define a method which takes the economic assumption type and just relays the relevant risk free rate to the value methods already defined (which take an AbstractBond and a scalar r).\n\nvalue(bond::AbstractBond,econ::EconomicAssumptions) = value(bond,econ.riskfree)\n\nvalue (generic function with 4 methods)\n\n\nNow this following works:\n\nvalue(ZeroCouponBond(100.0, 5), econ_baseline)\n\n78.35261664684589\n\n\nHere’s an example of how this would be used:\n\nportfolio = [\n    Cash(50.0),\n    CouponBond(100.0, 0.05, 5),\n    ZeroCouponBond(100.0, 5),\n]\n\nmap(asset-&gt; value(asset,econ_baseline), portfolio)\n\n3-element Vector{Float64}:\n 50.0\n 99.99999999999999\n 78.35261664684589\n\n\nThis is very close to the goal that we set out at the end of the section. We can complete it by reducing over the collection to sum up the value:\n\nmapreduce(asset -&gt; value(asset,econ_baseline), +, portfolio)\n\n228.3526166468459\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis code:\nmapreduce(asset-&gt; value(asset,econ_baseline), +, portfolio)\nis more verbose than what we set out do at the start (mapreduce(value,+,portfolio)) due to the two-argument value function requiring a second arugment for the economic variables. This works well! However, there is a way to define it which avoids the anonymous function, which in some cases will end up needing to be compiled more frequently than you want it to. Sometime we want a lightweight, okay-to-compile-on-the-fly function. Other times, we know it’s something that will be passed around in compute-intensive parts of the code. A technique in this situation is to define an object which “locks in” one of the arguments but behaves like the anonymous version. There is a pair of types in the Base module, Fix1 and Fix2, which represent partially-applied versions of the two-argument function f, with the first or second argument fixed to the value “x”.\nThis is, Base.Fix1(f, x) behaves like y-&gt;f(x, y) and Base.Fix2(f, x) behaves like y-&gt;f(y, x).\nIn the context of our valuation model, this would look like:\n\nval = Base.Fix2(value,econ_baseline)\nmapreduce(val,+,portfolio)\n\n228.3526166468459\n\n\n\n\nExtending the example, we can use a time-varying risk free rate instead of a constaint. For fun, let’s say that the risk free rate has a sinusoidal pattern:\n\necon_sin = EconomicAssumptions(t -&gt; 0.05 + sin(t) / 100)\n\nEconomicAssumptions{var\"#15#16\"}(var\"#15#16\"())\n\n\nNow value will not work, because we’ve only defined how value works on bonds if the given rate is a Float64 type:\n#| error: true\nvalue(ZeroCouponBond(100.0, 5), econ_sin)\nWe can extend our methods to account for this:\n\n1function value(bond::ZeroCouponBond, r::T) where {T&lt;:Function}\n2    return bond.par / (1 + r(bond.tenor))^bond.tenor\nend\n\n\n1\n\nThe r::T ... where {T&lt;:Function} says use this method if r is any concrete subtype of the (abstract) Function type.\n\n2\n\nr is a fuction, where we call the time to get the zero coupon bond (a.k.a. spot) rate for the given timepoint.\n\n\n\n\nvalue (generic function with 5 methods)\n\n\nNow it works:\n\nvalue(ZeroCouponBond(100.0, 5), econ_sin)\n\n82.03058910862806",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data and Types</span>"
    ]
  },
  {
    "objectID": "type-abstractions.html#objected-oriented-design",
    "href": "type-abstractions.html#objected-oriented-design",
    "title": "7  Data and Types",
    "section": "7.5 Objected Oriented Design",
    "text": "7.5 Objected Oriented Design\nThere’s enough general familiarity with object oriented (“OO”) design that it’s worth describing for understanding how it compares and contrasts to other design patterns. Object oriented systems attempt to form the analogy that various parts of the system are their own objects which encapsulate both data and behavior. Object oriented design is often one the first computer programming abstractions introduced because it very relatable1, however this comparative discussion will point out a number of its flaws as well. That said, much of OO design can be emulated in Julia except for data inheritance.\nWe bring up object oriented design not because of the authors (admittedly subjective) opinion that the object-oriented paradigm can be less suitable for financial modeling, but because by having a (potentially more relatable) contrasting approach we can better illuminate certain ideas and concepts.",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data and Types</span>"
    ]
  },
  {
    "objectID": "type-abstractions.html#assigning-behavior",
    "href": "type-abstractions.html#assigning-behavior",
    "title": "7  Data and Types",
    "section": "7.6 Assigning Behavior",
    "text": "7.6 Assigning Behavior\nThe value function is a good example of where the OO requriment to ascribe behavior to a single type (class) can lead to confusing design. If we had to assign value to one of the objects involved, should it be the economic parameteres of the asset contracts? The choice is not obvious at all. Isn’t it the market (economic parameters) that determines the value? But then if value were to be a method wholly owned by the economic parameters, how could it possible define in advance the valuation semantics of all types of assets? What if one wanted to extend the valuation to a new asset class? These are problems presented in traditional OO designs and that resolve so elegantly with multiple dispatch.",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data and Types</span>"
    ]
  },
  {
    "objectID": "type-abstractions.html#inheritance",
    "href": "type-abstractions.html#inheritance",
    "title": "7  Data and Types",
    "section": "7.7 Inheritance",
    "text": "7.7 Inheritance\nWe discussed the type hierarchy in Chapter 5 and in most OO implementations this hierarchy comes with inheriting both data and behavior. This is different from Julia where subtypes inherit behavior but not data from the parent type.\nInheriting the data tends to introduce a tight coupling between the parent and the child classes in OO systems. This tight coupling can lead to several issues, particularly as systems grow in complexity. For example, changes in the parent class can inadvertently affect the behavior of all its child classes, which can be problematic if these changes are not carefully managed. This is often referred to as the “fragile base class problem,” where base classes are delicate and changes to them can have widespread, unintended consequences.\nAnother issue with inheritance in OO design is the temptation to use it for code reuse, which can lead to inappropriate hierarchies. Developers might create deep inheritance structures just to reuse code, leading to a scenario where classes are not logically related but are forced into a hierarchy. This can make the system harder to understand and maintain.\nMoreover, inheritance can sometimes lead to the duplication of code across the hierarchy, especially if the inherited behavior needs to be slightly modified in different child classes. This goes against the DRY (Don’t Repeat Yourself) principle, which is a fundamental concept in software engineering advocating for the reduction of repetition in code.\n\n7.7.1 Composition over Inheritance\nTo mitigate some of the problems associated with inheritance, there’s a growing preference for composition. Composition involves creating objects that contain instances of other objects to achieve complex behaviors. This approach is more flexible than inheritance as it allows for the creation of more modular and reusable code. There is a general preference for “composition over inheritance” among professional developers these days.\nIn composition, objects are constructed from other objects, and behaviors are delegated to these contained objects. This approach allows for greater flexibility, as it’s easier to change the behavior of a system by replacing parts of it without affecting the entire hierarchy, as is often the case with inheritance.\nComposition looks like this:\nstruct CUSIP\n    code::string\nend\n\nstruct FixedIncome\n    coupon::Float64\n    tenor::Float64\nend\n\nstruct MunicipalBond\n    cusip::CUSIP\n    fi::FixedIncome\nend\n\nstruct ListedOption\n    cusip::CUSIP\n    #... other data fields\nend\n\nstruct UnlistedBond\n    fi::FixedIncome\nend\n\n\n# define behavior which relies on defining \nlast_transaction(c::CUSIP) = # ... perform lookup of data\nlast_transaction(asset) = last_transaction(asset.cusip)\n\nduration(f::FixedIncome) = # ... calculate duration\nduration(asset) = duration(asset.fi)\nIn the above example, there are number of asset classes that have CUSIP related attributes (i.e. the 9 character code) and behavior (e.g. being able to look up transaction data). Other assets have fixed income attributes (e.g. calculating a duration). But not all of these assets have a CUSIP! Composition lets us bundle the data and behavior together without needing complex chains of inheritance.\n\n\n\n\n\n\nNote\n\n\n\nA CUSIP (Committee on Uniform Security Identification Procedures) number, is a unique nine-character alphanumeric code assigned to securities, such as stocks and bonds, in the United States and Canada. This code is used to facilitate the clearing and settlement process of securities and to uniquely identify them in transactions and records.",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data and Types</span>"
    ]
  },
  {
    "objectID": "type-abstractions.html#footnotes",
    "href": "type-abstractions.html#footnotes",
    "title": "7  Data and Types",
    "section": "",
    "text": "“Many people who have no idea how a computer works find the idea of object-oriented programming quite natural. In contrast, many people who have experience with computers initially think there is something strange about object oriented systems.” - David Robson, “Object Oriented Software Systems” in Byte Magazine (1981).↩︎",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data and Types</span>"
    ]
  },
  {
    "objectID": "patterns-abstraction.html",
    "href": "patterns-abstraction.html",
    "title": "8  Higher Levels of Abstraction",
    "section": "",
    "text": "8.1 In this section\nWhy we talk about abstraction as a technique in and of itself, discussion of abstraction at the level of code organizaiton and interfaces.",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Higher Levels of Abstraction</span>"
    ]
  },
  {
    "objectID": "patterns-abstraction.html#introduction",
    "href": "patterns-abstraction.html#introduction",
    "title": "8  Higher Levels of Abstraction",
    "section": "8.2 Introduction",
    "text": "8.2 Introduction\nIn programming and modeling, as in mathematics, abstraction permits the definition of interchangeable components and patterns that can be reused. Abstraction is a selective ignorance—focusing on the aspects of the problem that are relevant, and ignoring the others. The last two chapters described what we might call “micro” level abstractions: specific functions or types. In this chapter, we zoom out and examine some principles that guide good model development and how that manifests itself in architectural conerns such as how different parts of the code are organized, what parts of the program are considered ‘public’ versus ‘private’, and patterns themselves.\nChapter 5 Described a number of tools that we can utilize as interfaces within our model. We use these tools that are provided by our programming language in service of the conceptual abstraction described above.\n\nFunctions let us implement behavior, where we need trouble ourselves with the low level details.\nData types provide a hierarchical structure to provide meaning to things, and to group those things together into more meaningful structures.\nModules allow us to combine data, and or function, into a related group of concepts which can be shared in different parts of our model",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Higher Levels of Abstraction</span>"
    ]
  },
  {
    "objectID": "patterns-abstraction.html#principles-for-abstraction",
    "href": "patterns-abstraction.html#principles-for-abstraction",
    "title": "8  Higher Levels of Abstraction",
    "section": "8.3 Principles for Abstraction",
    "text": "8.3 Principles for Abstraction\nHere is a list of some principles that arise when developing a particular abstraction. Not all abstractions serve all of these purposes but generally fit one or more of them.\n\n\n\n\nTable 8.1: Finding abstractions generally means finding patterns that fit into one of these principles.\n\n\n\n\n\n\n\n\n\n\n\nPrinciple\nWhat\nWhy\nExample\n\n\n\n\nSeparation of Concerns\nDivide the system into distinct parts, each addressing a separate concern\nPromote modularity and reduce high degree of dependence (coupling) between components\nSeparating data retrieval, data processing, and output generation steps in a process\n\n\nEncapsulation\nHide the internal details of a component and expose only a clean, well-defined set of functionality (interface)\nDon’t let other parts of the program modify internal data and make the system easier to understand and maintain\nDefining a type or module with well defined behavior and responsibiltiy\n\n\nComposability\nDesign simple components that can be combined to create more complex behaviors, as opposed to a single component that attempts to handle all behavior.\nPromote reuse and allow for the components to be combined creatively\nSeparate details about economic conditions into different types than contracts/instruments\n\n\nGeneralization\nIdentify common patterns and create generic components that can be specialized as needed. Often this means idenfifying the common behavior that arises repeatedly in a model\nAvoid duplication and make the system more expressive and extensible\nDefining a generic Instrument type that can be specialized for different asset classes\n\n\n\n\n\n\n\nThese principles provide guidance for creating abstractions that are modular, reusable, and maintainable. By following these principles, developers can create financial models that are easier to understand, extend, and adapt to changing requirements.\n\n8.3.1 Pragramatic Considerations for Model Design\n\n8.3.1.1 Behavior-Oriented\nThis strategies is to effectively group together components with a model that behaves similarly. So, in our example of bonds and interest-rate swaps fundamentally, they share many characteristics and are used in very similar ways within a model. Therefore, it might make sense to group them together when developing a model.\n\n\n8.3.1.2 Domain Expertise\nIt may be that components of the model require sufficient expertise that different persons or groups are involved in the development. This may warrant separating a models design, So that different groups contributing to the model can focus on any more narrow aspect, Regardless of inherent similarity of components. For example, at a higher vertical level of obstruction, financial derivatives may fall under similar grouping, but sufficient differences exist for equity credit or foreign exchange derivatives that the model should separate those three asset classes for development purposes.\n\n\n8.3.1.3 Composability versus All-in-One\nFor some model design goals, it may be warranted to attempt to bundle together more functionality instead of allowing users to compose a functionality that comes from different packages. For example, perhaps a certain visualization of a model result is particularly useful, It is not easy to create from scratch, And virtually everyone using the model, will desire to see the model output visualized that way. Instead of relying on the user to install a separate visualization package and develop the visualization themselves, it could make sense to bundle visualization functionality with a model that is otherwise unconcerned with graphical capabilities.\nIn general, though it is preferred to try to loosely couple systems, you can pick and choose which components you use and that those components work well together.",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Higher Levels of Abstraction</span>"
    ]
  },
  {
    "objectID": "patterns-abstraction.html#interfaces",
    "href": "patterns-abstraction.html#interfaces",
    "title": "8  Higher Levels of Abstraction",
    "section": "8.4 Interfaces",
    "text": "8.4 Interfaces\nInterfaces are the boundary between different encapsulated abstractions. The user-facing interface is the set of functionality and details that the user of the package or model must consider, which is separate from the intermediate variables, logic, and complexity that may be contained within.\n\n\n\n\n\n\nExample of an interface\n\n\n\nWhen looking up a ticker for a market quote, one need not be mindful of the underlying realtime databases, networking, rendering text to the screen, memory management, etc. The interface is “put in symbol, get out number”. By design there are multiple layers of interfaces and abstractions that are used but the financial modeler need only be actively concerned about the points that he or she comes in contact with, not the entire chain of complexity.\n\n\nFor a financial model this might mean that there is an interface for bonds, or there is an interface for interest-rate swaps. There may be a different interface for calculating risk metrics or visualizing the results.\nFinancial model this might mean that there is an interface for bonds, or there is an interface for interest-rate swaps. There may be a different interface for calculating risk metrics or visualizing the results. A better system design will separate the concern of visualizing output from the mechanics of a fixed income contract. This is what it means to put boundaries on different parts of a models logic. One of the easiest places to see this is with the available open source packages. There are packages available for visualizations, data frames, file, storage, statistical analysis, etc. for many of these it’s easy to see where the natural boundary lies.\nHowever, it’s often difficult to find where to draw lines within financial models. For example, should bonds and interest-rate swaps be in separate packages? Or both part of a broader fixed income package? This is where much of the art and domain expertise of the financial professional comes to bear in modeling. There would be no way for a pure software engineer to think about the right design for the system without understanding how underlying components share, similarities or differences and how those components interact.\n\n8.4.1 Defining Good Interfaces\nA well-designed interface should follow these principles:\n\nBe minimal and focused. The interface should provide only the essential functionality needed, without unnecessary clutter or features. This makes the interface easier to understand and and faciliates building the necessary complexity through digestable, composable components.\nBe consistent and intuitive. The interface should use consistent naming conventions, parameter orders, and behaviors. It should match the user’s mental model and expectations.\nHide implementation details. The interface should abstract away the internal complexity and expose only what the user needs to know. This of details allows the implementation to change without affecting users of the interface.\nBe documented and contractual. The interface should clearly specify what inputs it expects and what outputs or behaviors it provides. It forms a contract between the implementation and the users.\nBe testable. A good interface allows the functionality to be easily tested through the public interface, without needing to access internal details.\n\n\n\n8.4.2 Interfaces: A Financial Modeling Case Study\nAs a case study, we’ll look at the FinanceModels.jl and related packages to discuss some of the background and design choices that went into the functionality. This suite was written by one of the authors and is publically available as set of installable Julia packages.\n\n8.4.2.1 Background\nIn actuarial work, it is common to need to work with interest rate and bond yield curves to determine current forward rates, estimates of the shape of future yield curves, or discount a series of cashflows to determine a present value. Determining things like “given a par yield curve, what’s the implied discount factor for a cashflow at time 10” or “what is the 10 year BBB public corporate rate implied by the current curve in five years’ time” is cumbersome at best in a spreadsheet.\nFor example, to determine the answer to the first one (“a discount factor for time 10”) actually requires quite a bit of detail and assumption to derive:\n\nReference market data and a specification for how that market data should be interpreted. For example, if given the rate 0.05 for time 10, quoted as a continuous rate or annual effective? Is that a par rate, a zero-coupon bond (spot) rate, or a one-year-forward rate from time 10?\nSmoothing, interpolation, or extrapolation for noisy or sparse data. Should the rates be bootstrapped or fit to a parametrically specified curve?\n\nThis is the type of complexity that we wish to save the user from needing to keep front of mind when the primary goal is, e.g., valuation of a stream of riskless life insurance payments, which might look like this:\nrisk_free_rates = [0.05,0.06,...0.06]\ntenors = [1/12,3/12,...30]\nyield_curve = Yields.Par(risk_free_rates,tenors)\n\ncashflow_vector = [1e6,3e6,...,1e3]\npresent_value(yield_curve,cashflow_vector)\nThis is very clear from the variable and function names what the purpose and steps in the analysis are. Imagine starting with rates and cashflows in a spreadsheet, needing to perform the bootstrapping, interpolation, and discounting before getting to the simple present value sought in the analysis. What can be, with the right abstractions, distilled into five lines of code would take hundreds of cells in a spreadsheet. Providing abstractions like this at the hand of financial modelers is a productivity multiplier.\n\n\n8.4.2.2 Initial Versions\nThere were two main abstractions to talk about from early versions of the packages.\n\n8.4.2.2.1 Rates\nUtilizing the benefit of the type system, it was decided that it would be most useful to represent rates not as simple floating point numbers (e.g. 0.05) but instead with dedicated types to distinguish between rate conventions. The abstract type CompoudingFrequency had two subtypes: Continuous and Periodic sop that a 5% rate compounded continuously versus an effective per period rate would be distinguished via Continuous(0.05) versus Periodic(0.05,1). The two could be converted between by extending the built-in Base.convert function.\nThis was useful because once rates were converted into Rates within the ecosystem, that data contained within itself charateristics that could distinguish how downstream functionality should treat the rates.\n\n\n8.4.2.2.2 Yield Curves\nAt first, only bootstrapping was supported as a method to construct curve objects. This required that there was only one rate given per time period (no noisy data) and only supported linear, quadratic, and cubic splines.\nFurther, there was a specific constructor for different common types of instruments. From the old documentation:\n\n\nYields.Zero(rates,maturities) using a vector of zero, or spot, rates\nYields.Forward(rates,maturities) using a vector of one-period\nYields.Constant(rate) takes a single constant rate for all times\nYields.Par(rates,maturities) takes a series of yields for securities priced at par.Assumes that maturities &lt;= 1 year do not pay coupons and that after one year, pays coupons with frequency equal to the CompoundingFrequency of the corresponding rate.\nYields.CMT(rates,maturities) takes the most commonly presented rate data (e.g. Treasury.gov) and bootstraps the curve given the combination of bills and bonds.\nYields.OIS(rates,maturities) takes the most commonly presented rate data for overnight swaps and bootstraps the curve.\n\n\nThis covered a lot of lightweight use-cases, but made a lot of implicit assumptions about how the given rates should be interpreted.\n\n\n\n8.4.2.3 The Birth of FinanceModels\nThere were a multiple of insights that led to a more flexible interface in more recent versions.\n\n\n\nA conceptual sketch of FinanceModels.jl components.\n\n\nFirst, realizing that yield curves were just a particular kind of model - one that used interest rates to discount cashflows. But you can have different kinds of models - such as Black-Scholes option valuation or a Monte Carlo valuation approach. Likewise, the cashflows need not simply be a vector of floating point values, and instead it could be the representation of a generic financial contract. As long as the model knew how to value it, an appropriate present value could be derived.\nWhere previously it was:\npresent_value(yield_curve,cashflow_vector)\nNow, it was\npresent_value(model,contract)\nSecond, that a model was simple some generic box that had been “fit” to previously observed prices for similar types of contracts we would be trying to value in the model. The combination of a contract and a price constituted a “quote” and with multiple quotes a model could be fit using various algorithms.\nWith these changes, the package that was originally called Yields.jl was renamed to FinanceModels.jl. The updated code from the earlier example now would e implemented like this:\nrisk_free_rates = [0.05,0.06,...0.06]\ntenors = [1/12,3/12,...30]\nquotes = ParYield.(risk_free_rates,tenors)\nmodel = fit(Spline.Cublic(),quotes,Fit.Bootstrap())\n\ncashflow_vector = [1e6,3e6,...,1e3]\npresent_value(model,cashflow_vector)\nIt’s slightly more verbose, but notice how much more powerful and extensible fit(Spline.Cubic(), quotes, Fit.Boostrap() is than Yields.Par(risk_free_rates,tenors) . The end result is the same, but now the same package and interface can clearly interchange other options, such as a NelsonSiegelSvennson curve instead of a spline. And the quotes could be a combination of observed bonds of different technical parameters (though still sharing characteristics which make it relevant for the model being constructed).\nThe same pattern also applies for option valuation, such as this example of vanilla euro options with an assumed constant volatility assumption:\n\n1a = Option.EuroCall(CommonEquity(), 1.0, 1.0)\nb = Option.EuroCall(CommonEquity(), 1.0, 2.0)\n\n2qs = [\n    Quote(0.0541, a),\n    Quote(0.072636, b),\n]\n\n3model = Equity.BlackScholesMerton(0.01, 0.02, Volatility.Constant())\n\n4m = fit(model, qs)\n\n5present_value(m,qs[1].instrument)\n\n1\n\nThe arguments to EuroCall are the underlying asset type, strike, and maturity time.\n\n2\n\nA vector of observed option prices.\n\n3\n\nA BSM model with a given risk free rate, dividend yield, and a to-be-fit constant volatility component.\n\n4\n\nFits the model and derives an approximate volatility of 0.15 .\n\n5\n\nValues the contract and in such a simple, noiseless model we recover the original price of 0.0541\n\n\nWith a consistent interface able to handle a wide variety of situations, the modeler is free to expand the model in new directions of analysis with the built in functionality allowing him or her to compose pieces together that was not possible with the less abstracted design. For example, the equity option example had no parallel when all of the available constructors were Yields.Zero or Yields.Par and would have required a completely from-scratch implementation with newly defined functions.\nFurther, and critically, the new design allows modelers to create their own models or contracts1 and extend the existing methods rather than needing to create their own: the function signature fit(model,quotes) handles a very wide variety of cases, as does present_value(model,contract).",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Higher Levels of Abstraction</span>"
    ]
  },
  {
    "objectID": "patterns-abstraction.html#macros-homoiconicity",
    "href": "patterns-abstraction.html#macros-homoiconicity",
    "title": "8  Higher Levels of Abstraction",
    "section": "8.5 Macros & Homoiconicity",
    "text": "8.5 Macros & Homoiconicity\nWe’ve talked about transforming data and restructuring logic in order to make the model more effective. We can go still deeper!(Or is it higher level?) We can actually abstract the process of writing code itself! This subject is a bit advanced, so we are simply going to introduce it because you will likely find many convenient instances of it as a user even if you never find a need to implement this yourself.\nHomoiconicity refers to the property of a programming language where the language’s code can be represented and manipulated as a data structure in the language itself. In other words, the code is data and can be treated as such. This enables powerful metaprogramming (i.e. code that writes other code) capabilities, where code can be generated or transformed during the compilation process.\nMacros are a metaprogramming feature that leverage homoiconicity in Julia. They allow the programmer to write code that generates or manipulates other code at compile-time. Macros take code as input, transform it based on certain rules or patterns, and return the modified code which then gets compiled.\nFor example, a built-in macro is @time which will measure the elapsed runtime for a piece of code2.\n@time exp(rand())\nWill effectively expand to:\nt0 = time_ns()\nvalue = exp(rand())\nt1 = time_ns()\nprintln(\"elapsed time: \", (t1-t0)/1e9, \" seconds\")\nvalue\nHere it is when we run it:\n\n@time exp(rand())\n\n  0.000004 seconds\n\n\n2.585074682367439\n\n\n\n8.5.1 Metaprogramming in Financial Modeling\nIn the context of financial modeling, macros can be used to simplify repetitive or complex code patterns, enforce certain conventions or constraints, or generate code based on data or configuration.\nHere are a few potential use cases of macros in financial modeling. Again, these are more advanced use-cases but knowing that these paths exist may benefit your work in the future.\n\nDefining custom DSLs (Domain-Specific Languages): Macros can be used to create expressive and concise DSLs tailored to financial modeling. For example, a macro could allow defining financial contracts using a syntax closer to the domain language, which then gets expanded into the underlying implementation code.\nAutomating boilerplate code: Macros can help reduce code duplication by generating common patterns or boilerplate code. This can include generating accessor functions3, constructors, or serialization logic based on type definitions.\nEnforcing conventions and constraints: Macros can be used to enforce coding conventions, such as naming rules or type checks, by automatically transforming code that doesn’t adhere to the conventions. They can also be used to add runtime assertions or checks based on certain conditions.\nOptimizing performance: Macros can be used to perform code optimizations at compile-time. For example, a macro could unroll loops, inline functions, or specialize generic code based on specific types or parameters, resulting in more efficient runtime code.\nGenerating code from data: Macros can be used to generate code based on external data or configuration files. For example, a macro could read a specification file and generate the corresponding financial contract types and functions.\n\n\n\n8.5.2 Commonly Encountered Macros\n\nUseful macros for modeling work. There are others related to parallelism which will be covered in ?sec-parallelization.\n\n\n\n\n\n\nMacro\nDescription\n\n\n\n\nBenchmarkTools.@benchmark\nRuns the given expression multiple times, collecting timing and memory allocation statistics. Useful for benchmarking and performance analysis.\n\n\nBenchmarkTools.@btime\nSimilar to @benchmark, but focuses on the minimum execution time and provides a more concise output.\n\n\n@edit\nOpens the source code of a function or module in an editor for inspection or modification.\n\n\n@which\nDisplays the method that would be called for a given function call, helping to understand method dispatch.\n\n\n@code_warntype\nShows the type inference results for a given function call, highlighting any type instabilities or performance issues.\n\n\n@info, @warn, @error\nUsed for logging messages at different severity levels (info, warning, error) during program execution.\n\n\n@assert\nAsserts that a given condition is true, throwing an error if the condition is false. Useful for runtime checks and debugging.\n\n\n@view, @views\nAccess a subset of an array without copying the data in that slice. @views applies to all array slicing operations within the expressions that follow it.\n\n\nTest.@test, Test.@testset\nUsed for defining unit tests. @test checks that a condition is true, while @testset groups related tests together.\n\n\n@raw\nEncloses a string literal, disabling string interpolation and escape sequences. Useful for writing raw string data. This is especially helpful when working with filepaths where the \\ in Windows paths otherwise needs to be escaped with a leading slash (e.g. \\\\ ).\n\n\n@fastmath\nEnables aggressive floating-point optimizations within a block, potentially sacrificing strict IEEE compliance for performance.\n\n\n@inbounds\nDisables bounds checking for array accesses within a block, improving performance but removing safety checks.\n\n\n@inline\nSuggests to the compiler that a function should be inlined at its call sites, potentially improving performance by reducing function call overhead.",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Higher Levels of Abstraction</span>"
    ]
  },
  {
    "objectID": "patterns-abstraction.html#footnotes",
    "href": "patterns-abstraction.html#footnotes",
    "title": "8  Higher Levels of Abstraction",
    "section": "",
    "text": "And projections, which is handled by defining a ProjectionKind , such as a cashflow or accounting basis. This topic is covered in more detail in the FinanceModels.jl documentation.↩︎\n(time?) is a simple, built-in function. For true benchmarking purposes, see Section 30.1.↩︎\nAccessor functions are useful when working with nested data structures. For example, if you have a struct within a struct and want to conveniently access an inner structs field.↩︎",
    "crumbs": [
      "Conceptual Foundations: Abstractions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Higher Levels of Abstraction</span>"
    ]
  },
  {
    "objectID": "hardware.html",
    "href": "hardware.html",
    "title": "9  Hardware and Its Implications",
    "section": "",
    "text": "9.1 In this section\nA discussion of why a cursory understanding of modern computing hardware and architecture is important for making the right design decisions within a modeling context. Stack vs heap allocations, pointers, and bit types. A discussion of parallelism and the different kinds of parallelism.",
    "crumbs": [
      "Conceptual Foundations: Learning from Related Disciplines",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Hardware and Its Implications</span>"
    ]
  },
  {
    "objectID": "elements-of-compsci.html",
    "href": "elements-of-compsci.html",
    "title": "10  Elements of Computer Science",
    "section": "",
    "text": "10.1 In this section\nAdapting computer science concepts to work for financial professionals. Concepts like computability, computational complexity, the language of algorithms and problem solving.",
    "crumbs": [
      "Conceptual Foundations: Learning from Related Disciplines",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Elements of Computer Science</span>"
    ]
  },
  {
    "objectID": "elements-of-compsci.html#computer-science-for-financial-professionals",
    "href": "elements-of-compsci.html#computer-science-for-financial-professionals",
    "title": "10  Elements of Computer Science",
    "section": "10.2 Computer Science for Financial Professionals",
    "text": "10.2 Computer Science for Financial Professionals\nComputer science as a term can be a bit misleading because of the overwhelming association with the physical desktop or laptop machines that we call “computers”. The discipline of computer science is much richer than consumer electronics: at it’s core, computer science concerns itself with areas of reserach and answering tough questions:\n\nAlgorithms and Optimization. How can a problem be solved efficiently? How can that problem be solved at all? Given constraints, how can one find an optimal solution?\nInformation Theory. Given limited data, what can be known or inferred from it?\nTheory of Computation. What sorts of questions are even answerable? Is an answer easy to computer or will resolving it require more resources than the entire known universe? Will a computation ever stop calculating?\nData Structures. How to encode, store, and use data? How does that data relate to each other and what are the trade-offs between different representations of that data?\n\nFor a reader in the twenty-first century we hope that’s it’s patently obvious how impactful the applied computer science has been as an end-user of the internet, artificial intelligence, computational photography, safety control systems, etc., etc. have been to our lives. It is a testament to the utlity of being able to harness some of the ideas of this science is. Many of the most impactful advances occur at the boundary between two disciplines. It’s here in this chapter that we desire to bring together the financial discipline together with computer science and to provide the financial practitioner with the language and concepts to leverage some of computer science’s most relevant ideas.\nIn this section, we will refer back to a problem called the travelling salesperson problem (TSP).",
    "crumbs": [
      "Conceptual Foundations: Learning from Related Disciplines",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Elements of Computer Science</span>"
    ]
  },
  {
    "objectID": "elements-of-compsci.html#algorithms-complexity",
    "href": "elements-of-compsci.html#algorithms-complexity",
    "title": "10  Elements of Computer Science",
    "section": "10.3 Algorithms & Complexity",
    "text": "10.3 Algorithms & Complexity\nAlgorithms is a general term for a process that transforms an input to an output. It’s the dirty, down-to-earn implementation of a mathemetical function or process. Further, we should indicate that a process needs to be specified in sufficient detail to be able to call itself an algorithm versus a heuristic which does not indicate with enough detail how the process would unfold.\n\n10.3.1 Computational Complexity\nWe can characterize the computational complexity of a problem by looking at how long an algorithm takes to complete a task when given an input of size \\(n\\). We can then compare two approaches to see which is computationally less complex for a given \\(n\\).\n\nNote that computational complexity isn’t quite the same as how fast an algorithm will run on your computer, but it’s a very good guide. Modern computer architectures can sometimes execute multiple instructions in a single cycle of the CPU making an algorithm that is, on paper, slower than another actually run faster in practice. Additionally, sometimes algorithms are able to substantially limit the number of computations to be performed, at the expense of using a lot more memory and thereby trading CPU usage with RAM usage.\nYou can think of computational complexity as a measure of how much work is to be performed. Sometimes the computer is able to perform certain kinds of work more efficiently.\nFurther, when we analyze an algorithm recall that ultimately our code gets translated into instructions for the computer hardware. Some instructions are implemented in a way that for any type of number (e.g. floating point), it doesn’t matter if the number is 1.0 or 0.41582574300044717, the operation will take the exact same time and number of instructions to execute (e.g. for the addition operation).\nSometimes a higher level operation is implemented in a way that takes many machine instructions. For exmaple, division instructions may require many CPU cycles when compared to multiplication or division. Sometimes this is an important distiction and sometimes not, but for this book we will ignore this level of analysis.\n\n\n10.3.1.1 Example: Sum of Consecutive Integers\nTake for example the problem of determining the sum of integers from \\(1\\) to \\(n\\). We will explore three different algorithms and the associated computational complexity for them.\n\n\n10.3.1.2 Constant Time\nA mathematical proof can show a simple formula for the result. This allows us to compute the answer in constant time, which means that for any \\(n\\), our algorithm is essentially the same amount of work.\n\nnsum_constant(n) = n * (n + 1) ÷ 2\n\nnsum_constant (generic function with 1 method)\n\n\nIn this we see that we perform three operations: a mulitiplication, a sum, and a division, no matter what n is. If n is 10_000_000 we’d expect this to complete in about a single unit of time.\n\n\n10.3.1.3 Linear Time\nThis algorithm performs a number of operations which grows in proportion with \\(n\\) by individually summing up each element in \\(1\\) through \\(n\\):\n\nfunction nsum_linear(n)\n    result = 0\n    for i in 1:n\n        result += i\n    end\n\n    result\nend\n\nnsum_linear (generic function with 1 method)\n\n\nIf \\(n\\) were 10_000_000, we’d expect it to run with roughly 10 million operations, or about 3 million times as many operations as the constant time version. We can say that this version of the alogrithm will take approximately \\(n\\) steps to complete.\n\n\n10.3.1.4 Quadratic Time\nWhat if we were less efficient, and instead said that the operation \\(n + 42\\) was to be implemented not as the basic addition of two numbers, but that we should add one to \\(n\\) forty-two times? That is, we’ll see that we add a second loop which increments our result by a unit instead of simply adding the current i to the running total result:\n\nfunction nsum_quadratic(n)\n    result = 0\n1    for i in 1:n\n2        for j in 1:i\n            result += 1\n        end\n    end\n\n    result\nend\n\n\n1\n\nThe outer loop with iterator i.\n\n2\n\nThe inner loop with iterator j.\n\n\n\n\nnsum_quadratic (generic function with 1 method)\n\n\nBreaking down the steps:\n\nWhen i is 1 there is 1 addition in the inner loop\nWhen i is 2 there are 2 additions in the inner loop\n…\nWhen i is n there are n additions in the inner loop\n\nTherefore, this computation takes \\(1 + ... + (n-2) + (n-1) + n\\) steps to complete. We actually know that this simplifies down to our constant time formula \\(n * (n + 1) ÷ 2\\) or \\(n^2 + n ÷ 2\\) steps to complete.\n\n\n10.3.1.5 Comparison\n\n10.3.1.5.1 Big-O Notation\nWe can categorize the above implementations using a convention called Big-O Notation1 which is a way of distilling and classifying computational complexity. We characterize the algorithms by the most significant term in the total number of operations. Table 10.1 shows for the examples constructed above what the description, order, and order of magnitude complexity is.\n\n\n\nTable 10.1: Complexity comparison for the three sample cases of summing integers from \\(1\\) to \\(n\\).\n\n\n\n\n\n\n\n\n\n\n\n\nFunction\nComputational Cost\nComplexity Description\nBig-O Order\nSteps (\\(n=10,000\\))\n\n\n\n\nnsum_constant\nfixed\nConstant\n\\(O(1)\\)\n~1\n\n\nnsum_linear\n\\(n\\)\nLinear\n\\(O(n)\\)\n~10,000\n\n\nnsum_quadratic\n\\(n^2 + n ÷ 2\\)\nQuadratic\n\\(O(n^2)\\)\n~100,000,000\n\n\n\n\n\n\nTable 10.2 shows a comparison of a more extended set of complexity levels. For the most complex categories of problems, the cost to compute grows so fast that it boggles the mind. What sorts of problems fall into the most complex categories? \\(O(2^n)\\), or exponential complexity, examples include the traveling salesman problem if solved with dynamic programming or the recursive approach to calculating the \\(nth\\) Fibbonnaci number. The beastly \\(O(n!)\\) algorithms include brute force solving the traveling salesman problem or enumerating all partitions of a set. In financial modeling, we may encounter these sorts of problems in portfolio optimization (using the brute-force approach of testing every potential combination assets to optimize a portfolio).\n\n\n\nTable 10.2: Different Big-O Orders of Complexity\n\n\n\n\n\n\n\n\n\n\n\n\nBig-O Order\nDescription\n\\(n=10\\)\n\\(n=1,000\\)\n\\(n=1,000,000\\)\n\n\n\n\n\\(O(1)\\)\nConstant Time\n1\n1\n1\n\n\n\\(O(n)\\)\nLinear Time\n10\n1,000\n1,000,000\n\n\n\\(O(n^2)\\)\nQuadratic Time\n100\n1,000,000\n10^12\n\n\n\\(O(log(n))\\)\nLogarithmic Time\n3\n7\n14\n\n\n\\(O(n\\times log(n))\\)\nLinearithmic Time\n30\n7,000\n14,000,000\n\n\n\\(O(2^n)\\)\nExponential Time\n1,024\n~10^300\n~10^301029\n\n\n\\(O(n!)\\)\nFactorial Time\n3,628,800\n~10^2567\n~10^5565708\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe care only about the most significant term because when \\(n\\) is large, the most significant term tends to dominate. For example, in our quadratic time example which has \\(n^2 + n ÷ 2\\) steps, if n is a large number like 10 million, then we see that it will result in:\n\\[\nn^2 + n ÷ 2\n(10^6)^2 + 10^6 ÷ 2\n(10^12) + 5^6\n\\]\n\\(10^12\\) is significantly more important than \\(5^6\\) (sixty-four million times as important, to be precise).\nConversely, if n is small then we don’t really care about computational complexity in general. This is why Big-O notation reduces the problem down to only the most significant complexity cost term.\n\n\n\n\n10.3.1.5.2 Empirical Results\n\nusing BenchmarkTools\n@btime nsum_constant(10_000)\n\n  0.708 ns (0 allocations: 0 bytes)\n\n\n50005000\n\n\n\n@btime nsum_linear(10_000)\n\n  1.250 ns (0 allocations: 0 bytes)\n\n\n50005000\n\n\n\n@btime nsum_quadratic(10_000)\n\n  2.398 μs (0 allocations: 0 bytes)\n\n\n50005000\n\n\nThe preceding examples of constant, linear, and exponential times are conceptually correct but if we try to run them in practice we see that the description doesn’t seem to hold at all for the linear time version, as it runs as quickly as the constant time version.\nWhat happened was that the compiler was able to understand and optimize the linear version such that it effectively transformed it into the constant time version and avoid the iterative summation that we had written. For examples that are simple enough to use as a teaching problem, the compiler can often optimize different written code down to the same efficient machine code (this is the same Triangular Number optimization we saw in Section 5.5.3.4).\n\n\n\n\n10.3.2 Expected versus worst-case complexity\nAnother consideration is that there may be one approach which performs better in the majority of cases, at the expense of having very poor performance in specific cases. Sometimes we may risk those high cost cases if we expect the benefit to be worthwhile on the rest of the problem set.\n\n\n10.3.3 Complexity: Takeaways\nThe idea of algorithmic complexity is important because it grounds us in the harsh truth that some problems are very difficult to compute. It’s in these cases that a lot of the creativity and domain specific heuristics can become the foremost consideration. We must remember to be thoughful about the design of our models and when searching for additional performance to look for the loops-within-loops or combinatorical explosions. It’s often at this level, rather than micro-optimizations, that you can transform the performance of the overall model (unless the fundamental complexity of the problem at hand forbids it).",
    "crumbs": [
      "Conceptual Foundations: Learning from Related Disciplines",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Elements of Computer Science</span>"
    ]
  },
  {
    "objectID": "elements-of-compsci.html#data-structures",
    "href": "elements-of-compsci.html#data-structures",
    "title": "10  Elements of Computer Science",
    "section": "10.4 Data Structures",
    "text": "10.4 Data Structures\nData structures is the art and science of how to represent data in discrete objects. There are many common kinds and many specialized sub-kinds, and we will describe some of the most common ones here. Julia has many data structures available in the Base library, but an extensive collection of other data structures can be found in the DataStructures.jl package.\n\n10.4.1 Arrays\nAn array is a contiguous block of memory containing elements of the same type, accessed via integer indices. Arrays have fast random access and are the fastest data structure for linear/iterated access of data.\nIn Julia, an array is a very common data structure and is implmemented with a simple declaration, such as:\nx = [1,2,3]\nIn memory, the integers are stored as consecutive bits representing the integer values of 1, 2, and 3, and would look like this (with the different integers shown on new lines for clarity):\n0000000000000000000000000000000000000000000000000000000000000001\n0000000000000000000000000000000000000000000000000000000000000010\n0000000000000000000000000000000000000000000000000000000000000011\nThis is great for accessing the values one-by-one or in consecutive groups, but it’s not efficient if values need to be inserted in between. For example, if we wanted to insert 0 between the 1 and 2 in x, then we’d need to overwrite the second position in the array, ask the operating system to allocate more memory2, and re-write the bytes that come after our new value. Inserting values at the end (push!(array, value)) is usually fast unless more memory needs to be allocated.\n\n\n10.4.2 Linked Lists\nA linked list is a chain of nodes where each node contains a value and a pointer to the next node. Linked lists allow for efficient insertion and deletion but slower random access compared to arrays.\nIn Julia, a simple linked list node could be implemented as:\nmutable struct Node\n    value::Any\n1    next::Union{Node, Nothing}\nend\n\nz = Node(3,Nothing)\ny = Node(2,z)\nx = Node(1,y)\n\n1\n\nHere, ‘Nothing’ would represent the end of the linked list.\n\n\nInserting a new node between existing nodes is efficient - if we wanted to inser a new node between the ones with value 2 and 3, we could do this:\na = Node(0,z) # &lt;1&gt; Create a new `Node` with `next` equal to `z`\ny.next = a # &lt;2&gt; Set the reference to `next` in `y` to be the new `Node` `a`.\nHowever, accessing the nth element requires traversing the list from the beginning, making it O(n) time complexity for random access. Also, if tyou have an intermediate node such as y, y itself does not know about x so there’s no way to move ‘up’ the list to get to previous values.\n\n\n10.4.3 Records/Structs\nAn aggregate of named fields, typically of fixed size and sequence. Records group related data together. We’ve encountered structs in Section 5.5.7, but here we’ll add that simple structs with primitive fields can themselves be represented without creating pointers to the data stored:\n\nstruct SimpleBond\n    id::Int\n    par::Float64\nend\n\nstruct LessSimpleBond\n    id::String\n    par::Float64\nend\n\na = SimpleBond(1, 100.0)\nb = LessSimpleBond(\"1\", 100.0)\nisbits(a), isbits(b)\n\n(true, false)\n\n\nBecause a is comprised of simple elements, it can be represented as a contiguous set of bits in memory. It would look something like this in memory:\n0000000000000000000000000000000000000000000000000000000000000001\n0100000001011001000000000000000000000000000000000000000000000000\n\n1\n\nThe bits of 1\n\n2\n\nThe bits of 100.0\n\n\nIn contrast, the LessSimpleBond uses a String to represent the ID of the bond. Strings are essentially arrays of containers, and the arrays themselves are mutable containers which is by definition not a constant set of bits. In memory, b would look like:\n.... a pointer ...\n0100000001011001000000000000000000000000000000000000000000000000\n\n1\n\na pointer/reference to the array of characters that comprise the string ID\n\n2\n\nThe bits of 100.0\n\n\nIn performance critical code, having data that is represented with simple bits instead of references/pointers can be much faster (see Chapter 21 for an example).\n\n\n\n\n\n\nNote\n\n\n\nFor many mutable types, there are immutable, bits-types alternatives. For example:\n\nArrays have a StaticArray counterpart (from the StaticArrays.jl package).\nStrings have InlineStrings (from the InlineStrings.jl package) which use fixed-width representations of strings.\n\nThe downsides to the immutable alternatives (other than the loss of potentially desired flexibly that mutability provides) are that they can be harder on the compiler (more upfront compilation cost) to handle the specialized cases involved.\n\n\n\n\n10.4.4 Dictionaries (Hash Tables)\n\n10.4.4.1 Hashes and Hash Functions.\nHashes are the result of a hash function that maps arbitrary data to a fixed size value. It’s sort of a “one way” mapping to a simpler value which has the benefits of:\n\nOne way so that if someone knows the hashed value, it’s very difficult to guess what the original value was. This is most useful in cryptographic and security applications.\nCreating (probabilistically) unique IDs for a given set of data.\n\nFor example, we can calculate a type of has called an SHA hash on any data:\n\nimport SHA\nlet\n    a = SHA.sha256(\"hello world\") |&gt; bytes2hex\n    b = SHA.sha256(rand(UInt8, 10^6)) |&gt; bytes2hex\n    println(a)\n    println(b)\nend\n\nb94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9\n756291d8b685a1bb2547a0c01828b6458e34b3f713741b9ac389621311a1869a\n\n\nWe can easily verify that the sha256 hash of \"hello world\" is the same each time, but it’s virtually impossible to guess \"hello world\" if we are just given the resulting hash. This is the premise of trying to “crack” a password when the stored password hash is stolen.\nOne way to check if two set of data are the same is to compute the hash and see if the resulting hashes are equal. For example, maybe you want to see if two data files with different names contain the same data - comparing the hashes is a sure way to determine if they contain the same data.\n\n\n10.4.4.2 Dictionaries\nDictionaries map a key to a value. More specifically, they use the hash of a key to store a reference to the value.\nDictionaries offer constant-time average case access but must handle potential collisions of keys (generally, the more robust the collision handling means higher fixed cost for access).\n\n\n\n10.4.5 Graphs\nA graph is a collection of nodes (also called vertices) connected by edges to represent relationships or connections between entities. Graphs are versatile data structures that can model various real-world scenarios such as social networks, transportation systems, or computer networks.\nIn Julia, a simple graph could be implemented using a dictionary where keys are nodes and values are lists of connected nodes:\nstruct Graph\n    nodes::Dict{Any, Vector{Any}}\nend\n\nfunction add_edge!(graph::Graph, node1, node2)\n    push!(get!(graph.nodes, node1, []), node2)\n    push!(get!(graph.nodes, node2, []), node1)\nend\n\ng = Graph(Dict())\nadd_edge!(g, 1, 2)\nadd_edge!(g, 2, 3)\nadd_edge!(g, 1, 3)\nThis implementation represents an undirected graph. For a directed graph, you would only add the edge in one direction.\nGraphs can be traversed using various algorithms such as depth-first search (DFS) or breadth-first search (BFS). These traversals are useful for finding paths, detecting cycles, or exploring connected components.\nFor more advanced graph operations, the Graphs.jl package provides a comprehensive set of tools for working with graphs in Julia.\n\n\n10.4.6 Trees\nA tree is a hierarchical data structure with a root node and child subtrees. Each node in a tree can have zero or more child nodes, and every node (except the root) has exactly one parent node. Trees are widely used for representing hierarchical relationships, organizing data for efficient searching and sorting, and in various algorithms.\nA simple binary tree node in Julia could be implemented as:\nmutable struct TreeNode\n    value::Any\n    left::Union{TreeNode, Nothing}\n    right::Union{TreeNode, Nothing}\nend\n\n# Creating a simple binary tree\nroot = TreeNode(1, \n    TreeNode(2, \n        TreeNode(4, nothing, nothing), \n        TreeNode(5, nothing, nothing)\n    ), \n    TreeNode(3, \n        nothing, \n        TreeNode(6, nothing, nothing)\n    )\n)\nTrees have various specialized forms, each with its own properties and use cases:\n\nBinary Search Trees (BST): Each node has at most two children, with all left descendants less than the current node, and all right descendants greater.\nAVL Trees: Self-balancing binary search trees, ensuring that the heights of the two child subtrees of any node differ by at most one.\nB-trees: Generalization of binary search trees, allowing nodes to have more than two children. Commonly used in databases and file systems.\nTrie (Prefix Tree): Used for efficient retrieval of keys in a dataset of strings. Each node represents a common prefix of some keys.\n\nTrees support efficient operations like insertion, deletion, and searching, often with \\(O(log n)\\) time complexity for balanced trees. They are fundamental in many algorithms and data structures, including heaps, syntax trees in compilers, and decision trees in machine learning.\n\n\n10.4.7 Data Structures Conclusion\nData structures have strengths and weakness depending on whether you want to priortize computational efficiency, memory (space) efficiency, code simplicity, and/or mutability. Due to the complexity of real world modeling needs, it can be the case that different representations of the data are more natural or more efficient for the use case at hand.",
    "crumbs": [
      "Conceptual Foundations: Learning from Related Disciplines",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Elements of Computer Science</span>"
    ]
  },
  {
    "objectID": "elements-of-compsci.html#formal-verification",
    "href": "elements-of-compsci.html#formal-verification",
    "title": "10  Elements of Computer Science",
    "section": "10.5 Formal Verification",
    "text": "10.5 Formal Verification\nFormal verification is a technique used to prove or disprove the correctness of algorithms with respect to a certain formal specification or property. In essence, it’s a mathematical approach to ensuring that a system behaves exactly as intended under all possible conditions.\n\n10.5.1 Basic Concept\nIn formal verification, we use mathematical methods to:\n\nCreate a formal model of the system\nSpecify the desired properties or behaviors\nProve that the model satisfies these properties\n\nThis process can be automated using specialized software tools called theorem provers or model checkers.\n\n\n10.5.2 Formal Verification in Practice\nIt sounds like the perfect risk management and regulatory technique: prove that the system works exactly as intented. However, there has been very limited deployment of formal verification in industry. This is for serveral reasons:\n\nIncomplete Coverage: It’s often impractical to formally verify entire large-scale financial systems. Verification, if at all, is typically limited to critical components.\nIncomplete Specification: Actually reasoning through how the system should behave in all scenarios requires actually contemplating mathematically complete and rigorous possiblilities that could occur.\nModel-Reality Gap: The formal model may not perfectly represent the real-world system, especially in finance where market behavior can be unpredictable.\nChanging Requirements: Financial regulations and market conditions change rapidly, potentially outdating formal verifications.\nPerformance Trade-offs: Systems designed for easy formal verification might sacrifice performance or flexibility.\nCost: The process can be expensive in terms of time and specialized labor.\n\n\n\n10.5.3 Related Topics\n\n10.5.3.1 Property Based Testing\nTesting will be discussed in more detail in Chapter 11, but an intermediate concept between Formal Verification and typical software testing is property-based testing, which tests for general rules instead of specific examples.\nFor example, a function which is associative (\\((a + b) + c = a + (b + c)\\)) or commutative (\\(a + b\\) = \\(b + c\\)) can be tested with simple examples like:\nusing Test\n\nmyadd(a,b) = a + b\n\n@test myadd(1,2) == myadd(2,1)\n@test myadd(myadd(1,2),3) == myadd(1,myadd(2,3))\nHowever, we really haven’t proven the associative and commutative properties in general. There are techniques to do this, which is a more comprehensive alternative to testing specific examples above. Packages like Supposition.jl provide functionality for this. Note that like Formal Verification, property-based testing is a more advanced topic.\n\n\n10.5.3.2 Fuzzing\nFuzzing is kind of like property based testing, but instead of testing general rules, we generalize the simple examples using randomness. For example, we could test the commutative property using random numbers instead, therefore statistically checking that the property holds:\n@testset for i in 1:10000\n    a = rand()\n    b = rand()\n\n    @test myadd(a,b) == myadd(b,a)\nend\nThis is a good advancement over the simple @test myadd(1,2) == myadd(2,1), in terms of checking the correctness of myadd, but it comes at the cost of more computational time and non-deterministic tests.",
    "crumbs": [
      "Conceptual Foundations: Learning from Related Disciplines",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Elements of Computer Science</span>"
    ]
  },
  {
    "objectID": "elements-of-compsci.html#footnotes",
    "href": "elements-of-compsci.html#footnotes",
    "title": "10  Elements of Computer Science",
    "section": "",
    "text": "“Big-O”, so named because of the “O” in used in \\(O(1)\\). \\(O(n)\\), etc. Not one of the sciences’ more creative names.↩︎\nIn practice, the operating system may have already allocated space for an array that’s larger than what the program is actually using so far, so this step may be ‘quick’ at times, while other times the operating system may actually need to extend the block of memory allocated to the array.↩︎",
    "crumbs": [
      "Conceptual Foundations: Learning from Related Disciplines",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Elements of Computer Science</span>"
    ]
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "11  Applying Software Engineering Principles",
    "section": "",
    "text": "11.1 In this section\nWe describe modern software engineering practices such as version control, testing, documentation, and pipelines which can be utlizied by the financial professional to make their own work more robust and automated. Data practices and workflow advice.",
    "crumbs": [
      "Conceptual Foundations: Learning from Related Disciplines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Applying Software Engineering Principles</span>"
    ]
  },
  {
    "objectID": "software.html#testing",
    "href": "software.html#testing",
    "title": "11  Applying Software Engineering Principles",
    "section": "11.2 Testing",
    "text": "11.2 Testing\nNote some more advanced testing topics in Section 10.5.3.",
    "crumbs": [
      "Conceptual Foundations: Learning from Related Disciplines",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Applying Software Engineering Principles</span>"
    ]
  },
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "12  Statistical Inference and Information Theory",
    "section": "",
    "text": "12.1 In This Chapter\nA brief introduction to information theory and its foundational role in statistics. Entropy and probability distributions. Bayes’ rule and model selection comparison via likelihoods. A brief tour of modern Bayesian statistics.",
    "crumbs": [
      "Conceptual Foundations: Learning from Related Disciplines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Statistical Inference and Information Theory</span>"
    ]
  },
  {
    "objectID": "statistics.html#information-theory",
    "href": "statistics.html#information-theory",
    "title": "12  Statistical Inference and Information Theory",
    "section": "12.2 Information Theory",
    "text": "12.2 Information Theory\nProbability, statistics, machine learning, signal processing, and even physics have a foundational link in information theory which is the description and analysis of how much useful data is contained within something.\nLet’s consider the following number that we encounter while reading a report which contains estimates of total amount of assets held. Unfortunately, for one reason or another one of the digits is not visible to you. Here’s what you can read, with the _ indicating that the digit is not visible:\n32,000,_00\nNow you probably already formed an opinion on what the missing number is, but let’s look at how we can quantify the analysis.\nGiven that we know the number was an estimate and the tendency of humans to like nice round numbers, our prior assumption for what the probability of the missing digit is may be something like the \\(p(x_i)\\) row of Table 12.1. We shall call the individual outcomes \\(x_i\\) and the overall set of probabilities \\(\\{x_0,x_1,...x_9\\}\\) is called \\(X\\).\nThe information content of an outcome, \\(h(x)\\) is measured in bits and defined as1:\n\\[\nh(x_i) = \\text{log}_2\\frac{1}{p(x_i)}\n\\tag{12.1}\\]\nSo if we were to find out that the missing digit were indeed 0, we have gained less information relative to our expectation than if the missing digit were anything other than 0 .\nWe can characterize the entire distribution \\(X\\) via the entropy, \\(H(X)\\), of a probability set is the ensemble’s average information content:\n\\[\nH(X) = \\sum{p(x_i)\\text{log}_2}\\frac{1}{p(x_i)}\n\\tag{12.2}\\]\nThe entropy \\(H(X)\\) of the presumed outcomes in Table 12.1 distribution of outcomes is \\(0.722 \\text{bits}\\).\n\n\n\nTable 12.1: Probability distribution of missing digit, knowing the human inclination to prefer round numbers when estimating.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\\(p(x_i)\\)\n.91\n.01\n.01\n.01\n.01\n.01\n.01\n.01\n.01\n.01\n\n\n\\(h(x_i)\\)\n0.136\n6.644\n6.644\n6.644\n6.644\n6.644\n6.644\n6.644\n6.644\n6.644\n\n\n\n\n\n\nNote that we have take a view on the probability distribution for the missing digit, and we’ll refer to this as the prior assumption (or just prior). This is an opinionated assumption, so what if we had another colleague who believed humans are completely rational and without bias for certain numbers. They would then be arguing for a prior assumed distribution consistent with Table 12.2.\nWith the uniform prior assumption, \\(H(X) = 3.322 \\text{bits}\\) and \\(h(x_i)\\) is also uniform. We will not prove it here, but a uniform probability over a set of outcomes is the highest entropy distribution that can be assumed.\n\n\n\nTable 12.2: Probability distribution of missing digit with uniform, maximal entropy for the assumed probability distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\\(p(x_i)\\)\n.10\n.10\n.10\n.10\n.10\n.10\n.10\n.10\n.10\n.10\n\n\n\\(h(x_i)\\)\n3.322\n3.322\n3.322\n3.322\n3.322\n3.322\n3.322\n3.322\n3.322\n3.322\n\n\n\n\n\n\nThe choice of prior assumption can significantly impact the interpretation and analysis of the missing information. If we have strong reasons to believe that the human bias prior is more appropriate given the context (e.g., knowing that the number is an estimate), then we would expect the missing digit to be ‘0’ with high probability. However, if we have no specific knowledge about the nature of the number and prefer to make a more conservative assumption, the uniform prior may be more suitable.\nIn real-world scenarios, the choice of prior assumptions often depends on domain knowledge, available data, and the specific problem at hand. It is important to carefully consider and justify the prior assumptions used in information-theoretic and statistical analyses.\n\n12.2.1 Example: Classificaiton\nIn this example, we will determine the optimal splits for a decision tree2 based on the information gained at each node in the tree.\n\nusing DataFrames\n\nemployed = [true, false, true, true, true, false, false, true]\ngood_credit = [true, true, false, true, false, false, false, true]\ndefault = [true, false, true, true, true, true, false, true]\ndefault_data = DataFrame(; employed, good_credit, default)\n\n[ Info: Precompiling DataFrames [a93c6f00-e57d-5684-b7b6-d8193f3e46c0]\n\n\n\n\nTable 12.3: Fictional data regarding loan attributes and whether or not a loan defaulted before it’s maturity.\n\n\n\n8×3 DataFrame\n\n\n\nRow\nemployed\ngood_credit\ndefault\n\n\n\nBool\nBool\nBool\n\n\n\n\n1\ntrue\ntrue\ntrue\n\n\n2\nfalse\ntrue\nfalse\n\n\n3\ntrue\nfalse\ntrue\n\n\n4\ntrue\ntrue\ntrue\n\n\n5\ntrue\nfalse\ntrue\n\n\n6\nfalse\nfalse\ntrue\n\n\n7\nfalse\nfalse\nfalse\n\n\n8\ntrue\ntrue\ntrue\n\n\n\n\n\n\n\n\n\nThe entropy of the default rate data is, per Equation 12.2:\n\nH0 = let\n    p1 = sum(default_data.default) / nrow(default_data)\n    p2 = 1 - p1\n    p1 * log2(1 / p1) + p2 * log(1 / p2)\nend\n\n0.6578517147391054\n\n\nOur goal is to determine which attribute (employed or good_credit) to use as the first split in the decision tree. We will decide this by calculating the information gain, which is the difference in entropy between the prior node and the candidate node. In our case we start with H0 as calculated above for the output variable default and calculate the difference in entropy between it and the average entropy of the data if we split on that node. Information gain, \\(IG(inputs, attributes)\\), is:\n\\[\n...\n\\]\nLet’s first consider splitting the tree based on the employed status. We will calculate the entropy of each subset: with employment and without employment.\nIf we split the data based on being employed, we’d get two sub-datasets:\n\ndf_employed = filter(:employed =&gt; ==(true), default_data)\n\n5×3 DataFrame\n\n\n\nRow\nemployed\ngood_credit\ndefault\n\n\n\nBool\nBool\nBool\n\n\n\n\n1\ntrue\ntrue\ntrue\n\n\n2\ntrue\nfalse\ntrue\n\n\n3\ntrue\ntrue\ntrue\n\n\n4\ntrue\nfalse\ntrue\n\n\n5\ntrue\ntrue\ntrue\n\n\n\n\n\n\nand\n\ndf_unemployed = filter(:employed =&gt; ==(false), default_data)\n\n3×3 DataFrame\n\n\n\nRow\nemployed\ngood_credit\ndefault\n\n\n\nBool\nBool\nBool\n\n\n\n\n1\nfalse\ntrue\nfalse\n\n\n2\nfalse\nfalse\ntrue\n\n\n3\nfalse\nfalse\nfalse\n\n\n\n\n\n\nlet’s call it’s entropy H_employed, which should be zero because there is no variability in the default outcome for this subset.\n\nH_employed = let\n    p1 = sum(df_employed.default) / nrow(df_employed)\n    p2 = 1 - p1\n    # p1 * log2(1 / p1) + p2 * log(1 / p2) \n1    p1 * log2(1 / p1) + 0\nend\n\n\n1\n\nIn the case of \\(p_i = 0\\) the value of \\(h\\) (the second term in the sum above) is taken to be \\(0\\), which is consistent with the \\(\\lim_{p\\to0^+}p\\log (p) = 0\\).\n\n\n\n\n0.0\n\n\nAnd the corresponding candidate leaf is H_unemployed:\n\nH_unemployed = let\n    p1 = sum(df_unemployed.default) / nrow(df_unemployed)\n    p2 = 1 - p1\n    p1 * log2(1 / p1) + p2 * log(1 / p2)\nend\n\n0.7986309056458281\n\n\nThe average of the two is weighted by the size of the data that would fall into each leaf:\n\nH1_employment = let\n    p_emp = nrow(df_employed) / nrow(default_data)\n    p_unemp = 1 - p_emp\n\n    p_emp * H_employed + p_unemp * H_unemployed\nend\n\n0.29948658961718555\n\n\nThe information gain for splitting the tree using employment status is the difference between the root entropy and the entropy of the employment split:\n\nIG_employment = H0 - H1_employment\n\n0.35836512512191987\n\n\nWe could repeat the analysis to determine the information gain if we were to split the tree based on having good credit. However, given that there are only two attributes we can already conclude that employed is a better attribute to split the data on. This is because the information gain of IG_employment (0.358) is the majority of the overall entropy H0 (0.658). Entropy is always additive and you cannot have negative entropy, therefore no other other attribute could have greater information gain. This also matches our intuition when looking at Table 12.3 as the eye can spot a higher correlation between employed and default than good_credit and default.\nThe above example demonstrates how we can use information theory to create more optimal inferences on data.\n\n\n12.2.2 Maxium Entropy Distributions\nWhy is information theory a useful concept? Many financial models are statistical in nature and concepts of randomness and entropy are foundational. For example, when trying to estimate parameter distributions or assume a distribution for a random process you can lean on information theory to use the most conservative choice: the distribution with the highest entropy given known constraints. These distributions are referred to as maximum entropy distributions. Some discussion of maximum entropy distributions in the context of risk assessment is available in an article by Duracz3. probability distributions and risk asses\n\n\n\nTable 12.4: Maximum Entropy Distributions and the conditions under which they are applicable.\n\n\n\n\n\n\n\n\n\n\n\nConstraint\nDiscrete Distribution\nContinuous Distribution\n\n\n\n\n\nBounded range\nUniform (discrete)\nUniform (continuous)\n\n\n\nBounded range (0 to 1) with information about the mean or variance\n\nBeta\n\n\n\nMean is finite, two possible values\nBinomial\n\n\n\n\nMean is finite and positive\nGeometric\nExponential\n\n\n\nMean is finite and range is &gt; zero\n\nGamma\n\n\n\nMean and Variance is finite\n\nGuassian (Normal)\n\n\n\nPositive and equal mean and variance\nPoisson\n\n\n\n\n\n\n\n\nThe distributions in Table 12.4 arise again and again in nature because of the second law of thermodynamics - nature likes to have constantly increasing entropy and therefore it should be no surprise (random) processes that maximize entropy pop up all over the place. As an example, let’s look at processes that behave like the Gaussian (Normal) distribution.\n\n12.2.2.1 Processes that give rise to certain distributions\nA random walk can be viewed as the cumulative impact of nudges pushing in opposite directions. This behavior culminates in the random, terminal position being able to be described by a Gaussian distribution. The center of a Gaussian distribution is “thick” because there are many more ways for the cumulative total nudges to mostly cancel out, while its increasingly rare to end up further and further from the starting point (mean). The distribution then spreads out as flat (randomly) as it can while still maintaining the constraint of having a given, finite variance. Any other continuous distribution that has the same mean and variance has lower entropy than the Guassian.\n\n\n\nTable 12.5: Underlying processes create typical probability distributions. That there is significant overlap with the distributions in Section 12.2.2 is not a coincidence.\n\n\n\n\n\n\n\n\n\n\nProcess\nDistribution of Data\nExamples\n\n\n\n\nMany additive pluses and minus that move an outcome in one dimension\nNormal\nSum of many dice rolls, errors in measurements, sample means (Central Limit Theorem)\n\n\nMany multiplicative pluses and minus that move an outcome in one dimension\nLog-normal\nIncomes, sizes of cities, stock prices\n\n\nWaiting times between independent events occurring at a constant average rate\nExponential\nTime between radioactive decay events, customer arrivals\n\n\nDiscrete trials each with the same probability of success, counting the number of successes\nBinomial\nCoin flips, defective items in a batch\n\n\nDiscrete trials each with the same probability of success, counting the number of trials until the first success\nGeometric\nNumber of job applications until getting hired\n\n\nContinuous trials each with the same probability of success, measuring the time until the first success\nExponential\nTime until a component fails, time until a sales call results in a sale\n\n\nWaiting time until the r-th event occurs in a Poisson process\nGamma\nTime until the 3rd customer arrives, time until the 5th defect occurs\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Distributions\n\n\n\nThere are a lot of specialized distributions. There are lists of distributions you can find online or in references such as Leemis and McQueston (2008) which has a full-page network diagram of the relationships.\nThe information-theoretic and Bayesian perspective on it is to eschew memorization of a bunch of special cases and statistical tests. If you pull up the aforementioned diagram in Leemis and McQueston (2008), you can see just a handful of distributions that have the most central roles in the universe of distributions. Many distributions are simply transformations, limiting instances, or otherwise special cases of a more fundamental distribution. Instead of trying to memorize a bunch of probability distributions, it’s better to think critically about:\n\nThe fundamental processes that give rise to the randomness.\nTranformations of the data to make it nicer to work with, such as translations, scaling, or other non-destructive changes.\n\nThen when you encounter a wacky dataset you don’t need to comb the depths of Wikipedia to find the perfect probability distributions.\n\n\n\n\n12.2.2.2 Additive and Multiplicative Processes\nTable 12.5 describes some examples, let us discuss further what it means to have a process that arises via an additive vs multiplicative effect4.\nAn outcome is additive if it’s the sum or difference of multiple independent processes. One of the simplest examples of this is rolling multiple dice and taking their sum. Or a random walk along the natural numbers wherein with equal probability you take a step left or right. The distribution of the position after \\(n\\) steps converges rapidly to a normal distribution. Another common one is when you are looking at the mean of a sample - since you are summing up the individual measurements you end up with a normal distribution (the Central Limit Theorem).\nHowever, many processes are multiplicative in nature. For example the population density of cities is distributed in a log-normal fashion. If we think about the factors that contribute to choice of place to live, we can see how these factors multiply: an attractive city might make someone 10% more likely to move, a city with water features 15% more likely, high crime 30% less likely, etc. These forces combine in a multiplicative way in the generative process of deciding where to move.\n\n\n\n\n\n\nTip 12.1: Logarithms\n\n\n\nThe logarithm of a geometric process transforms the outcomes into “log-space”. The information is the same, but is often a more convenient form for the analysis. That is, if:\n\\[\nY = x_1 \\times x_2 \\times \\ldots \\times x_i\n\\]\nThen,\n\\[\nlog(Y) = log(x_1) + log(x_2) + \\ldots + \\log(x_i)\n\\]\nThis is effectively the transformation that gives rise to the Normal versus Log-Normal distribution.\n\nBringing this back to the context of computational thinking:\nFirst, we should think about how to transform data or modeling outcomes into a more convenient format. The log transform doesn’t eliminate any information but may map the information into a shape that is easier for an optimizer or Monte Carlo simulation to explore.\nSecond, per Chapter 5, floating point math is a lossy transformation of real numbers into a digital computer representation. Some information (in the literal Shannon information sense) is lost when computing and this tends to be worst with very small real numbers, such as those we encounter frequently in probabilities and likelihoods. Logarithms map very small numbers into negative numbers that don’t encounter the same degree of truncation error that tiny numbers do\nThird, modern CPUs are generally much faster at adding or subtracting numbers than multiplying or dividing. Therefore working with the logarithm of processes may be computationally faster than the direct process itself.",
    "crumbs": [
      "Conceptual Foundations: Learning from Related Disciplines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Statistical Inference and Information Theory</span>"
    ]
  },
  {
    "objectID": "statistics.html#bayes-rule",
    "href": "statistics.html#bayes-rule",
    "title": "12  Statistical Inference and Information Theory",
    "section": "12.3 Bayes’ Rule",
    "text": "12.3 Bayes’ Rule\nThe minister and statistician Thomas Bayes derived a relationship of conditional probabilities that we today know as Bayes’ Rule, commonly written as:\n\\[\nP(H|D) = \\frac{P(D|H) \\times P(H)}{P(D)}\n\\]\nThe components of this are:\n\n\\(P(H∣D)\\) is the conditional probability of event \\(H\\) occurring given that \\(D\\) is true.\n\\(P(D∣H)\\) is the conditional probability of event \\(D\\) occurring given that \\(H\\) is true.\n\\(P(H)\\) is the prior probability of event \\(H\\).\n\\(P(D)\\) is the prior probability of event \\(D\\).\n\nIf we take the following:\n\n\\(D\\) is the available data\n\\(H\\) is our hypothesis\n\nThen we can draw conclusions about the probability of a hypothesis being true given the observed data. When thought about this way, Bayes’ rule is often described as:\n\\[\n\\text{posterior} = \\frac{\\text{likelihood} \\times \\text{prior}}{\\text{evidence}}\n\\]\nThis is a very useful framework, which we’ll return to more completely in Section 12.4. First, let’s look at combining information theory and Bayes’ rule in an applied example.\n\n12.3.1 Example: Model Selection via Likelihoods\nLet’s say that we have competing hypothesis about a data generating process, such as: “given a set of data representing risk outcomes, what distribution best fits the data”?\nWe can compare these models using Bayes’ rules by observing the following: Suppose we have two models, \\(H_1\\) and \\(H_2\\), and we want to compare their likelihoods given the observed data, D. We can use Bayes’ rule to calculate the posterior probability of each model: $$ P(H_1|D) = (P(D|H_1) * P(H_1)) / P(D)\nP(H_2|D) = (P(D|H_2) * P(H_2)) / P(D) $$\nWhere:\n\n\\(P(H_1|D)\\) and \\(P(H_2|D)\\) are the posterior probabilities of models \\(H_1\\) and \\(H_2\\), respectively, given the data \\(D\\).\n\\(P(D|H_1)\\) and \\(P(D|H_2)\\) are the likelihoods of the data \\(D\\) under models \\(H_1\\) and \\(H_2\\), respectively.\n\\(P(H_1)\\) and \\(P(H_2)\\) are the prior probabilities of models \\(H_1\\) and \\(H_2\\), respectively.\n\\(P(D)\\) is the marginal likelihood of the data, which serves as a normalizing constant.\n\nTo compare the likelihoods of the two models, we can calculate the ratio of their posterior probabilities, known as the Bayes factor, \\(BF\\):\n\\[\nBF = \\frac{P(H_1|D)}{P(H_2|D)}\n\\]\nSubstituting the expressions for the posterior probabilities from Bayes’ rule, we get:\n\\[\nBF = \\frac{P(D|H_1) \\times P(H_1)} {P(D|H_2) \\times P(H_2)}\n\\]\nThe marginal likelihood \\(P(D)\\) cancels out since it appears in both the numerator and denominator. If we assume equal prior probabilities for the models, i.e., \\(P(H_1)\\) = \\(P(H_2)\\), then the Bayes factor simplifies to the likelihood ratio: \\[\nBF = \\frac{P(D|H_1)}{P(D|H_2)}\n\\]\nThe interpretation of the Bayes factor is as follows:\n\nIf \\(BF &gt; 1\\), the data favor \\(H_1\\) over \\(H_2\\).\nIf \\(BF &lt; 1\\), the data favor \\(H_2\\) over \\(H_1\\).\nIf \\(BF = 1\\), the data do not provide evidence in favor of either model.\n\nIn practice, the likelihoods \\(P(D|H_1)\\) and \\(P(D|H_2)\\) are often calculated using the probability density or mass functions of the models, evaluated at the observed data points. The prior probabilities \\(P(H_1)\\) and \\(P(H_2)\\) can be assigned based on prior knowledge or assumptions about the models. By comparing the likelihoods of the models using the Bayes factor, we can quantify the relative support for each model given the observed data, while taking into account the prior probabilities of the models.\nAnother way of interpreting this is the more simplistic evaluation of which model has the higher likelihood given the data: this is simply a matter of comparing the magnitude of the likelihoods.\n\n\n\n\n\n\nNull Hypthothesis Statistical Test\n\n\n\nNull Hypothesis Statistical Tests (NHST) is the idea of trying to statistically support an alternative hypothesis over a null hypothesis. The support in favor of alternative versus the null is reported via some statistical power, such as the p-value (the probability that the test result is as, or more extreme, than the value computed). The idea is that there’s some objective way to push science towards greater truths and NHST was seen as a methodology that avoided the subjectivity of the Bayesian approach. However, while pure in concept, the NHST choices of both null hypothesis and model contain significant amounts of subjectivity! We might as well call the null hypothesis a prior and stop trying to disprove it absolutely. Instead: focus on model comparison, model structure, and posterior probabilities of the competiting theories.\nOver 100 statistical tests have been developed in service of NHST Lewis (2013), but it’s widely viewed now that a focus on NHST has led to worse science due to a multitude of factors, such as:\n\n“P-hacking” or trying to find subsets of data which can (often only by chance) support rejecting some null\nCognitive anchoring to the importance of a p-value of 0.05 or less - why choose that number versus 0.01 or 0.001 or 0.49?\nBias in research processes where one may stop data collection or experimentation after achieving a favorable test result\nInappropriate application of the myriad of statistical tests\nFocus on p-values rather than effects that simply matter more or have greater effect\n\nFor example, which is of more interest to doctors? A study indicating a 1 in a billion chance of serious side effect (p-value 0.0001) or a study indicating a 1 in 3 chance (p-value 0.06)? Many journals would only publish the former study.\n\nDifficulty to determine causal relationships.\n\nThere is subjectivity in the null hypothesis, data collection methodologies, study design, handling of missing data, choice of data not to include, which statistical tests to perform, and interpretation of relationships.\nThe authors of this book recommend against basic NHST and memorization of statistical tests in favor of principled Bayesian approaches. For the actuarial readers, NHST is analogous to traditional credibility methods (of which the authors also prefer more modern statistical approaches).\n\n\nThe example we’ll look at relates to the annual rainfall totals for a specific location in California5, which could be useful for insuring flood risk or determining the value of a catastrophe bond. Acknowloging that we are attempting to create a geocentric model6 instead of a scientifically accurate weather model, we narrow the problem to finding a probability distribution that matches the historical rainfall totals. Our goal is to recommend a model that best fits the data and justify that recommendation quantitatively. Before even looking at the data, Table 12.6 shows three competing models based on thinking about the real-world outcome we are trying to model. These three are chosen for the increasingly sophisticated thought process that might lead the modeler to recommend them - but which is supportable by the statistics?\n\n\n\nTable 12.6: Three alternative hypothesis about the distribution of annual rainfall totals.\n\n\n\n\n\n\n\n\n\n\nHypothesis\nProcess\nPossible Rationale\n\n\n\n\n\\(H_1\\)\nA Normal (Gaussian) distribution\nThe sum of independent rainstorms creates annual rainfall totals that are normally distributed\n\n\n\\(H_2\\)\nA LogNormal distribution\nSince it’s normal-ish, but skewed and can’t be negative\n\n\n\\(H_3\\)\nA Gamma Distribution\nSince rainfall totals would be the sum of exponentially-distributed independent rainfall events\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the literature, \\(H_3\\) (the Gamma distribution) is known as the “Log-Pearson Type III distribution”. It’s actually recommended by the US Corps of Army Engineers as the recommended way to model rainfall totals.\n\n\n\nrain = [\n    39.51, 42.65, 44.09, 41.92, 28.42, 58.65, 30.18, 64.4, 29.02,\n    37.00, 32.17, 36.37, 47.55, 27.71, 58.26, 36.55, 49.57, 39.84,\n    82.22, 47.58, 51.18, 32.28, 52.48, 65.24, 51.12, 25.03, 23.27,\n    26.11, 47.3, 31.8, 61.45, 94.95, 34.8, 49.53, 28.65, 35.3, 34.8,\n    27.45, 20.7, 36.99, 60.54, 22.5, 64.85, 43.1, 37.55, 82.05, 27.9,\n    36.55, 28.7, 29.25, 42.32, 31.93, 41.8, 55.9, 20.65, 29.28, 18.4,\n    39.31, 20.36, 22.73, 12.75, 23.35, 29.59, 44.47, 20.06, 46.48,\n    13.46, 9.34, 16.51, 48.24\n];\n\nWhen we do plot it, we can see some of the characteristics that align with our prior assumptions and knowledge about the system itself, such as: the data being constrained to positive values and a skew towards having some extreme weather years with lots of rainfall.\n\nusing CairoMakie\nhist(rain)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/GtFuI/src/scenes.jl:227\n\n\n\n\n\n\n\n\nFigure 12.1: Annual rainfall totals for a specific location in California.\n\n\n\n\n\nWe will show the likelihood of the three models after deriving the maximum likelihood (MLE), which is simply finding the parameters that maximize the calculated likelihood. In general, this can be accomplished by an optimization routine, but here we will just use the functions built into Distributions.jl:\n\nusing StatsBase\nusing Distributions\n\nn = fit_mle(Normal, rain)\nln = fit_mle(Normal, log.(rain))\nlg = fit_mle(Gamma, log.(rain))\n@show n\n@show ln\n@show lg;\n\nn = Normal{Float64}(μ=38.91442857142857, σ=16.643603630714306)\nln = Normal{Float64}(μ=3.5690550009062663, σ=0.44148379736539156)\nlg = Gamma{Float64}(α=61.58531301458412, θ=0.05795302201453571)\n\n\n\nlet x = rain\n\n    range = 1:0.1:100\n    fig, ax, _ = lines(range, cdf.(n, range), label=\"Normal\", axis=(xgridvisible=false, ygridvisible=false,))\n    lines!(ax, range, cdf.(ln, log.(range)), label=\"LogNormal\")\n    lines!(range, cdf.(lg, log.(range)), label=\"LogGamma\")\n    lines!(quantile.(Ref(x), 0.01:0.01:0.99), 0.01:0.01:0.99, label=\"Data\", color=(:black, 0.6), linewidth=3)\n    fig[1, 2] = Legend(fig, ax, \"Model\", framevisible=false)\n    fig\nend\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/GtFuI/src/scenes.jl:227\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s look at the likelihoods. For the practical reasons described in Tip 12.1, we will compare the the log-likelihoods to maintain convention with what you’d likely see or deal with in practice. Taking the log of the likelihood does not change the ranking of the likelihoods.\n\n let\n    n_lik = sum(log.(pdf.(n, rain)))\n    ln_lik = sum(log.(pdf.(ln, log.(rain))))\n    lg_lik = sum(log.(pdf.(lg, log.(rain))))\n\n    @show n_lik\n    @show ln_lik\n    @show lg_lik\nend;\n\nn_lik = -296.1675156647812\nln_lik = -42.09272021737914\nlg_lik = -43.79151806348801\n\n\nThe results indicate that the LogNormal and the Gamma model for rainfall distribution are very superior to the Normal model, consistent with the visual inspection of the quantiles in Figure 12.2. We reach that conclusion by noting how much more likely the latter two are, as the likelihoods of \\(-42\\) and \\(-44\\) is much greater than \\(-296\\)7.\nWe evaluated the likelihood at a single point estimate of the parameters, but a true posterior probability of the parameters of the distributions will be represented by a distribution rather than a point. Expanding the analysis to account for that point will be the focus of the remainder of this chapter.",
    "crumbs": [
      "Conceptual Foundations: Learning from Related Disciplines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Statistical Inference and Information Theory</span>"
    ]
  },
  {
    "objectID": "statistics.html#sec-modern-bayes",
    "href": "statistics.html#sec-modern-bayes",
    "title": "12  Statistical Inference and Information Theory",
    "section": "12.4 Modern Bayesian Statistics",
    "text": "12.4 Modern Bayesian Statistics\n\n12.4.1 Background\nBayesian statistics is generally not taught in undergraduate statistics - Bayes’ rule is introduced, basic probability exercises are assigned, and then statistics moves on to a curriculum of regression and NHSTs. Why is the applied practice of statistics then gravitating towards Bayesian approaches? There are both philosophical and practical reasons why.\nPhilosophically, one of the main reasons why Bayesian thinking is appealing is its ability to provide a straightforward interpretation of statistical conclusions.\nFor example, when estimating an unknown quantity, a Bayesian probability interval can be directly understood as having a high probability of containing that quantity. In contrast, a Frequentist confidence interval is typically interpreted only in the context of a series of similar inferences that could be made in repeated practice. In recent years, there has been a growing emphasis on interval estimation rather than hypothesis testing in applied statistics. This shift has strengthened the Bayesian perspective since it is likely that many users of standard confidence intervals intuitively interpret them in a manner consistent with Bayesian thinking.\nAnother meaningful way to understand the contrast between Bayesian and Frequentist approaches is through the lens of decision theory, specifically how each view treats the concept of randomness. This perspective pertains to whether you regard the data being random or the parameters being random.\nFrequentist statistics treats parameters as fixed and unknown, and the data as random — this is reflective of the view that data you collect is but one realization of an infinitely repeatable random process. Consequently, Frequentist procedures, like hypothesis testing or confidence intervals, are generally based on the idea of long-run frequency or repeatable sampling.\nConversely, Bayesian statistics turns this on its head by treating the data as fixed — after all, once you’ve collected your data, it’s no longer random but a fixed observed quantity. Parameters, which are unknown, are treated as random variables. The Bayesian approach then allows us to use probability to quantify our uncertainty about these parameters.\nThe Bayesian approach tends to align more closely with our intuitive way of reasoning about problems. Often, you are given specific data and you want to understand what that particular set of data tells you about the world. You’re likely less interested in what might happen if you had infinite data, but rather in drawing the best conclusions you can from the data you do have.\nPractically, recent advances in computational power, algorithm development, and open-source libraries have enabled practitioners to adapt the Bayesian workflow.\nDeriving the posterior distribution is analytically intractable so computational methods must be used. Advances in raw computing power only in the 1990’s made non-trivial Bayesian analysis possible, and recent advances in algorithms have made the computations more efficient. For example, one of the most popular algorithms, NUTS, was only published in the 2010’s.\nMany problems require the use of compute clusters to manage runtime, but if there is any place to invest in understanding posterior probability distributions, it’s insurance companies trying to manage risk!\nThe availability of open-source libraries, such as Turing.jl, PyMC3, and Stan provide access to the core routines in an accessible interface. The exercise remains undoubtedly one that benefits from the computational thinking described in this book - understanding model complexity, model transformations and structure, data types and program organization, etc.\n\n12.4.1.1 Advantages of the Bayesian Approach\nThe main advantages of this approach over traditional actuarial techniques are:\n\nFocus on distributions rather than point estimates of the posterior’s mean or mode. We are often interested in the distribution of the parameters and a focus on a single parameter estimate will understate the risk distribution.\nModel flexibility. A Bayesian model can be as simple as an ordinary linear regression, but as complex as modeling a full insurance mechanics.\nSimpler mental model. Fundamentally, Bayes’ theorem could be distilled down to an approach where you count the ways that things could occur and update the probabilities accordingly.\nExplicit Assumptions.: Enumerating the random variables in your model and explicitly parameterizing prior assumptions avoids ambiguity of the assumptions inside the statistical model.\n\n\n\n12.4.1.2 Challenges with the Bayesian Approach\nWith the Bayesian approach, there are a handful of things that are challenging. Many of the listed items are not unique to the Bayesian approach, but there are different facets of the issues that arise.\n\nModel Construction. One must be thoughtful about the model and how variables interact. However, with the flexibility of modeling, you can apply (actuarial) science to makes better models!\nModel Diagnostics. Instead of R^2 values, there are unique diagnostics that one must monitor to ensure that the posterior sampling worked as intended.\nModel Complexity and Size of Data. The sampling algorithms are computationally intensive - as the amount of data grows and model complexity grows, the runtime demands cluster computing.\nModel Representation. The statistical derivation of the posterior can only reflect the complexity of the world as defined by your model. A Bayesian model won’t automatically infer all possible real-world relationships and constraints.\n\n\n\n\n\n\n\nSubjectivity of the Priors?\n\n\n\nThere are two ways one might react to subjectivity in a Bayesian context: It’s a feature that should be embraced or it’s a flaw that should be avoided.\n\n12.4.1.3 Subjectivity as a Feature\nA Bayesian approach to defining a statistical model is an approach that allows for explicitly incorporating professional judgment. Encoding assumptions into a Bayesian model forces the actuary to be explicit about otherwise fuzzy predilections. The explicit assumption is also more amenable to productive debate about its merits and biases than an implicit judgmental override.\n\n\n12.4.1.4 Subjectivity as a Flaw\nSubjectivity is inherent in all useful statistical methods. Subjectivity in traditional approaches include how the data was collected, which hypothesis to test, what significant levels to use, and assumptions about the data-generating processes.\nIn fact, the “objective” approach to null hypothesis testing is so prone to abuse and misinterpretation that in 2016, the American Statistical Association issued a statement intended to steer statistical analysis into a “post p&lt;0.05 era.” That “p&lt;0.05” approach is embedded in most traditional approaches to actuarial credibility8 and therefore should be similarly reconsidered.\n\n\n\n\n\n\n12.4.2 Implications for Financial Modeling\nLike Bayes’ Formula itself, another aspect of actuarial literature that is taught but often glossed over in practice is the difference between process risk (volatility), parameter risk, and model formulation risk. Often when performing analysis that relies on stochastic result, in practice only process/volatility risk is assessed.\nBayesian statistics provides the tools to help actuaries address parameter risk and model formulation. The posterior distribution of parameters derived is consistent with the observed data and modeled relationships. This posterior distribution of parameters can then be run as an additional dimension to the risk analysis.\nAdditionally, best practices include skepticism of the model construction itself, and testing different formulation of the modeled relationships and variable combinations to identify models which are best fit for purpose. Tools such as Information Criterion, posterior predictive checks, Bayes factors, and other statistical diagnostics can inform the actuary about trade-offs between different choices of model.\n\n\n\n\n\n\nBayesian Versus Machine Learning\n\n\n\nMachine learning (ML) is fully compatible with Bayesian analysis - one can derive posterior distributions for the ML parameters like any other statistical model and the combination of approaches may be fruitful in practice.\nHowever, to the extent that actuaries have leaned on ML approaches due to the shortcomings of traditional actuarial approaches, Bayesian modeling may provide an attractive alternative without resorting to notoriously finicky and difficult-to-explain ML models. The Bayesian framework provides an explainable model and offers several analytic extensions beyond the scope of this introductory chapter:\n\nCausal Modeling: Identifying not just correlated relationships, but causal ones, in contexts where a traditional experiment is unavailable.\nBayes Action: Optimizing a parameter for, e.g., a CTE95 level instead of a parameter mean.\nInformation Criterion: Principled techniques to compare model fit and complexity.\nMissing data: Mechanisms to handle the different kinds of missing data.\nModel averaging: Posteriors can be combined from different models to synthesize different approaches.\n\n\n\n\n\n12.4.3 Basics of Bayesian Modeling\nA Bayesian statistical model has four main components to focus on:\n\nPrior encoding assumptions about the random variables related to the problem at hand, before conditioning on the data.\nA Model that defines how the random variables give rise to the observed outcome.\nData that we use to update our prior assumptions.\nPosterior distributions of our random variables, conditioned on the observed data and our model\n\nHaving defined the first two components and collected our data, the workflow involves computationally sampling the posterior distribution, often using a technique called Markov Chain Monte-Carlo (MCMC). The result is a series of values that are sampled statistically from the posterior distribution.\n\n\n12.4.4 Markov-Chain Monte Carlo\nWhile computing the posterior distribution for most model parameters is analytically intractable, we can probabilistically sample from the posterior distribution and achieve an approximation of the posterior distribution. MCMC samplers, as they are called, do this by moving through the parameter space and travel to different points in proportion to the posterior probability. It is a Markov-Chain because the probability of the next point’s location is influenced by the prior sampling point’s location.\nHere is a simple example demonstrated with one of the oldest MCMC algorithm, called Metropolis-Hastings. The general idea is this:\n\nStart at an arbitrary point and make that the current_state.\nPropose a new point which is the current_state plus some movement that comes from a random distribution, proposal_dist.\nCalculate the likelihood ratio of the proposed versus current point (acceptance_ratio below).\nDraw a random number - if that random number is less than the acceptance_ratio, then move to that new point. Otherwise do not move.\nRepeat steps 2-4 until the distribution of points converges to a stable posterior distribution.\n\nHere is what a complete example looks like. We will try to find the posterior of an arbitrary 2D density function. Picking the product of an Exponential and Beta distribution looks like this:\n\ntarget_dist = product_distribution(Exponential(1.),Beta(2.,15.))\n\n# plot the target_distirubution\nxs = LinRange(-0.5, 3, 400)\nys = LinRange(0, 3, 400)\nzs = [pdf(target_dist,[x,y]) for x in xs, y in ys]\ncontour(xs, ys, zs)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/GtFuI/src/scenes.jl:227\n\n\n\n\n\nThe target probability density which we will attempt to infer via MCMC.\n\n\n\n\nWe will next define a probability distribution for the random step that we take from the current_point. We choose a 2D Guassian for this. The proposal_std controls how big of a movement is taken at each step.\n\n# Define the proposal distribution (2D Gaussian with diagonal covariance)\nproposal_std = 0.025\nproposal_dist = MvNormal(proposal_std * ones(2))\n\nZeroMeanDiagNormal(\ndim: 2\nμ: Zeros(2)\nΣ: [0.0006250000000000001 0.0; 0.0 0.0006250000000000001]\n)\n\n\nWe next define how many steps we want the chain to sample for, and implement the algorithm’s main loop containing the logic described above. The resulting chain contains a list of points that the algorithm has moved along during the sampling process. Note that there is a burn-in parameter. This is because we want the chain to be effectively independent of both (1) the starting point for the sample, and (2) so that different chains are effectively independent.\n\n# MCMC parameters\nnum_samples = 10000\nburn_in = 500\n\n# Initialize the Markov chain\ncurrent_state = [0.0, 0.0]\nchain = zeros(num_samples, 2)\n\n\n# MCMC sampling loop\nfor i in 1:num_samples\n    # Generate a new proposal\n    proposal = rand(proposal_dist) + current_state\n    \n    # Calculate the acceptance ratio\n    current_prob = pdf(target_dist, current_state)\n    proposal_prob = pdf(target_dist, proposal)\n    acceptance_ratio = proposal_prob / current_prob\n    \n    # Accept or reject the proposal\n    if rand() &lt; acceptance_ratio\n        current_state = proposal\n    end\n    \n    # Store the current state as a sample\n    chain[i, :] = current_state\nend\n\nchain\n\n10000×2 Matrix{Float64}:\n 0.0        0.0\n 0.0        0.0\n 0.0        0.0\n 0.0        0.0\n 0.0        0.0\n 0.0        0.0\n 0.0220077  0.0026304\n 0.0400813  0.0175625\n 0.0578822  0.0149467\n 0.0116069  0.0223141\n 0.0211373  0.0212291\n 0.0211373  0.0212291\n 0.0211373  0.0212291\n ⋮          \n 0.201805   0.136393\n 0.197762   0.100357\n 0.166989   0.0951181\n 0.162012   0.0847107\n 0.144701   0.0441389\n 0.20557    0.0157251\n 0.181378   0.029821\n 0.166476   0.0368301\n 0.118293   0.0871541\n 0.1443     0.0642386\n 0.175335   0.0497282\n 0.205826   0.055142\n\n\nAfter having performed the sampling, we can now visualize the chain versus the target_distribution. A few things to note:\n\nThe red line indicates the “warm up” or “burn-in” phase and we do not consider that as part of the sampled chain because those values are too correlated with the arbitrary starting point.\nThe blue line indicates the path traveled by the Metropolis-Hasting algorithm. Long asides into low-probability regions are possible, but in general the path will traverse areas in proportion to the probability of interest.\n\n\n# Plot the chain\nlet \n    f = Figure()\n    ax = Axis(f[1, 1])\n\n\n\n    contour!(ax,xs, ys, zs)\n    lines!(ax,Point2f.(eachrow(chain[1:burn_in,:])),color=(:red, 0.8))\n\n    \n1    for i in burn_in:num_samples\n        ps = Point2f.(eachrow(chain[i-1:i,:]))\n        lines!(ax,ps,color=(:blue,0.2))\n    end\n    f\nend\n\n\n1\n\nIterate through pairs of points to plot as separate line segments so that the transparent blue line visually stack and the higher density near the center is apparent.\n\n\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/GtFuI/src/scenes.jl:227\n\n\n\n\n\n\n\n\nFigure 12.3: The blue lines of the MCMC chain explore the posterior density of interest (after discarding the burn-in samples in red). Becuause of the sharp boundary at \\(x=0\\) right next to the highest density region, this will sample somewhat ineficiently since when the current state is near \\(x=0\\), it becomes increasingly likely that any proposal that moves further to the left will be rejected.\n\n\n\n\n\n\n\n12.4.5 MCMC Algorithms\nThe Metropolis-Hasting algorithm is simple, but somewhat inefficient. Some challenges with MCMC sampling are both mathematical and computational:\n\nOften times the algorithm will back-track (take a “U-Turn”), wasting steps in regions already explored.\nThe algorithm can have a very high rate of rejecting proposals if the proposal mechanism generates steps that would move the current state into a low-probability regions.\nThe choice of proposal distribution and parameters can greatly influence the speed of convergence. Too large of movement and key regions can be entirely skipped over, while small movements can take much longer than necessary to explore the space.\nAs the number of parameters grows, the dimensionality of the parameter space to explore also grows making posterior exploration much harder.\nThe shape of the posterior space can be more ore less difficult to explore. Complex models may have regions of density that are not nicely “round” - regions may be curved, donut shaped, or disjointed.\n\nThe problems above mean that MCMC sampling is very computationally expensive for more complex examples. Compared with Metropolis-Hastings, modern algorithms (such as the No-U-Turn (NUTS)) algorithm explore the posterior distribution more efficiently by avoiding back-tracking to already explored regions and dynamically adjusting the proposals to adaptively fit the posterior. Many of them take direct influence from particle physics, with the algorithm keeping track of the energy of the current state as it explores the posterior space.\nAlgortihms have only brought so much relief: much falls back onto the modeler to design models that are computationally efficient, transformed to eliminate oddly-shaped density regions, or find the right simplifications to the analysis in order to make the problem tractable.\n\n\n\n\n\n\nNote\n\n\n\nWhat does it mean to transform the parameter space?\nAn example will be shown in Chapter 25 where we want to ensure that a binomial variable is constrained to the region \\([0,1]\\) but the underlying factors are allowed to vary across the entire real numbers. We use a logit (or inverse logit, a.k.a. logistic) to transform the parameters to the required probability range for the binomial outcome.\nAnother common transform is “Normalizing” the data to center the data around zero and to scale the outcomes such that the sample standard deviation is equal to one.\n\n\n\n\n12.4.6 Rainfall Example (Continued)\nWe will construct a Bayesian model using the Turing.jl library, and fit the parameters of one of the competing models in order to demonstrate an MCMC analysis workflow and essential concepts.\nThe first thing that we will do is use Turing’s @model macro to define a model. This has a few components:\n\nThe “model” is really just a Julia function that takes in data and relates the data to the statistical outcomes modeled.\nThe ~ is the syntax to either relate a parameter to a prior assumptions.\nA loop (or broadcasted .~) that ties specific data observations to the random process.\n\nThink of the @model block really as a model constructor. When you pass data to the model, then you get an instantiated Model type9.\n\nusing Turing\n\n1@model function rainLogNormal(logdata)\n    \n    # Prior Assumptions for the (Log) Normal Parameters\n2    μ ~ Normal(4,1)\n3    σ ~ Exponential(0.5)\n\n    # Link observations to the random process\n    for i in 1:length(logdata)\n        logdata[i] ~ Normal(μ, σ)\n    end\nend\n\nm = rainLogNormal(log.(rain));\n\n\n1\n\nDefining the model uses the @model macro from Turing.\n\n2\n\nWe know that there will be positive rainfall and 96% of mean annual rainfall will be between \\(exp(2)\\) and \\(exp(6)\\), or 7 and 403 inches.\n\n3\n\nIn a LogNormal model, 0.5 deviations covers a lot of variation in outcomes.\n\n\n\n\nPrecompiling Turing\n  ✓ BangBang → BangBangDataFramesExt\n  ✓ Transducers → TransducersDataFramesExt\n  2 dependencies successfully precompiled in 17 seconds. 380 already precompiled.\n[ Info: Precompiling Turing [fce5fe82-541a-59a6-adf8-730c64b5f9a0]\n[ Info: Precompiling IntervalArithmeticDiffRulesExt [86439bda-9ede-5c22-99a0-f82df582fb22]\n[ Info: Precompiling IntervalArithmeticForwardDiffExt [ba47a815-ec9a-57c1-b718-e4e972ac9261]\n[ Info: Precompiling InverseFunctionsUnitfulExt [f5f6e0dd-5310-5802-bcb2-1cb72ad693d4]\n[ Info: Precompiling AccessorsUnitfulExt [0f33c9ce-b40b-5f58-839e-64dee873ac84]\nPrecompiling BangBangDataFramesExt\n  ✓ BangBang → BangBangDataFramesExt\n  1 dependency successfully precompiled in 1 seconds. 45 already precompiled.\n[ Info: Precompiling BangBangDataFramesExt [d787bcad-b5c5-56bb-adaa-6bfddb178a59]\n┌ Warning: Module BangBang with build ID fafbfcfd-e7c0-17cc-000a-06aa8b7bdacd is missing from the cache.\n│ This may mean BangBang [198e06fe-97b7-11e9-32a5-e1d131e6ad66] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing BangBangDataFramesExt [d787bcad-b5c5-56bb-adaa-6bfddb178a59].\nPrecompiling TransducersDataFramesExt\n  ✓ Transducers → TransducersDataFramesExt\n  1 dependency successfully precompiled in 2 seconds. 59 already precompiled.\n[ Info: Precompiling TransducersDataFramesExt [cefb4096-3352-5e5f-8501-71f024082a88]\n┌ Warning: Module Transducers with build ID fafbfcfd-cd4f-c17e-000a-06aab1436dd0 is missing from the cache.\n│ This may mean Transducers [28d57a85-8fef-5791-bfe6-a80928e7c999] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing TransducersDataFramesExt [cefb4096-3352-5e5f-8501-71f024082a88].\n[ Info: Precompiling IntervalArithmeticRecipesBaseExt [e3b91bd4-2888-5303-85ed-4cf5ebb38ff1]\nPrecompiling SciMLBaseMakieExt\n  ✓ SciMLBase → SciMLBaseMakieExt\n  1 dependency successfully precompiled in 9 seconds. 299 already precompiled.\n[ Info: Precompiling SciMLBaseMakieExt [565f26a4-c902-5eae-92ad-e10714a9d9de]\n┌ Warning: Module SciMLBase with build ID fafbfcfd-c557-f943-000a-06b32ed0f6c0 is missing from the cache.\n│ This may mean SciMLBase [0bca4576-84f4-4d90-8ffe-ffa030f20462] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing SciMLBaseMakieExt [565f26a4-c902-5eae-92ad-e10714a9d9de].\n\n\n\n12.4.6.1 Setting Priors\nIn the example above, we used “weakly informative” priors. We constrained the prior probability to plausible ranges, knowing enough about the system of study (rainfall) that it would be completely implausible for there to be a Uniform(0,Inf) distribution of mean log-rainfall total. We admit a level of subjectivity when we encode into the model some limitations on what we would believe to be true. Admittedly, we haven’t confirmed with a meteorologist that \\(exp(20)\\) (485 million) inches of rain per year is impossible. But such is the beauty of the transparency of Bayesian analysis that the prior assumption is right there! Front and center!\n“Strongly informative” priors would be something where we want to encode a stronger assumption about the plausible range of outcomes, such as if we knew enough about the problem domain that we could tell given the location of the rainfall, we’d expect 95% of the rainfall to be between, say, 10 and 30 inches per year.\n“Uninformative” priors use only maximum entropy or uniform priors to avoid encoding bias into the model.\n\n\n12.4.6.2 Sampling\nAnalysis should begin by evaluating the prior assumptions for reasonability and coverage over possible outcomes of the process we are trying to model. The top plot in Figure 12.7 shows the modeled rainfall outcomes taking on a wide range of possible outcomes. If we had more knowledge of the system we could enforce a stronger (narrower) prior assumption to constrain the model to a smaller set of values.\nThe object returned is an MCMCChains structure containing the samples as well as diagnostic information. Summary information gets printed below.\n\nchain_prior = sample(m, Prior(), 1000)\n\nSampling:   1%|▍                                        |  ETA: 0:00:11Sampling: 100%|█████████████████████████████████████████| Time: 0:00:00\n\n\n\n\n\nChains MCMC chain (1000×3×1 Array{Float64, 3}):\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 1.04 seconds\nCompute duration  = 1.04 seconds\nparameters        = μ, σ\ninternals         = lp\nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ⋯\n      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ⋯\n           μ    3.9818    0.9860    0.0306   1036.4826   1025.7469    0.9990   ⋯\n           σ    0.5039    0.5007    0.0157   1039.6729   1025.2091    1.0002   ⋯\n                                                                1 column omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n           μ    2.1100    3.2967    4.0059    4.6423    5.9364\n           σ    0.0157    0.1404    0.3363    0.7198    1.7820\n\n\n\n\nFigure 12.4: Model output for the sampled prior. This isn’t running an MCMC algorithm, it’s simply taking draws from the defined prior assumptions.\n\n\n\n\nWe now sample the posterior by using the No-U-Turns (NUTS) algorithm and drawing 1000 samples (not including the warm-up phase). This is the primary result we will analyze further.\n\nchain_posterior = sample(m,NUTS(),1000)\n\n┌ Info: Found initial step size\n└   ϵ = 0.4\n\n\n\n\n\nChains MCMC chain (1000×14×1 Array{Float64, 3}):\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 2.46 seconds\nCompute duration  = 2.46 seconds\nparameters        = μ, σ\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n           μ    3.5697    0.0546    0.0017   974.0726   675.4028    1.0024     ⋯\n           σ    0.4499    0.0399    0.0015   741.7311   651.2142    1.0013     ⋯\n                                                                1 column omitted\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n           μ    3.4657    3.5331    3.5688    3.6056    3.6772\n           σ    0.3824    0.4224    0.4479    0.4756    0.5318\n\n\n\n\nFigure 12.5: Model output for the sampled posterior.\n\n\n\n\n\n\n12.4.6.3 Diagnostics\nBefore analyzing the result itself, we should check a few things to ensure the model and sampler were well behaved. MCMC techniques are fundamentally stochastic and randomness can cause an errant sampling path. Or a model may be mis-specified such that the parameter space to explore is incompatible with the current algorithm (or any known so far).\nA few things we can check:\nFirst, the ess or effective sample size which adjusts the number of samples for the degree of autocorrelation in the chain. Ideally, we would be able to draw independent samples from the posterior but due to the Markov-Chain approach the samples can have autocorrelation between neighboring samples. Therefore we collect less information about the posterior in the presence of positive autocorrelation. An ess greater than our sample indicates that there was less (negative) autocorrelation than we would have expected for the chain. An ess much less than the number of samples indicates that the chain isn’t sampling very efficiently but, aside from needing to run more samples, isn’t necessarily a problem.\n\ness(chain_posterior)\n\n\nESS\n  parameters        ess   ess_per_sec \n      Symbol    Float64       Float64 \n           μ   974.0726      396.2867\n           σ   741.7311      301.7620\n\n\n\n\nSecond, the rhat (\\(\\hat{R}\\)) is the Gelman-Rubin convergence diagnostic and it’s value should be very close to 1.0 for a chain that has converged properly. Even a value of 1.01 may indicate an issue and quickly gets worse for higher values.\n\nrhat(chain_posterior)\n\n\nR-hat\n  parameters      rhat \n      Symbol   Float64 \n           μ    1.0024\n           σ    1.0013\n\n\n\n\nNext, we can look at the “trace” plots for the parameters being sampled (Figure 12.6). These are sometimes called “hairy caterpillar” plots because in a healthy chain sample, we should see a series without autocorrelation and that the values bounce around randomly between individual samples.\n\nlet\n    f = Figure()\n    ax1 = Axis(f[1,1],ylabel=\"μ\")\n    lines!(ax1,vec(get(chain_posterior,:μ).μ.data))\n    ax2 = Axis(f[2,1],ylabel=\"σ\")\n    lines!(ax2,vec(get(chain_posterior,:σ).σ.data))\n    f\nend\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/GtFuI/src/scenes.jl:227\n\n\n\n\n\n\n\n\nFigure 12.6: The trace plots indicate low autocorrelation which is desirable for an MCMC sample.\n\n\n\n\n\nThe ess, rhat, and trace plots all look good for our sampled chain so we can next we will analyze the results proper.\n\n\n12.4.6.4 Analysis\nLet’s see how it looks compared to the data first. Figure 12.7 shows 200 samples from the prior and posterior. The prior (top) shows how wide the range of possible rainfall outcomes could be using our weakly informative prior assumptions. The bottom shows that after having learned from the data, the posterior probability of rainfall has narrowed considerably.\n\nfunction chn_cdf!(axis,chain,rain)    \n    n = 200\n    s = sample(chain, n)\n    vals = get(s, [:μ, :σ])\n    ds = Normal.(vals.μ, vals.σ) # ,get(s,:σ)[i])\n    rg = 1:200\n    for (i, d) in enumerate(ds)\n        lines!(axis, rg,cdf.(d,log.(rg)),color=(:gray,0.3))\n    end\n\n    # plot the actual data\n    percentiles= 0.01:0.01:0.99\n    lines!(axis,quantile.(Ref(rain),percentiles),percentiles,linewidth=3)\nend\n\nlet \n    f = Figure()\n    ax1 = Axis(f[1,1],title=\"Prior\", xgridvisible=false, ygridvisible=false,ylabel=\"Quantile\")\n    chn_cdf!(ax1,chain_prior,rain)\n\n    ax2 = Axis(f[2,1],title=\"Posterior\", xgridvisible=false, ygridvisible=false,xlabel=\"Annual Rainfall (inches)\",ylabel=\"Quantile\")\n    chn_cdf!(ax2,chain_posterior,rain)\n\n    linkxaxes!(ax1, ax2)\n\n    f\nend\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/GtFuI/src/scenes.jl:227\n\n\n\n\n\n\n\n\nFigure 12.7: The fitted posterior model (bottom) has good coverage of the observed data (shown in blue).\n\n\n\n\n\nComparing to the maximum likelihood analysis from before by plotting the MLE point estimate onto the marginal densities in Figure 12.8. The peak of the the posterior is referred to as the maximum a posteriori (MAP) and would be the point estimate proposed by this Bayesian analysis. However, the Bayesian way of thinking about distributions of outcomes rather than point estimates is one of the main aspects we encourage for financial modelers. Using the posterior distribution of the parameters, we can assess parameter uncertainty directly instead of ignoring it as we tend to do with point estimates.\n\nlet\n    # get the parameters from the earlier MLE approach\n    p = params(ln)\n\n    f = Figure()\n\n    # plot μ posterior\n    ax1 = Axis(f[1,1],title=\"μ posterior\",xgridvisible=false)\n    hideydecorations!(ax1)\n    d = density!(ax1,vec(get(chain_posterior,:μ).μ.data))\n    l = vlines!(ax1,[p[1]],color=:red)\n    \n\n    # plot σ posterior\n    ax2 = Axis(f[2,1],title=\"σ posterior\", xgridvisible=false)\n    hideydecorations!(ax2)\n    density!(ax2,vec(get(chain_posterior,:σ).σ.data))\n    vlines!(ax2,[p[2]],color=:red)\n    \n    Legend(f[1,2],[d,l],[\"Posterior Density\", \"MLE Estimate\"])\n\n    f\nend\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/GtFuI/src/scenes.jl:227\n\n\n\n\n\n\n\n\nFigure 12.8: The MLE point estimate does not necessarily align with the posterior densities.\n\n\n\n\n\n\n\n12.4.6.5 Understanding Model Limitations\nWe have built and assessed a simple statistical model that could be used in the estimation of risk for a particular location. Nowhere in our model did we define a mechanism to capture a more sophisticated view of the world. There is no parameter for changes over time due to climate change, or inter-annual seasonality for El Niño or La Niña cycles, or any of a multitude of other real-world factors that can influence the forecasting. All we’ve defined is that there is a LogNormal process generating rainfall in a particular location. This may or may not be sufficient to capture the dynamics of the problem at hand.\nPart of the benefits of the Bayesian approach is that it allows us to extend the statistical model to be arbitrarily complex in order to capture our intended dynamics. We are limited by the availability of data, computational power and time, and our own expertise in the modeling. Regardless of the complexity of the model, the same fundamental techniques and idea apply in the Bayesian approach.\n\n\n12.4.6.6 Continuing the Analysis\nLike any good model, you can often continue the analysis in any number of directions, such as: collecting more data, evaluating different models, creating different visualizations, making predictions about future events, creating a multi-level model that predicts rainfall for multiple related locations simultaneously, among many other\nEarlier we discussed model comparison. To compute a real Bayes Factor in comparing the different models, we would take the average likelihood across the posterior samples instead of just comparing the maximum likelihood points as we did earlier. There are more sophisticated tools for estimating out of sample performance of the model, or measures that evaluate a model for over-fitting by penalizing the diagnostic statistic for the model having too many free parameters. See LOO (leave-one-out) cross-validation and various “information criteria” in the resources listed in Section 12.4.8.\n\n\n\n12.4.7 Conclusion\nThis chapter has attempted to make accessible the foundations of statistical inference and the modern tools and approaches available. Underlying this approach to thinking about statistical problems are informational theoretic and mathematical concepts that can be challenging to learn when traditional finance and actuarial curricula is not centered on the necessary computational foundations that are associated with modern statistical analysis. Further, the approach of treating estimation not as an exercise in determining a “best estimate” but instead as a range of outcomes will enhance financial analysis and quantification of risks.\n\n\n12.4.8 Further Reading\nBayesian approaches to statistical problems are rapidly changing the professional statistical field. To the extent that the actuarial profession incorporates statistical procedures, financial professionals should consider adopting the same practices. The benefits of this are a better understanding of the distribution of risk and return, results that are more interpretable and explainable, and techniques that can be applied to a wider range of problems. The combination of these things would serve to enhance best practices related to understanding and communicating about financial quantities.\nTextbooks recommended by the author are:\n\nStatistical Rethinking (McElreath)\nBayes Rules! (Johnson, Ott, Dogucu)\nBayesian Data Analysis (Gelman, et. al.)\n\nChi Feng has an interactive demonstration of different MCMC samplers available at: https://chi-feng.github.io/mcmc-demo/.\n\n\n\n\nLeemis, Lawrence M, and Jacquelyn T McQueston. 2008. “Univariate Distribution Relationships.” The American Statistician 62 (1): 45–53. https://doi.org/10.1198/000313008x270448.\n\n\nLewis, N D. 2013. 100 Statistical Tests. Createspace.",
    "crumbs": [
      "Conceptual Foundations: Learning from Related Disciplines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Statistical Inference and Information Theory</span>"
    ]
  },
  {
    "objectID": "statistics.html#footnotes",
    "href": "statistics.html#footnotes",
    "title": "12  Statistical Inference and Information Theory",
    "section": "",
    "text": "Log base two turns out to be the most natural representation of information content as it mimics the fundamental 0 or 1 value bit. A more complete introduction is available in “Information Theory, Inference, and Learning Algorithms” by David MacKay.↩︎\nA decision tree is a classification algorithm which attempts to optimally classify an output based on if/else type branches on the input variables.↩︎\nhttps://www.researchgate.net/publication/239752412_Derivation_of_Probability_Distributions_for_Risk_Assessment↩︎\nMulitiplicative process are often referred to as “geometric”, as in “geometric Brownian motion” or “geometric mean”. Additive processes are sometimes referred to as “arithmetic”. This root of this confusing terminology appears to be due to the fact that series involving repeated multiplication were solved via geometric (triangles, angles, etc.) methods while those using sums and differences were solved via arithmetic.↩︎\nhttps://data.ca.gov/dataset/annual-precipitation-data-for-northern-california-1944-current↩︎\nSee @sec-predictive-vs-explanatory.↩︎\nThe values are negative because we are taking the logarithm of a number less than \\(1\\). The likelihoods are less than \\(1\\) because the likelihood is the joint (multiplicative) probability of observing each of the individual outcomes.↩︎\nNote that the approach discussed here is much more encompassing than the Bühlmann-Straub Bayesian approach described in the actuarial literature.↩︎\nSpecifically: a DynamicPPL.Model type (PPL = Probabilistic Programming Language).↩︎",
    "crumbs": [
      "Conceptual Foundations: Learning from Related Disciplines",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Statistical Inference and Information Theory</span>"
    ]
  },
  {
    "objectID": "modeling.html",
    "href": "modeling.html",
    "title": "13  Modeling",
    "section": "",
    "text": "13.1 In This Chapter\nWe discuss how to approach a problem and identify the key attributes to include in the model, what are the inherent trade-offs with different approaches, and how to work with data that feeds your model.",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modeling</span>"
    ]
  },
  {
    "objectID": "modeling.html#parsimony",
    "href": "modeling.html#parsimony",
    "title": "13  Modeling",
    "section": "13.2 Parsimony",
    "text": "13.2 Parsimony",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Modeling</span>"
    ]
  },
  {
    "objectID": "autodiff.html",
    "href": "autodiff.html",
    "title": "14  Automatic Differentiation",
    "section": "",
    "text": "14.1 In This Chapter\nHarnessing the chain rule to compute derivatives not just of simple functions, but of complex programs.",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#motivation-for-automatic-derivatives",
    "href": "autodiff.html#motivation-for-automatic-derivatives",
    "title": "14  Automatic Differentiation",
    "section": "14.2 Motivation for (Automatic) Derivatives",
    "text": "14.2 Motivation for (Automatic) Derivatives\nDerivatives are one of the most useful analytical tools we have. Determining the rate of change with respect to an input is effectively sensitivity testing. Knowing the derivative let’s you optimize things faster (see Chapter 15). You can test properties and implications (monotonicy, maxima/minima).",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#finite-differentiation",
    "href": "autodiff.html#finite-differentiation",
    "title": "14  Automatic Differentiation",
    "section": "14.3 Finite Differentiation",
    "text": "14.3 Finite Differentiation\nFinite differentiation is evaluating a function \\(f(x)\\) at a value \\(x\\) and then at a nearby value \\(x+\\epsilon\\). The line drawn through these two point effectively estimates the line that is tangent to the function \\(f\\) at \\(x\\): effectively the derivative has been found by approximation. That is, we are looking to approximate the derivative using the property:\n\\[\nf'(x) = \\lim_{{\\epsilon \\to 0}} \\frac{{f(x_0 + \\epsilon) - f(x_0)}}{{\\epsilon}}\n\\]\nWe can approximate the result by simply choosing a small \\(\\epsilon\\).\nThere’s also flavors of finite differentiation to approximate derivatives to be aware of:\n\nforward difference is as defined in the above equation, where \\(\\epsilon\\) is added to \\(x_0\\)\nreverse difference is as defined in the above equation, where \\(\\epsilon\\) is subtracted from \\(x_0\\)\ncentral difference is where we evaluate at \\(x_0 \\pm \\epsilon\\) and then divide by \\(2\\epsilon\\)\n\nThe benefit of the central difference is that it limits issues around minima and maxima where the trough or peak respectively would seem much steeper if using forward or reverse. Here’s a picture of this:\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/iRM0c/src/scenes.jl:220\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/iRM0c/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\nOne benefit of the central difference method is that is often more accurate than forward or reverse. However it comes at the cost of needing to evaluate the function an additional time in many circumstances. Take, for example, the process of optimizing a function to find a maxima or minima. The process usually involves guessing an initial point, evaluating the function at that point, and determining what the derivative of the function is at that point. Both items are used to update the guess to one that’s closer to the solution. This approach is used in many optimization algorithms such as Newton’s Method.\nAt each step you need to evaluate the function three times: for \\(x\\), \\(x+\\epsilon\\), and \\(x-\\epsilon\\). With forward or reverse finite differences, you can reuse the prior function evaluation of the prior guess \\(x\\) As one of the components in the estimation of the derivative, thereby saving an evaluation of the function for each iteration.\nThere are additional challenges with the finite differences method. In practice, we are often interested in much more complex functions than \\(x^2\\). For example, we may actually be interested in the sum of a series that is many elements long or contains more complex operations than basic algebra. In the prior example, the \\(\\epsilon\\) is set unusually wide for demonstration purposes. As \\(\\epsilon\\) grow smaller generally, the accuracy of all three finite different methods increases. However, that’s not always the case due to both the complexity of the function that you may be trying to differentiate or due to numerical inaccuracies of floating point math.\nTo demonstrate, here is a more complex example using an arbitrary function\nfor this example we’ll show the results of the three methods calculated at different values of \\(\\epsilon\\):\n\nusing DataFrames\n\n\nf(x) = exp(x)\nϵ = 10 .^ (range(-16, stop=0, length=100))\nx0 = 1\nestimate = @. (f(x0 + ϵ) - f(x0 - ϵ)) / 2ϵ\n1actual = f(x0)\n\nfig = Figure()\nax = Axis(fig[1, 1], xscale=log10, yscale=log10, xlabel=\"ϵ\", ylabel=\"absolute error\")\nscatter!(ax, ϵ, abs.(estimate .- actual))\nfig\n\n\n1\n\nThe derivative of \\(f(x) = exp(x)\\) is itself. That is \\(f'(x) = f(x)\\) in this special case.\n\n\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/iRM0c/src/scenes.jl:220\n\n\n\n\n\n\n\n\nFigure 14.1: A log-log plot showing the absolute error of the finite differences. Further to the left, roundoff error dominates while further to the right, truncation error dominates.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe @. in the code example above is a macro that applies broadcasting each function to its right. @. (f(x0 + ϵ) - f(x0 - ϵ)) / 2ϵ is the same as (f.(x0 .+ ϵ) .- f.(x0 .- ϵ)) ./ (2 .* ϵ)\n\n\nA few observations:\n\nAt virtually every value of ϵ we observe some error from the true derivative.\nThat error is the sum of two parts: truncation error is inherent in that we are using a given value for ϵ and not determining the limiting analytic value as \\(\\epsilon \\to 0\\). The other component is roundoff error which arises due to the limited precision of floating point math.\n\nThe implications of this are that we need to often be careful about the choice of ϵ, as the optimal choice will vary depending on the function and the point we are attempting to evaluate. This presents a number of practical diffuclties in various algorithms.\nAdditionally,when computing the finite difference we must evaluate the function multiple times to determine a single estimate of the derivative. When performing something like optimization the process typically involves iteratively making many guesses — plus the number of guesses required to find the right answer can depends on the ability to accurate determine the derivative at a point!\nAdmittedly, despite the accuracy and computational overhead, finite differences can be very useful in many circumstances. However, a more appealing alternative approach will be covered next.",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#automatic-differentiation",
    "href": "autodiff.html#automatic-differentiation",
    "title": "14  Automatic Differentiation",
    "section": "14.4 Automatic Differentiation",
    "text": "14.4 Automatic Differentiation\nAutomatic differentiation (“autodiff” or “AD” for short) is essentially the practice of defining algorithmically what the derivatives of function should be. We are able to do this through a creative application of the chain rule. Recall that the chain rule allows us to compute the derivative of a composite function using the derivatives of the component functions:\n\\[\nh(x)=f(g(x))\n\\] \\[\nh'(x) = f'(g(x)) g'(x)\n\\]\nUsing this rule, we can define how elementary operations act when differentiated. Combined with the fact that most computer code is building up from a bunch of elementary operations, we can get a very long way in differentiating complex functions.\n\n14.4.1 Dual Numbers\nTo understand where we are going, let’s remind ourselves about complex numbers. Complex numbers are of the form which has an real part (\\(r\\)) and an imaginary part (\\(iq\\)):\n\\[\nr+iq\n\\]\nBy definition we say that \\(i^2 = -1\\). This is useful because it allows us to perform certain types of operations (e.g. finding a square root of a negative number) that is otherwise unsolvable with just the real numbers1. After defining how the normal algebraic operations (addition, multiplication, etc.) work for the imaginary number, we are able to utilize the imaginary numbers for a variety of practical mathematical tasks.\nWhat is meant by extending the algebraic operations for imaginary numbers? For example, stating how addition should work for imaginary numbers:\n\\[\n(r+iq) + (s+iu) = (r+s) + i(q+u)\n\\]\nIn a similar fashion as extending the Real (\\(\\mathbb{R}\\)) numbers with an imaginary part, for automatic differentiation we will extend them with a dual part. A dual number is one of the form:\n\\[\na + \\epsilon b\n\\]\nWhere \\(\\epsilon^2 = 0\\) and \\(\\epsilon \\neq 0\\) by defintion. For our purposes here, one can think of \\(b\\) as the derivative of the function evaluated at the same point as \\(a\\). An intial example should make this clearer. First let’s define a DualNumber:\n\n1struct DualNumber{T,U}\n    a::T\n    b::U\n2    function DualNumber(a::T, b::U=zero(a)) where {T,U}\n        return new{T,U}(a, b)\n    end\nend\n\n\n1\n\nWe define this type parametrically to handle all sorts of &lt;:Real types and allow a and b to vary types in case a mathematical operation causes a type change (e.g. as in the case of integers becoming a floating point number like 10/4 == 2.5)\n\n2\n\nzero(a) is a generic way to create a value equal to zero with the same type of the argument a. zero(12.0) == 0.0 and zero(12) == 0.\n\n\n\n\nNow let’s define how dual numbers work under addition. The mathematical rule is:\n\\[\n(a+\\epsilon b)+(c+\\epsilon d)=(a+c)+(b+d)\\epsilon\n\\]\nWe then need to define how it works for the combinations of numbers that we might receive as arguments to our function (this is an example where multiple dispatch greatly simplifies the code compare to object oriented single dispatch!):\n\nBase.:+(d::DualNumber, e::DualNumber) = DualNumber(d.a + e.a, d.b + e.b)\nBase.:+(d::DualNumber, x) = DualNumber(d.a + x, d.b)\nBase.:+(x, d::DualNumber) = d + x\n\nAnd here’s how we would get the derivative of a very simple function:\n\nf1(x) = 5 + x\n\nf1(DualNumber(10, 1))\n\nDualNumber{Int64, Int64}(15, 1)\n\n\nThat’s not super interesting though - the derivative of f1 is just 1 and we supplied that in the construction of DualNumber. We did at least prove that we can add the 10 and 5!\nLet’s make this more interesting by also defining the multiplication operation on dual numbers. We’ll follow the product rule:\n\\[\n(u \\times v)' = u ' \\times v + u \\times v'\n\\]\n\nBase.:*(d::DualNumber, e::DualNumber) = DualNumber(d.a * g.a, d.b * g.a + d.a * g.b)\nBase.:*(x, d::DualNumber) = DualNumber(d.a * x, d.b * x)\nBase.:*(d::DualNumber, x) = x * d\n\nNow what if we evaluate this function:\n\nf2(x) = 5 + 3x\n\nf2(DualNumber(10, 1))\n\nDualNumber{Int64, Int64}(35, 3)\n\n\nWe have found that the second component is 3, which is indeed the derivative of \\(5+3x\\) with respect to \\(x\\). And in the first part we have the value of f2 evaluated at 10.\n\n\n\n\n\n\nNote\n\n\n\nWhen calcualting the derivative, why do we start with 1 in the dual part of the number? Because the derivative of a variable with respect to itself is 1. From this unitary starting point, the various operations applied accumulate the derivative of the various operations in the \\(b\\) part of \\(a + \\epsilon b\\).\n\n\nWe can also define this for things like transcendental functions:\n\nBase.exp(d::DualNumber) = DualNumber(exp(d.a), exp(d.a) * d.b)\nBase.sin(d::DualNumber) = DualNumber(sin(d.a), cos(d.a) * d.b)\nBase.cos(d::DualNumber) = DualNumber(cos(d.a), -sin(d.a) * d.b)\nexp(DualNumber(1, 1))\n\nDualNumber{Float64, Float64}(2.718281828459045, 2.718281828459045)\n\n\n\nsin(DualNumber(0, 1))\n\nDualNumber{Float64, Float64}(0.0, 1.0)\n\n\n\ncos(DualNumber(0, 1))\n\nDualNumber{Float64, Float64}(1.0, -0.0)\n\n\nAnd finally, to put it all together in a more usable wrapper, we can define a function which will calcualte the derivative of another function at a certain point:\n\nderivative(f, x) = f(DualNumber(x, one(x))).b\n\nderivative (generic function with 1 method)\n\n\nAnd then evaluating it on a more complex function like \\(f(x) = 5e^{sin(x)}+3x\\) at \\(x = 0\\), we would analytically derive \\(8\\), which matches what we calculate next:\n\nlet\n    f(x) = 5 * exp(sin(x)) + 3x\n    derivative(f, 0)\nend\n\n8.0\n\n\nWe have demonstrated that through the clever use of dual numbers and the chain rule that complex expressions can be automatically differentiated by the computer to an exact level, limited only by the same machine precision that applies to our primary function of interst as well.",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#performance-of-automatic-differentiation",
    "href": "autodiff.html#performance-of-automatic-differentiation",
    "title": "14  Automatic Differentiation",
    "section": "14.5 Performance of Automatic Differentiation",
    "text": "14.5 Performance of Automatic Differentiation\nRecall that in the finite difference method, we generally had to evalue the function two or three times to approximate the derivative. Here we have a single function call that provides both the value and the derivative at that value. How does this compare performance-wise to simply evaluating the function a single time?\n\nusing BenchmarkTools\n@btime f2(rand())\n\n  2.583 ns (0 allocations: 0 bytes)\n\n\n6.167102117329243\n\n\n\n@btime f2(DualNumber(rand(), 1))\n\n  2.666 ns (0 allocations: 0 bytes)\n\n\nDualNumber{Float64, Int64}(7.41269815425763, 3)\n\n\nIn performing this computation, the compiler has been able to optimize it such that we effectively are able to compute the function and its derivative at effetcitly the same speed as just the evaluating the function itself! As the function gets more complex, the overhead does increase but is still a much preferred option versus finite differentiation. This advantage becomes more pronouced as we contemplate derivatives with respect to many variables at once or for higher-order derivatives.\n\n\n\n\n\n\nNote\n\n\n\nIn fact, it’s largely due to the advances in applications of automatic differentiation that has led to the explosion of machine learning and artificial intelligence techniques in the 2010s/2020s. The “learning” process relies on solving parameter weights and would be too computationally expensive if using finite differences.\nThese applications of autodiffertiation in specialized C++ libraries underpin the libraries like PyTorch, Tensorfow, and Keras. These libraries specialize in allowing for autodiff on a limited subset of operations. Julia’s available automatic differentiation is more general and can be applied to many more scenarios.",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#automatic-differentiation-in-practice",
    "href": "autodiff.html#automatic-differentiation-in-practice",
    "title": "14  Automatic Differentiation",
    "section": "14.6 Automatic Differentiation in Practice",
    "text": "14.6 Automatic Differentiation in Practice\nWe have, of course, not defined an exhaustive list of operations, covering only +, *, exp, sin, and cos. There are only a few more arithmetic (-, /) and trancendental (log, more trigonometric functions, etc.) before we would have a very robust set of algebraic operations defined for our DualNumber. In fact, it’s possible to go even further and to define the behavior through conditional expressions and iterations to differentiate fairly complex functions or to extend the mechanism to partial derivatives and higher-order derivatives as well.\n\nimport Distributions\nimport ForwardDiff\n\nN(x) = Distributions.cdf(Distributions.Normal(), x)\n\nfunction d1(S, K, τ, r, σ, q)\n    return (log(S / K) + (r - q + σ^2 / 2) * τ) / (σ * √(τ))\nend\n\nfunction d2(S, K, τ, r, σ, q)\n    return d1(S, K, τ, r, σ, q) - σ * √(τ)\nend\n\n\"\"\"\n    eurocall(parameters)\n\nCalculate the Black-Scholes implied option price for a european call where `parameters` is a vector with the following six elements:\n\n- `S` is the current asset price\n- `K` is the strike or exercise price\n- `τ` is the time remaining to maturity (can be typed with \\\\tau[tab])\n- `r` is the continuously compounded risk free rate\n- `σ` is the (implied) volatility (can be typed with \\\\sigma[tab])\n- `q` is the continuously paid dividend rate\n\"\"\"\nfunction eurocall(parameters)\n1    S, K, τ, r, σ, q = parameters\n    iszero(τ) && return max(zero(S), S - K)\n    d₁ = d1(S, K, τ, r, σ, q)\n    d₂ = d2(S, K, τ, r, σ, q)\n    return (N(d₁) * S * exp(τ * (r - q)) - N(d₂) * K) * exp(-r * τ)\nend\n\n\n1\n\nWe put the various variables inside a single parameters vector to allow calling a single gradient call instead of multiple derivative calls for each parameter.\n\n\n\n\neurocall\n\n\n\nS = 1.0\nK = 1.0\nτ = 30 / 365\nr = 0.05\nσ = 0.2\nq = 0.0\nparams = [S, K, τ, r, σ, q]\neurocall(params)\n\n0.02493376819403728\n\n\n\n\n\n\n\n\nTip\n\n\n\nSome terminology in differentiation:\n\nDerivative is generally the scalar rate of change in output relative to a scalar input and can be used in the context of partial derivatives for a multi-variate function (e.g. \\(\\frac{d}{dx}f(x,y,z)\\)).\nGradient is the first derivative with respect to all dimensions of a function that outputs a scalar. For a function \\(f(x,y,z)\\) the gradient would be a vector of partial derivatives such that you would get \\([\\frac{d}{dx},\\frac{d}{dy},\\frac{d}{dz}]\\)\nJacobian is the first derivative with respect to all dimensions of a function that outputs a vector.\nHessian is the second derivative with respect to all dimensions of a function that outputs a scalar.\n\n\n\nWith the above code, now we can get the partial derivaties with respect to each parameter. The first, third, fourth, fifth, and sixth correspond to the common “greeks” delta, theta, rho, vega, and epsilon respectively. The second term is the parital derivative with respect to the strike price:\n\nForwardDiff.gradient(eurocall, params)\n\n6-element Vector{Float64}:\n  0.5399635456230838\n -0.5150297774290467\n  0.16420676980838977\n  0.042331214583209334\n  0.11379886104405816\n -0.04438056539367815\n\n\nWe can also get the second order greeks with a simple call. In addtion to many uncommon second order parital derivatives. Gamma is in the [1,1] position for example:\n\nForwardDiff.hessian(eurocall, params)\n\n6×6 Matrix{Float64}:\n  6.92276    -6.92276    0.242297   0.568994   -0.0853491   -0.613375\n -6.92276     6.92276   -0.07809   -0.526663    0.199148     0.568994\n  0.242297   -0.07809   -0.846846   0.521448    0.685306    -0.559878\n  0.568994   -0.526663   0.521448   0.0432874  -0.0163683   -0.0467667\n -0.0853491   0.199148   0.685306  -0.0163683   0.00245525   0.007015\n -0.613375    0.568994  -0.559878  -0.0467667   0.007015     0.0504144\n\n\n\n14.6.1 Performance\nEarlier we examined the impact on performance for the derivatives using the DualNumber developed in this chapter on a very basic function. What about if we take a more realistic example like eurocall? We can observe approximately a 9x slowdown when computing all of the first order derivatives which isn’t bad considering we are computing 6x of the outputs!\n\n@btime eurocall($params)\n\n  36.500 ns (0 allocations: 0 bytes)\n\n\n0.02493376819403728\n\n\n\nlet\n1    g = similar(params)\n    @btime ForwardDiff.gradient!($g, eurocall, $params)\nend\n\n\n1\n\nTo avoid benchmarking allocating a new array we are able to pre-allocate the memory to store the result and then call gradient! to fill in g for each result.\n\n\n\n\n  341.509 ns (2 allocations: 704 bytes)\n\n\n6-element Vector{Float64}:\n  0.5399635456230838\n -0.5150297774290467\n  0.16420676980838977\n  0.042331214583209334\n  0.11379886104405816\n -0.04438056539367815",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#forward-mode-and-reverse-mode",
    "href": "autodiff.html#forward-mode-and-reverse-mode",
    "title": "14  Automatic Differentiation",
    "section": "14.7 Forward Mode and Reverse Mode",
    "text": "14.7 Forward Mode and Reverse Mode\nThe approach of autodiff outlined about is called forward mode auto-differetiation where the derivative is brought forward through the computation and accumulated through each step. The alternative to this is to first evaluate the function and then work backwards by accumulating the partial derivatives in what’s called reverse mode automatic differentiation.\nReverse mode requires more book-keeping because unlike the forward mode the derivative needs to be carried backwards, unlike the DualNumber approach of forward mode.",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#practical-tips-for-automatic-differentiation",
    "href": "autodiff.html#practical-tips-for-automatic-differentiation",
    "title": "14  Automatic Differentiation",
    "section": "14.8 Practical tips for Automatic Differentiation",
    "text": "14.8 Practical tips for Automatic Differentiation\nHere are a few practical tips to keep in mind.\n\n14.8.1 Choosing between Reverse Mode and Forward Mode\nForward mode is more efficient when the number of outputs is much larger than the number inputs. When the number of inputs is much larger than the number of outputs, then reverse mode will generally be more efficient. Examples of the number of inputs being larger than the outputs might be in a statistical analysis where many features are used to predict a limited number of outcome variables or a complex model with a lot of parameters.\n\n\n14.8.2 Mutation\nAuto-differentiation works through most code, but a particularly tricky part to get right is when values within arrays are mutated (changed). It’s possible to do so but may require a little bit more boilerplate to setup. As of 2024, Enzyme.jl has the best support for functions with mutation inside of them.\n\n\n14.8.3 Custom Rules\nCustom rules for new or unusal functions can be defined, but this is an area that should be explored equipped with a bit of calculus and a deeper understanding of both forward-mode and reverse-mode. ChainRules.jl provides an interface for defining additional rules that hook into the AD infrastrucutre in Julia as well as provide a good set of documentation on how to extend the rules for your custom function.\n\n\n14.8.4 Available Libraries\n\nForwardDiff.jl provides robust forward-mode AD.\nZygote.jl is a reverse-mode package with the innovations of being able to differentiate structs in addition to arrays and scalars.\nEnzyme.jl is a newer package which allows for both forward and reverse mode, but has the advantage of supporting array mutation. Additionally, Enzyme works at the level of LLVM code (an intermediate level between high level Julia code and machine code) which allows for different, sometimes better, optimizations.\n\nIn the authors experience, they would probably recommend ForwardDiff.jl first and then Enzyme.jl if reaching for more advanced functionality or looking for reverse mode.",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#references",
    "href": "autodiff.html#references",
    "title": "14  Automatic Differentiation",
    "section": "14.9 References",
    "text": "14.9 References\n\nhttps://book.sciml.ai/notes/08-Forward-Mode_Automatic_Differentiation_(AD)_via_High_Dimensional_Algebras/\nhttps://blog.esciencecenter.nl/automatic-differentiation-from-scratch-23d50c699555",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "autodiff.html#footnotes",
    "href": "autodiff.html#footnotes",
    "title": "14  Automatic Differentiation",
    "section": "",
    "text": "Richard Feynman has a wonderful, short lecture on algebra here: https://www.feynmanlectures.caltech.edu/I_22.html↩︎",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Automatic Differentiation</span>"
    ]
  },
  {
    "objectID": "optimization.html",
    "href": "optimization.html",
    "title": "15  Optimization",
    "section": "",
    "text": "15.1 In This Chapter\nOptimization as root finding or minimization/maximazation of defined objectives. Differentiable programming and the benefits to optimization problems. Model fitting as an optimization problem.",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "optimization.html#setup",
    "href": "optimization.html#setup",
    "title": "15  Optimization",
    "section": "15.2 Setup",
    "text": "15.2 Setup\n\nusing Flux\nusing LsqFit\nusing CairoMakie\n\n[ Info: Precompiling Flux [587475ba-b771-5e3f-ad9e-33799f191a9c]\nPrecompiling ZygoteColorsExt\n  ✓ Zygote → ZygoteColorsExt\n  1 dependency successfully precompiled in 12 seconds. 74 already precompiled.\n[ Info: Precompiling ZygoteColorsExt [e68c091a-8ea5-5ca7-be4f-380657d4ad79]\n┌ Warning: Module Zygote with build ID fafbfcfd-0d96-2484-0004-53d56b07ad13 is missing from the cache.\n│ This may mean Zygote [e88e6eb3-aa80-5325-afca-941959d7151f] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing ZygoteColorsExt [e68c091a-8ea5-5ca7-be4f-380657d4ad79].\n[ Info: Precompiling LsqFit [2fda8390-95c7-5789-9bda-21331edee243]",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "optimization.html#differentiable-programming",
    "href": "optimization.html#differentiable-programming",
    "title": "15  Optimization",
    "section": "15.3 Differentiable programming",
    "text": "15.3 Differentiable programming\nDifferentiable programming is an approach to programming where functions are defined using differentiable operations, allowing automatic differentiation to be applied to them. Automatic differentiation is a technique used to efficiently compute derivatives of functions, and it is crucial in many machine learning algorithms, optimization techniques, and scientific computing applications.\nElements in differentiable programming\n\nDifferentiable Functions: Functions are defined using operations that are differentiable. These operations include basic arithmetic operations (addition, subtraction, multiplication, division), as well as more complex operations like exponentials, logarithms, trigonometric functions, etc.\nAutomatic Differentiation (AD): Automatic differentiation is used to compute derivatives of functions with respect to their inputs or parameters. AD exploits the fact that every computer program, no matter how complex, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division), and elementary functions (exponentials, logarithms, trigonometric functions). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.\nOptimization and Machine Learning: Differentiable programming is particularly useful in optimization problems, where gradients or higher-order derivatives are required to find the minimum or maximum of a function. It’s also widely used in machine learning, where optimization algorithms like gradient descent are used to train models by adjusting their parameters to minimize a loss function.\n\n\nusing Flux\n\n# Define a differentiable function\nf(x) = 3x^2 + 2x + 1\n# Define an input value\nx = 2.0\n\n@show \"Value of f(x) at x=$x: \", f(x)\n@show \"Gradient of f(x) at x=$x: \", gradient(x -&gt; f(x), x)\n\n(\"Value of f(x) at x=$(x): \", f(x)) = (\"Value of f(x) at x=2.0: \", 17.0)\n(\"Gradient of f(x) at x=$(x): \", gradient((x-&gt;begin\n                #= In[4]:9 =#\n                f(x)\n            end), x)) = (\"Gradient of f(x) at x=2.0: \", (14.0,))\n\n\n(\"Gradient of f(x) at x=2.0: \", (14.0,))",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "optimization.html#model-fitting",
    "href": "optimization.html#model-fitting",
    "title": "15  Optimization",
    "section": "15.4 Model fitting",
    "text": "15.4 Model fitting\n\n15.4.1 Root finding\nRoot finding, also known as root approximation or root isolation, is the process of finding the values of the independent variable (usually denoted as \\(x\\)) for which a given function equals zero. In mathematical terms, if we have a function \\(f(x)\\), root finding involves finding values of \\(x\\) such that \\(f(x)=0\\).\nThere are various algorithms for root finding, each with its own advantages and disadvantages depending on the characteristics of the function and the requirements of the problem. One notable approach is Newton’s method, an iterative method that uses the derivative of the function to approximate the root with increasing accuracy in each iteration.\n\nusing Flux\n\n# Define a differentiable function\nf(x) = 3x^2 + 2x + 1\n# Define an initial value\nx = 1.0\n# tolerance of difference in value\ntol = 1e-6\n# maximum number of iteration of the algorithm\nmax_iter = 100\niter = 0\nwhile abs(f(x)) &gt; tol && iter &lt; max_iter\n    x -= f(x) / gradient(x -&gt; f(x), x)[1]\n    iter += 1\nend\nif iter == max_iter\n    @show \"Warning: Maximum number of iterations reached.\"\nelse\n    @show \"Root found after\", iter, \" iterations.\"\nend\n@show \"Approximate root: \", x\n\n\"Warning: Maximum number of iterations reached.\" = \"Warning: Maximum number of iterations reached.\"\n(\"Approximate root: \", x) = (\"Approximate root: \", -1.391591884376212)\n\n\n(\"Approximate root: \", -1.391591884376212)\n\n\n\n\n15.4.2 Best fitting curve\nIn model fitting, the “best fitting curve” refers to the curve or function that best describes the relationship between the independent and dependent variables in the data. The goal of model fitting is to find the parameters of the chosen curve or function that minimize the difference between the observed data points and the values predicted by the model.\nThe process of finding the best fitting curve typically involves:\n\nChoosing a model: Based on the nature of the data and the underlying relationship between the variables, a suitable model or family of models are selected.\nEstimating parameters: Using the chosen model, one estimates the parameters that best describe the relationship between the variables. This is often done using optimization techniques such as least squares regression, maximum likelihood estimation, or Bayesian inference.\nEvaluating the fit: Once the parameters are estimated, one evaluates the goodness of fit of the model by comparing the predicted values to the observed data. Common metrics for evaluating fit, or error functions, include the residual sum of squares, the coefficient of determination (R-squared), and visual inspection of the residuals.\nIterating if necessary: If the fit is not satisfactory, one may need to iterate on the model or consider alternative models until you find a satisfactory fit to the data.\n\n\nx_data = 0:0.1:10\ny_data = 2 .* sin.(x_data) .+ 0.5 .* randn(length(x_data))\n# Define the model function\nmodel(x, p) = p[1] * x.^2 + p[2] * x .+ p[3]\n# Initial parameter guess\np₀ = [1.0, 1.0, 1.0]\n# Fit the model to the data\nfit_result = curve_fit(model, x_data, y_data, p₀)\n# Extract the fitted parameters\nparams = coef(fit_result)\n# Evaluate the model with the fitted parameters\ny_fit = model(x_data, params)\n# Plot the data and the fitted curve\nfig = Figure()\nAxis(fig[1, 1], title = \"Curve Fitting\")\nscatter!(x_data, y_data, label=\"Data\")\nlines!(x_data, y_fit, label=\"Fitted Curve\", linestyle=:dash, color=:red)\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ND0gA/src/scenes.jl:220",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Optimization</span>"
    ]
  },
  {
    "objectID": "sensitivity-analysis.html",
    "href": "sensitivity-analysis.html",
    "title": "16  Sensitivity Analysis",
    "section": "",
    "text": "16.1 In This Chapter\nDifferent approaches to understanding the sensitivity of a model to changes in its inputs: derivatives, finite differences, global sensitivity analysis approaches, and statistical approaches.",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sensitivity Analysis</span>"
    ]
  },
  {
    "objectID": "sensitivity-analysis.html#setup",
    "href": "sensitivity-analysis.html#setup",
    "title": "16  Sensitivity Analysis",
    "section": "16.2 Setup",
    "text": "16.2 Setup\n\nusing CSV, DataFrames\nusing MortalityTables, Dates\nusing GlobalSensitivity\nusing QuasiMonteCarlo\nusing CairoMakie\n\n\n@enum Sex Female = 1 Male = 2\n@enum Risk Standard = 1 Preferred = 2\n\nmutable struct Policy\n    id::Int\n    sex::Sex\n    benefit_base::Float64\n    COLA::Float64\n    mode::Int\n    issue_date::Date\n    issue_age::Int\n    risk::Risk\nend",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sensitivity Analysis</span>"
    ]
  },
  {
    "objectID": "sensitivity-analysis.html#the-data",
    "href": "sensitivity-analysis.html#the-data",
    "title": "16  Sensitivity Analysis",
    "section": "16.3 The Data",
    "text": "16.3 The Data\n\nsample_csv_data =\n    IOBuffer(\n        raw\"id,sex,benefit_base,COLA,mode,issue_date,issue_age,risk\n         1,M,100000.0,0.03,12,1999-12-05,30,Std\"\n    )\n\nmort = Dict(\n    Male =&gt; MortalityTables.table(988).ultimate,\n    Female =&gt; MortalityTables.table(992).ultimate,\n)\n\nDict{Sex, OffsetArrays.OffsetVector{Float64, Vector{Float64}}} with 2 entries:\n  Male   =&gt; [0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.0225…\n  Female =&gt; [0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.0…\n\n\n\npolicies = let\n\n    # read CSV directly into a dataframe\n    # df = CSV.read(\"sample_inforce.csv\",DataFrame) # use local string for notebook\n    df = CSV.read(sample_csv_data, DataFrame)\n\n    # map over each row and construct an array of Policy objects\n    map(eachrow(df)) do row\n        Policy(\n            row.id,\n            row.sex == \"M\" ? Male : Female,\n            row.benefit_base,\n            row.COLA,\n            row.mode,\n            row.issue_date,\n            row.issue_age,\n            row.risk == \"Std\" ? Standard : Preferred,\n        )\n    end\n\nend\n\n1-element Vector{Policy}:\n Policy(1, Male, 100000.0, 0.03, 12, Date(\"1999-12-05\"), 30, Standard)\n\n\nGiven a basic insurance product, a pure whole of life (WOL) policy with level benefits and level premiums payable within the first 10 years, the reserve at the end of the \\(y^th\\) policy year is defined by\n\\[\nres(y) = \\sum_{t=age+y}^{120} (sur_(t-age-y) * mort_t * B_y * \\sqrt(1 + r)) - (P_y * sur_(t-age-y))\n\\]\nwhere\n\n\\(mort_t\\) is the mortality at age \\(t\\)\n\\(p_y\\) is the survival probability adjusted with COLA, with values of \\(p_(y-1) = 1\\) and \\(p_x = p_(x-1) * (1 - mort_(age+y)) / (1 + COLA)\\) for \\(x &gt;= y\\), and 0 for \\(x &lt; y - 1\\) or \\(age + x &gt;= 120\\), or ultimate age of the current mortality table\n\\(B_y\\) is the level benefit throughout the policy\n\\(P_y\\) is the level premium within the first 10 policy years which is 0 for policy years after 10\n\\(r\\) is the level interest rate throughout the policy\n\n\nfunction sur(y::Int, pol::Policy)\n    if y == 0\n        1\n    elseif y &lt; 0 || 120 - y &lt;= pol.issue_age\n        0\n    else\n        sur(y - 1, pol) * (1 - mort[pol.sex][pol.issue_age+y]) / (1 + pol.COLA)\n    end\nend\n\nfunction res(y::Int, pol::Policy, P::Float64)\n    s = 0.0\n    if y &gt;= 1 && y &lt;= 120 - pol.issue_age\n        for t in (pol.issue_age+y):120\n            prem = 0.0\n            if y &lt;= 9\n                prem = P\n            end\n            s += sur(t - pol.issue_age - y, pol) * mort[pol.sex][t] * pol.benefit_base - prem * sur(t - pol.issue_age - y, pol)\n        end\n    end\n    s\nend\n\nres (generic function with 1 method)",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sensitivity Analysis</span>"
    ]
  },
  {
    "objectID": "sensitivity-analysis.html#common-sensitivity-analysis-methodologies",
    "href": "sensitivity-analysis.html#common-sensitivity-analysis-methodologies",
    "title": "16  Sensitivity Analysis",
    "section": "16.4 Common Sensitivity Analysis Methodologies",
    "text": "16.4 Common Sensitivity Analysis Methodologies\n\n16.4.1 Finite Differences\nDefine a customized finite difference function with respect to the COLA, rippled by a small difference.\n\nfunction res_wrt_r_fd(y::Int, pol::Policy, P::Float64, r::Float64, h=1e-3)\n    p₊, p₋ = deepcopy(pol), deepcopy(pol)\n    p₊.COLA = r + h, p₋.COLA = r - h\n    (res(y, p₊, P) - res(y, p₋, P)) / (2h)\nend\n\nres_wrt_r_fd (generic function with 2 methods)\n\n\n\n\n16.4.2 Scenario Analyses\nScenarios can be generated following scenario generation methodologies to evaluate impacts. Refer to scenario generation chapter.\n\n\n16.4.3 Regression Analyses\n\nfunction r1_wrt_r(r)\n    p = deepcopy(policies[1])\n    p.COLA = r[2]\n    res(Int(floor(r[1])), p, r[3])\nend\n\ngsa(r1_wrt_r, RegressionGSA(), [[1, 1.01], [0.025, 0.035], [10000.0, 10000.1]], samples=1000)\n\nGlobalSensitivity.RegressionGSAResult{Matrix{Float64}, Nothing}([-0.002241307731791374 0.9997042921500664 0.0007396441619925974], [3.854653879225636e-5 0.9997044800328053 -0.0001211228928920829], [0.0015862542026721963 0.9997042955338378 -0.0038074548519504074], nothing, nothing, nothing)\n\n\n\n\n16.4.4 Sobol Indices\nSobol is a variance-based method, and it decomposes the variance of the output of the model or system into fractions which can be attributed to inputs or sets of inputs. This helps to get not just the individual parameter’s sensitivities, but also gives a way to quantify the affect and sensitivity from the interaction between the parameters.\nThe Sobol Indices are “order”ed, the first order indices given by ​ the contribution to the output variance of the main effect of ​ . Therefore, it measures the effect of varying ​ alone, but averaged over variations in other input parameters. It is standardized by the total variance to provide a fractional contribution. Higher-order interaction indices ​ and so on can be formed by dividing other terms in the variance decomposition by Var(Y).\n\nL, U = QuasiMonteCarlo.generate_design_matrices(1000, [1, 0.025, 10000.0], [1, 0.035, 10000.1], SobolSample())\ngsa(r1_wrt_r, Sobol(), L, U)\n\n┌ Warning: The `generate_design_matrices(n, d, sampler, R = NoRand(), num_mats)` method does not produces true and independent QMC matrices, see [this doc warning](https://docs.sciml.ai/QuasiMonteCarlo/stable/design_matrix/) for more context. \n│     Prefer using randomization methods such as `R = Shift()`, `R = MatousekScrambling()`, etc., see [documentation](https://docs.sciml.ai/QuasiMonteCarlo/stable/randomization/)\n└ @ QuasiMonteCarlo ~/.julia/packages/QuasiMonteCarlo/KvLfb/src/RandomizedQuasiMonteCarlo/iterators.jl:255\n\n\nGlobalSensitivity.SobolResult{Vector{Float64}, Nothing, Nothing, Nothing}([-0.0, 1.089514785129876, 3.0140159390210083e-6], nothing, nothing, nothing, [0.0, 1.0013980466757952, 1.689157120266108e-8], nothing)\n\n\n\n\n16.4.5 Morris Method\nThe Morris method also known as Morris’s OAT method where OAT stands for One At a Time can be described in the following steps:\n\\[\nEE_i = \\frac{f(x_1, x_2, ...x_i + \\Delta, ...x_k) - y}{\\Delta}\n\\]\nWe calculate local sensitivity measures known as “elementary effects”, which are calculated by measuring the perturbation in the output of the model on changing one parameter.\nThese are evaluated at various points in the input chosen such that a wide “spread” of the parameter space is explored and considered in the analysis, to provide an approximate global importance measure. The mean and variance of these elementary effects is computed. A high value of the mean implies that a parameter is important, a high variance implies that its effects are non-linear or the result of interactions with other inputs. This method does not evaluate separately the contribution from the interaction and the contribution of the parameters individually and gives the effects for each parameter which takes into consideration all the interactions and its individual contribution.\n\nm = gsa(r1_wrt_r, Morris(), [[1, 1.01], [0.025, 0.035], [10000.0, 10000.1]])\n\nGlobalSensitivity.MorrisResult{Matrix{Float64}, Vector{Any}}([0.0 1.3415886175575547e6 -17.3705883934486], [0.0 1.3415886175575547e6 17.3705883934486], [0.0 7.388987327704615e9 0.5973769562917856], Any[[0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, 0.0, -0.0, -0.0, -0.0, 0.0, -0.0, 0.0, -0.0, -0.0, 0.0, 0.0, -0.0, 0.0], [1.2735236409098222e6, 1.4143258782873524e6, 1.4225250165987348e6, 1.4225245164000192e6, 1.4225242663012377e6, 1.2855799678389216e6, 1.2880116718813693e6, 1.4335506759846301e6, 1.263957687706597e6, 1.2615815569131228e6, 1.4419167206638255e6, 1.2615819864526435e6, 1.2592113190762473e6, 1.259211533340271e6, 1.2242879614379266e6, 1.4363460493428556e6, 1.4363460493428556e6], [-16.7237287603869, -16.72372880360609, -16.72372880360609, -16.7237287603869, -16.723728774793297, -16.723728774793297, -16.7237287603869, -18.024881996174297, -18.000056822427716, -18.000056894459696  …  -16.615834319904348, -16.299669231666666, -16.299669231666666, -16.29966924553342, -16.29966924553342, -16.29966924553342, -18.200388060408965, -18.200388060408965, -18.225709793815426, -18.225709793815426]])\n\n\n\n\n16.4.6 Fourier Amplitude Sensitivity Tests\n\ngsa(r1_wrt_r, eFAST(), [[1, 1.01], [0.025, 0.035], [10000.0, 10000.1]], samples=1000)\n\nGlobalSensitivity.eFASTResult{Matrix{Float64}}([4.545440942097076e-11 0.997674735976488 1.6770867913419316e-8], [7.421006319452417e-7 0.9999975248447088 0.0023460871117844118])",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sensitivity Analysis</span>"
    ]
  },
  {
    "objectID": "sensitivity-analysis.html#benchmarking",
    "href": "sensitivity-analysis.html#benchmarking",
    "title": "16  Sensitivity Analysis",
    "section": "16.5 Benchmarking",
    "text": "16.5 Benchmarking",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Sensitivity Analysis</span>"
    ]
  },
  {
    "objectID": "stochastic.html#footnotes",
    "href": "stochastic.html#footnotes",
    "title": "17  Stochastic Modeling",
    "section": "",
    "text": "Kalos was a pioneer in Monte Carlo techniques, quoted via https://doi.org/10.1007/978-3-540-74686-7_3↩︎",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Stochastic Modeling</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "18  Visualizations",
    "section": "",
    "text": "18.1 In This Chapter\nThe evolved brain and pattern recognition, recommended principles for looking at data, and avoiding common mistakes. Exploratory visualization versus visualizations intended for an audience.",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "matrices.html",
    "href": "matrices.html",
    "title": "19  Matrices and Their Uses",
    "section": "",
    "text": "19.1 In This Chapter\nMatrices and their myriad uses: reframing problems through the eyes of linear algebra, an intuitive refreshing on applicable maths, and recurring patterns of matrix operations in financial modeling.",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Matrices and Their Uses</span>"
    ]
  },
  {
    "objectID": "matrices.html#setup",
    "href": "matrices.html#setup",
    "title": "19  Matrices and Their Uses",
    "section": "19.2 Setup",
    "text": "19.2 Setup\n\nusing LinearAlgebra\nusing Recommendation\nusing SparseArrays\nusing MLDataUtils\nusing Statistics\nusing MultivariateStats\n\nWARNING: using Recommendation.isdefined in module Main conflicts with an existing identifier.\n[ Info: Precompiling MLDataUtils [cc2ba9b6-d476-5e6d-8eaf-a92d5412d41d]\n[ Info: Precompiling MultivariateStats [6f286f6a-111f-5878-ab1e-185364afe411]",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Matrices and Their Uses</span>"
    ]
  },
  {
    "objectID": "matrices.html#matrix-manipulation",
    "href": "matrices.html#matrix-manipulation",
    "title": "19  Matrices and Their Uses",
    "section": "19.3 Matrix manipulation",
    "text": "19.3 Matrix manipulation\n\n19.3.1 Multiplication\n\n# Define two matrices\nA = [1 2 3;\n     4 5 6;\n     7 8 9]\nB = [9 8 7;\n     6 5 4;\n     3 2 1]\n# Perform matrix multiplication\nC = A * B\n# Display the result\nprintln(\"Result of matrix multiplication:\")\nprintln(C)\n\nResult of matrix multiplication:\n[30 24 18; 84 69 54; 138 114 90]\n\n\n\n\n19.3.2 Inversion\n\n# Define a matrix\nA = [1 2; 3 4]\n# Compute the inverse of the matrix\nA_inv = inv(A)\n# Display the result\nprintln(\"Inverse of matrix A:\")\nprintln(A_inv)\n\nInverse of matrix A:\n[-1.9999999999999996 0.9999999999999998; 1.4999999999999998 -0.4999999999999999]",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Matrices and Their Uses</span>"
    ]
  },
  {
    "objectID": "matrices.html#matrix-decomposition",
    "href": "matrices.html#matrix-decomposition",
    "title": "19  Matrices and Their Uses",
    "section": "19.4 Matrix decomposition",
    "text": "19.4 Matrix decomposition\n\n19.4.1 Eigenvalues\nEigenvalue decomposition, also known as eigendecomposition, is a matrix factorization that decomposes a matrix into its eigenvectors and eigenvalues.\n\n# Create a square matrix\nA = [1 2 3;\n     4 5 6;\n     7 8 9]\n# Perform eigenvalue decomposition\neigen_A = eigen(A)\n# Extract eigenvalues and eigenvectors\nλ = eigen_A.values\nV = eigen_A.vectors\n\n# Display the results\nprintln(\"Original Matrix:\")\nprintln(A)\nprintln(\"\\nEigenvalues:\")\nprintln(λ)\nprintln(\"\\nEigenvectors:\")\nprintln(V)\n\nOriginal Matrix:\n[1 2 3; 4 5 6; 7 8 9]\n\nEigenvalues:\n[-1.1168439698070434, -8.582743335036247e-16, 16.11684396980703]\n\nEigenvectors:\n[-0.7858302387420671 0.4082482904638635 -0.2319706872462857; -0.0867513392566285 -0.8164965809277261 -0.5253220933012335; 0.6123275602288101 0.4082482904638627 -0.8186734993561815]\n\n\n\n\n19.4.2 Singular values\nSingular value decomposition breaks a matrix into three matrices U, Σ, and V, representing the left singular vectors, the singular values (diagonal matrix), and the right singular vectors, respectively.\n\n# Create a random matrix\nA = rand(4, 3)\n# Perform Singular Value Decomposition (SVD)\nU, Σ, V = svd(A)\n# U: Left singular vectors\n# Σ: Singular values (diagonal matrix)\n# V: Right singular vectors (transpose)\n# Reconstruct original matrix\nA_reconstructed = U * Diagonal(Σ) * V'\n\n# Display the results\nprintln(\"Original Matrix:\")\nprintln(A)\nprintln(\"\\nLeft Singular Vectors:\")\nprintln(U)\nprintln(\"\\nSingular Values:\")\nprintln(Σ)\nprintln(\"\\nRight Singular Vectors:\")\nprintln(V)\nprintln(\"\\nReconstructed Matrix:\")\nprintln(A_reconstructed)\n\nOriginal Matrix:\n[0.21325580226021357 0.389496833665004 0.17808974883408513; 0.2337025927427112 0.3952331427431379 0.16037239627520417; 0.9674446054847815 0.7191492935055963 0.5580892396688999; 0.627280551451029 0.4399349096043559 0.009970933677968086]\n\nLeft Singular Vectors:\n[-0.2745224034026138 -0.3501747620974075 0.5656278970051957; -0.2820284573915028 -0.2651928275017744 0.5793821615854229; -0.8065038955854518 -0.258692452421888 -0.5313742933081439; -0.4411902828224041 0.8603072795760753 0.24904367764983204]\n\nSingular Values:\n[1.6376317364208604, 0.28919718334786065, 0.22734536205697808]\n\nRight Singular Vectors:\n[-0.7214394896059233 0.5281184077253699 -0.4479017863982736; -0.6060485347786332 -0.1686193629911879 0.7773497822197659; -0.33500781532647395 -0.8322610515013098 -0.4417140543656239]\n\nReconstructed Matrix:\n[0.21325580226021365 0.38949683366500415 0.1780897488340854; 0.23370259274271124 0.395233142743138 0.16037239627520428; 0.9674446054847815 0.7191492935055966 0.5580892396689001; 0.6272805514510289 0.43993490960435605 0.009970933677968169]\n\n\n\n\n19.4.3 Matrix factorization and fatorization machines\nMatrix factorization is a popular technique in recommendation systems for modeling user-item interactions and making personalized recommendations. The core idea behind matrix factorization is to decompose the user-item interaction matrix into two lower-dimensional matrices, capturing latent factors that represent user preferences and item characteristics. By learning these latent factors, the recommendation system can make predictions for unseen user-item pairs.\nFactorization Machines (FM) are a type of supervised machine learning model designed for tasks such as regression and classification, especially in the context of recommendation systems and predictive modeling with sparse data. FM models extend traditional linear models by incorporating interactions between features, allowing them to capture complex relationships within the data.\n\n# Generate synthetic user-item interaction data\nnum_users = 100\nnum_items = 50\nnum_ratings = 500\nuser_ids = rand(1:num_users, num_ratings)\nitem_ids = rand(1:num_items, num_ratings)\nratings = rand(1:5, num_ratings)\n# Create a sparse user-item matrix\nuser_item_matrix = sparse(user_ids, item_ids, ratings)\n# Split data into training and testing sets\ntrain_data, test_data = splitobs(user_item_matrix, 0.8)\n# Set parameters for matrix factorization\nnum_factors = 10\nnum_iterations = 10\n# Train matrix factorization model\ndata = DataAccessor(user_item_matrix)\nrecommender = MF(data) # FactorizationMachines(data) alternatively\nfit!(recommender)\n# Predict ratings for the test set\nrec = Dict()\nfor user in 1:num_users\n     rec[user] = recommend(recommender, user, num_items, collect(1:num_items))\nend\n# Evaluate model performance\npredictions = []\nfor (i, j, v) in zip(findnz(test_data.data)[1], findnz(test_data.data)[2], findnz(test_data.data)[3])\n     for p in rec[i]\n          if p[1] == j\n               push!(predictions, p[2])\n               break\n          end\n     end\nend\nrmse = measure(RMSE(), predictions, nonzeros(test_data.data))\nprintln(\"Root Mean Squared Error (RMSE): \", rmse)\n\nRoot Mean Squared Error (RMSE): 1.2955047477699404\n\n\n\n\n19.4.4 Principal component analysis\nPrincipal Component Analysis (PCA) is a widely used technique in various fields for dimensionality reduction, data visualization, feature extraction, and noise reduction. PCA can also be applied to detect anomalies or outliers in the data by identifying data points that deviate significantly from the normal patterns captured by the principal components. Anomalies may appear as data points with large reconstruction errors or as outliers in the low-dimensional space spanned by the principal components.\n\n# Generate some synthetic data\ndata = randn(100, 5)  # 100 samples, 5 features\n# Perform PCA\npca_model = fit(PCA, data; maxoutdim=2)  # Project to 2 principal components\n# Transform the data\ntransformed_data = transform(pca_model, data)\n# Access principal components and explained variance ratio\nprincipal_components = pca_model.prinvars\nexplained_variance_ratio = pca_model.prinvars / sum(pca_model.prinvars)\n\n# Print results\nprintln(\"Principal Components:\")\nprintln(principal_components)\nprintln(\"Explained Variance Ratio:\")\nprintln(explained_variance_ratio)\n\nPrincipal Components:\n[36.99984564067487, 29.433043788899607]\nExplained Variance Ratio:\n[0.556950720620069, 0.443049279379931]",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Matrices and Their Uses</span>"
    ]
  },
  {
    "objectID": "data-learning.html",
    "href": "data-learning.html",
    "title": "20  Learning from Data",
    "section": "",
    "text": "20.1 In this chapter\nUsing data to inform a model: fitting parameters, forecasting, and fundamental limitations on prediction. Also covered are elements of practical review such as static and dynamic validations, and implied rate analysis.",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Learning from Data</span>"
    ]
  },
  {
    "objectID": "data-learning.html#setup",
    "href": "data-learning.html#setup",
    "title": "20  Learning from Data",
    "section": "20.2 Setup",
    "text": "20.2 Setup\n\nusing MLJ\nusing StatsBase\nusing Flux\n\n[ Info: Precompiling MLJ [add582a8-e3ab-11e8-2d5e-e98b27df1bc7]\n┌ Warning: Module MLJBase with build ID fafbfcfd-6a14-ab4b-0001-d6175d31fa79 is missing from the cache.\n│ This may mean MLJBase [a7f614a8-145f-11e9-1d2a-a57a1082229d] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing MLJ [add582a8-e3ab-11e8-2d5e-e98b27df1bc7].\nPrecompiling MLJBase\n  ✓ BangBang → BangBangChainRulesCoreExt\n  ✓ BangBang → BangBangStaticArraysExt\n  ✓ MicroCollections\n  ✓ Transducers\n  ✓ FLoops\n    MLUtils Being precompiled by another process (pid: 60172, pidfile: /Users/alec/.julia/compiled/v1.10/MLUtils/S1qG5_gIMmk.ji.pidfile)\n  ✓ MLUtils\n    StatisticalMeasuresBase Being precompiled by another process (pid: 60141, pidfile: /Users/alec/.julia/compiled/v1.10/StatisticalMeasuresBase/xdFPe_gIMmk.ji.pidfile)\n  ✓ StatisticalMeasuresBase\n    MLJBase Being precompiled by another process (pid: 60138, pidfile: /Users/alec/.julia/compiled/v1.10/MLJBase/jaWQl_gIMmk.ji.pidfile)\n  ✓ MLJBase\n  8 dependencies successfully precompiled in 9 seconds. 115 already precompiled.\n[ Info: Precompiling MLJBase [a7f614a8-145f-11e9-1d2a-a57a1082229d]\nPrecompiling CategoricalArraysJSONExt\n  ✓ CategoricalArrays → CategoricalArraysJSONExt\n  1 dependency successfully precompiled in 0 seconds. 13 already precompiled.\n[ Info: Precompiling CategoricalArraysJSONExt [3659b7b4-73bc-5a91-ba4f-cc4948098fdb]\n┌ Warning: Module CategoricalArrays with build ID fafbfcfd-ad22-616f-0001-d622efcfa27f is missing from the cache.\n│ This may mean CategoricalArrays [324d7699-5711-5eae-9e2f-1d82baa6b597] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing CategoricalArraysJSONExt [3659b7b4-73bc-5a91-ba4f-cc4948098fdb].\n┌ Warning: The call to compilecache failed to create a usable precompiled cache file for MLJBase [a7f614a8-145f-11e9-1d2a-a57a1082229d]\n│   exception = Required dependency ScientificTypes [321657f4-b219-11e9-178b-2701a2544e81] failed to load from a cache file.\n└ @ Base loading.jl:1992\n[ Info: Precompiling ScientificTypes [321657f4-b219-11e9-178b-2701a2544e81]\n┌ Warning: Module CategoricalArrays with build ID fafbfcfd-ad22-616f-0001-d622efcfa27f is missing from the cache.\n│ This may mean CategoricalArrays [324d7699-5711-5eae-9e2f-1d82baa6b597] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing ScientificTypes [321657f4-b219-11e9-178b-2701a2544e81].\n[ Info: Precompiling CategoricalDistributions [af321ab8-2d2e-40a6-b165-3d674595d28e]\n┌ Warning: Module ScientificTypes with build ID ffffffff-ffff-ffff-0001-d62760f24022 is missing from the cache.\n│ This may mean ScientificTypes [321657f4-b219-11e9-178b-2701a2544e81] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing CategoricalDistributions [af321ab8-2d2e-40a6-b165-3d674595d28e].\nPrecompiling StatisticalMeasuresBase\n  ✓ BangBang → BangBangChainRulesCoreExt\n  ✓ BangBang → BangBangStaticArraysExt\n  ✓ BangBang → BangBangStructArraysExt\n    Transducers Being precompiled by another process (pid: 60324, pidfile: /Users/alec/.julia/compiled/v1.10/Transducers/cATK6_gIMmk.ji.pidfile)\n  ✓ Transducers\n    FLoops Being precompiled by another process (pid: 60322, pidfile: /Users/alec/.julia/compiled/v1.10/FLoops/99zMn_gIMmk.ji.pidfile)\n  ✓ FLoops\n    MLUtils Being precompiled by another process (pid: 60319, pidfile: /Users/alec/.julia/compiled/v1.10/MLUtils/S1qG5_gIMmk.ji.pidfile)\n  ✓ MLUtils\n    StatisticalMeasuresBase Being precompiled by another process (pid: 60296, pidfile: /Users/alec/.julia/compiled/v1.10/StatisticalMeasuresBase/xdFPe_gIMmk.ji.pidfile)\n  ✓ StatisticalMeasuresBase\n  7 dependencies successfully precompiled in 9 seconds. 75 already precompiled.\n[ Info: Precompiling StatisticalMeasuresBase [c062fc1d-0d66-479b-b6ac-8b44719de4cc]\n┌ Warning: Module CategoricalArrays with build ID fafbfcfd-ad22-616f-0001-d622efcfa27f is missing from the cache.\n│ This may mean CategoricalArrays [324d7699-5711-5eae-9e2f-1d82baa6b597] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing StatisticalMeasuresBase [c062fc1d-0d66-479b-b6ac-8b44719de4cc].\n[ Info: Precompiling MLUtils [f1d291b0-491e-4a28-83b9-f70985020b54]\n┌ Warning: The call to compilecache failed to create a usable precompiled cache file for FLoops [cc61a311-1640-44b5-9fba-1b764f453329]\n│   exception = Required dependency BangBang [198e06fe-97b7-11e9-32a5-e1d131e6ad66] failed to load from a cache file.\n└ @ Base loading.jl:1992\n[ Info: Skipping precompilation since __precompile__(false). Importing MLUtils [f1d291b0-491e-4a28-83b9-f70985020b54].\nPrecompiling FLoops\n    BangBangStaticArraysExt Being precompiled by another process (pid: 60092, pidfile: /Users/alec/.julia/compiled/v1.10/BangBangStaticArraysExt/I8ZlX_gIMmk.ji.pidfile)\n  ✓ BangBang → BangBangStaticArraysExt\n    BangBangStructArraysExt Being precompiled by another process (pid: 60092, pidfile: /Users/alec/.julia/compiled/v1.10/BangBangStructArraysExt/8XF7E_gIMmk.ji.pidfile)\n  ✓ BangBang → BangBangStructArraysExt\n    MicroCollections Being precompiled by another process (pid: 60092, pidfile: /Users/alec/.julia/compiled/v1.10/MicroCollections/UymYM_gIMmk.ji.pidfile)\n  ✓ MicroCollections\n    Transducers Being precompiled by another process (pid: 60092, pidfile: /Users/alec/.julia/compiled/v1.10/Transducers/cATK6_gIMmk.ji.pidfile)\n  ✓ Transducers\n    FLoops Being precompiled by another process (pid: 60092, pidfile: /Users/alec/.julia/compiled/v1.10/FLoops/99zMn_gIMmk.ji.pidfile)\n  ✓ FLoops\n  5 dependencies successfully precompiled in 3 seconds. 37 already precompiled.\n[ Info: Precompiling FLoops [cc61a311-1640-44b5-9fba-1b764f453329]\nPrecompiling BangBangChainRulesCoreExt\n  ✓ BangBang → BangBangChainRulesCoreExt\n  ✓ BangBang → BangBangStaticArraysExt\n  2 dependencies successfully precompiled in 1 seconds. 20 already precompiled.\n[ Info: Precompiling BangBangChainRulesCoreExt [47e8a63d-7df8-5da4-81a4-8f5796ea640c]\n┌ Warning: Module BangBang with build ID fafbfcfd-5ce8-4b62-0001-d62bc1114941 is missing from the cache.\n│ This may mean BangBang [198e06fe-97b7-11e9-32a5-e1d131e6ad66] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing BangBangChainRulesCoreExt [47e8a63d-7df8-5da4-81a4-8f5796ea640c].\n[ Info: Precompiling BangBangStaticArraysExt [a9f1882a-14fa-573e-a12d-824431257a23]\n┌ Warning: Module BangBang with build ID fafbfcfd-5ce8-4b62-0001-d62bc1114941 is missing from the cache.\n│ This may mean BangBang [198e06fe-97b7-11e9-32a5-e1d131e6ad66] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing BangBangStaticArraysExt [a9f1882a-14fa-573e-a12d-824431257a23].\nPrecompiling BangBangStructArraysExt\n  ✓ BangBang → BangBangStructArraysExt\n  1 dependency successfully precompiled in 0 seconds. 23 already precompiled.\n[ Info: Precompiling BangBangStructArraysExt [d139770a-8b79-56c4-91f8-7273c836fd96]\n┌ Warning: Module BangBang with build ID fafbfcfd-5ce8-4b62-0001-d62bc1114941 is missing from the cache.\n│ This may mean BangBang [198e06fe-97b7-11e9-32a5-e1d131e6ad66] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing BangBangStructArraysExt [d139770a-8b79-56c4-91f8-7273c836fd96].\n┌ Warning: The call to compilecache failed to create a usable precompiled cache file for FLoops [cc61a311-1640-44b5-9fba-1b764f453329]\n│   exception = Required dependency MicroCollections [128add7d-3638-4c79-886c-908ea0c25c34] failed to load from a cache file.\n└ @ Base loading.jl:1992\n[ Info: Precompiling Transducers [28d57a85-8fef-5791-bfe6-a80928e7c999]\n┌ Warning: Module BangBang with build ID fafbfcfd-5ce8-4b62-0001-d62bc1114941 is missing from the cache.\n│ This may mean BangBang [198e06fe-97b7-11e9-32a5-e1d131e6ad66] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing Transducers [28d57a85-8fef-5791-bfe6-a80928e7c999].\n[ Info: Precompiling MicroCollections [128add7d-3638-4c79-886c-908ea0c25c34]\n┌ Warning: Module BangBang with build ID fafbfcfd-5ce8-4b62-0001-d62bc1114941 is missing from the cache.\n│ This may mean BangBang [198e06fe-97b7-11e9-32a5-e1d131e6ad66] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing MicroCollections [128add7d-3638-4c79-886c-908ea0c25c34].\n[ Info: Precompiling NNlib [872c559c-99b0-510c-b3b7-b6c96a88d5cd]\n[ Info: Precompiling CategoricalArraysRecipesBaseExt [b7a5aa15-52bc-53b2-90b5-730883a6b80e]\n┌ Warning: Module CategoricalArrays with build ID fafbfcfd-ad22-616f-0001-d622efcfa27f is missing from the cache.\n│ This may mean CategoricalArrays [324d7699-5711-5eae-9e2f-1d82baa6b597] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing CategoricalArraysRecipesBaseExt [b7a5aa15-52bc-53b2-90b5-730883a6b80e].\nPrecompiling MLJEnsembles\n    MLUtils Being precompiled by another process (pid: 60565, pidfile: /Users/alec/.julia/compiled/v1.10/MLUtils/S1qG5_gIMmk.ji.pidfile)\n  ✓ MLUtils\n    StatisticalMeasuresBase Being precompiled by another process (pid: 60562, pidfile: /Users/alec/.julia/compiled/v1.10/StatisticalMeasuresBase/xdFPe_gIMmk.ji.pidfile)\n  ✓ StatisticalMeasuresBase\n  ✓ MLJEnsembles\n  3 dependencies successfully precompiled in 8 seconds. 116 already precompiled.\n  2 dependencies precompiled but different versions are currently loaded. Restart julia to access the new versions\n[ Info: Precompiling MLJEnsembles [50ed68f4-41fd-4504-931a-ed422449fee0]\n┌ Warning: Module CategoricalArrays with build ID fafbfcfd-ad22-616f-0001-d622efcfa27f is missing from the cache.\n│ This may mean CategoricalArrays [324d7699-5711-5eae-9e2f-1d82baa6b597] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing MLJEnsembles [50ed68f4-41fd-4504-931a-ed422449fee0].\nPrecompiling MLJTuning\n    StatisticalMeasures Being precompiled by another process (pid: 60557, pidfile: /Users/alec/.julia/compiled/v1.10/StatisticalMeasures/09gry_gIMmk.ji.pidfile)\n  ✓ StatisticalMeasures\n    ScientificTypesExt Being precompiled by another process (pid: 60092, pidfile: /Users/alec/.julia/compiled/v1.10/ScientificTypesExt/BrnMF_gIMmk.ji.pidfile)\n    DefaultMeasuresExt Being precompiled by another process (pid: 60092, pidfile: /Users/alec/.julia/compiled/v1.10/DefaultMeasuresExt/hjROW_gIMmk.ji.pidfile)\n  ✓ MLJBase → DefaultMeasuresExt\n    MLJTuning Being precompiled by another process (pid: 60092, pidfile: /Users/alec/.julia/compiled/v1.10/MLJTuning/m3zbf_gIMmk.ji.pidfile)\n  ✓ MLJTuning\n  ✓ StatisticalMeasures → ScientificTypesExt\n  4 dependencies successfully precompiled in 41 seconds. 125 already precompiled.\n  1 dependency had output during precompilation:\n┌ StatisticalMeasures → ScientificTypesExt\n│  ┌ Warning: The call to compilecache failed to create a usable precompiled cache file for StatisticalMeasures [a19d573c-0a75-4610-95b3-7071388c7541]\n│  │   exception = Required dependency CategoricalDistributions [af321ab8-2d2e-40a6-b165-3d674595d28e] failed to load from a cache file.\n│  └ @ Base loading.jl:1992\n└  \n[ Info: Precompiling MLJTuning [03970b2e-30c4-11ea-3135-d1576263f10f]\n┌ Warning: Module MLJBase with build ID ffffffff-ffff-ffff-0001-d62742eefcc3 is missing from the cache.\n│ This may mean MLJBase [a7f614a8-145f-11e9-1d2a-a57a1082229d] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing MLJTuning [03970b2e-30c4-11ea-3135-d1576263f10f].\n[ Info: Precompiling LatinHypercubeSampling [a5e1c1ea-c99a-51d3-a14d-a9a37257b02d]\nPrecompiling MLJModels\n  ✓ CategoricalArrays → CategoricalArraysRecipesBaseExt\n  ✓ CategoricalArrays → CategoricalArraysJSONExt\n  ✓ ScientificTypes\n  ✓ CategoricalDistributions\n  ✓ MLJModels\n  5 dependencies successfully precompiled in 5 seconds. 73 already precompiled.\n  4 dependencies precompiled but different versions are currently loaded. Restart julia to access the new versions\n[ Info: Precompiling MLJModels [d491faf4-2d78-11e9-2867-c94bc002c0b7]\n┌ Warning: Module ScientificTypes with build ID ffffffff-ffff-ffff-0001-d62760f24022 is missing from the cache.\n│ This may mean ScientificTypes [321657f4-b219-11e9-178b-2701a2544e81] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing MLJModels [d491faf4-2d78-11e9-2867-c94bc002c0b7].\nPrecompiling OpenML\n  ✓ ARFFFiles\n  ✓ OpenML\n  2 dependencies successfully precompiled in 1 seconds. 36 already precompiled.\n[ Info: Precompiling OpenML [8b6db2d4-7670-4922-a472-f9537c81ab66]\n┌ Warning: Module CategoricalArrays with build ID fafbfcfd-ad22-616f-0001-d622efcfa27f is missing from the cache.\n│ This may mean CategoricalArrays [324d7699-5711-5eae-9e2f-1d82baa6b597] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing OpenML [8b6db2d4-7670-4922-a472-f9537c81ab66].\n[ Info: Precompiling ARFFFiles [da404889-ca92-49ff-9e8b-0aa6b4d38dc8]\n┌ Warning: Module CategoricalArrays with build ID fafbfcfd-ad22-616f-0001-d622efcfa27f is missing from the cache.\n│ This may mean CategoricalArrays [324d7699-5711-5eae-9e2f-1d82baa6b597] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing ARFFFiles [da404889-ca92-49ff-9e8b-0aa6b4d38dc8].\nPrecompiling MLJFlow\n  ✓ BangBang → BangBangStructArraysExt\n  ✓ MLFlowClient\n  ✓ StatisticalMeasuresBase\n  ✓ MLJBase\n  ✓ StatisticalMeasures\nScientificTypesExt Waiting for background task / IO / timer.\n[pid 61108] waiting for IO to finish:\n Handle type        uv_handle_t-&gt;data\nThis means that a package has started a background task or event source that has not finished running. For precompilation to complete successfully, the event source needs to be closed explicitly. See the developer documentation on fixing precompilation hangs for more help.\n  ✓ StatisticalMeasures → ScientificTypesExt\n  ✓ MLJBase → DefaultMeasuresExt\n  ✓ MLJFlow\n  8 dependencies successfully precompiled in 51 seconds. 135 already precompiled.\n  3 dependencies precompiled but different versions are currently loaded. Restart julia to access the new versions\n  2 dependencies had output during precompilation:\n┌ MLJBase → DefaultMeasuresExt\n│  ┌ Warning: attempting to remove probably stale pidfile\n│  │   path = \"/Users/alec/.julia/compiled/v1.10/NNlib/A7zdE_gIMmk.ji.pidfile\"\n│  └ @ FileWatching.Pidfile ~/.julia/juliaup/julia-1.10.3+0.aarch64.apple.darwin14/share/julia/stdlib/v1.10/FileWatching/src/pidfile.jl:273\n└  \n┌ StatisticalMeasures → ScientificTypesExt\n│  [pid 61108] waiting for IO to finish:\n│   Handle type        uv_handle_t-&gt;data\n│  This means that a package has started a background task or event source that has not finished running. For precompilation to complete successfully, the event source needs to be closed explicitly. See the developer documentation on fixing precompilation hangs for more help.\n└  \n[ Info: Precompiling MLJFlow [7b7b8358-b45c-48ea-a8ef-7ca328ad328f]\n┌ Warning: Module MLJBase with build ID ffffffff-ffff-ffff-0001-d62742eefcc3 is missing from the cache.\n│ This may mean MLJBase [a7f614a8-145f-11e9-1d2a-a57a1082229d] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing MLJFlow [7b7b8358-b45c-48ea-a8ef-7ca328ad328f].\n[ Info: Precompiling MLFlowClient [64a0f543-368b-4a9a-827a-e71edb2a0b83]\n┌ Warning: Module HTTP with build ID fafbfcfd-82ce-fd39-0001-d63da5eaf7dc is missing from the cache.\n│ This may mean HTTP [cd3eb016-35fb-5094-929b-558a96fad6f3] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing MLFlowClient [64a0f543-368b-4a9a-827a-e71edb2a0b83].\nPrecompiling StatisticalMeasures\n  ✓ BangBang → BangBangStructArraysExt\n  1 dependency successfully precompiled in 0 seconds. 117 already precompiled.\n  1 dependency precompiled but a different version is currently loaded. Restart julia to access the new version\n[ Info: Precompiling StatisticalMeasures [a19d573c-0a75-4610-95b3-7071388c7541]\n┌ Warning: Module CategoricalArrays with build ID fafbfcfd-ad22-616f-0001-d622efcfa27f is missing from the cache.\n│ This may mean CategoricalArrays [324d7699-5711-5eae-9e2f-1d82baa6b597] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing StatisticalMeasures [a19d573c-0a75-4610-95b3-7071388c7541].\n[ Info: Precompiling DefaultMeasuresExt [fe5f6ac0-33d3-5066-8083-2d519bb7c971]\n┌ Warning: Module MLJBase with build ID ffffffff-ffff-ffff-0001-d62742eefcc3 is missing from the cache.\n│ This may mean MLJBase [a7f614a8-145f-11e9-1d2a-a57a1082229d] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing DefaultMeasuresExt [fe5f6ac0-33d3-5066-8083-2d519bb7c971].\n[ Info: Precompiling ScientificTypesExt [106586fb-0781-5151-be1c-b45c40028622]\n┌ Warning: Module ScientificTypes with build ID ffffffff-ffff-ffff-0001-d62760f24022 is missing from the cache.\n│ This may mean ScientificTypes [321657f4-b219-11e9-178b-2701a2544e81] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing ScientificTypesExt [106586fb-0781-5151-be1c-b45c40028622].\nPrecompiling MLJBalancing\n  ✓ MLJBalancing\n  1 dependency successfully precompiled in 1 seconds. 126 already precompiled.\n[ Info: Precompiling MLJBalancing [45f359ea-796d-4f51-95a5-deb1a414c586]\n┌ Warning: Module MLJBase with build ID ffffffff-ffff-ffff-0001-d62742eefcc3 is missing from the cache.\n│ This may mean MLJBase [a7f614a8-145f-11e9-1d2a-a57a1082229d] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing MLJBalancing [45f359ea-796d-4f51-95a5-deb1a414c586].\nPrecompiling MLJIteration\n  ✓ MLJIteration\n  1 dependency successfully precompiled in 1 seconds. 128 already precompiled.\n[ Info: Precompiling MLJIteration [614be32b-d00c-4edb-bd02-1eb411ab5e55]\n┌ Warning: Module MLJBase with build ID ffffffff-ffff-ffff-0001-d62742eefcc3 is missing from the cache.\n│ This may mean MLJBase [a7f614a8-145f-11e9-1d2a-a57a1082229d] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing MLJIteration [614be32b-d00c-4edb-bd02-1eb411ab5e55].\nPrecompiling Flux\n  ✓ OneHotArrays\n  ✓ Flux\n  2 dependencies successfully precompiled in 3 seconds. 111 already precompiled.\n[ Info: Precompiling Flux [587475ba-b771-5e3f-ad9e-33799f191a9c]\n┌ Warning: Module NNlib with build ID fafbfcfd-b639-1dcb-0001-d62df13d2011 is missing from the cache.\n│ This may mean NNlib [872c559c-99b0-510c-b3b7-b6c96a88d5cd] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing Flux [587475ba-b771-5e3f-ad9e-33799f191a9c].\n[ Info: Precompiling Optimisers [3bd65402-5787-11e9-1adc-39752487f4e2]\n[ Info: Precompiling Zygote [e88e6eb3-aa80-5325-afca-941959d7151f]\n[ Info: Precompiling ZygoteColorsExt [e68c091a-8ea5-5ca7-be4f-380657d4ad79]\n[ Info: Precompiling OneHotArrays [0b1bfda6-eb8a-41d2-88d8-f5af5cad476f]\n┌ Warning: Module NNlib with build ID fafbfcfd-b639-1dcb-0001-d62df13d2011 is missing from the cache.\n│ This may mean NNlib [872c559c-99b0-510c-b3b7-b6c96a88d5cd] does not support precompilation but is imported by a module that does.\n└ @ Base loading.jl:1948\n[ Info: Skipping precompilation since __precompile__(false). Importing OneHotArrays [0b1bfda6-eb8a-41d2-88d8-f5af5cad476f].",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Learning from Data</span>"
    ]
  },
  {
    "objectID": "data-learning.html#applications",
    "href": "data-learning.html#applications",
    "title": "20  Learning from Data",
    "section": "20.3 Applications",
    "text": "20.3 Applications\n\n20.3.1 Parameter fitting\nRefer to the chapter on Optimization for more details.\n\n\n20.3.2 Forecasting\n\n\n20.3.3 Static and dynamic validation\nStatic validation typically involves splitting the dataset into training and testing sets, where the testing set is held out and not used during model training. The model is trained on the training set and then evaluated on the held-out testing set to assess its performance. This approach helps to measure how well the model generalizes to unseen data.\nDynamic validation, on the other hand, involves using a rolling or expanding window to train and test the model iteratively over time. In each iteration, the model is trained on past data and tested on future data, simulating how the model would perform in a real-world scenario where new data becomes available over time. This approach helps to assess the model’s ability to adapt to changing patterns and trends in the data.\n\nnum_samples = 100\n# Generate synthetic time series data\ndata = rand(100)\nX = [ones(100) data]\ny = 2data .+ 1 .+ 0.1 * randn(num_samples, 1)  # dependent variable with noise\n# Train the model on the training set\nθ = X \\ y\n# Predictions\ny_pred = θ[2] .* data .+ θ[1]\n# Compute evaluation metrics\nmse = mean((y_pred .- y) .^ 2)\nmae = mean(abs.(y_pred .- y))\n\nprintln(\"Static validation results:\")\nprintln(\"Mean Squared Error (MSE): \", mse)\nprintln(\"Mean Absolute Error (MAE): \", mae)\n\n# Dynamic validation to update model over time and evaluate\nnum_updates = 5\nmse_dyn = Float64[]\nmae_dyn = Float64[]\nfor i in 1:num_updates\n    data = rand(100)\n    X = [ones(100) data]\n    y = 2data .+ 1 .+ 0.1 * randn(num_samples, 1)  # dependent variable with noise\n    # Train the model on the training set\n    θ = X \\ y\n    # Predictions\n    y_pred = θ[2] .* data .+ θ[1]\n    # Compute evaluation metrics\n    mse = mean((y_pred .- y) .^ 2)\n    mae = mean(abs.(y_pred .- y))\n    push!(mse_dyn, mse)\n    push!(mae_dyn, mae)\nend\n\nprintln(\"Dynamic validation results:\")\nprintln(\"Mean Squared Error (MSE): \", mean(mse_dyn))\nprintln(\"Mean Absolute Error (MAE): \", mean(mae_dyn))\n\nStatic validation results:\nMean Squared Error (MSE): 0.010192128501064917\nMean Absolute Error (MAE): 0.08168939460817073\nDynamic validation results:\nMean Squared Error (MSE): 0.010976280017658496\nMean Absolute Error (MAE): 0.08444382741105269\n\n\n\n\n20.3.4 Implied rate analysis\nImplied rates are rates that are derived from the prices of financial instruments, such as bonds or options. For example, in the context of bonds, the implied rate is the interest rate that equates the present value of future cash flows from the bond (coupons and principal) to its current market price.\n\n# Define the bond cash flows and prices\ncash_flows = [100, 100, 100, 100, 1000]  # Coupons and principal\nprices = [95, 96, 97, 98, 1050]           # Market prices\n# Define a function to calculate the present value of cash flows given a rate\nfunction present_value(rate, cash_flows)\n    pv = 0\n    for (i, cf) in enumerate(cash_flows)\n        pv += cf / (1 + rate)^i\n    end\n    return pv\nend\n# Define a function to calculate the implied rate using bisection method\nfunction implied_rate(cash_flows, price)\n    f(rate) = present_value(rate, cash_flows) - price\n    return rootassign(f, 0.0, 1.0)\nend\nfunction rootassign(f, l, u)\n    # Define an initial value\n    x = 1.0\n    # tolerance of difference in value\n    tol = 1e-6\n    # maximum number of iteration of the algorithm\n    max_iter = 100\n    iter = 0\n    while abs(f(x)) &gt; tol && iter &lt; max_iter\n        x -= f(x) / gradient(x -&gt; f(x), x)[1]\n        iter += 1\n    end\n    if iter &lt; max_iter && l &lt; x &lt; u\n        return x\n    else\n        return -1.0\n    end\nend\n# Calculate implied rates for each bond\nimplied_rates = [implied_rate(cash_flows, price) for price in prices]\n# Print the results\nfor (i, rate) in enumerate(implied_rates)\n    println(\"Implied rate for bond $i: $rate\")\nend\n\nImplied rate for bond 1: -1.0\nImplied rate for bond 2: -1.0\nImplied rate for bond 3: -1.0\nImplied rate for bond 4: -1.0\nImplied rate for bond 5: -1.0",
    "crumbs": [
      "Computational Thinking in an Actuarial and Financial Context",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Learning from Data</span>"
    ]
  },
  {
    "objectID": "stochastic-mortality.html",
    "href": "stochastic-mortality.html",
    "title": "21  Stochastic Mortality Projections",
    "section": "",
    "text": "21.1 In This Chapter\nA term life insurance policy is used to illustrate: selecting key model features, design tradeoffs between a few different approaches, and a discussion of the performance impacts of the different approaches to parallelism.",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Stochastic Mortality Projections</span>"
    ]
  },
  {
    "objectID": "stochastic-mortality.html#setup",
    "href": "stochastic-mortality.html#setup",
    "title": "21  Stochastic Mortality Projections",
    "section": "21.2 Setup",
    "text": "21.2 Setup\n\nusing CSV, DataFrames\nusing MortalityTables, ActuaryUtilities\nusing Dates\nusing ThreadsX\nusing BenchmarkTools\nusing Random\nusing CairoMakie\n\nDefine a datatype. Not strictly necessary, but will make extending the program with more functions easier.\nType annotations are optional, but providing them is able to coerce the values to be all plain bits (i.e. simple, non-referenced values like arrays are) when the type is constructed. This makes the whole data be stored in the stack and is an example of data-oriented design. It’s much slower without the type annotations (~0.5 million policies per second, ~50x slower).\n\n@enum Sex Female = 1 Male = 2\n@enum Risk Standard = 1 Preferred = 2\n\n\nstruct Policy\n    id::Int\n    sex::Sex\n    benefit_base::Float64\n    COLA::Float64\n    mode::Int\n    issue_date::Date\n    issue_age::Int\n    risk::Risk\nend",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Stochastic Mortality Projections</span>"
    ]
  },
  {
    "objectID": "stochastic-mortality.html#the-data",
    "href": "stochastic-mortality.html#the-data",
    "title": "21  Stochastic Mortality Projections",
    "section": "21.3 The Data",
    "text": "21.3 The Data\n\nsample_csv_data =\n    IOBuffer(\n        raw\"id,sex,benefit_base,COLA,mode,issue_date,issue_age,risk\n         1,M,100000.0,0.03,12,1999-12-05,30,Std\n         2,F,200000.0,0.03,12,1999-12-05,30,Pref\"\n    )\n\nIOBuffer(data=UInt8[...], readable=true, writable=false, seekable=true, append=false, size=152, maxsize=Inf, ptr=1, mark=-1)\n\n\n\npolicies = let\n\n    # read CSV directly into a dataframe\n    # df = CSV.read(\"sample_inforce.csv\",DataFrame) # use local string for notebook\n    df = CSV.read(sample_csv_data, DataFrame)\n\n    # map over each row and construct an array of Policy objects\n    map(eachrow(df)) do row\n        Policy(\n            row.id,\n            row.sex == \"M\" ? Male : Female,\n            row.benefit_base,\n            row.COLA,\n            row.mode,\n            row.issue_date,\n            row.issue_age,\n            row.risk == \"Std\" ? Standard : Preferred,\n        )\n    end\n\n\nend\n\n2-element Vector{Policy}:\n Policy(1, Male, 100000.0, 0.03, 12, Date(\"1999-12-05\"), 30, Standard)\n Policy(2, Female, 200000.0, 0.03, 12, Date(\"1999-12-05\"), 30, Preferred)\n\n\nDefine what mortality gets used:\n\nmort = Dict(\n    Male =&gt; MortalityTables.table(988).ultimate,\n    Female =&gt; MortalityTables.table(992).ultimate,\n)\n\nfunction mortality(pol::Policy, params)\n    return params.mortality[pol.sex]\nend\n\nmortality (generic function with 1 method)\n\n\nThis defines the core logic of the policy projection and will write the results to the given out container (here, a named tuple of arrays).\nThis is using a threaded approach where it could be operating on any of the computer’s available threads, thus acheiving thread-based parallelism - as opposed to multi-processor (multi-machine) or GPU-based computation, which requires formulating the problem a bit differently (array/matrix based). For the scale of computation here, I think I’d apply this model of parallelism.\n\nfunction pol_project!(out, policy, params)\n    # some starting values for the given policy\n    dur = duration(policy.issue_date, params.val_date)\n    start_age = policy.issue_age + dur - 1\n    COLA_factor = (1 + policy.COLA)\n    cur_benefit = policy.benefit_base * COLA_factor^(dur - 1)\n\n    # get the right mortality vector\n    qs = mortality(policy, params)\n\n    # grab the current thread's id to write to results container without conflicting with other threads\n    tid = Threads.threadid()\n\n    ω = lastindex(qs)\n\n    # inbounds turns off bounds-checking, which makes hot loops faster but first write loop without it to ensure you don't create an error (will crash if you have the error without bounds checking)\n    @inbounds for t in 1:min(params.proj_length, ω - start_age)\n\n        q = qs[start_age+t] # get current mortality\n\n        if (rand() &lt; q)\n            return # if dead then just return and don't increment the results anymore\n        else\n            # pay benefit, add a life to the output count, and increment the benefit for next year\n            out.benefits[t, tid] += cur_benefit\n            out.lives[t, tid] += 1\n            cur_benefit *= COLA_factor\n        end\n    end\nend\n\npol_project! (generic function with 1 method)\n\n\nParameters for our projection:\n\nparams = (\n    val_date=Date(2021, 12, 31),\n    proj_length=100,\n    mortality=mort,\n)\n\n(val_date = Date(\"2021-12-31\"), proj_length = 100, mortality = Dict{Sex, OffsetArrays.OffsetVector{Float64, Vector{Float64}}}(Male =&gt; [0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571  …  0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], Female =&gt; [0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745  …  0.376246, 0.386015, 0.393507, 0.398308, 0.4, 0.4, 0.4, 0.4, 0.4, 1.0]))\n\n\nCheck the number of threads we’re using:\n\nThreads.nthreads()\n\n4\n\n\n\nfunction project(policies, params)\n    threads = Threads.nthreads()\n    benefits = zeros(params.proj_length, threads)\n    lives = zeros(Int, params.proj_length, threads)\n    out = (; benefits, lives)\n    ThreadsX.foreach(policies) do pol\n        pol_project!(out, pol, params)\n    end\n    map(x -&gt; vec(reduce(+, x, dims=2)), out)\nend\n\nproject (generic function with 1 method)",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Stochastic Mortality Projections</span>"
    ]
  },
  {
    "objectID": "stochastic-mortality.html#running-the-projection",
    "href": "stochastic-mortality.html#running-the-projection",
    "title": "21  Stochastic Mortality Projections",
    "section": "21.4 Running the projection",
    "text": "21.4 Running the projection\nExample of a single projection:\n\nproject(repeat(policies, 100_000), params)\n\n(benefits = [5.63229016517325e10, 5.673942804299262e10, 5.709732414171803e10, 5.742625665444827e10, 5.772008698021762e10, 5.787901697358337e10, 5.800331364251663e10, 5.8043858008850426e10, 5.8050894713489845e10, 5.795186240464434e10  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [195335, 190456, 185495, 180542, 175561, 170353, 165181, 159956, 154783, 149535  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\n21.4.1 Stochastic Projection\nLoop through and calculate the reults n times (this is only running the two policies in the sample data” n times).\n\nfunction stochastic_proj(policies, params, n)\n\n    ThreadsX.map(1:n) do i\n        project(policies, params)\n    end\nend\n\nstochastic_proj (generic function with 1 method)\n\n\n\nstoch = stochastic_proj(policies, params, 1000)\n\n1000-element Vector{@NamedTuple{benefits::Vector{Float64}, lives::Vector{Int64}}}:\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 203279.41064604037, 209377.7929654216, 215659.12675438426, 222128.9005570158, 228792.76757372628, 235656.55060093806, 242726.24711896622, 250008.0345325352  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 406558.82129208074, 418755.5859308432, 431318.2535087685, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 418755.5859308432, 431318.2535087685, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n ⋮\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 209377.7929654216, 215659.12675438426, 222128.9005570158, 228792.76757372628, 235656.55060093806, 242726.24711896622, 250008.0345325352  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 215659.12675438426, 222128.9005570158, 228792.76757372628, 235656.55060093806, 242726.24711896622, 250008.0345325352  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 406558.82129208074, 418755.5859308432, 431318.2535087685, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 406558.82129208074, 418755.5859308432, 431318.2535087685, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\nlet\n    v = [pv(0.03, s.benefits) for s in stoch]\n    hist(v,\n        bins=15,\n        axis=(\n            xlabel=\"Present Value of Benefits\",\n            ylabel=\"Number of scenarios\"\n        )\n    )\nend\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/GtFuI/src/scenes.jl:227",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Stochastic Mortality Projections</span>"
    ]
  },
  {
    "objectID": "stochastic-mortality.html#benchmarking",
    "href": "stochastic-mortality.html#benchmarking",
    "title": "21  Stochastic Mortality Projections",
    "section": "21.5 Benchmarking",
    "text": "21.5 Benchmarking\nUsing a 2022 Macbook Air M2 laptop, about 30 million policies able to be stochastically projected per second:\n\npolicies_to_benchmark = 3_000_000\n# adjust the `repeat` depending on how many policies are already in the array\n# to match the target number for the benchmark\nn = policies_to_benchmark ÷ length(policies)\n\n@benchmark project(p, r) setup = (p = repeat($policies, $n); r = $params)\n\n\nBenchmarkTools.Trial: 55 samples with 1 evaluation.\n Range (min … max):  67.592 ms … 86.619 ms  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     72.970 ms              ┊ GC (median):    0.00%\n Time  (mean ± σ):   72.945 ms ±  3.845 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%\n  ▃  ▃▃ ▃█▃██          ▃ ▃  █ ▃▃    ▃ █  ▃▃                    \n  █▁▇██▇█████▁▇▁▁▇▁▇▇▁▁█▇█▁▁█▇██▇▇▁▇█▇█▇▁██▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▇ ▁\n  67.6 ms         Histogram: frequency by time        81.7 ms &lt;\n Memory estimate: 29.20 KiB, allocs estimate: 222.",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Stochastic Mortality Projections</span>"
    ]
  },
  {
    "objectID": "stochastic-mortality.html#further-optimization",
    "href": "stochastic-mortality.html#further-optimization",
    "title": "21  Stochastic Mortality Projections",
    "section": "21.6 Further Optimization",
    "text": "21.6 Further Optimization\nIn no particular order:\n\nthe RNG could be made faster: https://bkamins.github.io/julialang/2020/11/20/rand.html\nCould make the stochastic set distributed, but at the current speed the overhead of distributed computing is probably more time than it would save. Same thing with GPU projections\n…",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Stochastic Mortality Projections</span>"
    ]
  },
  {
    "objectID": "scenario-generation.html",
    "href": "scenario-generation.html",
    "title": "22  Scenario Generation",
    "section": "",
    "text": "22.1 In This Chapter\nHow to generate synthetic data for your model using sub-models, with applications to economic scenario generation and portfolio composition.",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Scenario Generation</span>"
    ]
  },
  {
    "objectID": "scenario-generation.html#setup",
    "href": "scenario-generation.html#setup",
    "title": "22  Scenario Generation",
    "section": "22.2 Setup",
    "text": "22.2 Setup\n\nusing CSV, DataFrames\nusing Random\nusing StatsBase, Distributions\nusing CairoMakie",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Scenario Generation</span>"
    ]
  },
  {
    "objectID": "scenario-generation.html#the-data",
    "href": "scenario-generation.html#the-data",
    "title": "22  Scenario Generation",
    "section": "22.3 The Data",
    "text": "22.3 The Data",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Scenario Generation</span>"
    ]
  },
  {
    "objectID": "scenario-generation.html#pseudo-random-number-generators",
    "href": "scenario-generation.html#pseudo-random-number-generators",
    "title": "22  Scenario Generation",
    "section": "22.4 Pseudo Random Number Generators",
    "text": "22.4 Pseudo Random Number Generators\nModern computers utilize Pseudo random number generators (PRNGs) to generate random-like numbers. PRNGs are algorithms used to generate sequences of numbers that appear to be random but are actually determined by an initial value, known as the seed. These generators are called “pseudo-random” because the sequences they produce are deterministic; if you provide the same seed, you’ll get the same sequence of numbers. In addition, they have a finite period, which means that after a certain number of generated values, the sequence will repeat. It’s important to choose or design PRNGs with a long enough period for practical applications.\n\n22.4.1 Common PRNGs\n\n22.4.1.1 Mersenne Twister\nOne of the strengths of the Mersenne Twister is its exceptionally long period. The period is \\(2^(19937)-1\\), which means it can generate \\(2^(19937)-1\\) pseudo random numbers before repeating. This long period is crucial for applications requiring a large number of independent random numbers. It is also known for its good statistical properties. It passes many standard tests for randomness and provides a relatively uniform distribution of random numbers. Moreover, it is designed to allow multiple independent instances to be used concurrently without interfering with each other. This makes it suitable for parallel computing. Although there are faster generators for specific use cases, the Mersenne Twister is still often favored for its balance between speed and quality.\n\n\n22.4.1.2 Xorshift\nXorshift is a family of PRNGs known for their simplicity and relatively fast operation. The name “xorshift” comes from the bitwise XOR (exclusive or) and bit-shifting operations that are the core of the algorithm. Xorshift generators are often used in applications where speed is a priority and cryptographic-strength randomness is not a strict requirement. Xorshift PRNGs use bitwise XOR, left shifts, and right shifts to update the internal state and generate pseudo-random numbers. The basic idea is to repeatedly apply these operations to the state to produce a sequence of numbers. The period of a typical xorshift generator is relatively short compared to some other PRNGs like the Mersenne Twister. However, there are variations of xorshift algorithms that can have longer periods. One of the main advantages of xorshift is its simplicity and speed. The bitwise XOR and bit-shifting operations can be efficiently implemented in hardware, making xorshift generators suitable for applications where fast random number generation is crucial.\n\n\n22.4.1.3 Xoshiro\nXoshiro is a family of PRNGs known for their high performance and good statistical properties. The name “Xoshiro” is derived from the Japanese word “xoroshiro,” meaning “random.” Xoshiro algorithms, including Xoshiro128 and others, use a combination of bitwise XOR, bit-shifting, and addition operations. They often have more complex update rules than basic Xorshift algorithms. In addition, they typically have longer periods, making them suitable for applications that require more pseudo-random numbers before repetition.\n\n\n\n22.4.2 Consistent Interface\nJulia offers a consistent interface for random numbers due to its design and multiple dispatch principles. Consider the following random numbers in different data types.\n\nrng = MersenneTwister(1234)\nrand(Int, (2, 3))\n\n2×3 Matrix{Int64}:\n -4553633257484113333  -6417478475750048536  -316803203281215731\n -4612771555669572001   -306045593666534508  3698543533142991023\n\n\n\nrng = MersenneTwister(1234)\nrand(Float64, (2, 3))\n\n2×3 Matrix{Float64}:\n 0.55306   0.604594  0.899876\n 0.458208  0.945436  0.102255\n\n\n\nrng = Xoshiro(1234)\nrand(Bool, (2, 3))\n\n2×3 Matrix{Bool}:\n 1  1  1\n 0  0  0",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Scenario Generation</span>"
    ]
  },
  {
    "objectID": "scenario-generation.html#common-economic-scenario-generation-approaches",
    "href": "scenario-generation.html#common-economic-scenario-generation-approaches",
    "title": "22  Scenario Generation",
    "section": "22.5 Common Economic Scenario Generation Approaches",
    "text": "22.5 Common Economic Scenario Generation Approaches\nEconomic scenario generation involves the development of plausible future economic scenarios to assess the potential impact on financial portfolios, investments, or decision-making processes. Various approaches are used to generate economic scenarios, including stochastic differential equations (SDEs) and Monte Carlo simulations.\n\n22.5.1 Interest Rate Models\n\n22.5.1.1 Vasicek and Cox Ingersoll Ross (CIR)\nThe Vasicek model is a one-factor model commonly used for simulating interest rate scenarios. It describes the dynamics of short-term interest rates using a stochastic differential equation (SDE). In a Monte Carlo simulation, we can use the Vasicek model to generate multiple interest rate paths. The CIR model is an extension of the Vasicek model with non-constant volatility. It addresses the issue of negative interest rates by ensuring that interest rates remain positive. Vasicek is defined as\n\\[\ndr(t) = \\kappa (\\theta - r(t)) \\, dt + \\sigma \\, dW(t)\n\\]\nwhere\n\n\\(r(t)\\) is the short-term interest rate at time \\(t\\).\n\\(κ\\) is the speed of mean reversion, representing how quickly the interest rate reverts to its long-term mean.\n\\(θ\\) is the long-term mean or equilibrium level of the interest rate.\n\\(σ\\) is the volatility of the interest rate.\n\\(dW(t)\\) is a Wiener process or Brownian motion, representing a random shock.\n\nAnd CIR is defined as\n\\[\ndr(t) = \\kappa (\\theta - r(t)) \\, dt + \\sigma \\sqrt{r(t)} \\, dW(t)\n\\]\nwhere\n\n\\(r(t)\\) is the short-term interest rate at time \\(t\\).\n\\(κ\\) is the speed of mean reversion, representing how quickly the interest rate reverts to its long-term mean.\n\\(θ\\) is the long-term mean or equilibrium level of the interest rate.\n\\(σ\\) is the volatility of the interest rate.\n\\(dW(t)\\) is a Wiener process or Brownian motion, representing a random shock.\n\nThe following code shows a simplified implementation of a CIR model. The specification of \\(dr\\) can be changed to become a Vasicek model.\n\n# Set seed for reproducibility\nRandom.seed!(1234)\n\n# CIR model parameters\nκ = 0.2       # Speed of mean reversion\nθ = 0.05      # Long-term mean\nσ = 0.1       # Volatility\n\n# Initial short-term interest rate\nr₀ = 0.03\n\n# Number of time steps and simulations\nnum_steps = 252\nnum_simulations = 1_000\n\n# Time increment\nΔt = 1/252\n\n# Function to simulate CIR process\nfunction cir_simulation(κ, θ, σ, r₀, Δt, num_steps, num_simulations)\n    interest_rate_paths = zeros(num_steps, num_simulations)\n    for j in 1:num_simulations\n        interest_rate_paths[1, j] = r₀\n        for i in 2:num_steps\n            dW = randn() * sqrt(Δt)\n            # for Vasicek\n            # dr = κ * (θ - interest_rate_paths[i-1, j]) * Δt + σ * dW\n            dr = κ * (θ - interest_rate_paths[i-1, j]) * Δt + σ * sqrt(interest_rate_paths[i-1, j]) * dW\n            interest_rate_paths[i, j] = max(interest_rate_paths[i-1, j] + dr, 0)  # Ensure non-negativity\n        end\n    end\n    return interest_rate_paths\nend\n\n# Run CIR simulation\ncir_paths = cir_simulation(κ, θ, σ, r₀, Δt, num_steps, num_simulations)\n\n# Plot the simulated interest rate paths\nf = Figure()\nAxis(f[1, 1])\nfor i in 1:num_simulations\n    lines!(1:num_steps, cir_paths[:, i])\nend\nf\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ND0gA/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\n\n\n22.5.1.2 Hull White\nThe Hull-White model is a one-factor model that extends the Vasicek model by allowing the mean reversion and volatility parameters to be time-dependent. It is commonly used for pricing interest rate derivatives. Brace-Gatarek-Musiela (BGM) Model extends the Hull-White model to incorporate more factors. It is one of the Libor Market Model (LMM) that describes the evolution of forward rates. It allows for the modeling of both the short-rate and the entire yield curve. It is defined as\n\\[\ndr(t) = (\\theta(t) - a r(t)) \\, dt + \\sigma(t) \\, dW(t)\n\\]\nwhere\n\n\\(r(t)\\) is the short-term interest rate at time \\(t\\).\n\\(θ\\) is the long-term mean or equilibrium level of the interest rate.\n\\(a\\) is the speed of mean reversion.\n\\(σ(t)\\) is the time-dependent volatility of the interest rate.\n\\(dW(t)\\) is a Wiener process or Brownian motion, representing a random shock.\n\n\n# Set seed for reproducibility\nRandom.seed!(1234)\n\n# Hull-White model parameters\nα = 0.1       # Mean reversion speed\nσ = 0.02      # Volatility\nr₀ = 0.03     # Initial short-term interest rate\n\n# Number of time steps and simulations\nnum_steps = 252\nnum_simulations = 1_000\n\n# Time increment\nΔt = 1/252\n\n# Function to simulate Hull-White process\nfunction hull_white_simulation(α, σ, r₀, Δt, num_steps, num_simulations)\n    interest_rate_paths = zeros(num_steps, num_simulations)\n    for j in 1:num_simulations\n        interest_rate_paths[1, j] = r₀\n        for i in 2:num_steps\n            dW = randn() * sqrt(Δt)\n            dr = α * (σ - interest_rate_paths[i-1, j]) * Δt + σ * dW\n            interest_rate_paths[i, j] = interest_rate_paths[i-1, j] + dr\n        end\n    end\n    return interest_rate_paths\nend\n\n# Run Hull-White simulation\nhull_white_paths = hull_white_simulation(α, σ, r₀, Δt, num_steps, num_simulations)\n\n# Plot the simulated interest rate paths\nf = Figure()\nAxis(f[1, 1])\nfor i in 1:num_simulations\n    lines!(1:num_steps, hull_white_paths[:, i])\nend\nf\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ND0gA/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\n\n\n\n22.5.2 Stock Models\n\n22.5.2.1 Geometric Brownian Motion (GBM)\nGBM is a stochastic process commonly used to model the price movement of financial instruments, including stocks. It assumes constant volatility and is characterized by a log-normal distribution. It is defined as\n\\[\ndS(t) = \\mu S(t) \\, dt + \\sigma S(t) \\, dW(t)\n\\]\nwhere\n\n\\(S(t)\\) is the stock price at time \\(t\\).\n\\(μ\\) is the drift coefficient (expected return).\n\\(σ\\) is the volatility coefficient.\n\\(dW(t)\\) is a Wiener process or Brownian motion, representing a random shock.\n\n\n# Set seed for reproducibility\nRandom.seed!(1234)\n\n# GBM parameters\nμ = 0.05       # Drift (expected return)\nσ = 0.2        # Volatility\n\n# Initial stock price\nS₀ = 100\n\n# Number of time steps and simulations\nnum_steps = 252\nnum_simulations = 1_000\n\n# Time increment\nΔt = 1/252\n\n# Function to simulate GBM\nfunction gbm_simulation(μ, σ, S₀, Δt, num_steps, num_simulations)\n    stock_price_paths = zeros(num_steps, num_simulations)\n    for j in 1:num_simulations\n        stock_price_paths[1, j] = S₀\n        for i in 2:num_steps\n            dW = randn() * sqrt(Δt)\n            dS = μ * S₀ * Δt + σ * S₀ * dW\n            stock_price_paths[i, j] = stock_price_paths[i-1, j] + dS\n        end\n    end\n    return stock_price_paths\nend\n\n# Run GBM simulation\ngbm_paths = gbm_simulation(μ, σ, S₀, Δt, num_steps, num_simulations)\n\n# Plot the simulated stock price paths\nf = Figure()\nAxis(f[1, 1])\nfor i in 1:num_simulations\n    lines!(1:num_steps, gbm_paths[:, i])\nend\nf\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ND0gA/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\n\n\n22.5.2.2 Generalized Autoregressive Conditional Heteroskedasticity (GARCH)\nGARCH models capture time-varying volatility. They are often used in conjunction with other models to forecast volatility. It is defined as\n\\[\n\\sigma^2_t = \\omega + \\alpha_1 r^2_{t-1} + \\beta_1 \\sigma^2_{t-1}\n\\]\n\\[\nr_t = \\varepsilon_t \\sqrt{\\sigma^2_t}\n\\]\n\n\\(σ^2_t\\) is the conditional variance at time \\(t\\)\n\\(r_t\\) is the return at time \\(t\\)\n\\(\\varepsilon_t\\) is a white noise or innovation process\n\\(\\omega\\), \\(\\alpha_1\\), \\(\\beta_1\\) are model parameters\n\n\n# Set seed for reproducibility\nRandom.seed!(1234)\n\n# GARCH(1,1) parameters\nα₀ = 0.01      # Constant term\nα₁ = 0.1       # Coefficient for lagged squared returns\nβ₁ = 0.8       # Coefficient for lagged conditional volatility\n\n# Number of time steps and simulations\nnum_steps = 252\nnum_simulations = 1_000\n\n# Time increment\nΔt = 1/252\n\n# Function to simulate GARCH(1,1) volatility\nfunction garch_simulation(α₀, α₁, β₁, num_steps, num_simulations)\n    volatility_paths = zeros(num_steps, num_simulations)\n    for j in 1:num_simulations\n        ε = randn(num_steps)\n        squared_returns = zeros(num_steps)\n        for i in 2:num_steps\n            squared_returns[i] = α₀ + α₁ * ε[i-1]^2 + β₁ * squared_returns[i-1]\n            volatility_paths[i, j] = sqrt(squared_returns[i])\n        end\n    end\n    return volatility_paths\nend\n\n# Run GARCH simulation\ngarch_paths = garch_simulation(α₀, α₁, β₁, num_steps, num_simulations)\n\n# Plot the simulated volatility paths\nf = Figure()\nAxis(f[1, 1])\nfor i in 1:num_simulations\n    lines!(1:num_steps, garch_paths[:, i])\nend\nf\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ND0gA/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\n\n\n\n22.5.3 Copulas\nSimulating data using copulas involves generating multivariate samples with specified marginal distributions and a copula structure.\n\n# Set seed for reproducibility\nRandom.seed!(1234)\n\n# Marginal distributions (e.g., normal)\nmarginal1 = Normal(0, 1)\nmarginal2 = Normal(0, 1)\n\n# Clayton copula parameters\ntheta = 0.5\n\n# Number of data points\nnum_points = 1000\n\n# Generate independent samples from marginals\nu1 = rand(marginal1, num_points)\nu2 = rand(marginal2, num_points)\n\n# Clayton copula simulation\nfunction clayton_copula_simulation(u1, u2, theta)\n    v1 = u1\n    v2 = u2 .* ((theta .* u1).^(-1/theta - 1))\n    return v1, v2\nend\n\n# Simulate Clayton copula\nv1, v2 = clayton_copula_simulation(u1, u2, theta)\n\n# Plot the simulated bivariate data\nf = Figure()\nAxis(f[1, 1])\nscatter!(v1, v2)\nf\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ND0gA/src/scenes.jl:220",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Scenario Generation</span>"
    ]
  },
  {
    "objectID": "scenario-generation.html#benchmarking",
    "href": "scenario-generation.html#benchmarking",
    "title": "22  Scenario Generation",
    "section": "22.6 Benchmarking",
    "text": "22.6 Benchmarking",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Scenario Generation</span>"
    ]
  },
  {
    "objectID": "similarity-calculation.html",
    "href": "similarity-calculation.html",
    "title": "23  Similarity Analysis",
    "section": "",
    "text": "23.1 In This Chapter\nGiven a set of interest, understanding the relative similarity (or not) of features of interest is useful in classification and data compression techniques.",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Similarity Analysis</span>"
    ]
  },
  {
    "objectID": "similarity-calculation.html#setup",
    "href": "similarity-calculation.html#setup",
    "title": "23  Similarity Analysis",
    "section": "23.2 Setup",
    "text": "23.2 Setup\n\nusing CSV, DataFrames\nusing LinearAlgebra\nusing StatsBase, TableTransforms\nusing CairoMakie\nusing NearestNeighbors",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Similarity Analysis</span>"
    ]
  },
  {
    "objectID": "similarity-calculation.html#the-data",
    "href": "similarity-calculation.html#the-data",
    "title": "23  Similarity Analysis",
    "section": "23.3 The Data",
    "text": "23.3 The Data\nStored data can generally be categorized into two formats: tabular (structured) and non-tabular (unstructured). Structured data format is a structured way of organizing and presenting data in rows and columns, resembling a table. This format is widely used for storing and representing structured datasets, making it easy to read, analyze, and manipulate data. The most common example of structured data is a spreadsheet, where data is organized into rows and columns. Structured data can also be stored in relational databases for easier lookups and matching. On the other hand, unstructured data refers to data that lacks a predefined data model or structure. Unlike structured data, which fits neatly into tables or databases, unstructured data does not have a predefined schema. It can include text documents, images, audio files, video files, social media posts, and more.\nStructured data can be further categorized into numerical and categorical data based on the types of values they represent. The following data tables will be referenced throughout the chapter. Real numerical data can easily be converted or normalized to a series of floating points, and real categorical data to a series of binary literals through one-hot encoding procedures.\n\nsample_csv_data =\n    IOBuffer(\n        raw\"id,sex,benefit_base,education,occupation,issue_age\n         1,M,100000.0,college,1,30.0\n         2,F,200000.0,master,3,20.0\n         3,M,150000.0,high_school,4,40.0\n         4,F,50000.0,college,2,60.0\n         5,M,250000.0,college,1,40.0\n         6,F,200000.0,high_school,2,30.0\"\n    )\n\nIOBuffer(data=UInt8[...], readable=true, writable=false, seekable=true, append=false, size=278, maxsize=Inf, ptr=1, mark=-1)\n\n\n\ndf = CSV.read(sample_csv_data, DataFrame)\ndf_num = apply(MinMax(), df[:, [:benefit_base, :issue_age]])[1]\n\n6×2 DataFrame\n\n\n\nRow\nbenefit_base\nissue_age\n\n\n\nFloat64\nFloat64\n\n\n\n\n1\n0.25\n0.25\n\n\n2\n0.75\n0.0\n\n\n3\n0.5\n0.5\n\n\n4\n0.0\n1.0\n\n\n5\n1.0\n0.5\n\n\n6\n0.75\n0.25\n\n\n\n\n\n\n\narr_cat = hcat(indicatormat(df.sex)', indicatormat(df.education)', indicatormat(df.occupation)')\n\n6×9 Matrix{Bool}:\n 0  1  1  0  0  1  0  0  0\n 1  0  0  0  1  0  0  1  0\n 0  1  0  1  0  0  0  0  1\n 1  0  1  0  0  0  1  0  0\n 0  1  1  0  0  1  0  0  0\n 1  0  0  1  0  0  1  0  0\n\n\nFor unstructured data, due to the nature of their variety, the choice of representation depends on the type of data and the specific task at hand. For text data, a Word2Vec embedding is commonly used, while Convolutional Neural Networks (CNNs) are for image data and wave transforms are for audio data. No matter which transformation is applied, unstructured data can generally be converted to a series of floating points, just like numerical structured data.",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Similarity Analysis</span>"
    ]
  },
  {
    "objectID": "similarity-calculation.html#common-similarity-measures",
    "href": "similarity-calculation.html#common-similarity-measures",
    "title": "23  Similarity Analysis",
    "section": "23.4 Common Similarity Measures",
    "text": "23.4 Common Similarity Measures\nThe following measures are commonly used to calculate similarities.\n\n23.4.1 Euclidean Distance (L2 norm)\nEuclidean distance, also known as the L2 norm, is defined as \\[\nd = \\sqrt{\\sum_{i=1}^{n} (w_i - v_i)^2}\n\\] The distance is usually meaningful when applied to numerical data. The following Julia code shows the Euclidean distance for the first two rows in df_num.\n\n#d₁₂ = √(∑((Array(df_num[1, :]) .- Array(df_num[2, :])) .* (Array(df_num[1, :]) .- Array(df_num[2, :]))))\nd₁₂ = LinearAlgebra.norm(Array(df_num[1, :]) .- Array(df_num[2, :]))\n\n0.5590169943749475\n\n\n\n\n23.4.2 Manhattan Distance (L1 Norm)\nManhattan distance, also known as the L1 norm, is defined as \\[\nd = \\sum_{i=1}^{n} |w_i - v_i|\n\\] The distance is also usually meaningful when applied to numerical data. The following Julia code shows the Euclidean distance for the first two rows in df_num.\n\n#d₁₂ = ∑(abs.(Array(df_num[1, :]) .- Array(df_num[2, :])))\nd₁₂ = LinearAlgebra.norm1(Array(df_num[1, :]) .- Array(df_num[2, :]))\n\n0.75\n\n\n\n\n23.4.3 Cosine Similarity\nCosine similarity is defined as \\[\nd = \\frac{\\sum_{i=1}^{n} w_i \\cdot v_i}{\\sqrt{\\sum_{i=1}^{n} w_i^2} \\cdot \\sqrt{\\sum_{i=1}^{n} v_i^2}}\n\\] The distance would be meaningful when applied to both numerical and categorical data.\nThe following Julia code shows the cosine similarity for the first two rows in df_num.\n\nd₁₂ = (Array(df_num[1, :]) ⋅ Array(df_num[2, :])) / norm(df_num[1, :]) / norm(df_num[2, :])\n\n0.7071067811865475\n\n\nThe following Julia code shows the cosine similarity for the first and the third rows in arr_cat.\n\nd₁₃ = (arr_cat[1, :] ⋅ arr_cat[3, :]) / norm(arr_cat[1, :]) / norm(arr_cat[3, :])\n\n0.33333333333333337\n\n\nNote how similar the syntax of processing for numerical or categorical data is. Multiple dispatch allows Julia to identify most efficient underlying procedure for different types of data. For categorical data, the \\(dot\\) operation on binary vectors is essentially count of 1’s, while for numerical data it is the \\(dot\\) operation for most numerical processing libraries.\n\n\n23.4.4 Jaccard Similarity\nJaccard similarity is defined as \\[\nd = \\frac{|W \\cap V|}{|W \\cup V|}\n\\] The distance is usually meaningful when applied to categorical data. The following Julia code shows the Jaccard similarity for the first and the third rows in arr_cat.\n\nd₁₃ = (arr_cat[1, :] ⋅ arr_cat[3, :]) / sum(arr_cat[1, :] .| arr_cat[3, :])\n\n0.2\n\n\n\n\n23.4.5 Hamming Distance\nHamming distance is defined as d = Number of positions at which w and v differ. The distance is usually meaningful when applied to categorical data. The following Julia code shows the Hamming distance for the first and the third rows in arr_cat.\n\nd₁₃ = sum(arr_cat[1, :] .⊻ arr_cat[3, :])\n\n4",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Similarity Analysis</span>"
    ]
  },
  {
    "objectID": "similarity-calculation.html#k-nearest-neighbor-knn-clustering",
    "href": "similarity-calculation.html#k-nearest-neighbor-knn-clustering",
    "title": "23  Similarity Analysis",
    "section": "23.5 k-Nearest Neighbor (kNN) Clustering",
    "text": "23.5 k-Nearest Neighbor (kNN) Clustering\nkNN is primarily known as a classification algorithm, but it can also be used for clustering, particularly in the context of density-based clustering. Density-based clustering identifies regions in the data space where the density of data points is higher, and it groups points in these high-density regions. The core idea of kNN clustering is to assign each data point to a cluster based on the density of its neighbors. A data point becomes a core point if it has at least a specified number of neighbors within a certain distance.\n\n# Create a kNN model\nk = 1\nknn_model = KDTree(Array(df_num))\n\n# Query point for prediction\nquery_point = rand(2)\n\n# Find k nearest neighbors\nindices, distances = knn(knn_model, query_point, k)\n\n# Display results\nprintln(\"Query Point: $query_point\")\nprintln(\"Nearest Neighbors Indices: $indices\")\nprintln(\"Distances to Neighbors: $distances\")\n\nf = Figure()\nAxis(f[1, 1])\nscatter!(df_num[:, 1], df_num[:, 2])\nscatter!(query_point[1], query_point[2])\nf\n\nQuery Point: [0.961042975940568, 0.7987463116803185]\nNearest Neighbors Indices: [1]\nDistances to Neighbors: [0.712711944993806]\n\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ND0gA/src/scenes.jl:220",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Similarity Analysis</span>"
    ]
  },
  {
    "objectID": "similarity-calculation.html#benchmarking",
    "href": "similarity-calculation.html#benchmarking",
    "title": "23  Similarity Analysis",
    "section": "23.6 Benchmarking",
    "text": "23.6 Benchmarking",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Similarity Analysis</span>"
    ]
  },
  {
    "objectID": "portfolio-optimization.html",
    "href": "portfolio-optimization.html",
    "title": "24  Portfolio Optimization",
    "section": "",
    "text": "24.1 In This Chapter\nOptimization in a portfolio context with examples of asset selection under different constraints and objectives.",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Portfolio Optimization</span>"
    ]
  },
  {
    "objectID": "portfolio-optimization.html#setup",
    "href": "portfolio-optimization.html#setup",
    "title": "24  Portfolio Optimization",
    "section": "24.2 Setup",
    "text": "24.2 Setup\n\nusing CSV, DataFrames\nusing JuMP, Ipopt, LinearAlgebra\nusing CairoMakie",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Portfolio Optimization</span>"
    ]
  },
  {
    "objectID": "portfolio-optimization.html#the-data",
    "href": "portfolio-optimization.html#the-data",
    "title": "24  Portfolio Optimization",
    "section": "24.3 The Data",
    "text": "24.3 The Data\n\nμ = [0.1, 0.15, 0.12] # returns\nρ = [0.1 0.05 0.03;\n    0.05 0.12 0.04;\n    0.03 0.04 0.08] # covariances\nnₐ = length(μ) # number of assets\n\n3",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Portfolio Optimization</span>"
    ]
  },
  {
    "objectID": "portfolio-optimization.html#theory",
    "href": "portfolio-optimization.html#theory",
    "title": "24  Portfolio Optimization",
    "section": "24.4 Theory",
    "text": "24.4 Theory\nHarry Markowitz introduced the modern portfolio theory in 1952. The main idea is that investors are pursuing to maximize their expected return of a portfolio given a certain amount of risk. By definition any portfolio yielding a higher return must have higher amount of risk, so there is a trade-off between desired expected returns and allowable risks. The risk versus maximized expected return relationship can be plotted out as a curve, a.k.a. the efficient frontier.",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Portfolio Optimization</span>"
    ]
  },
  {
    "objectID": "portfolio-optimization.html#mathematical-tools",
    "href": "portfolio-optimization.html#mathematical-tools",
    "title": "24  Portfolio Optimization",
    "section": "24.5 Mathematical tools",
    "text": "24.5 Mathematical tools\n\n24.5.1 Mean-variance optimization model\nMean-variance optimization is a mathematical framework that seeks to maximize expected returns while minimizing portfolio variance (or standard deviation). It involves calculating the expected return and risk of individual assets and finding the optimal combination of assets to achieve the desired risk-return tradeoff.\n\\[\\begin{align*}\n\\text{minimize} \\quad &{w}^{T}\\Sigma{w}\\\\\n\\text{subject to} \\quad &{R}^{T}\\geq{\\mu}_{\\text{target}}\\\\\n&{1}^{T}{w}={1}\\\\\n&{w}\\geq{0}\n\\end{align*}\\]\n\n# Create an optimization model\nmodel = Model(optimizer_with_attributes(Ipopt.Optimizer, \"print_level\" =&gt; 0))\n# Set up weights as variables to optimize\n@variable(model, w[1:nₐ] &gt;= zero(0.0))\n# Objective: minimize portfolio variance\n@objective(model, Min, sum(w[i] * ρ[i, j] * w[j] for i in 1:nₐ, j in 1:nₐ))\n# Constraints: Sum of portfolio weights should equal to 1, and all weights should be zero or positive\n@constraint(model, sum(w) == 1)\n# May also add additional constraints\n# target_return = 0.1\n# @constraint(model, dot(μ, w) &gt;= target_return)\n# Solve the optimization problem\noptimize!(model)\n# Print results\n@show \"Optimal Portfolio Weights:\"\nfor i = 1:nₐ\n    @show (\"Asset \", i, \": \", value.(w)[i])\nend\n\n\n******************************************************************************\nThis program contains Ipopt, a library for large-scale nonlinear optimization.\n Ipopt is released as open source code under the Eclipse Public License (EPL).\n         For more information visit https://github.com/coin-or/Ipopt\n******************************************************************************\n\n\"Optimal Portfolio Weights:\" = \"Optimal Portfolio Weights:\"\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 1, \": \", 0.3333333012309821)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 2, \": \", 0.16666675086886984)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 3, \": \", 0.4999999479001481)\n\n\n\n\n24.5.2 Efficient frontier analysis\nThe efficient frontier represents the set of portfolios that offer the highest expected return for a given level of risk or the lowest risk for a given level of return. Efficient frontier analysis involves plotting risk-return combinations for different portfolios and identifying the optimal portfolio on the frontier.\n\n# Create an optimization model\nmodel = Model(optimizer_with_attributes(Ipopt.Optimizer, \"print_level\" =&gt; 0))\n# Set up weights as variables to optimize\n@variable(model, w[1:nₐ] &gt;= zero(0.0))\n# Define objective function: minimize portfolio variance\nportfolio_variance = w'ρ * w\n@objective(model, Min, portfolio_variance)\n# Constraints: Sum of portfolio weights should equal to 1, and all weights should be zero or positive\n@constraint(model, sum(w) == 1)\n# Generate a range of target returns\npoints = 100\ntarget_returns = range(minimum(μ), maximum(μ), length=points)\n\nefficient_frontier = []\nfor target_return in target_returns\n    # Add additional constraint for target return\n    @constraint(model, c, dot(μ, w) == target_return)\n    # Solve the problem\n    optimize!(model)\n    # Show solution\n    if termination_status(model) == MOI.LOCALLY_SOLVED\n        push!(efficient_frontier, (sqrt(objective_value(model)), target_return))\n    end\n    unregister(model, :c)\n    delete(model, c)\nend\n# Plot Efficient Frontier\nfig = Figure()\nAxis(fig[1, 1])\nlines!(map(x -&gt; x[1], efficient_frontier), map(x -&gt; x[2], efficient_frontier))\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ND0gA/src/scenes.jl:220\n\n\n\n\n\n\n\n\n\n\n\n24.5.3 Black-Litterman\nThe Black-Litterman model combines the views of investors with market equilibrium assumptions to generate optimal portfolios. It starts with a market equilibrium portfolio and adjusts it based on investor views and confidence levels. The model incorporates subjective opinions while maintaining diversification and risk management principles.\n\\[\\begin{align*}\n\\text{maximize} \\quad & \\mu^T w - \\lambda \\cdot \\frac{1}{2} w^T \\Sigma w \\\\\n\\text{subject to} \\quad & \\sum_{i=1}^{N} w_i = 1 \\\\\n& w_i \\geq 0, \\quad \\forall i\n\\end{align*}\\]\n\nλ = 2.5 # risk aversion\nrfr = 0.02 # risk free rate\n# Market equilibrium parameters (prior)\nμ_market = [0.08, 0.08, 0.08] # Market equilibrium return\nΣ_market = ρ # Market equilibrium covariance matrix\n# Investor views\nQ = μ # Expected returns on assets according to investor views\nP = [1 0 0; 0 1 0; 0 0 1]     # Pick matrix specifying which assets views are on\nΩ = [0.001^2 0.0 0.0; 0.0 0.002^2 0.0; 0.0 0.0 0.003^2]  # Views uncertainty (covariance matrix)\n\n# Create an optimization model\nmodel = Model(optimizer_with_attributes(Ipopt.Optimizer, \"print_level\" =&gt; 0))\n# Set up weights as variables to optimize\n@variable(model, w[1:nₐ] &gt;= zero(0.0))\n# Black-Litterman expected return adjustment\nΣ_prior_inv = inv(Σ_market)\nτ = 0.05 # Scaling factor\n# Calculate the posterior expected returns\nμ_posterior = Σ_prior_inv * (τ * Σ_market * (Σ_prior_inv + P' * inv(Ω) * P)) \\\n              (τ * Σ_market * (Σ_prior_inv * μ_market + P' * inv(Ω) * Q) + Σ_prior_inv * μ_market)\n# Objective: maximize sharpe ratio\nsr = (w' * μ_posterior - rfr) / (λ / 2 * w' * Σ_market * w)\n@objective(model, Max, sr)\n# Constraints: Sum of portfolio weights should equal to 1, and all weights should be zero or positive\n@constraint(model, sum(w) == 1)\n# Solve the optimization problem\noptimize!(model)\n# Print results\nv = sqrt(value.(w)' * Σ_market * value.(w))\n@show \"Optimal Portfolio Weights, Expected Portfolio Return, Portfolio Volatility:\", v\nfor i = 1:nₐ\n    @show (\"Asset \", i, \": \", value.(w)[i], value.(w)[i] * μ_posterior[i])\nend\n\n(\"Optimal Portfolio Weights, Expected Portfolio Return, Portfolio Volatility:\", v) = (\"Optimal Portfolio Weights, Expected Portfolio Return, Portfolio Volatility:\", 0.2606134101929549)\n(\"Asset \", i, \": \", value.(w)[i], value.(w)[i] * μ_posterior[i]) = (\"Asset \", 1, \": \", 2.0824146208647934e-8, 2.5581726822010987e-10)\n(\"Asset \", i, \": \", value.(w)[i], value.(w)[i] * μ_posterior[i]) = (\"Asset \", 2, \": \", 0.2311617288917621, 0.009281526926739852)\n(\"Asset \", i, \": \", value.(w)[i], value.(w)[i] * μ_posterior[i]) = (\"Asset \", 3, \": \", 0.7688382502840917, 0.03861300028744048)\n\n\n\n\n24.5.4 Risk Parity\nRisk parity is an asset allocation strategy that allocates capital based on risk rather than traditional measures such as market capitalization or asset prices. It aims to balance risk contributions across different assets or asset classes to achieve a more stable portfolio. Risk parity portfolios often include assets with different risk profiles, such as stocks, bonds, and commodities.\n\\[\\begin{align*}\n\\text{minimize} \\quad & \\sum_{i=1}^{N} (w_i \\cdot \\sqrt{\\sigma_i})^2 \\\\\n\\text{subject to} \\quad & \\sum_{i=1}^{N} w_i = 1 \\\\\n& w_i \\geq 0, \\quad \\forall i\n\\end{align*}\\]\n\n# Create an optimization model\nmodel = Model(optimizer_with_attributes(Ipopt.Optimizer, \"print_level\" =&gt; 0))\n# Set up weights as variables to optimize\n@variable(model, w[1:nₐ] &gt;= zero(0.0))\n# Objective: minimize portfolio variance\nportfolio_variance = w'ρ * w\nmargin = (ρ * w ./ sqrt(portfolio_variance)) .* w\nrisk_contributions = margin ./ sum(margin)\ntarget = repeat([1.0 / nₐ], nₐ)\n@objective(model, Max, sum((risk_contributions .- target) .^ 2))\n# Constraints: Sum of portfolio weights should equal to 1, and all weights should be zero or positive\n@constraint(model, sum(w) == 1)\n# Solve the optimization problem\noptimize!(model)\n# Print results\n@show \"Optimal Portfolio Weights:\"\nfor i = 1:nₐ\n    @show (\"Asset \", i, \": \", value.(w)[i])\nend\n\n\"Optimal Portfolio Weights:\" = \"Optimal Portfolio Weights:\"\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 1, \": \", -6.957484531612737e-9)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 2, \": \", 1.0000000131375544)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 3, \": \", -6.180069741122123e-9)\n\n\n\n\n24.5.5 Sharpe Ratio Maximization\nThe Sharpe ratio measures the risk-adjusted return of a portfolio and is calculated as the ratio of excess return to volatility. Maximizing the Sharpe ratio involves finding the portfolio allocation that offers the highest risk-adjusted return. This approach focuses on achieving the best tradeoff between risk and return.\n\\[\\begin{align*}\n\\text{maximize} \\quad & \\frac{E[R_p] - R_f}{\\sigma_p} \\\\\n\\text{subject to} \\quad & \\sum_{i=1}^{N} w_i = 1 \\\\\n& w_i \\geq 0, \\quad \\forall i\n\\end{align*}\\]\n\n# Create an optimization model\nmodel = Model(optimizer_with_attributes(Ipopt.Optimizer, \"print_level\" =&gt; 0))\n# Set up weights as variables to optimize\n@variable(model, w[1:nₐ] &gt;= zero(0.0))\n# Objective: minimize portfolio variance\nrfr = 0.05 # risk free rate\n@objective(model, Max, (dot(μ, w) - rfr) / sqrt(sum(w[i] * ρ[i, j] * w[j] for i in 1:nₐ, j in 1:nₐ)))\n# Constraints: Sum of portfolio weights should equal to 1, and all weights should be zero or positive\n@constraint(model, sum(w) == 1)\n# Solve the optimization problem\noptimize!(model)\n# Print results\n@show \"Optimal Portfolio Weights:\"\nfor i = 1:nₐ\n    @show (\"Asset \", i, \": \", value.(w)[i])\nend\n\n\"Optimal Portfolio Weights:\" = \"Optimal Portfolio Weights:\"\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 1, \": \", 0.010841995514843134)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 2, \": \", 0.5352292318109132)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 3, \": \", 0.45392877267424375)\n\n\n\n\n24.5.6 Robust Optimization\nRobust optimization techniques aim to create portfolios that are resilient to uncertainties and fluctuations in market conditions. These techniques consider a range of possible scenarios and optimize portfolios to perform well across different market environments. Robust optimization may involve incorporating stress tests, scenario analysis, or robust risk measures into the portfolio construction process.\n\\[\\begin{align*}\n\\text{minimize} \\quad & w^T \\Sigma w + \\gamma \\|w - w_0\\|_2^2 \\\\\n\\text{subject to} \\quad & \\sum_{i=1}^{N} w_i = 1 \\\\\n& w_i \\geq 0, \\quad \\forall i \\\\\n& \\|(\\Sigma^{1/2} (w - w_0))\\|_2 \\leq \\epsilon\n\\end{align*}\\]\n\n# Create an optimization model\nmodel = Model(optimizer_with_attributes(Ipopt.Optimizer, \"print_level\" =&gt; 0))\n# Set up weights as variables to optimize\n@variable(model, w[1:nₐ] &gt;= zero(0.0))\n# Objective: minimize portfolio variance\nε = 0.05  # Uncertainty level\nγ = 0.1  # Robustness parameter\nw₀ = [0.3, 0.4, 0.3]\n@objective(model, Min, dot(w, ρ * w) + γ * sum((w[i] - w₀[i])^2 for i in 1:nₐ))\n# Constraints: Sum of portfolio weights should equal to 1, and all weights should be zero or positive\n@constraint(model, sum(w) == 1)\n@constraint(model, sum((ρ[i, j] * (w[i] - w₀[i]) * (w[j] - w₀[j])) for i in 1:nₐ, j in 1:nₐ) &lt;= ε)\n# Solve the optimization problem\noptimize!(model)\n# Print results\n@show \"Optimal Portfolio Weights:\"\nfor i = 1:nₐ\n    @show (\"Asset \", i, \": \", value.(w)[i])\nend\n\n\"Optimal Portfolio Weights:\" = \"Optimal Portfolio Weights:\"\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 1, \": \", 0.31250000098314346)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 2, \": \", 0.31250000376951037)\n(\"Asset \", i, \": \", value.(w)[i]) = (\"Asset \", 3, \": \", 0.37499999524734623)",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Portfolio Optimization</span>"
    ]
  },
  {
    "objectID": "bayesian-mortality.html",
    "href": "bayesian-mortality.html",
    "title": "25  Bayesian Mortality Modeling",
    "section": "",
    "text": "25.1 Generating fake data\nThe problem of interest is to look at mortality rates, which are given in terms of exposures (whether or not a life experienced a death in a given year).\nWe’ll grab some example rates from an insurance table, which has a “selection” component: When someone enters observation, say at age 50, their mortality is path dependent (so for someone who started being observed at 50 will have a different risk/mortality rate at age 55 than someone who started being observed at 45).\nAddtionally, there may be additional groups of interest, such as:\nThe example data will start with only the risk classification above. ““”\nusing MortalityTables\nusing Turing\nusing UUIDs\nusing DataFramesMeta\nusing MCMCChains\nusing LinearAlgebra\nusing CairoMakie\nusing StatsBase\nusing OffsetArrays\nn = 10_000\ninforce = map(1:n) do i\n    (\n        id=uuid1(),\n        issue_age=rand(30:70),\n        risk_level=rand(1:3),\n    )\n\nend\n\n10000-element Vector{@NamedTuple{id::UUID, issue_age::Int64, risk_level::Int64}}:\n (id = UUID(\"2286ed86-3da5-11ef-205c-7dc0677b4e32\"), issue_age = 63, risk_level = 3)\n (id = UUID(\"2286eda4-3da5-11ef-1f2e-e56428a23a1b\"), issue_age = 60, risk_level = 2)\n (id = UUID(\"2286eda4-3da5-11ef-38b6-0b079961e7c4\"), issue_age = 64, risk_level = 1)\n (id = UUID(\"2286eda4-3da5-11ef-00b5-65a75c687f62\"), issue_age = 34, risk_level = 2)\n (id = UUID(\"2286edae-3da5-11ef-0127-2f189365b9d8\"), issue_age = 31, risk_level = 3)\n (id = UUID(\"2286edae-3da5-11ef-1460-73245dc6adf3\"), issue_age = 33, risk_level = 3)\n (id = UUID(\"2286edae-3da5-11ef-2aed-b3b5c16bea5c\"), issue_age = 66, risk_level = 3)\n (id = UUID(\"2286edb8-3da5-11ef-235a-b370e0b4cfe9\"), issue_age = 56, risk_level = 3)\n (id = UUID(\"2286edb8-3da5-11ef-1a85-1d67295ada2d\"), issue_age = 70, risk_level = 3)\n (id = UUID(\"2286edb8-3da5-11ef-1b94-5147832d9411\"), issue_age = 61, risk_level = 3)\n ⋮\n (id = UUID(\"22876748-3da5-11ef-1630-1f709a9d9043\"), issue_age = 66, risk_level = 3)\n (id = UUID(\"22876748-3da5-11ef-17ab-ef7a80c61b21\"), issue_age = 53, risk_level = 1)\n (id = UUID(\"22876748-3da5-11ef-22f5-c347cfc6b4fb\"), issue_age = 66, risk_level = 3)\n (id = UUID(\"22876752-3da5-11ef-1b42-e3687b55a329\"), issue_age = 53, risk_level = 2)\n (id = UUID(\"22876752-3da5-11ef-0489-433e9d283bd3\"), issue_age = 63, risk_level = 2)\n (id = UUID(\"22876752-3da5-11ef-1da1-538ce9cfd057\"), issue_age = 63, risk_level = 1)\n (id = UUID(\"2287675c-3da5-11ef-2d95-adaf31928f52\"), issue_age = 60, risk_level = 1)\n (id = UUID(\"2287675c-3da5-11ef-2a7b-3511b13fbd68\"), issue_age = 55, risk_level = 3)\n (id = UUID(\"2287675c-3da5-11ef-1d5c-a1c801f9be0e\"), issue_age = 46, risk_level = 2)\nbase_table = MortalityTables.table(\"2001 VBT Residual Standard Select and Ultimate - Male Nonsmoker, ANB\")\n\nfunction tabular_mortality(params, issue_age, att_age, risk_level)\n    q = params.ultimate[att_age]\n    if risk_level == 1\n        q *= 0.7\n    elseif risk_level == 2\n        q = q\n    else\n        q *= 1.5\n    end\nend\n\ntabular_mortality (generic function with 1 method)\nfunction model_outcomes(inforce, assumption, assumption_params; n_years=5)\n\n    outcomes = map(inforce) do pol\n        alive = 1\n        sim = map(1:n_years) do t\n            att_age = pol.issue_age + t - 1\n            q = assumption(\n                assumption_params,\n                pol.issue_age,\n                att_age,\n                pol.risk_level\n            )\n            if rand() &lt; q\n                out = (att_age=att_age, exposures=alive, death=1)\n                alive = 0\n                out\n            else\n                (att_age=att_age, exposures=alive, death=0)\n            end\n        end\n        filter!(x -&gt; x.exposures == 1, sim)\n\n    end\n\n\n    df = DataFrame(inforce)\n\n    df.outcomes = outcomes\n    df = flatten(df, :outcomes)\n\n    df.att_age = [x.att_age for x in df.outcomes]\n    df.death = [x.death for x in df.outcomes]\n    df.exposures = [x.exposures for x in df.outcomes]\n    select!(df, Not(:outcomes))\n\n\nend\n\nexposures = model_outcomes(inforce, tabular_mortality, base_table)\ndata = combine(groupby(exposures, [:issue_age, :att_age])) do subdf\n    (exposures=nrow(subdf),\n        deaths=sum(subdf.death),\n        fraction=sum(subdf.death) / nrow(subdf))\nend\n\n\ndata2 = combine(groupby(exposures, [:issue_age, :att_age, :risk_level])) do subdf\n    (exposures=nrow(subdf),\n        deaths=sum(subdf.death),\n        fraction=sum(subdf.death) / nrow(subdf))\nend\n\n615×6 DataFrame590 rows omitted\n\n\n\nRow\nissue_age\natt_age\nrisk_level\nexposures\ndeaths\nfraction\n\n\n\nInt64\nInt64\nInt64\nInt64\nInt64\nFloat64\n\n\n\n\n1\n30\n30\n1\n68\n0\n0.0\n\n\n2\n30\n30\n2\n76\n0\n0.0\n\n\n3\n30\n30\n3\n81\n0\n0.0\n\n\n4\n30\n31\n1\n68\n0\n0.0\n\n\n5\n30\n31\n2\n76\n0\n0.0\n\n\n6\n30\n31\n3\n81\n0\n0.0\n\n\n7\n30\n32\n1\n68\n0\n0.0\n\n\n8\n30\n32\n2\n76\n1\n0.0131579\n\n\n9\n30\n32\n3\n81\n0\n0.0\n\n\n10\n30\n33\n1\n68\n0\n0.0\n\n\n11\n30\n33\n2\n75\n0\n0.0\n\n\n12\n30\n33\n3\n81\n0\n0.0\n\n\n13\n30\n34\n1\n68\n0\n0.0\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n604\n70\n71\n1\n93\n3\n0.0322581\n\n\n605\n70\n71\n2\n62\n0\n0.0\n\n\n606\n70\n71\n3\n75\n2\n0.0266667\n\n\n607\n70\n72\n1\n90\n1\n0.0111111\n\n\n608\n70\n72\n2\n62\n2\n0.0322581\n\n\n609\n70\n72\n3\n73\n3\n0.0410959\n\n\n610\n70\n73\n1\n89\n1\n0.011236\n\n\n611\n70\n73\n2\n60\n2\n0.0333333\n\n\n612\n70\n73\n3\n70\n3\n0.0428571\n\n\n613\n70\n74\n1\n88\n3\n0.0340909\n\n\n614\n70\n74\n2\n58\n0\n0.0\n\n\n615\n70\n74\n3\n67\n4\n0.0597015",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Bayesian Mortality Modeling</span>"
    ]
  },
  {
    "objectID": "bayesian-mortality.html#generating-fake-data",
    "href": "bayesian-mortality.html#generating-fake-data",
    "title": "25  Bayesian Mortality Modeling",
    "section": "",
    "text": "high/medium/low risk classification\nsex\ngroup (e.g. company, data source, etc.)\ntype of insurance product offered",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Bayesian Mortality Modeling</span>"
    ]
  },
  {
    "objectID": "bayesian-mortality.html#a-single-binomial-parameter-model",
    "href": "bayesian-mortality.html#a-single-binomial-parameter-model",
    "title": "25  Bayesian Mortality Modeling",
    "section": "25.2 1: A single binomial parameter model",
    "text": "25.2 1: A single binomial parameter model\nEstiamte \\(q\\), the average mortality rate, not accounting for any variation within the population/sample. Our model is defines as:\n\\[\nq ~ Beta(1,1)\np(death) ~ Binomial(q)\n\\]\n\n@model function mortality(data, deaths)\n    q ~ Beta(1, 1)\n    for i = 1:nrow(data)\n        deaths[i] ~ Binomial(data.exposures[i], q)\n    end\nend\n\nm1 = mortality(data, data.deaths)\n\n\nDynamicPPL.Model{typeof(mortality), (:data, :deaths), (), (), Tuple{DataFrame, Vector{Int64}}, Tuple{}, DynamicPPL.DefaultContext}(Main.Notebook.mortality, (data = 205×5 DataFrame\n Row │ issue_age  att_age  exposures  deaths  fraction   \n     │ Int64      Int64    Int64      Int64   Float64    \n─────┼───────────────────────────────────────────────────\n   1 │        30       30        225       0  0.0\n   2 │        30       31        225       0  0.0\n   3 │        30       32        225       1  0.00444444\n   4 │        30       33        224       0  0.0\n   5 │        30       34        224       0  0.0\n   6 │        31       31        259       1  0.003861\n   7 │        31       32        258       0  0.0\n   8 │        31       33        258       0  0.0\n  ⋮  │     ⋮         ⋮         ⋮        ⋮         ⋮\n 199 │        69       72        231      11  0.047619\n 200 │        69       73        220       5  0.0227273\n 201 │        70       70        235       5  0.0212766\n 202 │        70       71        230       5  0.0217391\n 203 │        70       72        225       6  0.0266667\n 204 │        70       73        219       6  0.0273973\n 205 │        70       74        213       7  0.0328638\n                                         190 rows omitted, deaths = [0, 0, 1, 0, 0, 1, 0, 0, 0, 0  …  5, 5, 4, 11, 5, 5, 5, 6, 6, 7]), NamedTuple(), DynamicPPL.DefaultContext())\n\n\n\n\n25.2.1 Sampling from the posterior\nWe use a No-U-Turn-Sampler (NUTS) technique to sample multile chains at once:\n\nnum_chains = 4\nchain = sample(m1, NUTS(), MCMCThreads(), 400, num_chains)\n\n\nChains MCMC chain (400×13×4 Array{Float64, 3}):\n\nIterations        = 201:1:600\nNumber of chains  = 4\nSamples per chain = 400\nWall duration     = 0.54 seconds\nCompute duration  = 1.87 seconds\nparameters        = q\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk    ess_tail      rhat    ⋯\n      Symbol   Float64   Float64   Float64    Float64     Float64   Float64    ⋯\n\n           q    0.0073    0.0004    0.0000   878.7937   1126.0829    1.0003    ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           q    0.0066    0.0071    0.0073    0.0076    0.0081\n\n\n\n\nHere, we have asked for the outcomes to be modeled via a single parameter for the population. We see that the posterior distirbution of \\(q\\) is very close to the overall population mortality rate:\n\nsum(data.deaths) / sum(data.exposures)\n\n0.007319545823195458\n\n\nHowever, We can see that the sampling of possible posterior parameters doesn’t really fit the data very well since our model was so simplified. The lines represent the posterior binomial probability.\nThis is saying that for the observed data, if there really is just a single probability p that governs the true process that came up with the data, there’s a pretty narrow range of values it could possibly be:\n\nlet\n    data_weight = log.(data.exposures)\n    #data_weight = .√(data_weight ./ maximum(data_weight) .* 20)\n    f = Figure(title=\"Parametric Bayseian Mortality\"\n    )\n    ax = Axis(f[1, 1],\n        xlabel=\"age\",\n        ylabel=\"mortality rate\",\n        # ylims=(0.0, 0.25),\n    )\n    scatter!(ax,\n        data.att_age,\n        data.fraction,\n        markersize=data_weight,\n        color=(:blue, 0.5),\n        label=\"Experience data point (size indicates relative exposure quantity)\",)\n\n    # show n samples from the posterior plotted on the graph\n    n = 300\n    ages = sort!(unique(data.att_age))\n\n    q_posterior = sample(chain, n)[:q]\n\n\n    for i in 1:n\n\n        hlines!(ax, [q_posterior[i]], color=(:grey, 0.1))\n    end\n\n    # Need to simulate at indivudal level and then aggregate?\n\n\n    sim05 = Float64[]\n    sim95 = Float64[]\n    for r in eachrow(data)\n        outcomes = map(1:n) do i\n            rand(Binomial(r.exposures, q_posterior[i]), 500)\n        end\n        push!(sim05, quantile(Iterators.flatten(outcomes), 0.05) / r.exposures)\n        push!(sim95, quantile(Iterators.flatten(outcomes), 0.95) / r.exposures)\n\n\n    end\n\n\n\n    f\nend\n\n\n\n\n\n\n\n\n\nlet\n    n = 300\n    q_posterior = sample(chain, n)[:q]\n\n\nend\n\n2-dimensional AxisArray{Float64,2,...} with axes:\n    :iter, 1:300\n    :chain, 1:1\nAnd data, a 300×1 Matrix{Float64}:\n 0.008160220037846218\n 0.0078051007558187615\n 0.007280273613288957\n 0.007642004145553929\n 0.0073207789064674055\n 0.007430768553875488\n 0.0072422285253099985\n 0.007740062785602893\n 0.007979997171807914\n 0.007225707117235694\n ⋮\n 0.007251355033807228\n 0.0071986577785974545\n 0.006955678855852861\n 0.007316390812237514\n 0.007470518869725419\n 0.006998075230207476\n 0.007287033357751358\n 0.007618117732005956\n 0.007566696380891601",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Bayesian Mortality Modeling</span>"
    ]
  },
  {
    "objectID": "bayesian-mortality.html#parametric-model",
    "href": "bayesian-mortality.html#parametric-model",
    "title": "25  Bayesian Mortality Modeling",
    "section": "25.3 2. Parametric model",
    "text": "25.3 2. Parametric model\nIn this example, we utilize a MakehamBeard parameterization because it’s already very similar in form to a logistic function. This is important because our desired output is a probability (ie the probablity of a death at a given age), so the value must be constrained to be in the interval between zero and one.\nThe prior values for a,b,c, and k are chosen to constrain the hazard (mortality) rate to be between zero and one.\nThis isn’t an ideal parameterization (e.g. we aren’t including information about the select underwriting period), but is an example of utilizing Bayesian techniques on life experience data. ”\n\n@model function mortality2(data, deaths)\n    a ~ Exponential(0.1)\n    b ~ Exponential(0.1)\n    c = 0.0\n    k ~ truncated(Exponential(1), 1, Inf)\n\n    # use the variables to create a parametric mortality model\n    m = MortalityTables.MakehamBeard(; a, b, c, k)\n\n    # loop through the rows of the dataframe to let Turing observe the data \n    # and how consistent the parameters are with the data\n    for i = 1:nrow(data)\n        age = data.att_age[i]\n        q = MortalityTables.hazard(m, age)\n        deaths[i] ~ Binomial(data.exposures[i], q)\n    end\nend\n\nmortality2 (generic function with 2 methods)\n\n\nWe combine the model with the data and sample from the posterior using a similar call as before:\n\nm2 = mortality2(data, data.deaths)\n\nchain2 = sample(m2, NUTS(), MCMCThreads(), 400, num_chains)\n\n\nChains MCMC chain (400×15×4 Array{Float64, 3}):\n\nIterations        = 201:1:600\nNumber of chains  = 4\nSamples per chain = 400\nWall duration     = 141.62 seconds\nCompute duration  = 160.26 seconds\nparameters        = a, b, k\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n\n           a    0.1761    0.3050    0.1510     6.9143    16.6988    1.6048     ⋯\n           b    1.4238    2.3147    1.1457     6.9757    13.3508    1.5940     ⋯\n           k    3.2573    2.3792    1.1115     7.0573    12.9727    1.5785     ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           a    0.0000    0.0000    0.0001    0.1761    0.7042\n           b    0.0772    0.0853    0.0900    1.4379    5.4318\n           k    1.0155    1.3981    2.0893    7.1247    7.1252\n\n\n\n\nsummarize(chain2)\nplot(chain2)\n\n25.3.1 Plotting samples from the posterior\nWe can see that the sampling of possible posterior parameters fits the data well:\n\nlet\n    data_weight = data.exposures ./ sum(data.exposures)\n    data_weight = .√(data_weight ./ maximum(data_weight) .* 20)\n\n    p = scatter(\n        data.att_age,\n        data.fraction,\n        markersize=data_weight,\n        alpha=0.5,\n        label=\"Experience data point (size indicates relative exposure quantity)\",\n        axis=(\n            xlabel=\"age\",\n            limits=(nothing, nothing, 0.0, 0.25),\n            ylabel=\"mortality rate\",\n            title=\"Parametric Bayseian Mortality\"\n        )\n    )\n\n\n    # show n samples from the posterior plotted on the graph\n    n = 300\n    ages = sort!(unique(data.att_age))\n\n    for i in 1:n\n        s = sample(chain2, 1)\n        a = only(s[:a])\n        b = only(s[:b])\n        k = only(s[:k])\n        c = 0\n        m = MortalityTables.MakehamBeard(; a, b, c, k)\n        lines!(ages, age -&gt; MortalityTables.hazard(m, age), alpha=0.1, label=\"\")\n    end\n    p\nend\n\n\n\n\n\n\n\n\n\nlet\n    data_weight = log.(data.exposures)\n    #data_weight = .√(data_weight ./ maximum(data_weight) .* 20)\n    f = Figure(title=\"Parametric Bayseian Mortality\"\n    )\n    ax = Axis(f[1, 1],\n        xlabel=\"age\",\n        ylabel=\"mortality rate\",\n        # ylims=(0.0, 0.25),\n    )\n    scatter!(ax,\n        data.att_age,\n        data.fraction,\n        markersize=data_weight,\n        color=(:blue, 0.5),\n        label=\"Experience data point (size indicates relative exposure quantity)\",)\n\n    # show n samples from the posterior plotted on the graph\n    n = 300\n    ages = sort!(unique(data.att_age))\n\n    for i in 1:n\n        s = sample(chain2, 1)\n        a = only(s[:a])\n        b = only(s[:b])\n        k = only(s[:k])\n        c = 0\n        m = MortalityTables.MakehamBeard(; a, b, c, k)\n        qs = MortalityTables.hazard.(m, ages)\n        lines!(ax, ages, qs, color=(:grey, 0.1))\n    end\n    f\nend",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Bayesian Mortality Modeling</span>"
    ]
  },
  {
    "objectID": "bayesian-mortality.html#parametric-model-1",
    "href": "bayesian-mortality.html#parametric-model-1",
    "title": "25  Bayesian Mortality Modeling",
    "section": "25.4 3. Parametric model",
    "text": "25.4 3. Parametric model\nThis model extends the prior to create a multi-level model. Each risk class (risk_level) gets its own \\(a\\) paramater in the MakhamBeard model. The prior for \\(a_i\\) is determined by the hyperparameter \\(\\bar{a}\\).\n\n@model function mortality3(data, deaths)\n    risk_levels = length(levels(data.risk_level))\n    b ~ Exponential(0.1)\n    ā ~ Exponential(0.1)\n    a ~ filldist(Exponential(ā), risk_levels)\n    c = 0\n    k ~ truncated(Exponential(1), 1, Inf)\n\n    # use the variables to create a parametric mortality model\n\n    # loop through the rows of the dataframe to let Turing observe the data \n    # and how consistent the parameters are with the data\n    for i = 1:nrow(data)\n        risk = data.risk_level[i]\n\n        m = MortalityTables.MakehamBeard(; a=a[risk], b, c, k)\n        age = data.att_age[i]\n        q = MortalityTables.hazard(m, age)\n        deaths[i] ~ Binomial(data.exposures[i], q)\n    end\nend\n\nm3 = mortality3(data2, data2.deaths)\n\nchain3 = sample(m3, NUTS(), 1000)\n\nsummarize(chain3)\n\n\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n\n           b    0.0880    0.0061    0.0004   280.0867   305.2382    1.0052     ⋯\n           ā    0.0002    0.0004    0.0000   410.1920   335.2501    1.0010     ⋯\n        a[1]    0.0000    0.0000    0.0000   287.8220   305.0496    1.0060     ⋯\n        a[2]    0.0000    0.0000    0.0000   289.3920   237.9717    1.0082     ⋯\n        a[3]    0.0001    0.0000    0.0000   277.0825   378.0138    1.0066     ⋯\n           k    1.8186    0.7903    0.0306   314.5374   164.9161    1.0024     ⋯\n                                                                1 column omitted\n\n\n\n\n\nlet data = data2\n\n    data_weight = data.exposures ./ sum(data.exposures)\n    data_weight = .√(data_weight ./ maximum(data_weight) .* 20)\n    color_i = data.risk_level\n\n    p = scatter(\n        data.att_age,\n        data.fraction,\n        markersize=data_weight,\n        alpha=0.5,\n        color=color_i,\n        label=\"Experience data point (size indicates relative exposure quantity)\",\n        axis=(\n            xlabel=\"age\",\n            limits=(nothing, nothing, 0.0, 0.25),\n            ylabel=\"mortality rate\",\n            title=\"Parametric Bayseian Mortality\"\n        )\n    )\n\n\n    # show n samples from the posterior plotted on the graph\n    n = 100\n\n    ages = sort!(unique(data.att_age))\n    for r in 1:3\n        for i in 1:n\n            s = sample(chain3, 1)\n            a = only(s[Symbol(\"a[$r]\")])\n            b = only(s[:b])\n            k = only(s[:k])\n            c = 0\n            m = MortalityTables.MakehamBeard(; a, b, c, k)\n            if i == 1\n                lines!(ages, age -&gt; MortalityTables.hazard(m, age), label=\"risk level $r\", alpha=0.2, color=(CairoMakie.Makie.wong_colors()[r], 0.2))\n            else\n                lines!(ages, age -&gt; MortalityTables.hazard(m, age), label=\"\", alpha=0.2, color=(CairoMakie.Makie.wong_colors()[r], 0.2))\n            end\n        end\n    end\n    p\nend",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Bayesian Mortality Modeling</span>"
    ]
  },
  {
    "objectID": "bayesian-mortality.html#handling-non-unit-exposures",
    "href": "bayesian-mortality.html#handling-non-unit-exposures",
    "title": "25  Bayesian Mortality Modeling",
    "section": "25.5 Handling non-unit exposures",
    "text": "25.5 Handling non-unit exposures\nThe key is to use the Poisson distribution, which is a continuous approximation to the Binomial distribution:\n\n@model function mortality4(data, deaths)\n    risk_levels = length(levels(data.risk_level))\n    b ~ Exponential(0.1)\n    ā ~ Exponential(0.1)\n    a ~ filldist(Exponential(ā), risk_levels)\n    c ~ Beta(4, 18)\n    k ~ truncated(Exponential(1), 1, Inf)\n\n    # use the variables to create a parametric mortality model\n\n    # loop through the rows of the dataframe to let Turing observe the data \n    # and how consistent the parameters are with the data\n    for i = 1:nrow(data)\n        risk = data.risk_level[i]\n\n        m = MortalityTables.MakehamBeard(; a=a[risk], b, c, k)\n        age = data.att_age[i]\n        q = MortalityTables.hazard(m, age)\n        deaths[i] ~ Poisson(data.exposures[i] * q)\n    end\nend\n\nm4 = mortality4(data2, data2.deaths)\n\nchain4 = sample(m4, NUTS(), 1000)\n\n\nChains MCMC chain (1000×19×1 Array{Float64, 3}):\n\nIterations        = 501:1:1500\nNumber of chains  = 1\nSamples per chain = 1000\nWall duration     = 23.67 seconds\nCompute duration  = 23.67 seconds\nparameters        = b, ā, a[1], a[2], a[3], c, k\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std      mcse   ess_bulk   ess_tail      rhat   e ⋯\n      Symbol   Float64   Float64   Float64    Float64    Float64   Float64     ⋯\n\n           b    0.1029    0.0094    0.0006   233.2661   204.1708    1.0022     ⋯\n           ā    0.0001    0.0001    0.0000   257.0754   291.2055    0.9991     ⋯\n        a[1]    0.0000    0.0000    0.0000   229.7994   203.3633    1.0007     ⋯\n        a[2]    0.0000    0.0000    0.0000   227.9004   225.4640    1.0009     ⋯\n        a[3]    0.0000    0.0000    0.0000   229.9824   210.3709    1.0014     ⋯\n           c    0.0010    0.0003    0.0000   348.9915   450.5228    1.0017     ⋯\n           k    2.0173    0.9972    0.0429   295.1656   175.3704    1.0009     ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n           b    0.0850    0.0965    0.1027    0.1092    0.1220\n           ā    0.0000    0.0000    0.0000    0.0001    0.0003\n        a[1]    0.0000    0.0000    0.0000    0.0000    0.0000\n        a[2]    0.0000    0.0000    0.0000    0.0000    0.0000\n        a[3]    0.0000    0.0000    0.0000    0.0000    0.0001\n           c    0.0004    0.0007    0.0009    0.0012    0.0017\n           k    1.0192    1.2917    1.7145    2.4544    4.6541\n\n\n\n\n\nrisk_factors4 = [mean(chain4[Symbol(\"a[$f]\")]) for f in 1:3]\n\nrisk_factors4 ./ risk_factors4[2]\n\nlet data = data2\n\n    data_weight = data.exposures ./ sum(data.exposures)\n    data_weight = .√(data_weight ./ maximum(data_weight) .* 20)\n    color_i = data.risk_level\n\n    p = scatter(\n        data.att_age,\n        data.fraction,\n        markersize=data_weight,\n        alpha=0.5,\n        color=color_i,\n        label=\"Experience data point (size indicates relative exposure quantity)\",\n        axis=(xlabel=\"age\",\n            limits=(nothing, nothing, 0.0, 0.25),\n            ylabel=\"mortality rate\",\n            title=\"Parametric Bayseian Mortality\"\n        )\n    )\n\n\n    # show n samples from the posterior plotted on the graph\n    n = 100\n\n    ages = sort!(unique(data.att_age))\n    for r in 1:3\n        for i in 1:n\n            s = sample(chain4, 1)\n            a = only(s[Symbol(\"a[$r]\")])\n            b = only(s[:b])\n            k = only(s[:k])\n            c = 0\n            m = MortalityTables.MakehamBeard(; a, b, c, k)\n            if i == 1\n                lines!(ages, age -&gt; MortalityTables.hazard(m, age), label=\"risk level $r\", alpha=0.2, color=(CairoMakie.Makie.wong_colors()[r], 0.2))\n            else\n                lines!(ages, age -&gt; MortalityTables.hazard(m, age), label=\"\", alpha=0.2, color=(CairoMakie.Makie.wong_colors()[r], 0.2))\n            end\n        end\n    end\n    p\nend",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Bayesian Mortality Modeling</span>"
    ]
  },
  {
    "objectID": "bayesian-mortality.html#predictions",
    "href": "bayesian-mortality.html#predictions",
    "title": "25  Bayesian Mortality Modeling",
    "section": "25.6 Predictions",
    "text": "25.6 Predictions\nWe can generate predictive estimates by passing a vector of missing in place of the outcome variables and then calling predict.\nWe get a table of values where each row is the the prediction implied by the corresponding chain sample, and the columns are the predicted value for each of the outcomes in our original dataset.\n\npreds = predict(mortality4(data2, fill(missing, length(data2.deaths))), chain4)\n\n\nChains MCMC chain (1000×615×1 Array{Float64, 3}):\n\nIterations        = 1:1:1000\nNumber of chains  = 1\nSamples per chain = 1000\nparameters        = deaths[1], deaths[2], deaths[3], deaths[4], deaths[5], deaths[6], deaths[7], deaths[8], deaths[9], deaths[10], deaths[11], deaths[12], deaths[13], deaths[14], deaths[15], deaths[16], deaths[17], deaths[18], deaths[19], deaths[20], deaths[21], deaths[22], deaths[23], deaths[24], deaths[25], deaths[26], deaths[27], deaths[28], deaths[29], deaths[30], deaths[31], deaths[32], deaths[33], deaths[34], deaths[35], deaths[36], deaths[37], deaths[38], deaths[39], deaths[40], deaths[41], deaths[42], deaths[43], deaths[44], deaths[45], deaths[46], deaths[47], deaths[48], deaths[49], deaths[50], deaths[51], deaths[52], deaths[53], deaths[54], deaths[55], deaths[56], deaths[57], deaths[58], deaths[59], deaths[60], deaths[61], deaths[62], deaths[63], deaths[64], deaths[65], deaths[66], deaths[67], deaths[68], deaths[69], deaths[70], deaths[71], deaths[72], deaths[73], deaths[74], deaths[75], deaths[76], deaths[77], deaths[78], deaths[79], deaths[80], deaths[81], deaths[82], deaths[83], deaths[84], deaths[85], deaths[86], deaths[87], deaths[88], deaths[89], deaths[90], deaths[91], deaths[92], deaths[93], deaths[94], deaths[95], deaths[96], deaths[97], deaths[98], deaths[99], deaths[100], deaths[101], deaths[102], deaths[103], deaths[104], deaths[105], deaths[106], deaths[107], deaths[108], deaths[109], deaths[110], deaths[111], deaths[112], deaths[113], deaths[114], deaths[115], deaths[116], deaths[117], deaths[118], deaths[119], deaths[120], deaths[121], deaths[122], deaths[123], deaths[124], deaths[125], deaths[126], deaths[127], deaths[128], deaths[129], deaths[130], deaths[131], deaths[132], deaths[133], deaths[134], deaths[135], deaths[136], deaths[137], deaths[138], deaths[139], deaths[140], deaths[141], deaths[142], deaths[143], deaths[144], deaths[145], deaths[146], deaths[147], deaths[148], deaths[149], deaths[150], deaths[151], deaths[152], deaths[153], deaths[154], deaths[155], deaths[156], deaths[157], deaths[158], deaths[159], deaths[160], deaths[161], deaths[162], deaths[163], deaths[164], deaths[165], deaths[166], deaths[167], deaths[168], deaths[169], deaths[170], deaths[171], deaths[172], deaths[173], deaths[174], deaths[175], deaths[176], deaths[177], deaths[178], deaths[179], deaths[180], deaths[181], deaths[182], deaths[183], deaths[184], deaths[185], deaths[186], deaths[187], deaths[188], deaths[189], deaths[190], deaths[191], deaths[192], deaths[193], deaths[194], deaths[195], deaths[196], deaths[197], deaths[198], deaths[199], deaths[200], deaths[201], deaths[202], deaths[203], deaths[204], deaths[205], deaths[206], deaths[207], deaths[208], deaths[209], deaths[210], deaths[211], deaths[212], deaths[213], deaths[214], deaths[215], deaths[216], deaths[217], deaths[218], deaths[219], deaths[220], deaths[221], deaths[222], deaths[223], deaths[224], deaths[225], deaths[226], deaths[227], deaths[228], deaths[229], deaths[230], deaths[231], deaths[232], deaths[233], deaths[234], deaths[235], deaths[236], deaths[237], deaths[238], deaths[239], deaths[240], deaths[241], deaths[242], deaths[243], deaths[244], deaths[245], deaths[246], deaths[247], deaths[248], deaths[249], deaths[250], deaths[251], deaths[252], deaths[253], deaths[254], deaths[255], deaths[256], deaths[257], deaths[258], deaths[259], deaths[260], deaths[261], deaths[262], deaths[263], deaths[264], deaths[265], deaths[266], deaths[267], deaths[268], deaths[269], deaths[270], deaths[271], deaths[272], deaths[273], deaths[274], deaths[275], deaths[276], deaths[277], deaths[278], deaths[279], deaths[280], deaths[281], deaths[282], deaths[283], deaths[284], deaths[285], deaths[286], deaths[287], deaths[288], deaths[289], deaths[290], deaths[291], deaths[292], deaths[293], deaths[294], deaths[295], deaths[296], deaths[297], deaths[298], deaths[299], deaths[300], deaths[301], deaths[302], deaths[303], deaths[304], deaths[305], deaths[306], deaths[307], deaths[308], deaths[309], deaths[310], deaths[311], deaths[312], deaths[313], deaths[314], deaths[315], deaths[316], deaths[317], deaths[318], deaths[319], deaths[320], deaths[321], deaths[322], deaths[323], deaths[324], deaths[325], deaths[326], deaths[327], deaths[328], deaths[329], deaths[330], deaths[331], deaths[332], deaths[333], deaths[334], deaths[335], deaths[336], deaths[337], deaths[338], deaths[339], deaths[340], deaths[341], deaths[342], deaths[343], deaths[344], deaths[345], deaths[346], deaths[347], deaths[348], deaths[349], deaths[350], deaths[351], deaths[352], deaths[353], deaths[354], deaths[355], deaths[356], deaths[357], deaths[358], deaths[359], deaths[360], deaths[361], deaths[362], deaths[363], deaths[364], deaths[365], deaths[366], deaths[367], deaths[368], deaths[369], deaths[370], deaths[371], deaths[372], deaths[373], deaths[374], deaths[375], deaths[376], deaths[377], deaths[378], deaths[379], deaths[380], deaths[381], deaths[382], deaths[383], deaths[384], deaths[385], deaths[386], deaths[387], deaths[388], deaths[389], deaths[390], deaths[391], deaths[392], deaths[393], deaths[394], deaths[395], deaths[396], deaths[397], deaths[398], deaths[399], deaths[400], deaths[401], deaths[402], deaths[403], deaths[404], deaths[405], deaths[406], deaths[407], deaths[408], deaths[409], deaths[410], deaths[411], deaths[412], deaths[413], deaths[414], deaths[415], deaths[416], deaths[417], deaths[418], deaths[419], deaths[420], deaths[421], deaths[422], deaths[423], deaths[424], deaths[425], deaths[426], deaths[427], deaths[428], deaths[429], deaths[430], deaths[431], deaths[432], deaths[433], deaths[434], deaths[435], deaths[436], deaths[437], deaths[438], deaths[439], deaths[440], deaths[441], deaths[442], deaths[443], deaths[444], deaths[445], deaths[446], deaths[447], deaths[448], deaths[449], deaths[450], deaths[451], deaths[452], deaths[453], deaths[454], deaths[455], deaths[456], deaths[457], deaths[458], deaths[459], deaths[460], deaths[461], deaths[462], deaths[463], deaths[464], deaths[465], deaths[466], deaths[467], deaths[468], deaths[469], deaths[470], deaths[471], deaths[472], deaths[473], deaths[474], deaths[475], deaths[476], deaths[477], deaths[478], deaths[479], deaths[480], deaths[481], deaths[482], deaths[483], deaths[484], deaths[485], deaths[486], deaths[487], deaths[488], deaths[489], deaths[490], deaths[491], deaths[492], deaths[493], deaths[494], deaths[495], deaths[496], deaths[497], deaths[498], deaths[499], deaths[500], deaths[501], deaths[502], deaths[503], deaths[504], deaths[505], deaths[506], deaths[507], deaths[508], deaths[509], deaths[510], deaths[511], deaths[512], deaths[513], deaths[514], deaths[515], deaths[516], deaths[517], deaths[518], deaths[519], deaths[520], deaths[521], deaths[522], deaths[523], deaths[524], deaths[525], deaths[526], deaths[527], deaths[528], deaths[529], deaths[530], deaths[531], deaths[532], deaths[533], deaths[534], deaths[535], deaths[536], deaths[537], deaths[538], deaths[539], deaths[540], deaths[541], deaths[542], deaths[543], deaths[544], deaths[545], deaths[546], deaths[547], deaths[548], deaths[549], deaths[550], deaths[551], deaths[552], deaths[553], deaths[554], deaths[555], deaths[556], deaths[557], deaths[558], deaths[559], deaths[560], deaths[561], deaths[562], deaths[563], deaths[564], deaths[565], deaths[566], deaths[567], deaths[568], deaths[569], deaths[570], deaths[571], deaths[572], deaths[573], deaths[574], deaths[575], deaths[576], deaths[577], deaths[578], deaths[579], deaths[580], deaths[581], deaths[582], deaths[583], deaths[584], deaths[585], deaths[586], deaths[587], deaths[588], deaths[589], deaths[590], deaths[591], deaths[592], deaths[593], deaths[594], deaths[595], deaths[596], deaths[597], deaths[598], deaths[599], deaths[600], deaths[601], deaths[602], deaths[603], deaths[604], deaths[605], deaths[606], deaths[607], deaths[608], deaths[609], deaths[610], deaths[611], deaths[612], deaths[613], deaths[614], deaths[615]\ninternals         = \n\nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ⋯\n      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ⋯\n\n   deaths[1]    0.0800    0.2893    0.0101    823.8474    839.2649    0.9991   ⋯\n   deaths[2]    0.1050    0.3288    0.0111    867.2257    850.7642    0.9990   ⋯\n   deaths[3]    0.1410    0.3785    0.0133    815.2475    829.9658    0.9995   ⋯\n   deaths[4]    0.0850    0.2998    0.0099    942.1352    955.8923    0.9991   ⋯\n   deaths[5]    0.1080    0.3353    0.0113    872.9503    874.7944    1.0037   ⋯\n   deaths[6]    0.1520    0.4013    0.0125   1018.3354    994.3935    0.9996   ⋯\n   deaths[7]    0.0670    0.2731    0.0086   1006.6870   1016.3682    0.9991   ⋯\n   deaths[8]    0.1020    0.3431    0.0113    962.0040    811.2027    1.0002   ⋯\n   deaths[9]    0.1540    0.3853    0.0132    856.7941    867.6040    0.9992   ⋯\n  deaths[10]    0.0750    0.2783    0.0085   1075.4465   1010.1399    0.9992   ⋯\n  deaths[11]    0.1180    0.3467    0.0122    826.4943    869.0158    1.0002   ⋯\n  deaths[12]    0.1310    0.3549    0.0123    827.8854    825.8416    0.9990   ⋯\n  deaths[13]    0.0900    0.3066    0.0097    997.2699   1009.9563    0.9991   ⋯\n  deaths[14]    0.1160    0.3445    0.0119    880.8107    927.8549    0.9993   ⋯\n  deaths[15]    0.1500    0.3920    0.0121   1035.3036   1014.2939    1.0017   ⋯\n  deaths[16]    0.1000    0.3132    0.0101    963.8286    964.2805    0.9993   ⋯\n  deaths[17]    0.1150    0.3223    0.0123    691.9285    706.1928    1.0025   ⋯\n      ⋮           ⋮         ⋮         ⋮          ⋮           ⋮          ⋮      ⋱\n                                                   1 column and 598 rows omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n\n   deaths[1]    0.0000    0.0000    0.0000    0.0000    1.0000\n   deaths[2]    0.0000    0.0000    0.0000    0.0000    1.0000\n   deaths[3]    0.0000    0.0000    0.0000    0.0000    1.0000\n   deaths[4]    0.0000    0.0000    0.0000    0.0000    1.0000\n   deaths[5]    0.0000    0.0000    0.0000    0.0000    1.0000\n   deaths[6]    0.0000    0.0000    0.0000    0.0000    1.0000\n   deaths[7]    0.0000    0.0000    0.0000    0.0000    1.0000\n   deaths[8]    0.0000    0.0000    0.0000    0.0000    1.0000\n   deaths[9]    0.0000    0.0000    0.0000    0.0000    1.0000\n  deaths[10]    0.0000    0.0000    0.0000    0.0000    1.0000\n  deaths[11]    0.0000    0.0000    0.0000    0.0000    1.0000\n  deaths[12]    0.0000    0.0000    0.0000    0.0000    1.0000\n  deaths[13]    0.0000    0.0000    0.0000    0.0000    1.0000\n  deaths[14]    0.0000    0.0000    0.0000    0.0000    1.0000\n  deaths[15]    0.0000    0.0000    0.0000    0.0000    1.0000\n  deaths[16]    0.0000    0.0000    0.0000    0.0000    1.0000\n  deaths[17]    0.0000    0.0000    0.0000    0.0000    1.0000\n      ⋮           ⋮         ⋮         ⋮         ⋮         ⋮\n                                                598 rows omitted",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Bayesian Mortality Modeling</span>"
    ]
  },
  {
    "objectID": "other-techniques.html",
    "href": "other-techniques.html",
    "title": "26  Other Useful Techniques",
    "section": "",
    "text": "26.1 In this chapter\nOther useful techniques are surveyed, such as: memoization to avoid repeated computations, psuedo-monte carlo, creating a model office, and tips on modeling a complete balance sheet.",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Other Useful Techniques</span>"
    ]
  },
  {
    "objectID": "other-techniques.html#conceptual-techniques",
    "href": "other-techniques.html#conceptual-techniques",
    "title": "26  Other Useful Techniques",
    "section": "26.2 Conceptual Techniques",
    "text": "26.2 Conceptual Techniques\n\n26.2.1 Taking things to the Extreme\nConsider what happens if something is taken to an extreme. For example, what happens in the model if we input negative rates? Where should negative rates be allowed and can the model handle them?\n\n\n26.2.2 Range Bounding\nSometimes you just need to know that an outcome is within a certain range - if you can develop a “high” and “low” estimate by making assumptions that you know are outside of feasible ranges, then you can determine whether something is reasonable or within tolerances.\nTo take an example from the pages of interview questions: say you need to determine if a mortgaged property’s value is greater than the amount of the outstanding loan (say $100,000). You don’t have an appraisal, but know that it’s in reasonable condition and that (1) a comparable house with many more issues sold for $100 per square foot. You also don’t know the square footage of the house, but know from the number of rooms and layout that it must be at least 1000 square feet. Therefore you know that the value should at least be greater than:\n\\[\n\\frac{\\$100}{\\text{sq. ft}} \\times 1000 \\text{sq. ft} = \\$100,000\n\\]\nWe’d then conclude that the value of the house very likely exceeds the outstanding balance of the loan and resolves our query without complex modeling or expensive appraisals.",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Other Useful Techniques</span>"
    ]
  },
  {
    "objectID": "other-techniques.html#modeling-techniques",
    "href": "other-techniques.html#modeling-techniques",
    "title": "26  Other Useful Techniques",
    "section": "26.3 Modeling Techniques",
    "text": "26.3 Modeling Techniques\n\n26.3.1 Serialization",
    "crumbs": [
      "Applications in Practice",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Other Useful Techniques</span>"
    ]
  },
  {
    "objectID": "julia.html",
    "href": "julia.html",
    "title": "27  Set up Julia and the Computing Environment",
    "section": "",
    "text": "27.1 Installation\nJulia is open source and can be downloaded from JuliaLang.org and is available for all major operating systems. After you download and install, then you have Julia installed and can access the REPL, or Read-Eval-Print-Loop, which can run complete programs or function as powerful day-to-day calculator. However, many people find it more comfortable to work in a text editor or IDE (Integrated Development Environment).\nIf you are looking for managed installations with a curated set of packages for use within an organization, there are ways to self-host package repositories and otherwise administratively manage packages. Julia Computing offers managed support with enterprise solutions, including push-button cloud compute capabilities.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Set up Julia and the Computing Environment</span>"
    ]
  },
  {
    "objectID": "julia.html#package-management",
    "href": "julia.html#package-management",
    "title": "27  Set up Julia and the Computing Environment",
    "section": "27.2 Package Management",
    "text": "27.2 Package Management\nJulia comes with Pkg, a built-in package manger. With it, you can install packages, pin certain versions, recreate environments with the same set of dependencies, and upgrade/remove/develop packages easily. It’s one of the things that just works and makes Julia stand out versus alternative languages that don’t have a de-facto way of managing or installing packages.\nPackage installation is accomplished interactively in the REPL or executing commands.\n\nIn the REPL, you can change to the Package Management Mode by hitting ] and, e.g., add DataFrames CSV to install the two packages. Hit [backspace] to exit that mode in the REPL.\nThe same operation without changing REPL modes would be: using Pkg; Pkg.add([\"DataFrames\", \"CSV\"])\n\nRelated to packages, are environments which are a self-contained workspaces for your code. This lets you install only packages that are relevant to the current work. It also lets you ‘remember’ the exact set of packages and versions that you used. In fact, you can share the environment with others, and it will be able to recreate the same environment as when you ran the code. This is accomplished via a Project.toml file, which tracks the direct dependencies you’ve added, along with details about your project like its version number. The Manifest.toml tracks the entire dependency tree.\nReproducibility via the environment tools above is a really key aspect that will ensure Julia code is consistent across time and users, which is important for financial controls.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Set up Julia and the Computing Environment</span>"
    ]
  },
  {
    "objectID": "julia.html#editors",
    "href": "julia.html#editors",
    "title": "27  Set up Julia and the Computing Environment",
    "section": "27.3 Editors",
    "text": "27.3 Editors\nBecause Julia is very extensible and amenable to analysis of its own code, you can typically find plugins for whatever tool you prefer to write code in. A few examples:\n\n27.3.1 Visual Studio Code\nVisual Studio Code is a free editor from Microsoft. There’s a full-featured Julia plugin available, which will help with auto-completion, warnings, and other code hints that you might find in a dedicated editor (e.g. PyCharm or RStudio). Like those tools, you can view plots, search documentation, show datasets, debug, and manage version control.\n\n\n27.3.2 Notebooks\nNotebooks are typically more interactive environments than text editors - you can write code in cells and see the results side-by-side.\nThe most popular notebook tool is Jupyter (“Julia, Python, R”). It is widely used and fits in well with exploratory data analysis or other interactive workflows. It can be installed by adding the IJulia.jl package.\nPluto.jl is a newer tool, which adds reactivity and interactivity. It is also more amenable to version control than Jupyter notebooks because notebooks are saved as plain Julia scripts. Pluto is unique to Julia because of the language’s ability to introspect and analyze dependencies in its own code. Pluto also has built-in package/environment management, meaning that Pluto notebooks contains all the code needed to reproduce results (as long as Julia and Pluto are installed).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Set up Julia and the Computing Environment</span>"
    ]
  },
  {
    "objectID": "julia.html#repl",
    "href": "julia.html#repl",
    "title": "27  Set up Julia and the Computing Environment",
    "section": "27.4 REPL",
    "text": "27.4 REPL\n\n27.4.1 Help Mode",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Set up Julia and the Computing Environment</span>"
    ]
  },
  {
    "objectID": "environment.html",
    "href": "environment.html",
    "title": "28  Environment and Package Management",
    "section": "",
    "text": "28.1 In This Section\nHow to effectively utilize environments to ensure consistent and reproducible results. How to use and manage packages. How to create a package and share with others. How to use local registries.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Environment and Package Management</span>"
    ]
  },
  {
    "objectID": "environment.html#projects-manifests-and-dependencies",
    "href": "environment.html#projects-manifests-and-dependencies",
    "title": "28  Environment and Package Management",
    "section": "28.2 Projects, Manifests, and Dependencies",
    "text": "28.2 Projects, Manifests, and Dependencies\nJulia comes bundled with Pkg.jl, an environment and package manager. It enables installation of packages from registries, pinning versions for compatibility, and analyzing your dependencies. It uses a couple of files to record this to your project: Project.toml and Manifest.toml.\n\n28.2.1 Project.toml\nA Project.toml file defines attributes about the current project and its dependencies. Julia uses this to understand how to reference your current project and what dependencies it should look for from registries when instantiating the project.\n\n\n\n\n\n\nNote\n\n\n\nTOML (Tom’s Obvious Markup Language) is a modern configuration file format used to store settings and data in a human-readable, plaintext format.\n\n\nThis is a bit abstract, so here is a quick, annotated tour of an example Project.toml file:\n1name = \"FinanceCore\"\n2uuid = \"b9b1ffdd-6612-4b69-8227-7663be06e089\"\nauthors = [\"alecloudenback &lt;alecloudenback@users.noreply.github.com&gt; and contributors\"] \n3version = \"2.1.0\"\n\n4[deps]\nDates = \"ade2ca70-3891-5945-98fb-dc099432e06a\"\nLoopVectorization = \"bdcacae8-1622-11e9-2a5c-532679323890\"\nRoots = \"f2b01f46-fcfa-551c-844a-d8ac1e96c665\"\n\n5[compat]\nDates = \"1\"\nLoopVectorization = \"^0.12\"\nRoots = \"^1.0, 2\"\njulia = \"1.6\"\n\n1\n\nThe name is the name of your current project which only matters if you turn your project into a package.\n\n2\n\nA UUID is a unique identifier and can be created with Julia’s UUIDs standard library.\n\n3\n\nThe version follows Semantic Versioning (“SemVer”) to convey to Pkg (and users!) information that ties a specific version to a specific code commit1.\n\n4\n\nThe deps section records the name of direct dependencies and their UUIDs so that Julia can know which packages to grab in order to make your project run.\n\n5\n\nThe compat section defines compatibility with packages can be enforced (via SemVer) to clarify which versions are allowed to be installed in case incompatibilities arise.\n\n\nWhen you instantiate a project (see Section 28.3 for more), Julia will essentially add the packages listed under deps, and will resolve the compatible versions, generally picking the highest version number for the packages so long as the compat section rule are note broken.\nWhen adding the dependencies, those packages themselves likely specify their own set of dependencies and Julia must resolve the entire dependency graph or dependency tree to allow your current project to work.\n\n\n\n\n\n\nSemantic Versioning\n\n\n\nSemantic Versioning (“SemVer”) is a scheme which uses the three-component version code to convey meaning about different versions of a package to both users and computer systems. With the version scheme vMAJOR.MINOR.PATCH, the meaning is roughly as follows:\n\nMAJOR increments denote changes to the code which make it incompatible with prior versions.\nMINOR increments denote changes which add features that are compatible with the prior versions.\nPATCH increments denote changes which fix issues in prior versions and code written against the prior version is still compatible.\n\nAs an example, say we are currently using v2.10.4 of a package, and the following theoretical options are available for us to upgrade to:\n\nv2.10.5 - The 4 to 5 indicates that something may have been broken in the prior release and so we should upgrade without fear that we need to make changes to our code (unless we relied on the previously broken code!).\nv2.11.0 - The 10 to 11 bump suggests that the new release contains some features which should not require us to change any of our previously written code.\nv3.0.0 - The 2 to 3 indicates that we will potentially have to modify code that we have written that interfaces with this dependency.\n\nSemVer cannot distill all possible compatibility and upgrade information about a set of packages (e.g. an author may release an update with a MINOR version which also includes fixes).\n\n\n\n\n28.2.2 Manifest.toml\nThe Manifest.toml file includes a record of all external dependencies used by the project at hand. Unlike Project.toml, this file gets machine generated when Julia instantiates or updates the environment. The contents are basically a long list of your direct dependencies and the dependencies of those direct dependencies and looks something like this:\njulia_version = \"1.10.0\"\nmanifest_format = \"2.0\"\nproject_hash = \"5fea00df4808d89f9c977d15b8ee992bd408081b\"\n\n[[deps.AbstractFFTs]]\ndeps = [\"LinearAlgebra\"]\ngit-tree-sha1 = \"d92ad398961a3ed262d8bf04a1a2b8340f915fef\"\nuuid = \"621f4979-c628-5d54-868e-fcf4e3e8185c\"\nversion = \"1.5.0\"\nweakdeps = [\"ChainRulesCore\", \"Test\"]\n\n    [deps.AbstractFFTs.extensions]\n    AbstractFFTsChainRulesCoreExt = \"ChainRulesCore\"\n    AbstractFFTsTestExt = \"Test\"\n\n... many more lines\n\n\n\n\n\n\nNote\n\n\n\nStarting in Julia 1.11, Manifest files will include a version indication, making it nicer to work with multiple Julia versions at one time on a single system.\n\n\n\n\n28.2.3 Reproducibility\nReproducibility fulfills both practical and principled goals. Practical in that we can record the complex chain of dependencies that is used in modern computing in order to potentially re-create a result or demonstrate an audit trail of the tools used. Principled in that there are circumstances (like science research) in which we want to be able to replicate results. The combination of Project.toml and Manifest.toml go a long way towards accomplishing this, as you can share both and with the same hardware and Julia version should be able to get the exact same set of dependencies and therefore run the same code. In practice, this level of reproducibility isn’t usually needed, as most time a set of code can be run accurately without requiring the exact same set of dependencies.\nSince dependencies can have variation between systems (Windows/Mac) and architectures (x86 vs x64), you may not be able to recreate the Manifest exactly. Nevertheless, it’s a fairly low bar if you are trying to maintain the utmost level of rigor around the toolchain and Julia is one of the most robust languages regarding tools to support open replication of results.\n\n\n\n\n\n\nArtifacts\n\n\n\nJulia has a system called artifacts which allows specification of a location and hash (a cryptographic key) for data and binaries. The artifact system used to download and verify the contents of a file match the hash. This is designed for more permanent data and less end-user workflows, but we call it out here as another example where Julia takes steps to promote consistency and reproducibility.\nFor more on data workflows for the end-user, see Chapter 11.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Environment and Package Management</span>"
    ]
  },
  {
    "objectID": "environment.html#sec-environment-details",
    "href": "environment.html#sec-environment-details",
    "title": "28  Environment and Package Management",
    "section": "28.3 Environemnts",
    "text": "28.3 Environemnts\nEnvironment is meant to mean, in general, the computer you use and software installed in it. When we speak about environments in the Julia context, this means the Julia version and packages available to the current Julia code. For example, from the current code is a given package installed and usable?\nIf you open a Julia REPL, by default you will be in the global environment. If you hit ] to enter Pkg mode, you should see:\n(@v1.10) pkg&gt;\nThe (@1.10) indicates that you are using the global environment for the current Julia version (there is no global environment which applies across all Julia versions installed). You can activate a new environment with activate [environment name].\n(@v1.10) pkg&gt; activate MyNewEnv\n  Activating new project at `~/MyNewEnv`\nThis will… not do anything. Yet! When we add a package to this environment, then it will create a Project.toml and Manifest.toml file in that directory. Now that directory is a full fledged Julia project!\n\n\n\n\n\n\nTip\n\n\n\nActivate a temporary environment with activate --temp. This will give you a temporary environment with a random name, which is very useful for testing out things in a clean, simplified environment (the global environment, like @1.10 still applies.)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Environment and Package Management</span>"
    ]
  },
  {
    "objectID": "environment.html#packages",
    "href": "environment.html#packages",
    "title": "28  Environment and Package Management",
    "section": "28.4 Packages",
    "text": "28.4 Packages\n\n28.4.1 Packages versus Projects\n\n\n28.4.2 Basic Package Structure\n\n\n28.4.3 Extension Packages",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Environment and Package Management</span>"
    ]
  },
  {
    "objectID": "environment.html#regisries",
    "href": "environment.html#regisries",
    "title": "28  Environment and Package Management",
    "section": "28.5 Regisries",
    "text": "28.5 Regisries\n\n28.5.1 Local Registries",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Environment and Package Management</span>"
    ]
  },
  {
    "objectID": "environment.html#footnotes",
    "href": "environment.html#footnotes",
    "title": "28  Environment and Package Management",
    "section": "",
    "text": "When registering a package to a repository, the repository will record the version indicated in the Project.toml file to the git commit id of the package when it is registered.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Environment and Package Management</span>"
    ]
  },
  {
    "objectID": "ecosystem.html",
    "href": "ecosystem.html",
    "title": "29  The Julia Ecosystem Today",
    "section": "",
    "text": "A tour of relevant available packages as of 2023.\nThe Julia ecosystem favors composability and interoperability, enabled by multiple dispatch. In other words, because it’s easy to automatically specialize functionality based on the type of data being used, there’s much less need to bundle a lot of features within a single package.\nAs you’ll see, Julia packages tend to be less vertically integrated because it’s easier to pass data around. Counterexamples of this in Python and R:\n\nNumpy-compatible packages that are designed to work with a subset of numerically fast libraries in Python\nspecial functions in Pandas to read CSV, JSON, database connections, etc.\nThe Tidyverse in R has a tightly coupled set of packages that works well together but has limitations with some other R packages\n\nJulia is not perfect in this regard, but it’s neat to see how frequently things just work. It’s not magic, but because of Julia features outside the scope of this article it’s easy for package developers (and you!) to do this.\nJulia also has language-level support for documentation, so packages can follow a consistent style of help-text and have the docs be auto-generated into web pages available locally or online.\nThe following highlighted packages were chosen for their relevance to typical actuarial work, with a bias towards those used regularly by the authors. This is a small sampling of the over 6000 registered Julia Packages[^2]\n\n29.0.1 Data\nJulia offers a rich data ecosystem with a multitude of available packages. Perhaps at the center of the data ecosystem are CSV.jl and DataFrames.jl. CSV.jl is for reading and writing files text files (namely CSVs) and offers top-class read and write performance. DataFrames.jl is a mature package for working with dataframes, comparable to Pandas or dplyr.\nOther notable packages include ODBC.jl, which lets you connect to any database (given you have the right drivers installed), and Arrow.jl which implements the Apache Arrow standard in Julia.\nWorth mentioning also is Dates, a built-in package making date manipulation straightforward and robust.\nCheck out JuliaData org for more packages and information.\n\n\n29.0.2 Plotting\nPlots.jl is a meta-package providing an interface to consistently work with several plotting backends, depending if you are trying to emphasize interactivity on the web or print-quality output. You can very easily add animations or change almost any feature of a plot.\nStatsPlots.jl extends Plots.jl with a focus on data visualization and compatibility with dataframes.\nMakie.jl supports GPU-accelerated plotting and can create very rich, beautiful visualizations, but it’s main downside is that it has not yet been optimized to minimize the time-to-first-plot.\n\n\n29.0.3 Statistics\nJulia has first-class support for missing values, which follows the rules of three-valued logic so other packages don’t need to do anything special to incorporate missing values.\nStatsBase.jl and Distributions.jl are essentials for a range of statistics functions and probability distributions respectively.\nOthers include:\n\nTuring.jl, a probabilistic programming (Bayesian statistics) library, which is outstanding in its combination of clear model syntax with performance.\nGLM.jl for any type of linear modeling (mimicking R’s glm functionality).\nLsqFit.jl for fitting data to non-linear models.\nMultivariateStats.jl for multivariate statistics, such as PCA.\n\nYou can find more packages and learn about them here.\n\n\n29.0.4 Machine Learning\nFlux, Gen, Knet, and MLJ are all very popular machine learning libraries. There are also packages for PyTorch, Tensorflow, and SciKitML available. One advantage for users is that the Julia packages are written in Julia, so it can be easier to adapt or see what’s going on in the entire stack. In contrast to this design, PyTorch and Tensorflow are built primarily with C++.\nAnother advantage is that the Julia libraries can use automatic differentiation to optimize on a wider range of data and functions than those built into libraries in other languages.\n\n\n29.0.5 Differentiable Programming\nSensitivity testing is very common in actuarial workflows: essentially, it’s understanding the change in one variable in relation to another. In other words, the derivative!\nJulia has unique capabilities where almost across the entire language and ecosystem, you can take the derivative of entire functions or scripts. For example, the following is real Julia code to automatically calculate the sensitivity of the ending account value with respect to the inputs:\njulia&gt; using Zygote\n\njulia&gt; function policy_av(pol)\n    COIs = [0.00319, 0.00345, 0.0038, 0.00419, 0.0047, 0.00532]\n    av = 0.0\n    for (i,coi) in enumerate(COIs)\n        av += av * pol.credit_rate\n        av += pol.annual_premium\n        av -= pol.face * coi\n    end\n    return av                # return the final account value\nend\n\njulia&gt; pol = (annual_premium = 1000, face = 100_000, credit_rate = 0.05);\n\njulia&gt; policy_av(pol)        # the ending account value\n4048.08\n\njulia&gt; policy_av'(pol)       # the derivative of the account value with respect to the inputs\n(annual_premium = 6.802, face = -0.0275, credit_rate = 10972.52)\nWhen executing the code above, Julia isn’t just adding a small amount and calculating the finite difference. Differentiation is applied to entire programs through extensive use of basic derivatives and the chain rule. Automatic differentiation, has uses in optimization, machine learning, sensitivity testing, and risk analysis. You can read more about Julia’s autodiff ecosystem here.\n\n\n29.0.6 Utilities\nThere are also a lot of quality-of-life packages, like Revise.jl which lets you edit code on the fly without needing to re-run entire scripts.\nBenchmarkTools.jl makes it incredibly easy to benchmark your code - simply add @benchmark in front of what you want to test, and you will be presented with detailed statistics. For example:\njulia&gt; using ActuaryUtilities, BenchmarkTools\n\njulia&gt; @benchmark present_value(0.05,[10,10,10])\n\nBenchmarkTools.Trial: 10000 samples with 994 evaluations.\n Range (min … max):  33.492 ns … 829.015 ns  ┊ GC (min … max): 0.00% … 95.40%\n Time  (median):     34.708 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   36.599 ns ±  33.686 ns  ┊ GC (mean ± σ):  4.40% ±  4.55%\n\n  ▁▃▆▆▆██▇▄▃▂         ▁                                        ▂\n  █████████████▆▆▇█▇████▇██▇█▇█▇▇▆▆▅▅▅▅▅▄▅▄▄▅▅▅▅▄▄▁▅▄▄▅▄▄▅▅▆▅▆ █\n  33.5 ns       Histogram: log(frequency) by time      45.6 ns &lt;\n\n Memory estimate: 112 bytes, allocs estimate: 1.\nTest is a built-in package for performing testsets, while Documenter.jl will build high-quality documentation based on your inline documentation.\nClipData.jl lets you copy and paste from spreadsheets to Julia sessions.\n\n\n29.0.7 Other packages\nJulia is a general-purpose language, so you will find packages for web development, graphics, game development, audio production, and much more. You can explore packages (and their dependencies) at https://juliahub.com/.\n\n\n29.0.8 Actuarial packages\nSaving the best for last, the next article in the series will dive deeper into actuarial packages, such as those published by JuliaActuary for easy mortality table manipulation, common actuarial functions, financial math, and experience analysis.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>The Julia Ecosystem Today</span>"
    ]
  },
  {
    "objectID": "debugging.html",
    "href": "debugging.html",
    "title": "30  Debugging and Performance Measurement",
    "section": "",
    "text": "30.1 Benchmarking",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Debugging and Performance Measurement</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Leemis, Lawrence M, and Jacquelyn T McQueston. 2008. “Univariate\nDistribution Relationships.” The American Statistician\n62 (1): 45–53. https://doi.org/10.1198/000313008x270448.\n\n\nLewis, N D. 2013. 100 Statistical Tests. Createspace.",
    "crumbs": [
      "Appendices",
      "References"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "The approach\nThe authors of the book are practicing actuaries, but we intend for the content to be applicable to nearly all practitioners in the financial industry. The discussion and examples may have an orientation towards insurance topics, but the concepts and patterns are applicable to a wide variety of related disciplines.\nWe will pull from examples on both sides of the balance sheet: the left (assets) and right (liabilities). We may also take the liberty to, at times, abuse traditional accounting notions: a liability is just an asset with the obligor and obligee switched. When the accounting conventions are important (such as modeling a total balance sheet) we will be mindful in explaining the accounting perspective. In practice, this means that we’ll take examples that use examples of assets (fixed income, equity, derivatives) or liabilities (life insurance, annuities, long term care) and show that similar modeling techniques can be used for both.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#what-you-will-learn",
    "href": "intro.html#what-you-will-learn",
    "title": "Introduction",
    "section": "What you will learn",
    "text": "What you will learn\nIt is our hope that with the help of this book, you will find it more efficient to discuss aspects of modeling with colleagues, borrow problem solving language from computer science, spot recurring structural patterns in problems that arise, and understand how best to make use of the “bicycle for your mind” in the context of financial modeling.\nIt is the experience of the authors that many professsionals that do complex modeling as a part of their work have gotten to be very proficient in spite of not having substantive formal training on problem solving, algorithms, or model architecture. This book serves to fill that gap and provide the “missing semester” (or “years of practical learning”!). After reading this book, we hope that you will appreciate the attributes of Microsoft Excel that made it so ubiquitous, but that you prefer to use a programming language for the ability to more naturally express the relevant abstractions which make your models simpler, faster, or more usuable by others.\nEven if your direct responsibility does not entail hands-on-coding, be it management or “low-code”, the ideas and language should prove useful in guiding the work to a cleaner, more efficient solution.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#the-journey-ahead",
    "href": "intro.html#the-journey-ahead",
    "title": "Introduction",
    "section": "The Journey Ahead",
    "text": "The Journey Ahead\nLearning a new topic, especially one that’s not well trodden in a given field, can be intimidating. There are many resources available online, this book will recommend some others, and there are community support resources available - check the chat and forums and look for the users talking about the topics that interest you. One of the wonderful things about the technology community is the degree to which content is available online for learning and reference.\nFurther, moving substantial parts of the financial services industry towards a digital-first, modern workflow is a monumental effort and you should seek partners on both the finance and information technology side. In general, good ideas and processes will prevail and the trick to encouraging adoption is finding the right place to plug a new idea or suggestion. One of the secondary, but still important, things this book should provide is the language and technical knowledge to partner with others (such as peers and IT) to make pragmatic decisions about the tradeoffs that will need to be made.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#prerequisites",
    "href": "intro.html#prerequisites",
    "title": "Introduction",
    "section": "Prerequisites",
    "text": "Prerequisites\nBasic experience with financial modeling is not strictly required, but it will benefit the reader to be familar so that the examples will not be attempting to teach both financial maths and computer science simultaneously.\nAdvanced financial maths (e.g. stochastic calculus) is not required. Indeed, this book is not oriented to the advanced technicalities of Wall Street “quants” and is instead directed at the multitudes of financial practitioners focused on producing results that are not measured in the microseconds of high-frequency trading.\nPrior programming experience is not required either: 5  Elements of Programming introduces the basic syntax and concepts while 27  Set up Julia and the Computing Environment covers setting up your environment to follow along. For readers with background in programming, we recommend skimming 5  Elements of Programming and reading in full the sections which have a  symbol in the margin, which is our way of highlighting Julia-specific content to be aware of.\n\n\n\n\n\n\nTODO\n\n\n\nCreate a venn diagram showing financial modeling at the intersection of statistics, financial math, computer science.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#the-contents-of-this-book",
    "href": "intro.html#the-contents-of-this-book",
    "title": "Introduction",
    "section": "The Contents of This Book",
    "text": "The Contents of This Book\nPart 1 of the book addresses the theoretical and technical foundations of programming, as well as the conceptual basis for financial modelling. It familiarizes the readers with key functional programming principles, alongside introducing important aspects of software engineering relevant to financial modelling.\nParts 2 and 3 bridge the gap between theory and practical applications, underlining the features of Julia that make it a robust tool for real-world financial and actuarial contexts. Through a careful exploration of topics like sensitivity analysis, optimization, stochastic modeling, visualization, and practical financial applications, the book demonstrates how Julia’s high-level, high-performance programming capabilities can enhance accuracy and efficiency in financial modelling. As an up-and-coming language loved for its speed and simplicity, Julia is ripe for wide adoption in the financial sector. The time for this book is ripe, as it will satiate the growing demand for professionals who want to blend programming skills with financial modelling acumen.\nWhile we have chosen to use Julia for the examples in this book, the vast majority of the concepts presented are not Julia-specific. We will attempt to motivate why Julia works so well as a language for financial modeling but like mathematics and applied mathematics, the concepts are portable even if the numbers (language) changes. Readers are encouraged to follow along the examples on their own computer (see instructions for Julia in 27  Set up Julia and the Computing Environment) and the entire book is available on GitHub at [#TODO: determine book URL].\n\nNotes on formatting\nWhen a concept is defined for the first time, the term will be bold. Code, or references to pieces of code will be formatted in inline code style like 1+1 or in separate code blocks:\n\"This is a code block that doesn't show any results\"\n\n\"This is a code block that does show output\"\n\n\"This is a code block that does show output\"\n\n\nWhen we show inline commands are to be sent to Pkg mode in the REPL (see 28.1 In This Section), such as such as add DataFrames, we will try to make it clear in the context. If using Pkg mode in standalone codeblocks, it will be presented showing the full prompt, such as:\n(@v1.10) pkg&gt; add DataFrames\nThere will be various callout blocks which indiate tips or warnings. These should be self-evident but we wanted to point to a particular callout which is intended to convey advice that stems from practical modeling experience of the authors:\n\n\n\n\n\n\nFinancial Modeling Pro-tip\n\n\n\nThis box indicates a side note that’s particularly applicable to improving your financial modeling.\n\n\n\n\nColophon\nThe HTML and PDF book were rendered using Quarto and Quarto’s open source dependencies like PanDoc.\nThe HTML version of this book uses Lato for the body font and JuliaMono for the monospace font.\nThe PDF version of this book uses TeX Gyre Pagella for the body font and JuliaMono for the monospace font.\nThe cover was designed by Alec Loudeback using Affinity Designer with the graphic used under permission by user cormullion on Github.\nThis book was rendered on June 10, 2024. The system used to generate the code and benchmarks was:\n\nversioninfo()\n\nJulia Version 1.10.3\nCommit 0b4590a5507 (2024-04-30 10:59 UTC)\nBuild Info:\n  Official https://julialang.org/ release\nPlatform Info:\n  OS: macOS (arm64-apple-darwin22.4.0)\n  CPU: 8 × Apple M3\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-15.0.7 (ORCJIT, apple-m1)\nThreads: 4 default, 0 interactive, 2 GC (on 4 virtual cores)\nEnvironment:\n  JULIA_NUM_THREADS = auto",
    "crumbs": [
      "Introduction"
    ]
  }
]