[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Thinking for Actuaries and Financial Professionals",
    "section": "",
    "text": "Preface\nThis book is intended to enable practitioners and advanced students of financial disciplines to utilize the tools, language, and ideas of computational sciences in their own discipline."
  },
  {
    "objectID": "cover_draft.html",
    "href": "cover_draft.html",
    "title": "1  Draft of Cover",
    "section": "",
    "text": "Draft of Cover"
  },
  {
    "objectID": "intro.html#the-approach",
    "href": "intro.html#the-approach",
    "title": "Introduction",
    "section": "The approach",
    "text": "The approach\nThe authors of the book are practicing actuaries, but we intend for the content to be applicable to nearly all practitioners in the financial industry. The discussion and examples may have an orientation towards insurance topics, but the concepts and patterns are applicable to a wide variety of related disciplines.\nWe will pull from examples on both sides of the balance sheet: the left (assets) and right (liabilities). We may also take the liberty to, at times, abuse traditional accounting notions: a liability is just an asset with the obligor and obligee switched. When the accounting conventions are important (such as modeling a total balance sheet) we will be mindful in explaining the accounting perspective. In practice, this means that we’ll take examples that use examples of assets (fixed income, equity, derivatives) or liabilities (life insurance, annuities, long term care) and show that similar modeling techniques can be used for both."
  },
  {
    "objectID": "intro.html#what-you-will-learn",
    "href": "intro.html#what-you-will-learn",
    "title": "Introduction",
    "section": "What you will learn",
    "text": "What you will learn\nIt is our hope that with the help of this book, you will find it more efficient to discuss aspects of modeling with colleagues, borrow problem solving language from computer science, spot recurring structural patterns in problems that arise, and understand how best to make use of the “bicycle for your mind” in the context of financial modeling.\nIt is the experience of the authors that many professsionals that do complex modeling as a part of their work have gotten to be very proficient in spite of not having substantive formal training on problem solving, algorithms, or model architecture. This book serves to fill that gap and provide the “missing semester” (or “years of practical learning”!). After reading this book, we hope that you will appreciate the attributes of Microsoft Excel that made it so ubiquitous, but that you prefer to use a programming language for the ability to more naturally express the relevant abstractions which make your models simpler, faster, or more usuable by others."
  },
  {
    "objectID": "intro.html#prerequisites",
    "href": "intro.html#prerequisites",
    "title": "Introduction",
    "section": "Prerequisites",
    "text": "Prerequisites\nBasic experience with financial modeling is not strictly required, but it will benefit the reader to be familar so that the examples will not be attempting to teach both financial maths and computer science simultaneously.\nAdvanced financial maths (e.g. stochastic calculus) is not required. Indeed, this book is not oriented to the advanced technicalities of Wall Street “quants” and is instead directed at the multitudes of financial practitioners focused on producing results that are not measured in the microseconds of high-frequency trading.\nPrior programming experience is not required either: Chapter 5 introduces the basic syntax and concepts while Chapter 23 covers setting up your environment to follow along. For readers with background in programming, we recommend skimming Chapter 5 and reading in full the sections which have a  symbol in the margin, which is our way of highlighting Julia-specific content to be aware of.\n\n\n\n\n\n\nTODO\n\n\n\nCreate a venn diagram showing financial modeling at the intersection of statistics, financial math, computer science."
  },
  {
    "objectID": "intro.html#the-contents-of-this-book",
    "href": "intro.html#the-contents-of-this-book",
    "title": "Introduction",
    "section": "The Contents of This Book",
    "text": "The Contents of This Book\nPart 1 of the book addresses the theoretical and technical foundations of programming, as well as the conceptual basis for financial modelling. It familiarizes the readers with key functional programming principles, alongside introducing important aspects of software engineering relevant to financial modelling.\nParts 2 and 3 bridge the gap between theory and practical applications, underlining the features of Julia that make it a robust tool for real-world financial and actuarial contexts. Through a careful exploration of topics like sensitivity analysis, optimization, stochastic modeling, visualization, and practical financial applications, the book demonstrates how Julia’s high-level, high-performance programming capabilities can enhance accuracy and efficiency in financial modelling. As an up-and-coming language loved for its speed and simplicity, Julia is ripe for wide adoption in the financial sector. The time for this book is ripe, as it will satiate the growing demand for professionals who want to blend programming skills with financial modelling acumen.\nWhile we have chosen to use Julia for the examples in this book, the vast majority of the concepts presented are not Julia-specific. We will attempt to motivate why Julia works so well as a language for financial modeling but like mathematics and applied mathematics, the concepts are portable even if the numbers (language) changes. Readers are encouraged to follow along the examples on their own computer (see instructions for Julia in Chapter 23) and the entire book is available on GitHub at [#TODO: determine book URL].\n\nNotes on formatting\nWhen a concept is defined for the first time, the term will be bold. Code, or references to pieces of code will be formatted in inline code style like 1+1 or in separate code blocks:\n\"This is a code block that doesn't show any results\"\n\n\"This is a code block that does show output\"\n\n\"This is a code block that does show output\"\n\n\nThere will be various callout blocks which indiate tips or warnings. These should be self-evident but we wanted to point to a particular callout which is intended to convey advice that stems from practical modeling experience of the authors:\n\n\n\n\n\n\nFinancial Modeling Pro-tip\n\n\n\nThis box indicates a side note that’s particularly applicable to improving your financial modeling."
  },
  {
    "objectID": "why-program.html#in-this-chapter",
    "href": "why-program.html#in-this-chapter",
    "title": "2  Why Program?",
    "section": "2.1 In this Chapter",
    "text": "2.1 In this Chapter\nWe motivate why a financial professional should adopt programming skills which will improve their own capabilities and enjoyment of the discipline, whilst allowing themselves to better themselves and the industry we work in."
  },
  {
    "objectID": "why-program.html#the-long-view",
    "href": "why-program.html#the-long-view",
    "title": "2  Why Program?",
    "section": "2.2 The Long View",
    "text": "2.2 The Long View\nIt might be odd to say that technology and its use in insurance is on a one-hundred-year cycle, but that seems to be the case.\n130 years ago, actuaries crowded into a room at a meeting of the Actuarial Society of America to watch a demonstration that would revolutionize the industry: Herman Hollerith’s tabulating punch card machine1.\nFor the next half-century, the increasing automation — from tabulating machines to early-adopting mainframes and computers — was a critical competitive differentiator. Companies like Prudential, MetLife, and others partnered with technology companies in the development of hardware and software2.\nThe dramatic embodiment of this information-driven cycle was portrayed in the infamous Billion Dollar Bubble movie, which showcased the power and abstraction of the computer to commit millions of dollars of fraud by creating and maintaining fake insurance policies.\nThe movie also starts to hint at the oscillation away from the technological-competitive focus of insurance companies. I argue that the focus on technology was lost over the last 50 years with the rise of Wall Street finance, investment-oriented life insurance, industry consolidation, and the explosion of financial structuring like derivatives, reserve financing, or other advanced forms of reinsurance.\nValue-add came from the C-Suite, not from the underlying business processes, operations, and analysis. The result is, e.g., ever-more complicated reinsurance treaties layered into mainframes and admin systems older than most of the actuaries interfacing with them.\nThe pace of strategic value-add isn’t slowing, though it must stretch further (in complexity and risk) to find comparable opportunities as the past. Having more agile, data-oriented operations enables companies to be able to react to and implement those opportunities. Technological value-add can improve a company’s bottom line through lower expenses and higher top-line growth, but often with a more favorable risk profile than some of the “strategic” opportunities.\nToday, there is a trend reverting back to technological value-creation and is evident across many traditional sectors. Tesla claims that it’s a technology company; Amazon is the #1 product retailer because of its vehement focus on internal information sharing3; Airlines are so dependent on their systems that the skies become quieter on the rare occasion that their computers give way.\nWhy is it, that companies that are so involved in things (cars, shopping) and physical services (flights) are so much more focused on improving their technological operations than insurance companies whose very focus is ‘information-based’? The market has rewarded those who have prioritized their internal technological solutions.\nCommoditized investing services and low yield environments have reduced insurance companies’ comparative advantage to “manage money”. Yield compression and the explosion of consumer-oriented investment services means a more competitive focus on the ability to manage the entire policy lifecycle efficiently (digitally), perform more real-time analysis of experience and risk management, and handle the growing product and regulatory complexity.\nThese are problems that have technological solutions and are waiting for insurance company adoption.\nCompanies that treat data like coordinates on a grid (spreadsheets) will get left behind. Two main hurdles have prevented technology companies from breaking into insurance:\n\nHigh regulatory barriers to entry, and\nDifficulty in selling complex insurance products without traditional distribution.\n\nOnce those two walls are breached, traditional insurance companies without a strong technology core will struggle to keep up. The key to thriving is not just adding “developers” to an organization; it’s going to be getting domain experts like actuaries to be an integral part of the technology transformation."
  },
  {
    "objectID": "why-program.html#whats-coding-got-to-do-with-this",
    "href": "why-program.html#whats-coding-got-to-do-with-this",
    "title": "2  Why Program?",
    "section": "2.3 What’s coding got to do with this?",
    "text": "2.3 What’s coding got to do with this?\nEverything. Programming is the optimal way to interact between the computer and actuary — and importantly between computer and computer. Programming is the actionable expression of ideas, math, analysis, and information. Think of programming as the 21st-century leap in the actuary’s toolkit, just as spreadsheets were in the preceding 40 years. Versus a spreadsheet-oriented workflow:\n\nMore natural automation of, and between processes\nBetter reproducibility\nScaling to fit any size dataset and workload\nStatistics and machine learning capabilities\nAdvanced visualizations to garner new views into your data\n\nThis list isn’t comprehensive and some benefits are subtle — when you are code-oriented instead of spreadsheet-oriented, you tend to want to structure your data in a portable and shareable way. For example, relying more on data warehouses instead of email attachments. This, in turn, enables data discovery and insights that otherwise wouldn’t be there. Investing in a code-oriented workflow is playing the long-game.\nThe actuary of the future needs to have coding as one of their core skills. Already today, the advances of business processes, insurance products, and financial ingenuity are written with lines of code — not spreadsheets. Not being able to code necessarily means that you are following what others are doing today.\nIt’s commonly accepted now that to gather insights from your data, you need to know how to code. Similar to your data, your business architecture, modeling needs, and product peculiarities are often better suited to customized solutions. Why stop at data science when learning how to solve problems with a computer?"
  },
  {
    "objectID": "why-program.html#the-10x-actuary",
    "href": "why-program.html#the-10x-actuary",
    "title": "2  Why Program?",
    "section": "2.4 The 10x Actuary",
    "text": "2.4 The 10x Actuary\nAs we swing back to a technological focus, we do not leave the finance-driven complexity behind. The increasingly complex business needs will highlight a large productivity difference between an actuary who can code and one who can’t — simply because the former can react, create, synthesize, and model faster than the latter. From the efficiency of transforming administration extracts, summarizing and aggregating valuation output, to analyzing claims data in ways that spreadsheets simply can’t handle, you can become a “10x Actuary”4.\nFlipping switches in a graphical user interface versus being able to build models is the difference between having a surface-level familiarity and having full command over the analysis and the concepts involved — with the flexibility to do what your software can’t.\nYour current software might be able to perform the first layer of analysis but be at a loss when you want to visualize, perform sensitivity analysis, statistics, stochastic analysis, or process automation. Things that, when done programmatically, are often just a few lines of additional code.\nDo I advocate dropping the license for your software vendor? No, not yet anyway. But the ability to supplement and break out of the modeling box has been an increasingly important part of most actuaries’ work.\nAdditionally, code-based solutions can leverage the entire-technology sector’s progress to solve problems that are hard otherwise: scalability, data workflows, integration across functional areas, version control and versioning, model change governance, reproducibility, and more.\n30-40 years ago, there were no vendor-supplied modeling solutions and so you had no choice but to build models internally. This shifted with the advent of vendor-supplied modeling solutions. Today, it’s never been better for companies to leverage open source to support their custom modeling, risk analysis/monitoring, and reporting workflows."
  },
  {
    "objectID": "why-program.html#risk-governance",
    "href": "why-program.html#risk-governance",
    "title": "2  Why Program?",
    "section": "2.5 Risk Governance",
    "text": "2.5 Risk Governance\nCode-based workflows are highly conducive to risk governance frameworks as well. If a modern software project has all of the following benefits, then why not a modern insurance product and associated processes?\n\nAccess control and approval processes\nVersion control, version management, and reproducibility\nContinuous testing and validation of results\nOpen and transparent design\nMinimization of manual overrides, intervention, and opportunity for user error\nAutomated trending analysis, system metrics, and summary statistics\nContinuously updated, integrated, and self-generating documentation\nIntegration with other business processes through a formal boundary (e.g. via an API)\nTools to manage collaboration in parallel and in sequence"
  },
  {
    "objectID": "why-program.html#managing-and-leading-the-transformation",
    "href": "why-program.html#managing-and-leading-the-transformation",
    "title": "2  Why Program?",
    "section": "2.6 Managing and Leading the Transformation",
    "text": "2.6 Managing and Leading the Transformation\nThe ability to understand the concepts, capabilities, challenges, and lingo is not a dichotomy, it’s a spectrum. Most actuaries, even at fairly high levels, are still often involved in analytical work. Still above that, it’s difficult to lead something that you don’t understand.\nConversely, the skill and practice of coding enhances managerial capabilities. When you are really skilled at pulling apart a problem or process into its constituent parts and designing optimal solutions; that’s a core attribute of leadership: having the vision of where the organization should be instead of thinking about where it is now.\nNor is the skillset described here limiting in any other aspect of career development any more than mathematical ability, project collaboration, or financial acumen — just to name a few."
  },
  {
    "objectID": "why-program.html#outlook",
    "href": "why-program.html#outlook",
    "title": "2  Why Program?",
    "section": "2.7 Outlook",
    "text": "2.7 Outlook\nIt will increasingly be essential for companies to modernize to remain competitive. That modernization isn’t built with big black-box software packages; it will be with domain experts who can translate the expertise into new forms of analysis - doing it faster and more robustly than the competition.\nSpaceX doesn’t just hire rocket scientists - they hire rocket scientists who code.\nBe an actuary who codes."
  },
  {
    "objectID": "why-program.html#footnotes",
    "href": "why-program.html#footnotes",
    "title": "2  Why Program?",
    "section": "",
    "text": "Co-evolution of Information Processing Technology and Use: Interaction Between the Life Insurance and Tabulating Industries↩︎\nFrom Tabulators to Early Computers in the U.S. Life Insurance Industry↩︎\nHave you had your Bezos moment? What you can learn from Amazon↩︎\nThe 10x [Rockstar] developer is NOT a myth↩︎"
  },
  {
    "objectID": "why-julia.html#expressiveness-and-syntax",
    "href": "why-julia.html#expressiveness-and-syntax",
    "title": "3  Why use Julia?",
    "section": "3.1 Expressiveness and Syntax",
    "text": "3.1 Expressiveness and Syntax\nExpressiveness is the manner in which and scope of ideas and concepts that can be represented in a programming language. Syntax refers to how the code looks on the screen and its readability.\nIn a language with high expressiveness and pleasant syntax, you:\n\nGo from idea in your head to final product faster.\nEncapsulate concepts naturally and write concise functions.\nCompose functions and data naturally.\nFocus on the end-goal instead of fighting the tools.\n\nExpressiveness can be hard to explain, but perhaps two short examples will illustrate.\n\n3.1.1 Example: Retention Analysis\nThis is a really simple example relating Cessions, Policys, and Lives to do simple retention analysis.\nFirst, let’s define our data:\n\n# Define our data structures\nstruct Life\n  policies\nend\n\nstruct Policy\n  face\n  cessions\n end\n\nstruct Cession\n  ceded\nend\nNow to calculate amounts retained. First, let’s say what retention means for a Policy:\n# define retention\nfunction retained(pol::Policy)\n  pol.face - sum(cession.ceded for cession in pol.cessions)\nend\nAnd then what retention means for a Life:\nfunction retained(l::Life)\n  sum(retained(policy) for policy in life.policies)\nend\nIt’s almost exactly how you’d specify it English. No joins, no boilerplate, no fiddling with complicated syntax. You can express ideas and concepts the way that you think of them, not, for example, as a series of dataframe joins or as row/column coordinates on a spreadsheet.\nWe defined retained and adapted it to mean related, but different things depending on the specific context. That is, we didn’t have to define retained_life(...) and retained_pol(...) because Julia can be dispatch based on what you give it. This is, as some would call it, unreasonably effective.\nLet’s use the above code in practice then.\nThe julia&gt; syntax indicates that we’ve moved into Julia’s interactive mode (REPL mode):\n# create two policies with two and one cessions respectively\njulia&gt; pol_1 = Policy( 1000, [ Cession(100), Cession(500)] )\njulia&gt; pol_2 = Policy( 2500, [ Cession(1000) ] )\n\n# create a life, which has the two policies\njulia&gt; life = Life([pol_1, pol_2])\njulia&gt; retained(pol_1)\n400\njulia&gt; retained(life)\n1900\nAnd for the last trick, something called “broadcasting”, which automatically vectorizes any function you write, no need to write loops or create if statements to handle a single vs repeated case:\njulia&gt; retained.(life.policies) # retained amount for each policy\n[400 ,  1500]\n\n\n3.1.2 Example: Random Sampling\nAs another motivating example showcasing multiple dispatch, here’s random sampling in Julia, R, and Python.\nWe generate 100:\n\nUniform random numbers\nStandard normal random numbers\nBernoulli random number\nRandom samples with a given set\n\n\n\n\nTable 3.1: A comparison of random outcome generation in Julia, R, and Python.\n\n\n\n\n\n\n\nJulia\nR\nPython\n\n\n\n\nusing Distributions\n\nrand(100)\nrand(Normal(), 100)\nrand(Bernoulli(0.5), 100)\nrand([\"Preferred\",\"Standard\"], 100)\nrunif(100)\nrnorm(100)\nrbern(100, 0.5)\nsample(c(\"Preferred\",\"Standard\"),\n100, replace=TRUE)\n\nimport scipy.stats as sps\nimport numpy as np\n\n\nsps.uniform.rvs(size=100)\nsps.norm.rvs(size=100)\nsps.bernoulli.rvs(p=0.5,size=100)\nnp.random.choice([\"Preferred\",\"Standard\"],\nsize=100)\n\n\n\n\n\nBy understanding the different types of things passed to rand(), it maintains the same syntax across a variety of different scenarios. We could define rand(Cession) and have it generate a random Cession like we used above."
  },
  {
    "objectID": "why-julia.html#the-speed",
    "href": "why-julia.html#the-speed",
    "title": "3  Why use Julia?",
    "section": "3.2 The Speed",
    "text": "3.2 The Speed\nAs the journal Nature said, “Come for the Syntax, Stay for the Speed”.\nRecall the Solvency II compliance which ran 1000x faster than the prior vendor solution mentioned earlier: what does it mean to be 1000x faster at something? It’s the difference between something taking 10 seconds instead of 3 hours — or 1 hour instead of 42 days.\nWhat analysis would you like to do if it took less time? A stochastic analysis of life-level claims? Machine learning with your experience data? Daily valuation instead of quarterly?\nSpeaking from experience, speed is not just great for production time improvements. During development, it’s really helpful too. When building something, I can see that I messed something up in a couple of seconds instead of 20 minutes. The build, test, fix, iteration cycle goes faster this way.\nAdmittedly, most workflows don’t see a 1000x speedup, but 10x to 1000x is a very common range of speed differences vs R or Python or MATLAB.\nSometimes you will see less of a speed difference; R and Python have already circumvented this and written much core code in low-level languages. This is an example of what’s called the “two-language” problem where the language productive to write in isn’t very fast. For example, more than half of R packages use C/C++/Fortran and core packages in Python like Pandas, PyTorch, NumPy, SciPy, etc. do this too.\nWithin the bounds of the optimized R/Python libraries, you can leverage this work. Extending it can be difficult: what if you have a custom retention management system running on millions of policies every night?\nJulia packages you are using are almost always written in pure Julia: you can see what’s going on, learn from them, or even contribute a package of your own!"
  },
  {
    "objectID": "why-julia.html#more-of-julias-benefits",
    "href": "why-julia.html#more-of-julias-benefits",
    "title": "3  Why use Julia?",
    "section": "3.3 More of Julia’s benefits",
    "text": "3.3 More of Julia’s benefits\nJulia is easy to write, learn, and be productive in:\n\nIt’s free and open-source\n\nVery permissive licenses, facilitating the use in commercial environments (same with most packages)\n\nLarge and growing set of available packages\nWrite how you like because it’s multi-paradigm: vectorizable (R), object-oriented (Python), functional (Lisp), or detail-oriented (C)\nBuilt-in package manager, documentation, and testing-library\nJupyter Notebook support (it’s in the name! Julia-Python-R)\nMany small, nice things that add up:\n\nUnicode characters like α or β\nNice display of arrays\nSimple anonymous function syntax\nWide range of text editor support\nFirst-class support for missing values across the entire language\nLiterate programming support (like R-Markdown)\n\nBuilt-in Dates package that makes working with dates pleasant\nAbility to directly call and use R and Python code/packages with the PyCall and RCall packages\nError messages are helpful and tell you what line the error came from, not just the type of error\nDebugger functionality so you can step through your code line by line\n\nFor power-users, advanced features are easily accessible: parallel programming, broadcasting, types, interfaces, metaprogramming, and more.\nThese are some of the things that make Julia one of the world’s most loved languages on the StackOverflow Developer Survey.\nFor those who are enterprise-minded: in addition to the liberal licensing mentioned above, there are professional products from organizations like Julia Computing that provide hands-on support, training, IT governance solutions, behind-the-firewall package management, and deployment/scaling assistance."
  },
  {
    "objectID": "why-julia.html#the-tradeoff",
    "href": "why-julia.html#the-tradeoff",
    "title": "3  Why use Julia?",
    "section": "3.4 The Tradeoff",
    "text": "3.4 The Tradeoff\nJulia is fast because it’s compiled, unlike R and Python where (loosely speaking) the computer just reads one line at a time. Julia compiles code “just-in-time”: right before you use a function for the first time, it will take a moment to pre-process the code section for the machine. Subsequent calls don’t need to be re-compiled and are very fast.\nA hypothetical example: running 10,000 stochastic projections where Julia needs to precompile but then runs each 10x faster:\n\nJulia runs in 2 minutes: the first projection takes 1 second to compile and run, but each 9,999 remaining projections only take 10ms.\nPython runs in 17 minutes: 100ms of a second for each computation.\n\nTypically, the compilation is very fast (milliseconds), but in the most complicated cases it can be several seconds. One of these is the “time-to-first-plot” issue because it’s the most common one users encounter: super-flexible plotting libraries have a lot of things to pre-compile. So in the case of plotting, it can take several seconds to display the first plot after starting Julia, but then it’s remarkably quick and easy to create an animation of your model results. The time-to-first plot is a solvable problem that’s receiving a lot of attention from the core developers and will get better with future Julia releases.\nFor users working with a lot of data or complex calculations (like actuaries!), the runtime speedup is worth a few seconds at the start."
  },
  {
    "objectID": "why-julia.html#package-ecosystem",
    "href": "why-julia.html#package-ecosystem",
    "title": "3  Why use Julia?",
    "section": "3.5 Package Ecosystem",
    "text": "3.5 Package Ecosystem\nUsing packages as dependencies in your project is assisted by Julia’ bundled package manager.\nFor each project, you can track the exact set of dependencies and replicate the code/process on another machine or another time. In R or Python, dependency management is notoriously difficult and it’s one of the things that the Julia creators wanted to fix from the start.\nPackages can be one of the thousands of publicly available, or private packages hosted internally behind a firewall.\nAnother powerful aspect of the package ecosystem is that due to the language design, packages can be combined/extended in ways that are difficult for other common languages. This means that Julia packages often interop without any additional coordination.\nFor example, packages that operate on data tables work without issue together in Julia. In R/Python, many features tend to come bundled in a giant singular package like Python’s Pandas which has Input/Output, Date manipulation, plotting, resampling, and more. There’s a new Consortium for Python Data API Standards which seeks to harmonize the different packages in Python to make them more consistent (R’s Tidyverse plays a similar role in coordinating their subset of the package ecosystem).\nIn Julia, packages tend to be more plug-and-play. For example, every time you want to load a CSV you might not want to transform the data into a dataframe (maybe you want a matrix or a plot instead). To load data into a dataframe, in Julia the practice is to use both the CSV and DataFrames packages, which help separate concerns. Some users may prefer the Python/R approach of less modular but more all-inclusive packages."
  },
  {
    "objectID": "why-julia.html#conclusion",
    "href": "why-julia.html#conclusion",
    "title": "3  Why use Julia?",
    "section": "3.6 Conclusion",
    "text": "3.6 Conclusion\nLooking at other great tools like R and Python, it can be difficult to summarize a single reason to motivate a switch to Julia, but hopefully this article piqued an interest to try it for your next project.\nThat said, Julia shouldn’t be the only tool in your tool-kit. SQL will remain an important way to interact with databases. R and Python aren’t going anywhere in the short term and will always offer a different perspective on things!\nIn an earlier article, I talked about becoming a 10x Actuary which meant being proficient in the language of computers so that you could build and implement great things. In a large way, the choice of tools and paradigms shape your focus. Productivity is one aspect, expressiveness is another, speed one more. There are many reasons to think about what tools you use and trying out different ones is probably the best way to find what works best for you.\nIt is said that you cannot fully conceptualize something unless your language has a word for it. Similar to spoken language, you may find that breaking out of spreadsheet coordinates (and even a dataframe-centric view of the world) reveals different questions to ask and enables innovated ways to solve problems. In this way, you reward your intellect while building more meaningful and relevant models and analysis."
  },
  {
    "objectID": "why-julia.html#footnotes",
    "href": "why-julia.html#footnotes",
    "title": "3  Why use Julia?",
    "section": "",
    "text": "Python first appeared in 1990. R is an implementation of S, which was created in 1976, though depending on when you want to place the start of an independent R project varies (1993, 1995, and 2000 are alternate dates). The history of these languages is long and substantial changes have occurred since these dates.↩︎\nAviva Case Study↩︎"
  },
  {
    "objectID": "elements-of-financial-modeling.html#in-this-chapter",
    "href": "elements-of-financial-modeling.html#in-this-chapter",
    "title": "4  Elements of Financial Modeling",
    "section": "4.1 In this Chapter",
    "text": "4.1 In this Chapter\nWe explain what constitutes a financial model and what are common uses of a model. We explain what makes an adept practitioner."
  },
  {
    "objectID": "elements-of-financial-modeling.html#what-is-a-model",
    "href": "elements-of-financial-modeling.html#what-is-a-model",
    "title": "4  Elements of Financial Modeling",
    "section": "4.2 What is a model?",
    "text": "4.2 What is a model?\nA model represents aspects of the world around us distilled down into simpler, more tractable components. It is impossible to fully capture the everything that may affect the objects of our interest.\nFor example, say we want to simulate the returns for the stocks in our retirement portfolio. It would be impossible to try to build a model which would capture all of the individual people working jobs and making decisions, weather events that damage property, political machinations, etc. Instead, we try to capture certain fundamental characteristics. For example, it is common to model equity returns as cumulative pluses and minuses from random movements where those movements have certain theoretical or historical characteristics.\nWhether we are using this model of equity returns to estimate available retirement income or replicate an exotic option price, a key aspect of the model is the assumptions used therein. For the retirement income scenario we might assume a healthy eight percent return on stocks and conclude that such a return will be sufficient to retire at age 53. Alternatively, we may assume that future returns will follow a stochastic path with a certain distribution of volatility and drift. These two assumption sets will produce output - results from our model that must be inpsected, questioned, and understood in the context of the “small world” of the model’s mechanistic workings. Lastly, to be effective practitioners we must be able to contextualize the “small world” results withing the “large world” that exists around us.\nMore on the “small world” vs “large world”: say that our model is one that discounts a fixed set of future cashflows using the US Treasury rate curve. If I run my model using current rates today, and then re-run my model tomorrow with the same future cashlows and the present value of those cashflows has increased by 5% I may ask why the result has changed so much in such a short period of time! In the “small”, mechanistic world of the model I may be able to see that the rates I used to discount the cashflows with have fallen substantially. The “small world” answer is that the inputs have changed which produced a mechanical change in the output. The “big world” answer may be that the Federal Reserve lowered the Federal Funds Rate to prevent the economy from entering a deflationary recession. Of course, we can’t completely explain the relation between our model and the real world (otherwise we could capture that relationship in our model!). An effective practitioner will always try to look up from the immediate work and take stock of how the world at large is or is not relfected in the model."
  },
  {
    "objectID": "elements-of-financial-modeling.html#what-is-a-financial-model",
    "href": "elements-of-financial-modeling.html#what-is-a-financial-model",
    "title": "4  Elements of Financial Modeling",
    "section": "4.3 What is a Financial Model?",
    "text": "4.3 What is a Financial Model?\nFinancial models are those used extensively to ascertain better understanding of complex contracts, perform scenario analysis, and inform market participants’ decisions related to perceived value (and therefore price). It can’t be quantified directly, but it is likely not an exaggeration that many billions of dollars is transacted each day as a result of decisions made from the output of financial models.\nMost financial models can be characterized with a focus on the first or both of:\n\nAttempting to project pattern of cashflows or obligations at future timepoints\nReducing the projected obligations into a current value\n\nExamples of this:\n\nProjecting a retiree’s savings through time (1), and determining how much the should be saving today for their retirement goal (2)\nProjecting the obligation of an exotic option across different potential paths (1), and determining the premium for that option (2)\n\nModels are sometimes taken a step further, such as transforming the underlying economic view into an accounting or regulatory view (such as representing associated debits and credits, capital requirements, or associated intangible, capitalized balances).\nWe should also distinguish a financial model from a purely statistical model, where the often the inputs and output data are known and the intention is to estimate relationships between variables (example: linear regressions). That said, a financial model may have statistical components and many aspects of modeling is shared between the two kinds."
  },
  {
    "objectID": "elements-of-financial-modeling.html#the-process-of-building-a-financial-model",
    "href": "elements-of-financial-modeling.html#the-process-of-building-a-financial-model",
    "title": "4  Elements of Financial Modeling",
    "section": "4.4 The Process of Building a Financial Model",
    "text": "4.4 The Process of Building a Financial Model\n\n\n\n\n\n\nTODO: Describe model building process and make associated diagram\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n    Z[Other Inputs] --&gt; D\n    A[Market Data Inputs] --&gt; D\n    D[Model Assumptions] --&gt; B[Model]\n    B --&gt; C[Model Output]\n    C --&gt;|Relates output back to| A"
  },
  {
    "objectID": "elements-of-financial-modeling.html#predictive-versus-explanatory-models",
    "href": "elements-of-financial-modeling.html#predictive-versus-explanatory-models",
    "title": "4  Elements of Financial Modeling",
    "section": "4.5 Predictive versus Explanatory Models",
    "text": "4.5 Predictive versus Explanatory Models\nGiven a set of inputs, our model will generate an output and we are generally interested in its accuracy. The model need not have a realistic mechanism for how the world works. That is, we may primarily be interested in accurately calculating an output value without the model having any scientific, explanatory power of how different parts of the real-world system interact.\n\n4.5.1 A Historical Example\nConsider the classic underdog story where Copernicus overthrew the status quo when he proposed (correctly) that the earth orbited the sun instead of the other way around1.\nThe existing Ptolemic model used a geocentric view of the solar system in which the planets and sun orbited the Earth in perfect circles with an epicycle used to explain retrograde motion (as see in Figure 4.1). Retrograde motion is the term used to describe the apparent, temporarily reversed motion of a planet as viewed from Earth when the Earth is overtaking the other planet in orbit around the sun. This was accurate enough to match the obersvational data that described the position of the planets in the sky.\n\n\n\nFigure 4.1: In the Ptolemic solar model, the retrograde motion of the planets was explained by adding an epicycle to the circular orbit around the earth.\n\n\nFamously, Copernicus came along and said that the sun, not the Earth, should be at the center (a heliocentric model). Earth revolves around the sun! Today, we know this to be a much better description of reality than one in which the Earth arrogantly sits at the center of the universe. However the model was actually slightly less accurate in predicting the apparent position of the planets (to the limits of observational precision at the time)! Why would this be?\nFirst, the Copernican proposal still used perfectly circular orbits with an epicycle adjustment, which we know today to be inaccurate (in favor of an elliptical orbit consistent with the theory of gravity). Despite being more scientifically correct, it was still not the complete picture.\nSecond, the geocentric model was already very accurate because it was essentially a Taylor-series approximation which described to sufficient observational accuracy the apparent position of the planet relative to the Earth. The heliocentric model was effectively a re-parameterization of the orbital approximation.\nThird, we have considered a limited criteria for which we are evaluating the model for accuracy, namely apparent position of the planets. It’s not until we contemplate other observational data that the Copernican model would demonstrate greater modeling accuracy: apparent brightness of the planets as they undergo retrograde motion and angular relationship of the planets to the sun.\nFor modelers today, this demonstrates a few things to keep in mind:\n\nPredictive models need not have a scientific, causal structure to make accurate predictions.\nIt is difficult to capture the complete scientific inter-relationships of a system and much care and thought needs to be given in what aspects are included in our model.\nWe should look at, or seek out, additional data that is related to our model because we may accurately fit (or overfit) to one outcome while achieving an increasingly poor fit to other related variables.\n\nStriving to better understand the world is a good thing to do but trying to include more components into the model is not always going to help achieve our goals.\n\n\n4.5.2 Examples in the Financial Context\n\n4.5.2.1 Home Prices\nAmerican home prices which have a strong degree of seasonality and have the strongest prices around April of each year. We may find that including a simple oscillating term in our model captures the variability in prices better than if we tried to imperfectly capture the true market dynamics of home sales: supply and demand curves varying by personal (job bonus payment timing, school calendars), local (new homes built, company relocation), and national (monetary policy, tax incentives for home-ownership). In other words, one could likely predict a stable pattern like this with a model that contains a simple sinusoidal periodic component. One could likely spend months trying to build a more scientific model and not achieve as good of fit, even though the latter tries to be more conceptually accurate.\n\n\n\n\nFigure 4.2: House prices peak around April of each year.\n\n\n\n\n\n4.5.2.2 Replicating Portfolio\nAnother example in the financial modeling realm: in attempting to value a portfolio of insurance contracts a replicating portfolio of hypothetical assets will sometimes be constructed2. The point of this is to create a basket of assets that can be more quickly (minutes to hours) valued in response to changing market conditions than it would take to run the actuarial model (hours to days). This is an example where the basket of assets has no ability to explain why the projected cashflows are what they are - but retains strong predictive accuracy."
  },
  {
    "objectID": "elements-of-financial-modeling.html#what-makes-a-good-model",
    "href": "elements-of-financial-modeling.html#what-makes-a-good-model",
    "title": "4  Elements of Financial Modeling",
    "section": "4.6 What makes a good model?",
    "text": "4.6 What makes a good model?\nThe answer is: it depends.\n\n4.6.1 Achieving original purpose\nA model is built for a specific set of reasons and therefore we must evaluate a model in terms of achieving that goal. We should not critique a model if we want to use it outside of what it was inteded to do. This includes: contents of output and required level of accuracy.\nA model may have been created to for scenario analysis to value all assets in a portfolio to within half a percent of a more accurate, but much more computationally expensive model. If we try to add a never-before-seen asset class or use the model to order trades we may be extending the design scope of the original model.\n\n\n4.6.2 Usability\nHow easy is it for someone to use? Does it require pages and pages of documentation, weeks of specialized training and an on-call help desk? All else equal, it is an indicator of how usable the model is by the amount of support and training. However, one may sometimes wish to create a highly capable, complex model which is known to require a high amount of experience and expertise. An analogy here might be the cockpit of a small Cessna aircraft versus a fighter jet: the former is a lot simpler and takes less training to master but is also more limited.\nFigure 4.3 illustrates this concept and shows that if your goal is very high capability that you may need to expect to develop training materials and support the more complex model. On this view, a better model is one that is able to have a shorter amount of time and experience to acheive the same level of capability.\n\n\n\n\n\nFigure 4.3: Tradeoff between complexity and capability\n\n\n\n\n\n\n4.6.3 Performance\nFinancial models are generally not used for their awe-inspiring beauty - users are results oriented and the faster a model returns the requested results, the better. Aside from direct computational costs such as server runtime, a shorter model runtime means that one can iterate faster, test new ideas on the fly, and stay focused on the problem at hand.\nMany readers may be familiar with the cadence of (1) try running model overnight, (2) see results failed in the morning, (3) spend day developing, (4) repeat step 1. It is preferred if this cycle can be measured in minutes instead of hours or days.\nOf course, requirements must be considered here too: needs for high frequency trading, daily portfolio rebalancing, and quarterly valuations are different when it comes to performance.\n\n\n4.6.4 Separation of Model Logic and Data\nWhen business logic is embedded within data, or data inputs are spread out across multiple locations it’s tough to keep track of things. Using a spreadsheet as an example, often times it’s incredibly difficult to ascertain a model’s operation if inputs are spread out across locations on many tabs. Or if related calculations are performed in multiple locations, or if it’s not clear where the line is drawn between calculations performed in the worksheets or in macros."
  },
  {
    "objectID": "elements-of-financial-modeling.html#what-makes-a-good-modeler",
    "href": "elements-of-financial-modeling.html#what-makes-a-good-modeler",
    "title": "4  Elements of Financial Modeling",
    "section": "4.7 What makes a good modeler?",
    "text": "4.7 What makes a good modeler?\nA model is nothing without it’s operator, and a skilled practitioner is worth their weight in gold. What elements separate a good modeler from a mediocre modeler?\n\n4.7.1 Domain Expertise\nAn expert who knows enough about all of the domains that are applicable is crucial. Imagine if someone said let’s emulate an architect by having a construction worker and an artist work together. It’s all too common for business to attempt to pair a business expert with an information technologist in the same way.\nUnfortunately, this means that there’s generally no easy way out of learning enough about finance, actuarial science, computers, and/or programming in order to be an effective modeler.\nAlso, a word of warning for the financial analysts out there: the computer scientists may find it easier to learn applied financial modeling than the other way around since the tools, techniques, and language of problem solving is already more a more general and flexible skill-set. There’s more technologists starting banks than there are financiers starting technology companies.\n\n\n4.7.2 Model Theory\nIf it is granted that financial modeling must involve, as the essential part, a building up of modeler’s knowledge, the next issue is to characterize that knowledge more explicitly. The modeler’s knowledge should be regarded as a theory, in the sense of Ryle’s3 “Concept of the Mind.” Very briefly: a person who has or possesses a theory in this sense knows how to do certain things and in addition can support the actual doing with explanations, justifications, and answers to queries, about the model and it’s results4.\nA financial model is rarely left in a final state. Regulatory changes, additional mechanics, sensitivity testing, market dynamics, new products, and new systems to interact with force a model to undergo change and development through its entire life. And like a living thing, it must have nurturing caregivers. This metaphor sounds extended, but Naur’s point is that unless the model also lives in the heads of it’s developers then it cannot successfully be maintained through time:\n\nThe conclusion seems inescapable that at least with certain kinds of large programs, the continued adaption, modification, and correction of errors in them, is essentially dependent on a certain kind of knowledge possessed by a group of programmers who are closely and continuously connected with them.\n\nAssume that we need to adapt the model to fit a new product. One possessing a high degree of model theory includes:\n\nthe ability to describe the trade-offs between alternate approaches that would accomplish the desired change\nrelate the proposed change to the design of the current system and any challenges that will arise as a result of prior design decisions\nprovide a quantitative estimation for the impact the change will have: runtime, risk metrics, valuation changes, etc.\nAnalogize how the system works to themselves or to others\nDescribe key limitations that the model has and where it is most divorced from the reality it seeks to represent.\n\nAbstractions and analogies of the system are a critical aspect of model theory, as the human mind cannot retain perfectly precise detail about how the system works in each sub-component. The ability to, at some times, collapse and compartmentalize parts of the model to limit the mental overload while at others recall important implementation details requires training - and is enhanced by learning concepts like those which will be covered in this book.\nAn example of how the right abstractions (and language describing those abstractions) can be helpful in simplifying the mental load:\nInstead of:\nThe valuation process starts by reading an extract into three tabs of the spreadsheet. A macro loops through the list of policies on the first tab and in column C it gives the name of the applicable statutory valuation ruleset. The ruleset is defined as the combination of (1) the logic in the macro in the “Valuation” VBA module with, (2) the underlying rate tables from the tabs named XXX to ZZZ, along with (3) the additional policy level detail on the second tab. The valuation projection is then run with the current policy values taken from the third tab of the spreadsheet and the resulting reserve (equal to the actuarial present value of claims) is saved and recorded in column J of the first tab. Finally, a pivot table is used to sum up the reserves by different groups.\nWe could instead design the process so that the following could be said instead:\nPolicy extracts are parsed into a Policy datatype which contains a subtype ValuationKind indicating the applicable statutory ruleset to apply. From there, we map the valuation function over the set of Policys and perform an additive reduce to determine the total reserve.\nThere are terminologies and concepts in the second example which we will develop over the course of this section of the book - we don’t want to dwell on the details bright now. However, we do want to emphasize that the process itself being able to condensed down to descriptions that are much more meaningful to the understanding of the model is a key differentiator for a code-based model instead of spreadsheets. It is not exaggerating that we could develop a handful of compartmentalized logics such that our primary valuation process described above could look like this in real code:\npolicies = parse(Policy,CSV.File(\"extract.csv\")) \nreserve = mapreduce(+,value,policies)\nWe’ve abstracted the mechanistic workings of the model into concise and meaningful symbols that not only perform the desired calculations but also make it obvious to an informed but unfamiliar reader what it’s doing.\nparse , mapreduce, + , value , Policy are all imbued with meaning - the first three would be understood by any computer scientist by the nature of their training. The latter two are unique to our model and have “real world” meaning that our domain expert modeler would understand which analogizes very directly to the way we would suggest implementing the details of value or Policy. The benefit of this, again, is to provide tools and concepts which let us more easily develop model theory.\n\n\n4.7.3 Curiosity\n\n\n4.7.4 Rigor\n\n\n4.7.5 Toolset\n… The skills in this book!"
  },
  {
    "objectID": "elements-of-financial-modeling.html#footnotes",
    "href": "elements-of-financial-modeling.html#footnotes",
    "title": "4  Elements of Financial Modeling",
    "section": "",
    "text": "Prof. Richard Fitzpatrick has excellent coverage of the associated mathematics and implications in “A Modern Almagest”: https://farside.ph.utexas.edu/books/Syntaxis/Almagest/Almagest.html↩︎\nSee, e.g., SOA Investment Symposium March 2010. Replicating Portfolios in the Insurance Industry (Curt Burmeister Mike Dorsel Patricia Matson)↩︎\nRyle, G. The Concept of Mind. Harmondsworth, England, Penguin, 1963, first published 1949. Applying “Theory Building”↩︎\nThe idea of “model theory” is adapted from Peter Naur’s 1985 essay, “Programming as Theory Building”. Indeed, this whole paragraph is only a slightly modified version of Naur’s description of theory in the programming context.↩︎"
  },
  {
    "objectID": "foundations-of-programming.html#in-this-section",
    "href": "foundations-of-programming.html#in-this-section",
    "title": "5  Elements of Programming",
    "section": "5.1 In this section",
    "text": "5.1 In this section\nStart building up computer science concepts by introducing tangible programming essentials. Data types, variables, control flow, functions, and scope are introduced."
  },
  {
    "objectID": "foundations-of-programming.html#computer-science-programming-and-coding",
    "href": "foundations-of-programming.html#computer-science-programming-and-coding",
    "title": "5  Elements of Programming",
    "section": "5.2 Computer Science, Programming, and Coding",
    "text": "5.2 Computer Science, Programming, and Coding\nComputer Science is the study of computing and information. As a science, it is distinct from programming languages which are merely coarse implementations of specific computer science concepts1. Programming (or “coding”) is the art and science of writing code in programming languages to have the computer perform desired tasks. While this may sound mechanistic, programming truly is one of the highest forms of abstract thinking and the design space of potential solutions is so large and potentially complex that much art and experience is needed to create a well-made program.\nThe language of computer science also provides a lexicon so that financial practitioners can discuss model architecture and problem characteristics. Having the language to describe a concept will also help see aspects of the problem in new ways, opening one up to more innovative solutions.\nIn the context of this financial modeling that we do, we can consider a financial model to be a type of computer program. It takes as input abstract information (data), performs calculations (an algorithm), and returns new data as an output. In this context, we generally do not need to consider many things that a software engineer may contemplate such as a graphical user interface, networking, or access restrictions. But there are many similarities: a good financial modeler must understand data types, algorithms, and some hardware details.\nWe will build up the concepts over this and the following chapter:\n\nThis chapter will provide a survey of important concepts in computer science that will prove useful for our financial modeling. First, we will talk about data types, boolean logic, and basic expressions. We’ll build on those to discuss algorithms (functions) which perform useful work and use control flow and recursion.\nThe following chapter will step back and discuss higher level concepts: the “schools of thought” around organizing the relationship between data and functions (functional versus object-oriented programming), design patterns, computational complexity, and compilation.\n\n\n\n\n\n\n\nTip\n\n\n\nThere will be brief references to hardware considerations for completeness, but hardware knowledge is not necessary to understand most programming languages (including Julia). It’s impossible to completely avoid talking about hardware when you care about the performance of your code, so feel free to gloss over the reference to hardware details on the first read and come back later after Chapter 8.\n\n\nIt’s highly recommended that you follow along and have a Julia session open (e.g. a REPL or a notebook) when first going through this chapter. See Chapter 23 if you haven’t gotten that set up yet. Follow along with the examples as we go.\n\n\n\n\n\n\nTip\n\n\n\nYou can get some help in the REPL by typing a ? followed by the symbol you want help with, for example:\n help?&gt; sum\nsearch: sum sum! summary cumsum cumsum! ...\n\n  sum(f, itr; [init])\n\n\n  Sum the results of calling function f on each element of itr.\n\n... More text truncated...\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThis introductory chapter is intended to provide a survey of the important concepts and building blocks, not to be a complete reference. For full details on available functions, more complete defintions, and a more complete tour of all language features, see the Manual at docs.julialang.org."
  },
  {
    "objectID": "foundations-of-programming.html#assignment-and-variables",
    "href": "foundations-of-programming.html#assignment-and-variables",
    "title": "5  Elements of Programming",
    "section": "5.3 Assignment and Variables",
    "text": "5.3 Assignment and Variables\nOne of the first things it will be convenient to understand is the concept of variables. In virtually every programming language, we can assign values to make our program more organized and meaningful to the human reader. In the following example, we assign values to intermediate symbols to benefit us humans as we convert (silly!) American distance units:\n\nfeet_per_yard = 3\nyards_per_mile = 1760\n\nfeet = 3000\nmiles = feet / feet_per_yard / yards_per_mile\n\n0.5681818181818182\n\n\nBeyond readability, variables are a form of abstraction which allows us to think beyond specific instances of data and numbers to a more general representation. For example, the last line in the prior code example is a very generic computation of a unit conversion relationship and feet could be any number and the expression remains a valid calculation.\nWe will return to this subject in more detail in (ref-assignment?)."
  },
  {
    "objectID": "foundations-of-programming.html#data-types",
    "href": "foundations-of-programming.html#data-types",
    "title": "5  Elements of Programming",
    "section": "5.4 Data Types",
    "text": "5.4 Data Types\nData types are a way of categorizing information by intrinsic characteristics. We instinctively know that 13.24 is different than \"this set of words\" and types are how we will formalize this distinction. This is a key conceptual point, and mathematically it’s like we have different sets of objects to perform specialized operations on. Beyond this set-like abstraction is implementation details related to computer hardware. You probably know that computers only natively “speak” in binary zeros and ones. Data types are a primary way that a computer can understand if it should interpret 01000010 as B or as 662.\nEach 0 or 1 within a computer is called a bit and eight bits in a row form a byte (such as 01000010). This is where we get terms like “gigabytes” or “kilobits per second” as a measure of the quantity or rate of bits something can handle3.\n\n5.4.1 Numbers\nNumbers are usually grouped into two categories: integers and floating-point4 numbers. Integers are like the mathematical set of integers while floating-point is a way of representing decimal numbers. Both have some limitations since computers can only natively represent a finite set of numbers due to the hardware (more on this in Chapter 8). Here are three integers that are input into the REPL (Read-Eval-Print-Loop)5 and the result is printed below the input:\n\n2\n\n2\n\n\n\n423\n\n423\n\n\n\n1929234\n\n1929234\n\n\nAnd three floating-point numbers:\n\n0.2\n\n0.2\n\n\n\n-23.3421\n\n-23.3421\n\n\n\n14e3      # the same as 14,000.0\n\n14000.0\n\n\nOn most systems, 0.2 will be interpreted as a 64-bit floating point type called Float64 in Julia since most architectures these days are 64-bit6, while on a 32-bit system 0.2 would be interpreted as a Float32. Given that there are a finite amount of bits attempting to represent a continuous, infinite set of numbers means that some numbers are not able to be represented with perfect precision. For example, if we ask for 0.2, the closest representations in 64 and 32 bit are:\n\n0.20000000298023223876953125 in 32-bit\n0.200000000000000011102230246251565404236316680908203125 in 64-bit\n\nThis leads to special considerations that computers take when performing calculations on floating point maths, some of which will be covered in more detail in Chapter 8. For now, just note that floating point numbers have limited precision and even if we input 0.2, your computations will use the above decimal representations even if it will print out a number with fewer digits shown:\n\n1x = 0.2\n\n2big(x)\n\n\n1\n\nHere, we assign the value 0.2 to a variable x. More on variables/assignments in Section 5.5.3.\n\n2\n\nbig(x) is a arbitrary precision floating point number and by default prints the full precision that was embedded in our variable x, which was originally Float64.\n\n\n\n\n0.200000000000000011102230246251565404236316680908203125\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote the difference in what printed between the last example and when we input 0.2 earlier in the chapter. The former had the same (not-exactly equal to \\(0.2\\)) value, but it printed an abbreviated set of digits as a nicety for the user, who usally doesn’t want to look at floating point numbers with their full machine precision. The system has the full precision (0.20...3125) but is truncating the ouput.\nIn the last example, we’ve converted the normal Float64 to a BigFloat which will not truncate the output when printing.\n\n\nIntegers are similarly represented as 32 or 64 bits (with Int32 and Int64) and are limited to exact precision:\n\n-32,767 to 32,767 for Int32\n-2,147,483,647 to 2,147,483,647 for Int64\n\nAdditional range in the positive direction if one chooses to use “unsigned”, non-negative numbers (UInt32 and UInt64). Unlike floating point numbers, the integers have a type Int which will use the system bit architecture by default (that is, Int(30) will create a 64 bit integer on 64-bit systems and 32-bit on 32-bit systems).\n\n\n\n\n\n\nFinancial Modeling Pro-tip\n\n\n\nExcel’s numeric storage and routine is complex and not quite the same as most programming languages, which follow the Institute of Electrical and Electronics Engineer’s standards (such as the IEEE 754 standard for double precision floating point numbers). Excel uses IEEE for the computations but results (and therefore the cells that comprise many calculations interim values) are stored with 15 significant digits of information. In some ways this is the worst of both worlds: having the sometimes unusual (but well-defined) behavior of floating point arithmetic and having additional modifications to various steps of a calculation. In general, you can assume that the programming language result (following the IEEE 754 standard) is a better result because there are aspects to the IEEE 754 defines techniques to minimize issues that arise in floating point math. Some of the issues (round-off or truncation) can be amplified instead of minimized with Excel.\nIn practice, this means that it can be difficult to exactly replicate a calculation in Excel in a progamming language and vice-versa. It’s best to try to validate a programming model versus Excel model using very small unit calculations (e.g. a single step or iteration of a routine) instead of an all-in result. You may need to define some tolerance threshold for comparison of a value that is the result of a long chain of calculation.\n\n\n\n\n5.4.2 Type Hierarchy\nWe can describe a hierarchy of types. Both Float64 and Int64 are examples of Real numbers (here, Real is an abstract Julia type which corresponds to the mathematical set of real numbers commonly denoted with \\(\\mathbb{R}\\) ). Both Float64 and Int32 are Real numbers, so why not just define all numbers as a Real type? Because for performant calculations, the computer must know in advance how many bits each number is represented with.\nFigure 5.1 shows the type hiearchy for most built-in Julia number types.\n\n\n\n\n\ngraph TD\n    Number --&gt; Real\n    Number --&gt; Complex\n\n    Real --&gt; Integer\n    Real --&gt; AbstractFloat\n    Real --&gt; Rational\n    Real --&gt; Irrational\n\n    Integer --&gt; Signed\n    Integer --&gt; Unsigned\n\n    Signed --&gt; Int8\n    Signed --&gt; Int16\n    Signed --&gt; Int32\n    Signed --&gt; Int64\n    Signed --&gt; Int128\n    Signed --&gt; BigInt\n\n    Unsigned --&gt; UInt8\n    Unsigned --&gt; UInt16\n    Unsigned --&gt; UInt32\n    Unsigned --&gt; UInt64\n    Unsigned --&gt; UInt128\n\n    AbstractFloat --&gt; Float16\n    AbstractFloat --&gt; Float32\n    AbstractFloat --&gt; Float64\n    AbstractFloat --&gt; BigFloat\n\n\nFigure 5.1: Numeric Type Hierarchy in Julia. Leafs of the tree are concrete types.\n\n\n\n\nThe integer and floating point types described in the prior section are known as concrete types because there are no possible sub types (child types). Further, a concrete type can be a bit type if the data type will always have the same number of bits in memory: a Float32 will always be 32 bits in memory, for example. Contrast this with strings (described below) which can contain an arbitrary number of characters.\n\n\n5.4.3 Arrays\n\n\n Julia has very powerful and friendly array types.\nArrays are the most common way to represent a collection of similar data. For example, we can represent a set of integers as follows:\n\n[1, 10, 300]\n\n3-element Vector{Int64}:\n   1\n  10\n 300\n\n\nAnd a floating point array:\n\n[0.2, 1.3, 300.0]\n\n3-element Vector{Float64}:\n   0.2\n   1.3\n 300.0\n\n\nNote the above two arrays are different types of arrays. The first is Vector{Int64} and the second is Vector{Float64}. These are arrays of concrete types and so Julia will know that each element of an array is the same amount of bits which will enable more efficient computations. With the following set of mixed numbers, Julia will promote the integers to floating point since the integers can be accurately represented7 in floating point.\n\n[1, 1.3, 300.0, 21]\n\n4-element Vector{Float64}:\n   1.0\n   1.3\n 300.0\n  21.0\n\n\nHowever, if we explicitly ask Julia to use a Real-typed array, the type is now Vector{Real}. Recall that Real is an abstract type. Having heterogeneous types within the array is conceptually fine, but in practice limits performance. Again, this will be covered in more detail in Chapter 8.\nIn Julia, arrays can be multi-dimensional. Here are are two three-dimensional arrays with length three in each dimension:\n\nrand(3, 3, 3)\n\n3×3×3 Array{Float64, 3}:\n[:, :, 1] =\n 0.629573  0.0990195  0.588411\n 0.845872  0.481827   0.997151\n 0.840776  0.406063   0.215226\n\n[:, :, 2] =\n 0.69587   0.527636   0.785346\n 0.903311  0.0612682  0.102936\n 0.32704   0.870227   0.626142\n\n[:, :, 3] =\n 0.814583  0.0610576  0.48594\n 0.244347  0.0512894  0.198395\n 0.291835  0.86858    0.870544\n\n\n\n[x + y + z for x in 1:3, y in 11:13, z in 21:23]\n\n3×3×3 Array{Int64, 3}:\n[:, :, 1] =\n 33  34  35\n 34  35  36\n 35  36  37\n\n[:, :, 2] =\n 34  35  36\n 35  36  37\n 36  37  38\n\n[:, :, 3] =\n 35  36  37\n 36  37  38\n 37  38  39\n\n\nThe above example demonstrates array comprehension syntax which is a convienient way to create arrays in Julia.\nA two-dimensional array has the rows by semi-colons (;):\n\nx = [1 2 3; 4 5 6]\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn Julia, a Vector{Float64} is simply a one-dimensional array of floating pointsand a Matrix{Float64} is a two-dimentional array. More precisely, they are type aliases of the more generic Array{Float64,1} and Array{Float64,2} names.\n\n\n\n5.4.3.1 Array indexing\nArray elements are accessed with the integer position, starting at 1 for the first element8 9:\n\nv = [10, 20, 30, 40, 50]\nv[2]\n\n20\n\n\nWe can also access a subset of the vector’s contents by passing a range:\n\nv[2:4]\n\n3-element Vector{Int64}:\n 20\n 30\n 40\n\n\nAnd we can generically reference the array’s contents, such as:\n\nv[begin+1:end-1]\n\n3-element Vector{Int64}:\n 20\n 30\n 40\n\n\nWe can assign values into the array as well, as well as combine arrays and push new elements to the end:\n\nv[2] = -1\npush!(v, 5)\nvcat(v, [1, 2, 3])\n\n9-element Vector{Int64}:\n 10\n -1\n 30\n 40\n 50\n  5\n  1\n  2\n  3\n\n\n\n\n5.4.3.2 Array Alignment\nWhen you have an MxN matrix (M rows, N columns), a choice must be made as to which elements are next to each other in memory. Typical math convention and fundamental computer linear algebra libraries (dating back decades!) are column major and Julia follows that legacy. Column major means that elements going down the rows of a column are stored next to each other in memory. This is important to know so that (1) you remember that vectors are treated like a column vector when working with arrays (a N element 1D vector is like a Nx1 matrix), and (2) when iterating through an array, it will be faster for the computer to access elements next to each other column-wise. A 10x10 matrix is actually stored in memory as 100 elements coming in order, one after another in single file.\nThis 3x4 matrix is stored with the elements of columns next to each other, which we can see with vec:\n\nmat = [1 2 3; 4 5 6; 7 8 9]\n\n3×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n 7  8  9\n\n\n\nvec(mat)\n\n9-element Vector{Int64}:\n 1\n 4\n 7\n 2\n 5\n 8\n 3\n 6\n 9\n\n\n\n\n\n5.4.4 Characters, Strings, and Symbols\nCharacters are represented in most programming languages as letters within quotation marks. In Julia, individual characters are represented using single quotes:\n\n'a'\n\n'a': ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\n\n\nLetters and other characters present more difficulties than numbers to represent within a computer (think of how many languages and alphabets exist!), and it essentially only works because the world at large has agreed to a given representation. Originally ASCII (American Standard Code for Information Interchange) was used to represent just 95 of the most common English characters (“a” through “z”, zero through nine, etc.). Now, UTF (Unicode Transformation Format) can encode more than a million characters and symbols from many human languages.\nStrings are a collection10 of characters, and can be created in Julia with double quotes:\n\n\"hello world\"\n\n\"hello world\"\n\n\nIt’s easy to ascertain how ‘normal’ characters can be inserted into a string, but what about things like new lines or tabs? They are represented by their own characters but are normally not printed in computer output. However, those otherwise invisible characters do exist. For example, here we will use a string literal (indicated by the \"\"\" ) to tell Julia to interpret the string as given, including the invisible new line created by hitting return on the keyboard between the two words:\n\n\"\"\"\nhello\nworld\n\"\"\"\n\n\"hello\\nworld\\n\"\n\n\nThe output above shows the \\n character contained within the string.\nSymbols are a way of representing an identifier which cannot be seen as a collection of individual characters. :helloworld is distinct from \"helloworld\" - you can kind of think of the former as an un-executed bit of code - if we were to execute it (with eval(:helloworld)), we would get an error UndefVarError: `a` not defined . Symbols can look like strings but do not behave like them. For now, it is best to not worry about symbols but it is an important aspect of Julia which allows the language to represent aspects of itself as data. This allows for powerful self-reference and self-modification of code but this is a more advanced topic generally out of scope of this book.\n\n\n5.4.5 Tuples\nTuples are a set of values that belong together and are denoted by a values inside parenthesis and separated by a comma. An example might be x-y coordinates in 2 dimensional space:\n\nx = 3\ny = 4\np1 = (x, y)\n\n(3, 4)\n\n\nTuple’s values can be accessed like arrays:\n\np1[1]\n\n3\n\n\nTuples fill a middle ground between scalar types and arrays in more ways that one:\n\nTuples have no problem having heterogeneous types in the different slots.\nTuples are immutable, meaning that you cannot overwrite the value in memory (an error will be thrown if we try to do p[1] = 5).\nIt’s generally expected that within an array, you would be able to apply the same operation to all the elements (e.g. square each element) or do something like sum all of the elements together which isn’t generally case for a tuple.\nTuples are generally stack allocated instead of being heap allocated like arrays11, meaning that a lot of times they can be faster than arrays.\n\n\n5.4.5.1 Named Tuples\nNamed tuples provide a way to give each field within the tuple a specific name. For example, our x-y coordinate example above could become:\n\np2 = (x=3, y=4)\n\n(x = 3, y = 4)\n\n\nThe benefit is that we can give more meaning to each field and access the values in a nicer way. Previously, we used location[1] to access the x-value, but with the new definition we can access it by name:\n\np2.x\n\n3\n\n\n\n\n\n5.4.6 Parametric Types\nWe just saw how tuples can contain heterogeneous types of data inside a common container. Let’s look at this a little bit closer by looking at the full type:\n\ntypeof(p1)\n\nTuple{Int64, Int64}\n\n\nlocation is a Tuple{Int64,Int64} type, which means that its first and second elements are both Int64. Contrast this with:\n\ntypeof((\"hello\", 1.0))\n\nTuple{String, Float64}\n\n\nThese tuples are both of the form Tuple{T,U} where T and U are both types. Why does this matter? We and the compiler can distinguish between a Tuple{Int64,Int64} and a Tuple{String,Float64} which allows us to reason about things (“I can add the first element of tuple together only if both are numbers”) and the compiler to optimize (sometimes it can know exactly how many bits in memory a tuple of a certain kind will need and be more efficient about memory use). Further, we will see how this can become a powerful force in writing appropriately abstracted code and more logically organize our entire program when we encounter “multiple dispatch” later on.\n\n\n5.4.7 Types for things not there\nnothing represents that there’s nothing to be returned - for example if there’s no solution to an optimization problem or if a function just doesn’t have any value to return (such as in the case with input/output like println).\nmissing is to represent something should be there but it’s not, as is all too common in real-world data. Julia natively supports missing and three-value logic, which an an extension of the two-value boolean (true/false) logic, to handle missing logical values:\n\n\nTable 5.1: Three value logic with true, missing, and false.\n\n\n\n\n(a) Not logic\n\n\nNOT (!)\nValue\n\n\n\n\ntrue\nfalse\n\n\nmissing\nmissing\n\n\nfalse\ntrue\n\n\n\n\n\n\n(b) And logic\n\n\nAND (&)\ntrue\nmissing\nfalse\n\n\n\n\ntrue\ntrue\nmissing\nfalse\n\n\nmissing\nmissing\nmissing\nfalse\n\n\nfalse\nfalse\nfalse\nfalse\n\n\n\n\n\n\n(c) Or Logic\n\n\nOR (|)\ntrue\nmissing\nfalse\n\n\n\n\ntrue\ntrue\ntrue\ntrue\n\n\nmissing\ntrue\nmissing\nmissing\n\n\nfalse\ntrue\nmissing\nfalse\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nMissing and Nothing are the types while missing and nothing are the values here. This is analagous to Float64 being a type and 2.0 being a value.\n\n\n\n\n5.4.8 Union Types\nWhen two types may arise in a context, union types are a way to represent that. For example, if we have a data feed and we know that it will produce either a Float64 or a Missing type then we can say that the value for this is Union{Float64,Missing}. This is much better for the compiler (and our performance!) than saying that the type of this is Any.\n\n\n5.4.9 Creating User Defined Types\nWe’ve talked about some built-in types but so much additional capabilities come from being able to define our own types. For example, taking the x-y-coordinate example from above, we could do the following instead of defining a tuple:\n\nstruct BasicPoint\n    x::Int64\n    y::Int64\nend\n\np3 = BasicPoint(3, 4)\n\nBasicPoint(3, 4)\n\n\nBasicPoint is a composite type because it is composed of elements of other types. Fields are accessed the same way as named tuples:\n\n1p3.x, p3.y\n\n\n1\n\nNote that here, Julia will return a tuple instead of a single value due to the comma separated expressions.\n\n\n\n\n(3, 4)\n\n\nstructs in Julia are immutable like tuples above.\nBut wait, didn’t tuples let us mix types too via parametric types? Yes, and we can do the same with our type!\n\nstruct Point{T}\n    x::T\n    y::T\nend\n\nLine 1 The {T} after the type’s name allows for different Points to be created depending on what the type of the underlying x and y is.\nHere’s two new points which now have different types:\n\np4 = Point(1, 4)\np5 = Point(2.0, 3.0)\n\np4, p5\n\n(Point{Int64}(1, 4), Point{Float64}(2.0, 3.0))\n\n\nNote that the types are not equal because they have different type parameters!\n\ntypeof(p4), typeof(p5), typeof(p4) == typeof(p5)\n\n(Point{Int64}, Point{Float64}, false)\n\n\nBut both are now subtypes of PPoint2D. The expression X isa Y is true when X is a (sub)type of Y:\n\np4 isa Point, p5 isa Point\n\n(true, true)\n\n\nNote though, that the x and y are both of the same type in each PPoint2D that we created. If instead we wanted to allow the coordinates to be of different types, then we could have defined PPoint2D as follows:\nstruct Point{T,U}\n    x::T\n    y::U\nend\n\n\n\n\n\n\nNote\n\n\n\nCan we define the structs above without indicating a (parametric) type? Yes!\nstruct Point\n    x # no type here!\n    y # no type declared here either!\nend\nBut! x and y will both be allowed to be Any, which is the fallback type where Julia says that it doesn’t know any more about the type until runtime (the time at which our program encounters the data when running). This means that the compiler (and us!) can’t reason about or optimize the code as effectively as when the types are explicit or parametric. This is an example of how Julia can provide a nice learning curve - don’t worry about the types until you start to get more sophicistited about the program design or need to extract more performance from the code.\n\n\nThe above structs that we have defined are examples of concrete types types which hold data. Abstract types don’t directly hold data themselves but are used to define a hiearchy of types which we will later exploit (Chapter 6) to implement custom behavior depending on what type our data is.\nHere’s an example of (1) defining a set of related types that sits above our Point2D:\n\nabstract type Coordinate end\nabstract type CartesianCoordinate &lt;: Coordinate end\nabstract type PolarCoordinate &lt;: Coordinate end\n\nstruct Point2D{T} &lt;: CartesianCoordinate\n    x::T\n    y::T\nend\n\nstruct Point3D{T} &lt;: CartesianCoordinate\n    x::T\n    y::T\n    z::T\nend\n\nstruct Polar2D{T} &lt;: PolarCoordinate\n    r::T\n    θ::T\nend\n\n\n\n\n\n\n\nUnicode Characters\n\n\n\nJulia has wonderful Unicode support, meaning that it’s not a problem to include characters like θ. The character can be typed in Julia editors by entering \\theta and then pressing the TAB key on the keyboard.\nUnicode is helpful for following conventions that you may be used to in math. For example, the math formula \\(\\text{circumference}(r) = 2 \\times r \\times \\pi\\) can be written in Julia with circumference(r) = 2 * r * π.\nThe name for the characters follows the same for LaTeX, so you can search the internet for,e.g. “theta LaTeX” to find the appropriate name. Furhter, you can use the REPL help mode to find out how to enter a character if you can copy and paste it from somewhere:\nhelp?&gt; θ\n\"θ\" can be typed by \\theta&lt;tab&gt;\n\n\n\n\n5.4.10 Mutable structs\nIt is possible to define structs where the data can be modified - such a data field is said to be mutable because it can be changed or mutated. Here’s an example of what it would look like if we made Point2D mutable:\nmutable struct Point2D{T}\n    x::T\n    y::T\nend\nYou may find that this more naturally represents what you are trying to do. However, recall that an advantage of an immutable datatype is that costly memory doesn’t necessarily have to be allocated for it. So you may think that you’re being more efficient by re-using the same object… but it may not actually be faster. Again, more will be revealed in Chapter 8.\n\n\n\n\n\n\nFinancial Modeling Pro-tip\n\n\n\nGenerally you should default to using immutable types and consider only moving to mutable types in specific circumstances. You’ll see some examples in the applications later in the book.\n\n\n\n\n5.4.11 Constructors\nConstructors are functions that return a data type (functions will be covered more generally later in the chapter). When we declare a struct, an implicit function is defined that takes a tuple of arguments and returns the data type that was declared. In the following example, after we define MyType the struct, Julia creates a function (also called MyType) which takes two arguments and will return the datatype MyType:\n\nstruct MyDate\n    year::Int\n    month::Int\n    day::Int\nend\n\nmethods(MyDate)\n\n# 2 methods for type constructor: MyDate(year::Int64, month::Int64, day::Int64) in Main at In[38]:2  MyDate(year, month, day) in Main at In[38]:2 \n\n\nImplicit constructors are nice in that you don’t have to define a default method and the language does it for you. Sometimes there’s reasons to want to control how an object is created, either for convenience or to enforce certain restrictions.\nWe can use an inner constructor (i.e. inside the struct block) to enforce restrictions:\n\nstruct MyDate\n    year::Int\n    month::Int\n    day::Int\n\n    function MyDate(y,m,d)\n        if ~(m in 1:12)\n            error(\"month is not between 1 and 12\")\n        else if ~(d in 1:31)\n            error(\"day is not between 1 and 31\")\n        else\n            return new(y,m,d)\n        end\n\n    end\n                \nend\nAnd outer constructors are simply functions defined that have the same name as the data type , but are not defined inside the struct block. Extending the MyDate example, say we want to provide a default constructor for if no day is given such that the date returns the 1st of the month:\nfunction MyDate(y,m)\n    return MyDate(y,m,1)\nend"
  },
  {
    "objectID": "foundations-of-programming.html#expressions-and-control-flow",
    "href": "foundations-of-programming.html#expressions-and-control-flow",
    "title": "5  Elements of Programming",
    "section": "5.5 Expressions and Control Flow",
    "text": "5.5 Expressions and Control Flow\nHaving already seen some more illustrative examples above, we can zoom in onto smaller pieces called expressions which are effectively the basic block of code that gets evaluated. Here is an expression that adds two integers together that evaluate to a new integer (3 in this case):\n\n1 + 2\n\n3\n\n\n\n5.5.1 Compound Expression\nThere’s two kinds of blocks where we can ensure that subexpressions get evaluated in order and return the last expression as the overall return value: begin and let blocks.\n\nc = begin\n    a = 3\n    b = 4\n    a + b\nend\n\na, b, c\n\n(3, 4, 7)\n\n\nThe variables inside the begin block are evaluated in the same scope as c and therefore have the assigned values when we call a and b in the last line. Contrast that with the let block below, where d and e are not available when we try to get the value of f. This is because let creates a new inner scope that’s not available in f’s scope. More on scope later in the chapter.\n\nf = let\n    d = 1\n    e = 2\n    d + e\nend\nf\n\n3\n\n\n\nd\n\nLoadError: UndefVarError: `d` not defined\n\n\n\n\n5.5.2 Conditional Expressions\nConditionals are expressions that evaluate to a boolean true or false. This is the beginning of really being able to assemble complex logic to perform useful work. Here are a handful expressions that would evaluate to true:\n1 &gt; 0\n1 == 1 # check for equality\nFloat64 isa Rational\n(5 &gt; 0) & (-1 &lt; 2) # \"and\" expression\n(5 &gt; 0) | (-1 &gt; 2) # \"or\" expression\n1 != 2\n\n\n\n\n\n\nNote\n\n\n\nIn Julia, the booleans have an integer equality: true is equal to 1 (true == 1) and false is equal to 0 (false == 0). However:\n\ntrue != 5. Only 1 is equal to true (in some languages, any non-zero number is “truthy”).\ntrue is not egal to 1 (egal is defined later in this chapter).\n\n\n\nConditionals can be used to assemble different logical paths for the program to follow and the general pattern is an if block:\nif condition\n    # do one thing\nelseif condition\n    # do something else\nelse\n    # do something if none of the \n    # other conditions are met\nend\nA complete example:\n\nfunction buy_or_sell(my_value, market_price)\n    if my_value &gt; market_price\n        \"buy more\"\n    elseif my_value &lt; market_price\n        \"sell\"\n    else\n        \"hold\"\n    end\nend\n\nbuy_or_sell(10, 15), buy_or_sell(15, 10), buy_or_sell(10, 10)\n\n(\"sell\", \"buy more\", \"hold\")\n\n\n\n5.5.2.1 Equality\nThe “Ship of Theseus12” problem is an example of how equality can be philosophically complex concept. In computer science we have the advantage that while we may not be able to resolve what’s the “right” type of equality, we can be more precise about it.\nHere is an example for which we can see the difference between two types of equality:\n\nEgal equality is when a program could not distinguish between two objects at all\nEqual equality is when the values of two objects are the same\n\nIf two things are egal, then they are also equal.\nIn the following example, s and t are equal but not egal:\n\ns = [1, 2, 3]\nt = [1, 2, 3]\ns == t, s === t\n\n(true, false)\n\n\nOne way to think about this is that while the values are equal, there is a way that one of the arrays could be made not equal to the other:\n\nt[2] = 5\nt\n\n3-element Vector{Int64}:\n 1\n 5\n 3\n\n\nNow t is no longer equal to s:\n\ns == t\n\nfalse\n\n\nRecall that arrays are able to be modified, but other types like tuples are immutable. Immutable types with the same value are egal because there is no way for us to make them different:\n\n(2, 4) === (2, 4)\n\ntrue\n\n\nUsing this terminology, we could now interpret the “Ship of Theseus” as that his ship is “equal” but not “egal”.\n\n\n\n5.5.3 Assignment and Variables\nWhen we say x = 2 we are assigning the integer value of 2 to the variable x. This is an expression that lets us bind a something to a variable so that it can be referenced more concisely or in different parts of our code. When we re-assign the variable we are not mutating the value: x = 3 does not change the 2.\nWhen we have a mutable object (e.g. an Array or a mutable struct), we can mutate the value inside the referenced container. For example:\n\n1x = [1, 2, 3]\n2x[1] = 5\nx\n\n\n1\n\nx refers to the array which currently contains the elements 1, 2, and 3.\n\n2\n\nWe re-assign the first element of the array to be the value 5 instead of 1\n\n\n\n\n3-element Vector{Int64}:\n 5\n 2\n 3\n\n\nIn the above example, x has not been reassigned. It is possible for two variables to refer to the same object:\n\nx = [1, 2, 3]\n1y = x\nx[1] = 6\ny\n\n\n1\n\ny refers to the same underlying array as x\n\n\n\n\n3-element Vector{Int64}:\n 6\n 2\n 3\n\n\n\n\n5.5.4 Loops\nLoops are ways for the program to move through a program and repeat expressions while we want it to. There are two primary loops: for and while.\nfor loops are loops that iterate over a defined range or set of values. Let’s assume that we have the array v = [6,7,8]. Here are multiple examples of using a for loop in order to print each value to output (println):\n# use fixed indices\nfor i in 1:3\n    println(v[i])\nend\n# use indices the of the array\nfor i in eachindex(v)\n    println(v[i])\nend\n# use the elements of the array\nfor x in v\n    println(x)\nend\n# use the elements of the array\nfor x ∈ v          # ∈ is typed \\in&lt;tab&gt;\n    println(x)\nend\nwhile loops will run repeatedly until an expression is false. Here’s some examples of printing each value of v again:\n# index the array\ni = 1\nwhile i &lt;= length(v) \n    println(v[i])\n1    global i += 1\nend\n\n1\n\nglobal is used to incrment i by 1. i is defined outside the scope of the while loop (see Section 5.7).\n\n\n# index the array\ni = 1\nwhile true\n    println(v[i])\n    if i &gt;= length(v)\n1        break\n    end\n    global i += 1 \nend\n\n1\n\nbreak is used to terminate the loop manually, since the condition that follows the while will never be false.\n\n\n\n\n5.5.5 Performance of loops\nLoops are highly performant in Julia and often the fastest way to accomplish things. Those coming from Python or R may have developed a habit to avoid writing loops. Fear the for loop not!"
  },
  {
    "objectID": "foundations-of-programming.html#functions",
    "href": "foundations-of-programming.html#functions",
    "title": "5  Elements of Programming",
    "section": "5.6 Functions",
    "text": "5.6 Functions\nFunctions are a set of expressions that take inputs and return specified outputs.\n\n5.6.1 Special Operators\nOperators are the glue of expressions which combine values. We’ve already seen quite a few, but let’s develop a little bit of terminology for these functions.\nUnary operators are operators which only take a single argument. Examples include the ! which negates a boolean value or - which negates a number:\n\n!true, -5\n\n(false, -5)\n\n\nBinary operators take two arguments and are some of the most common functions we encounter, such as + or - or &gt;:\n\n1 + 2, 1 - 2, 1 &gt; 2\n\n(3, -1, false)\n\n\nThe above unary and binary operators are special kinds of functions which don’t require the use of parenthesis. However, they can be written with parathesis for greater clarity:\n\n!(true), -(5), +(1, 2), -(1, 2)\n\n(false, -5, 3, -1)\n\n\nIn Julia, we distinguish between functions which define behavior that maps a set of inputs to outputs. But a single function can adapt its behavior to the arguments themselves. We have just seen the function - be used in two different ways: negation and subtraction depending on whether it had one or two arguments given to it. In this way there is a conceptual hierarchy of functions that complements the hierarchy we have discussed in relation to types:\n\n- is the overall function\n-(x) is a unary function which negates its values, -(x,y) subtracts y from x\nSpecific methods are then created for each combination of concrete types: -(x::Float64) is a different method than -(x::Int)\n\nMethods are specific compiled versions of the function for specific types. This is important because at a hardware level, operations for different types (e.g. integers versus floating point) differ considerably. By optimizing for the speicifc types Julia is able to achieve nearly ideal performance without the same sacrifices of other dynamic languages. We will develop more with respect to methods when we talk about dispatch in Chapter 6.\n\n\n5.6.2 General Functions\nFuncitons more generally are defined like so:\n\n1function distance(point)\n2    return sqrt(point.x^2 + point.y^2)\nend\n\n\n1\n\nA function block is declared with the name distance which takes a single argumemnt called point\n\n2\n\nWe compute the distance formla for a point with x and y coordinates. The return value make explicit what value the function will output.\n\n\n\n\ndistance (generic function with 1 method)\n\n\n\n\n\n\n\n\nNote\n\n\n\nAn alternate, simpler function syntax for distance would be:\ndistance(point) = sqrt(point.x^2 + point.y^2)\n\n\nHowever, we might at this point note a flaw in our function’s defintion if we think about the various Coordinates we defined earler: our definition would currently only work for Point2D. For example, if we try a Point3D we will get the wrong answer:\n\ndistance(Point3D(1, 1, 1,))\n\n1.4142135623730951\n\n\nThe above value should be \\(\\sqrt(3)\\), or approximately \\(1.73205\\). What we need to do is define a refined distance for each type, which we’ll call dist to distinguish from the earlier definition. We’ll also use the opportunity to introduce the syntax for documenting functions in Julia, which is simply to put a string (\"...\") or string literal (\"\"\"...\"\"\") right above the defintion.\n\n\"\"\"\n    dist(point)\n\nThe euclidean distance of a point from the origin.\n\"\"\"\ndist(p::Point2D) = sqrt(p.x^2 + p.y^2)\ndist(p::Point3D) = sqrt(p.x^2 + p.y^2 + p.z^2)\ndist(p::Polar2D) = p.r\n\ndist (generic function with 3 methods)\n\n\nNow our result will be correct:\n\ndist(Point3D(1, 1, 1,))\n\n1.7320508075688772\n\n\nIn the next chapter we’ll develop some more tools, which would, for example let us define the function dist(p::CartesianCoordinate) and generically define the distance for all of CartesianCoordinate’s subtypes.\n\n\n\n\n\n\nDefining Methods for Parametric Types\n\n\n\nWe learned that Float64 &lt;: Real in the type hierarchy. However, note that Tuple{Float64} is not a subtype of Tuple{Real}. This is called being invariant in type theory… but for our purposes this just practically means that when we define a method we need to specify that we want it to apply to all subtypes.\nFor example, myfunction(x::Tuple{Real}) would not be called if x was a Tuple{Float64} because it’s not a subtype of Tuple{Real}. To act the way we want, would define the method with the signature of myfunction(Tuple{&lt;:Real}) or myfunction{Tuple{T}} where {T&lt;:Real}.\n\n\n\n\n5.6.3 Keyword Arguments\nKeyword arguments are arguments that are passed to a function but do not use position to pass data to functions but instead used named arguments. In the following example, filepath is a positional argument while the two arguments after the semicolon (;) are keyword arguments.\nfunction read_data(filepath; normalize_names, has_header_row)\n    # ... function would be defined here\nend\nThe function would need to be called and have the two keyword arguments specified:\nread_data(\"results.csv\"; normalizenames=true, hasheaderrow=false)\n\n\n5.6.4 Default Arguments\nWe are able to define default arguments for both positional and keyword arguments via an assignment expression in the function signature. For example, we can make it so that the user need not specify all the options for each call. Modifying the prior example so that typical CSVs work with less customization from the user:\nfunction read_data(filepath;\n    normalizenames = true,\n    hasheader = false\n)\nThis is a simplified example, but if you look at the documentation for most data import packages you’ll see a lot of functionality defined via keyword arguments which have sensible defaults so that most of the the time you need not worry about modifying them.\n\n\n5.6.5 Anonymous Functions\nAnonymous functions are functions that have no name and are used in contexts where the name does not matter. The syntax is x -&gt; ...expression with x.... As an example, say that we want to create a vector from another where each element is squared. map applies a function to each member of a given collection:\n\nv = [4, 1, 5]\n1map(x -&gt; x^2, v)\n\n\n1\n\nThe x -&gt; x^2 is the anonymous function in this example.\n\n\n\n\n3-element Vector{Int64}:\n 16\n  1\n 25\n\n\nThey are often used when constructing something from another value, or defining a function within optimization or solving routines.\n\n\n5.6.6 Passing by Sharing\nArguments to a function in Julia are *passed-by-sharing** which means that an outside variable can be mutated from within a function. We can modify the array in the outer scope (scope discussed later in this chapter) from within the function. In this example, we modify the array that is assigned to v by doubling each element:\n\nv = [1, 2, 3]\n\nfunction double!(v)\n    for i in eachindex(v)\n        v[1] = 2 * v[i]\n    end\nend\n\ndouble!(v)\n\nv\n\n3-element Vector{Int64}:\n 6\n 2\n 3\n\n\n\n\n\n\n\n\nTip\n\n\n\nConvention in Julia is that a function that modifies it’s arguments has a ! in it’s name and we follow this convention in double! above. Another example would be the built-in function sort! which will sort an array in-place without allocating a new array to store the sorted values.\n\n\nWe won’t discuss all potential ways that programming languages can behave in this regard, but an alternative that one may have seen before (e.g. in Matlab) is pass-by-value where a modification to an argument only modifies the value within the scope. Here’s how to replicate that in Julia by copying the value before handing it to a function. This time, v is not modified because we only passed a copy of the array and not the array itself:\n\nv = [1, 2, 3]\ndouble!(copy(v))\nv\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\n\n\n5.6.7 Broadcasting\nLooking at the prior definition of dist, what if we wanted to compute the squared distance from the origin for a set of points? If those points are stored in an array, we can broadcast functions to all members of a collection at the same time. This is accomplished using the dot-syntax as follows:\n\npoints = [Point2D(1, 2), Point2D(3, 4), Point2D(6, 7)]\ndist.(points) .^ 2\n\n3-element Vector{Float64}:\n  5.000000000000001\n 25.0\n 85.0\n\n\nLet’s unpack that a bit more:\n\nThe . in dist.(points) tells Julia to apply the function dist to each element in points.\nThe . in .^ tells Julia to square each values as well\n\nWhy broadcasting is useful:\n\nWithout needing any redefinition of functions we were able to transform the function dist and exponentiation (^) to work on a collection of data. This means that we can keep our code simpler and easier to reason about (operating on individual things is easier than adding logic to handle collections of things).\nWhen multiple broadcasted operations are joined together, Julia can fuse the operations so that each operation is performed at the same time instead of each step sequentially. That is, if the operation were not fused, the computer would first calculate dist for each point, and then apply the square on the collection of distances. When it’s fused, the operations can happen at the same time without creating an interim set of values.\n\n\n\n\n\n\n\nNote\n\n\n\nFor readers coming from numpy-flavored Python or R, broadcasting is a way that can feel familiar to the array-oriented behavior of those two languages. Once you feel comfortable with Julia in general, you may find yourself relaxing and relying less on array-oriented design and instead picking whichever iteration paradigm feels most natural for the problem at hand: loops or broadcasting over arrays.\n\n\n\n5.6.7.1 Broadcasting Rules\nWhat happens if one of the collections is not the same size as the others? When broadcasting, singleton dimensions (i.e. the 1 in 1xN, “1-by-N”, dimensions) will be expanded automatically when it makes sense. For example, if you have a single element and a one dimensional array, the single element will be expanded in the function call without using any additional memory (if that dimension matches one of the dimensions of the other array).\nThe rules with an MxN and a PxQ array:\n\neither (M and P) or (N and Q) need to be the same, and\none of the non-matching dimensions needs to be 1\n\nSome examples might clarify. This 1x1 element is being combined with a 4x1, so there is a compatible dimension (N and Q match, M is 1):\n\n2 .^ [0, 1, 2, 3]\n\n4-element Vector{Int64}:\n 1\n 2\n 4\n 8\n\n\nHere, this 1x3 works with the 2x3 (N and Q match, M is 1)\n\n[1 2 3] .+ [1 2 3; 4 5 6]\n\n2×3 Matrix{Int64}:\n 2  4  6\n 5  7  9\n\n\nThis 3x1 isn’t compatible with this 2x3 array (neither M and P nor N and Q match)\n\n[1, 2, 3] .+ [1 2 3; 4 5 6]\n\nLoadError: DimensionMismatch: arrays could not be broadcast to a common size; got a dimension with lengths 3 and 2\n\n\nThis 2x4 isn’t compatible with the 2x3 (M and P match, but N nor Q is 1):\n\n[1 2; 3 4] .+ [1 2 3; 4 5 6]\n\nLoadError: DimensionMismatch: arrays could not be broadcast to a common size; got a dimension with lengths 2 and 3\n\n\n\n\n5.6.7.2 Not Broadcasting\nWhat if you do not want the array to be used element-wise when broadcasting? Then you can wrap the array in a Ref, which is used in broadcasting to make the array be treated like a scalar. In the example below, in(needle,haystack) searches a collection (haystack) for an item (needle) and returns true or false if the item is in the collection:\n\nin(4, [1 2 3; 4 5 6])\n\ntrue\n\n\nWhat if we had an array of things (“needles”) that we wanted to search for? By default, broadcasting would effectively split the array up into collections of individual elements to search:\n\nin.([1, 9], [1 2 3; 4 5 6])\n\n2×3 BitMatrix:\n 1  0  0\n 0  0  0\n\n\nEffectively, the result above is the result of this broadcasted result:\nin(1, [1,2,3]) # the first row of the above result\nin(9, [4,5,6])\nIf we were expecting Julia to return [1,0] (that the first needle is in the haystack but the second needle is not), then we need to tell Julia not to broadcast along the second array with Ref:\n\nin.([1, 9], Ref([1 2 3; 4 5 6]))\n\n2-element BitVector:\n 1\n 0\n\n\n\n\n\n5.6.8 First Class Nature\nFunctions in many languages including Julia are first class which means that functions can be assigned and moved around like data variables.\nIn this example, we have a general approach to calculate the error of a modeled result compared to a known truth. In this context, there are different ways to measure error of the modeled result and we can simplify the implementation of loss by keeping the different kinds of error defined separately. Then, we can assign a function to a variable and use it as an argument to another function:\n\nfunction square_error(guess, correct)\n    (correct - guess)^2\nend\n\nfunction abs_error(guess, correct)\n    abs(correct - guess)\nend\n\n# obs meaning \"observations\"\nfunction loss(modeled_obs,\n    actual_obs,\n1    loss_function\n)\n    sum(\n        loss_function.(modeled_obs, actual_obs)\n    )\nend\n\n2let\n3    a = loss([1, 5, 11], [1, 4, 9], square_error)\n    b = loss([1, 5, 11], [1, 4, 9], abs_error)\n    a, b\nend\n\n\n1\n\nloss_function is a variable that will refer to a function instead of data.\n\n2\n\nUsing a let block here is good practice to not have temporary variables a and b scattered around our workspace.\n\n3\n\nUsing a function as an argument to another function is an example of functions being treated as “first class”.\n\n\n\n\n(5, 3)"
  },
  {
    "objectID": "foundations-of-programming.html#sec-scope",
    "href": "foundations-of-programming.html#sec-scope",
    "title": "5  Elements of Programming",
    "section": "5.7 Scope",
    "text": "5.7 Scope\nIn projects of even modest complexity, it can be challenging to come up with unique identifiers for different functions or variables. Scope refers to the bounds for which an identifier is available. We will often talk about the local scope that’s inside some expression that creates a narrowly defined scope (such as a function or let or module block) or the global scope which is the top level scope that contains everything else inside of it. Here are a few examples to demonstrate scope.\n\n1i = 1\n2let\n3    j = 3\n    i + j\nend\n\n\n1\n\ni is defined in the global scope and would be available to other inner scopes.\n\n2\n\nThe let ... end block creates a local scope which inherits the defined global scope definitions.\n\n3\n\nj is only defined in the local scope created by the let block.\n\n\n\n\n4\n\n\nIn fact, if we try to use j outside of the scope defined above we will get an error:\n\nj\n\nLoadError: UndefVarError: `j` not defined\n\n\nHere is an example with functions:\n\nx = 2\nbase = 10\n1foo() = base^x\n2foo(x) = base^x\n3foo(x, base) = base^x\n\nfoo(), foo(4), foo(4, 4)\n\n\n1\n\nBoth base and x are inherited from the global scope.\n\n2\n\nx is based on the local scope from the function’s arguments and base is inherited from the global scope.\n\n3\n\nBoth base and x are defined in the local scope via the function’s arguments.\n\n\n\n\n(100, 10000, 256)\n\n\nIn Julia, it’s always best to explicitly pass arguments to functions rather than relying on them coming from an inherited scope. This is more straight-forward and easier to reason about and it also allows Julia to optimize the function to run faster because all relevant variables coming from outside the function are defined at the function’s entry point (the arguments).\n\n5.7.1 Modules and Namespaces\nModules are ways to encapsulate related functionality together. Another benefit is that the variables inside the module don’t “pollute” the namespace of your current scope. Here’s an example:\n\n1module Shape\n\nstruct Triangle{T}\n    base::T\n    height::T\nend\n\n2function area(t::Triangle)\n    return 1 / 2 * t.base * t.height\nend\nend\n\n3t = Shape.Triangle(4, 2)\n4area = Shape.area(t)\n\n\n1\n\nmodule defines an encapsulated block of code which is anchored to the namespace Shape\n\n2\n\nHere, area a function defined within the Shape module.\n\n3\n\nOutside of Shape module, we can access the definitions inside via the Module.identifier syntax.\n\n4\n\nHere, area is a variable in our global scope that does not conflict with the area defined within the Shape module. If Shape.area were not within a module then when we said area = ... we would have reassigned area to no longer refer to the function and instead would refer to the area of our triangle.\n\n\n\n\n4.0\n\n\n\n\n\n\n\n\nNote\n\n\n\nSummarizing related terminology:\n\nA module is a block of code such as module MySimulation ... end\nA package is a module that has a specific set of files and associated metadata. Essentially, it’s a module with a Project.toml file that has a name and unique identifier listed, and a file in a src/ directory called MySimulation.jl\n\nLibrary is just another name for a package, and the most common context this comes up is when talking about the packages that are bundled with Julia itself called the standard library (stdlib)."
  },
  {
    "objectID": "foundations-of-programming.html#footnotes",
    "href": "foundations-of-programming.html#footnotes",
    "title": "5  Elements of Programming",
    "section": "",
    "text": "Said differently, computer science may contemplate ideas and abstractions more generally than a specific implementation, as in mathematics where a theorem may be proved (\\(a^2 + b^2 = c^2\\)) without resorting to specific numeric examples (\\(3^2 + 4^2 = 5^2\\)).↩︎\nThis binary representations correspond to B and 66 with the ASCII character set and 8-bit integer encodings respectely, discussed later in this chapter.↩︎\nSome distinctions you may encounter: in short-form, “kb” means kilobits while the upper-case “B” in “kB” means kilobytes. Also confusingly, sometimes the “k” can be binary or decimal - because computers speak in binary, a binary “k” means 1024 (equal to 2^10) instead of the usual decimal 1000. In most computer contexts, the binary (multiples of 1024) is more common.↩︎\nThe term floating point refers to the fact that the number’s radix (decimal) point can “float” between the significant digits of the number.↩︎\nThat is, it reads the code input from the user, evaluates what code was given to it, prints the result of the input to the screen, and loops through the process again.↩︎\nThis means that their central processing units (CPUs) use instructions that are 64 bits long.↩︎\nAccurate only to a limited precision, as described in Section 5.4.1.↩︎\nWhether an index starts at 1 or 0 is sometimes debated. Zero-based indexing is natural in the context of low-level programming which deal with bits and positional offsets in computer memory. For higher level programming one-based indexing is more natural: in a set of data stored in an array, it is much more natural to reference the first (through \\(n^{th}\\)) datum instead of the zeroth (through \\((n-1)^{th}\\) datum.↩︎\nArrays in Julia can actually be indexed with an arbitrary starting point: see the package OffsetArrays.jl↩︎\nUnder the hood, strings are essentially a vector of characters but there are complexities with character encoding that don’t allow a lossless conversion to individual characters of uniform bit length. This is for historical compatibility reasons and to avoid making most documents’ file sizes larger than it needs to be.↩︎\nWhat this means will be explained in Chapter 8 .↩︎\nThe Ship of Theseus problem specifically refers to a legendary ancient Greek ship, owned by the hero Theseus. The paradox arises from the scenario where, over time, each wooden part of the ship is replaced with identical materials, leading to the question of whether the fully restored ship is still the same ship as the original. The Ship of Theseus problem is a thought experiment in philosophy that explores the nature of identity and change. It questions whether an object that has had all of its components replaced remains fundamentally the same object.↩︎"
  },
  {
    "objectID": "patterns-abstraction.html#in-this-section",
    "href": "patterns-abstraction.html#in-this-section",
    "title": "6  Patterns of Abstraction",
    "section": "6.1 In this section",
    "text": "6.1 In this section\nWe extend the building blocks from the prior section and talk about how to combine them into more abstract patterns to simplify the design of our programs. Data driven design, object oriented design versus composition, multiple dispatch, and interfaces."
  },
  {
    "objectID": "patterns-abstraction.html#introduction",
    "href": "patterns-abstraction.html#introduction",
    "title": "6  Patterns of Abstraction",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nAbstraction is a selective ignorance—focusing on the aspects of the problem that are relevant, and ignoring the others. At different times we are interested in different ladder of abstraction: sometimes we are interested in the small details, but other times we are interested in understanding the behavior of systems at a higher level.\nSay we are an insurance company with a portfolio of fixed income assets supporting long term insurance liabilities. We might delineate different levels of abstraction like so:\n\n\n\n\n\nThink about moving up and down a ladder of abstraction when analyzing a problem.\n\n\n\n\nTable 6.1: An example of the different levels of abstraction when thinking about modeling an insurance company’s assets and liabilites.\n\n\n\nItem\n\n\n\n\nMore Abstract\nSensitivity of an entire company’s solvency position\n\n\n\nSensitivity of a portfolio of assets\n\n\n\nBehavior over time of an individual contract\n\n\nMore granular\nMechanics of an individual bond or insurance policy\n\n\n\n\nAt different times, we are often interested in different aspects of a problem. In general, you start to be able to obtain more insights and a greater understanding of the system when you move up the ladder of abstraction.\nIn fact, a lot of designing a model is essentially trying to figure out where to put the right abstractions. What is the right level of detail to model this in and what is the right level of detail to expose to other systems?\nLet us also distinguish between vertical abstraction, as described above, and horizontal abstraction which will refer to encapsulating different properties, or mechanics of components of model that effectively exist on the same level of vertical abstraction. For example, both asset and liability mechanics sit at the most granular level in Table 6.1, But it may make sense in our model to separate the data and behavior from each other. If we were to do that, that would be an example of creating horizontal abstraction in service of our overall modeling goals."
  },
  {
    "objectID": "patterns-abstraction.html#interfaces",
    "href": "patterns-abstraction.html#interfaces",
    "title": "6  Patterns of Abstraction",
    "section": "6.3 Interfaces",
    "text": "6.3 Interfaces\nInterfaces are the boundary between different encapsulated abstractions. Financial model this might mean that there is an interface for bonds, or there is an interface for interest-rate swaps. There may be a different interface for calculating risk metrics or visualizing the results.\nFinancial model this might mean that there is an interface for bonds, or there is an interface for interest-rate swaps. There may be a different interface for calculating risk metrics or visualizing the results. A better system design will separate the concern of visualizing output from the mechanics of a fixed income contract. This is what it means to put boundaries on different parts of a models logic.\nOne of the easiest places to see this is with the available open source packages. There are packages available for visualizations, data frames, file, storage, statistical analysis, etc. for many of these it’s easy to see where the natural boundary lies, However, it’s often difficult to find where to draw lines within financial models. For example, should bonds and interest-rate swaps be in separate packages? Or both part of a broader fixed income package? This is where much of the art and domain expertise of the financial professional comes to bear in modeling. There would be no way for a pure software engineer to think about the right design for the system without understanding how underlying components share, similarities or differences and how those components interact.\n\n6.3.1 Conceptual Strategies\nLet’s consider some stategies that you could think about when deciding where to draw different boundaries inside the model.\n\n6.3.1.1 Behavior-Oriented\nThis strategies is to effectively group together components with a model that behaves similarly. So, in our example of bonds and interest-rate swaps fundamentally, they share many characteristics and are used in very similar ways within a model. Therefore, it might make sense to group them together when developing a model.\n\n\n6.3.1.2 Domain Expertise\nIt may be that components of the model require sufficient expertise that different persons or groups are involved in the development. This may warrant separating a models design, So that different groups contributing to the model can focus on any more narrow aspect, Regardless of inherent similarity of components. For example, at a higher vertical level of obstruction, financial derivatives may fall under similar grouping, but sufficient differences exist for equity credit or foreign exchange derivatives that the model should separate those three asset classes for development purposes.\n\n\n6.3.1.3 Composability versus All-in-One\nFor some model design goals, it may be warranted to attempt to bundle together more functionality instead of allowing users to compose a functionality that comes from different packages. For example, perhaps a certain visualization of a model result is particularly useful, It is not easy to create from scratch, And virtually everyone using the model, will desire to see the model output visualized that way. Instead of relying on the user to install a separate visualization package and develop the visualization themselves, it could make sense to bundle visualization functionality with a model that is otherwise unconcerned with graphical capabilities.\nIn general, though it is preferred to try to loosely couple systems, you can pick and choose which components you use and that those components work well together."
  },
  {
    "objectID": "patterns-abstraction.html#programming-interfaces-and-patterns",
    "href": "patterns-abstraction.html#programming-interfaces-and-patterns",
    "title": "6  Patterns of Abstraction",
    "section": "6.4 Programming Interfaces and Patterns",
    "text": "6.4 Programming Interfaces and Patterns\nChapter 5 Described a number of tools that we can utilize as interfaces within our model. We use these tools that are provided by our programming language in service of the conceptual abstraction described above.\n\nFunctions let us implement behavior, where we need trouble ourselves with the low level details.\nData types provide a hierarchical structure to provide meaning to things, and to group those things together into more meaningful structures.\nModules allow us to combine data, and or function, into a related group of concepts which can be shared in different parts of our model\n\nWe will also discuss here some patterns which are ways of doing things that seem to appear repeatedly and specific design choices have proven to work well in the past and should be considered when similar conditions arise in the future.\nLet’s develop a simplified system to value simple fixed income assets in order to illustrate some patterns. Inside a module called Asset, we’ll define a short hiearchy of types and then a function value with multiple methods for the relevant types.\n\nmodule Asset\n\n## Data type definitions\n1abstract type AbstractAsset end\n\nstruct Cash &lt;: AbstractAsset\n    balance::Float64\nend\n\n2abstract type AbstractBond &lt;: AbstractAsset end\n\nstruct CouponBond &lt;: AbstractBond\n    par::Float64\n    coupon::Float64\n    tenor::Int\nend\n\nstruct ZeroCouponBond &lt;: AbstractBond\n    par::Float64\n    tenor::Int\nend\n\n\n## Functions\n\n\"\"\" \n    value(asset,discount_rate)\n\nThe value of an asset with the given discount rate for it's cashflows.\n\"\"\"\nvalue(asset::Cash, r) = asset.balance\n\n4function value(asset::AbstractBond, r)\n    discount_factor = 1.0\n    value = 0.0\n    for t in 1:asset.tenor\n3        discount_factor /= (1 + r)\n        value += discount_factor * cashflow(asset, t)\n    end\n    return value\nend\n\nfunction cashflow(bond::CouponBond, time)\n    if time == bond.tenor\n        (1 + bond.coupon) * bond.par\n    else\n        bond.coupon * bond.par\n    end\nend\n\n5function value(bond::ZeroCouponBond, r)\n    return bond.par / (1 + r)^bond.tenor\nend\n\nend\n\n\n1\n\nGeneral convention is to name abstract types beginning with Abstract...\n\n2\n\nWe define two simple bonds: a coupon paying and a zero-coupon instrument.\n\n3\n\nx /= y, x += y, etc. are shorthand ways to write x = x / y or x = x + y\n\n4\n\nvalue is defined for AbstractBonds in general…\n\n5\n\n… and then more specifically for ZeroCouponBonds. This will be explained when discussing “dispatch” below.\n\n\n\n\nHere’s an example of how this would be used:\n\nportfolio = [\n    Asset.Cash(50.0),\n    Asset.CouponBond(100.0, 0.05, 5),\n    Asset.ZeroCouponBond(100.0, 5),\n]\n\nAsset.value.(portfolio, 0.05)\n\n3-element Vector{Float64}:\n 50.0\n 99.99999999999999\n 78.35261664684589\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the example above, a docstring was included over value(asset::Cash) - but not over the others. That’s okay. Julia will show docstrings for the function value not just individual methods.\n\n\nThere are quite a few things demonstrated here: dispatch, programming paradigms, and [!!what else?] . As each are addressed in turn, we will review how we could have designed the interface differently.\n\n6.4.1 (Multiple) Dispatch\nWhen a function is called, the computer has to decide which method to use. In the example above, when we want to value a ZeroCouponBond, does the value(asset::AbstractBond, r) or value(bond::ZeroCouponBond, r) version get used? Dispatch is the process of determining the right method to use and the rule is that the most specific defined method gets used. In this case, that means that even though our ZeroCouponBond is an AbstractBond, the routine that will used is the more specific value(bond::ZeroCouponBond, r).\nAlready, this is a powerful tool to simplify our code. Imagine the alternative of a long chain of conditional statements trying to find the right logic to use:\n# don't do this!\nfunction value(asset,r)\n    if asset.type == \"ZeroCouponBond\"\n        # special code for Zero coupon bonds\n        # ...\n    elseif asset.type == \"ParBond\"\n        # special code for Par bonds\n        # ...\n    elseif asset.type == \"AmortizingBond\"\n        # special code for Amortizing Bonds\n        # ...\n    else\n        # here define the generic AbstractBond logic\n    end\nend\nA more general concept is that of multiple dispatch, where the types of all arguments are used to determine which method to use. This is a very general paradigm, and in many ways is more extensible than traditional object oriented approaches, (more on that in the next section).\nIn our definition of value above, we used a simple scalar interest rate to determine the rate to discount the cash flows. What if instead of a scalar interest rate value we wanted to instead pass an object that represented a term structure of interest rates? All we have to do is define a new method where the first argument is our AbstractBonds as already written above, but the second argument is our new type, which might look like this:\nstruct MyCurve{F} where {F&lt;:Function}\n    discount_rate::F\nend\n\nfunction value(bond::ZeroCouponBond, c::MyCurve)\n    return bond.par * c.discount_rate(bond.tenor)\nend\nIn this way, multiple dispatch allows us to naturally define methods based on the combination of types.\nwhat if we wanted to extend from a rate to a yield curve and fixed to floating. Where does logic lie? In yield curve or in bond?)\n\n\n6.4.2 Programming Paradigms\n\n6.4.2.1 Objected Oriented Design\nThere’s enough general familiarity with object oriented (“OO”) design that it’s worth describing for understanding how it compares and contrasts to other design patterns. Object oriented systems attempt to form the analogy that various parts of the system are their own objects which encapsulate both data and behavior. Object oriented design is often one the first computer programming abstractions introduced because it very relatable1, however this comparative discussion will point out a number of its flaws as well. That said, much of OO design can be emulated in Julia except for data inheritance.\nWe bring up object oriented design not because of the authors (admittedly subjective) opinion that the object-oriented paradigm can be less suitable for financial modeling, but because by having a (potentially more relatable) contrasting approach we can better illuminate certain ideas and concepts.\n\n\n6.4.2.2 Inheritance\nWe discussed the type hierarchy in Chapter 5 and in most OO implementations this hierarchy comes with inheriting both data and behavior. This is different from Julia where subtypes inherit behavior but not data from the parent type.\nInheriting the data tends to introduce a tight coupling between the parent and the child classes in OO systems. This tight coupling can lead to several issues, particularly as systems grow in complexity. For example, changes in the parent class can inadvertently affect the behavior of all its child classes, which can be problematic if these changes are not carefully managed. This is often referred to as the “fragile base class problem,” where base classes are delicate and changes to them can have widespread, unintended consequences.\nAnother issue with inheritance in OO design is the temptation to use it for code reuse, which can lead to inappropriate hierarchies. Developers might create deep inheritance structures just to reuse code, leading to a scenario where classes are not logically related but are forced into a hierarchy. This can make the system harder to understand and maintain.\nMoreover, inheritance can sometimes lead to the duplication of code across the hierarchy, especially if the inherited behavior needs to be slightly modified in different child classes. This goes against the DRY (Don’t Repeat Yourself) principle, which is a fundamental concept in software engineering advocating for the reduction of repetition in code.\n\n\n\n6.4.3 Composition over Inheritance\nTo mitigate some of the problems associated with inheritance, there’s a growing preference for composition. Composition involves creating objects that contain instances of other objects to achieve complex behaviors. This approach is more flexible than inheritance as it allows for the creation of more modular and reusable code. There is a general preference for “composition over inheritance” among professional developers these days.\nIn composition, objects are constructed from other objects, and behaviors are delegated to these contained objects. This approach allows for greater flexibility, as it’s easier to change the behavior of a system by replacing parts of it without affecting the entire hierarchy, as is often the case with inheritance.\nComposition looks like this:\nstruct CUSIP\n    code::string\nend\n\nstruct FixedIncome\n    coupon::Float64\n    tenor::Float64\nend\n\nstruct MunicipalBond\n    cusip::CUSIP\n    fi::FixedIncome\nend\n\nstruct ListedOption\n    cusip::CUSIP\n    #... other data fields\nend\n\nstruct UnlistedBond\n    fi::FixedIncome\nend\n\n\n# define behavior which relies on defining \nlast_transaction(c::CUSIP) = # ... perform lookup of data\nlast_transaction(asset) = last_transaction(asset.cusip)\n\nduration(f::FixedIncome) = # ... calculate duration\nduration(asset) = duration(asset.fi)\nIn the above example, there are number of asset classes that have CUSIP related attributes (i.e. the 9 character code) and behavior (e.g. being able to look up transaction data). Other assets have fixed income attributes (e.g. calculating a duration). But not all of these assets have a CUSIP! Composition lets us bundle the data and behavior together without needing complex chains of inheritance.\n\n\n\n\n\n\nNote\n\n\n\nA CUSIP (Committee on Uniform Security Identification Procedures) number, is a unique nine-character alphanumeric code assigned to securities, such as stocks and bonds, in the United States and Canada. This code is used to facilitate the clearing and settlement process of securities and to uniquely identify them in transactions and records.\n\n\n\n\n6.4.4 Method Dispatch\nAn alternative and more limiting approach would be to be forced to assign the ownership of the method to one of the associated types, as is done in single-dispatch.\n\nAlternative designs:\n\nZeroCouponBond(par,tenor) = CouponBond(par,0.0,tenor)"
  },
  {
    "objectID": "patterns-abstraction.html#macros-homoiconicity",
    "href": "patterns-abstraction.html#macros-homoiconicity",
    "title": "6  Patterns of Abstraction",
    "section": "6.5 Macros & Homoiconicity",
    "text": "6.5 Macros & Homoiconicity"
  },
  {
    "objectID": "patterns-abstraction.html#misc-techniques",
    "href": "patterns-abstraction.html#misc-techniques",
    "title": "6  Patterns of Abstraction",
    "section": "6.6 Misc Techniques",
    "text": "6.6 Misc Techniques\n\n6.6.1 Recursion\n\n\n6.6.2 Iterators"
  },
  {
    "objectID": "patterns-abstraction.html#footnotes",
    "href": "patterns-abstraction.html#footnotes",
    "title": "6  Patterns of Abstraction",
    "section": "",
    "text": "“Many people who have no idea how a computer works find the idea of object-oriented programming quite natural. In contrast, many people who have experience with computers initially think there is something strange about object oriented systems.” - David Robson, “Object Oriented Software Systems” in Byte Magazine (1981).↩︎"
  },
  {
    "objectID": "elements-of-compsci.html#in-this-section",
    "href": "elements-of-compsci.html#in-this-section",
    "title": "7  Elements of Computer Science",
    "section": "7.1 In this section",
    "text": "7.1 In this section\nAdapting computer science concepts to work for financial professionals. Concepts like computability, computational complexity, the language of algorithms and problem solving, looking for and using patterns, and adopting digital-first practices to automate the boring parts of the job."
  },
  {
    "objectID": "elements-of-compsci.html#computer-science-for-financial-professionals",
    "href": "elements-of-compsci.html#computer-science-for-financial-professionals",
    "title": "7  Elements of Computer Science",
    "section": "7.2 Computer Science for Financial Professionals",
    "text": "7.2 Computer Science for Financial Professionals\nComputer science as a term can be a bit misleading because of the overwhelming association with the physical desktop or laptop machines that we call “computers”. The discipline of computer science is much richer than consumer electronics: at it’s core, computer science concerns itself with areas of reserach and answering tough questions:\n\nAlgorithms and Optimization. How can a problem be solved efficiently? How can that problem be solved at all? Given constraints, how can one find an optimal solution?\nInformation Theory. Given limited data, what can be known or inferred from it?\nTheory of Computation. What sorts of questions are even answerable? Is an answer easy to computer or will resolving it require more resources than the entire known universe? Will a computation ever stop calculating?\nData Structures. How to encode, store, and use data? How does that data relate to each other and what are the trade-offs between different representations of that data?\n\nFor a reader in the twenty-first century we hope that’s it’s patently obvious how impactful the applied computer science has been as an end-user of the internet, artificial intelligence, computational photography, safety control systems, etc., etc. have been to our lives. It is a testament to the utlity of being able to harness some of the ideas of this science is. Many of the most impactful advances occur at the boundary between two disciplines. It’s here in this chapter that we desire to bring together the financial discipline together with computer science and to provide the financial practitioner with the language and concepts to leverage some of computer science’s most relevant ideas."
  },
  {
    "objectID": "elements-of-compsci.html#algorithms",
    "href": "elements-of-compsci.html#algorithms",
    "title": "7  Elements of Computer Science",
    "section": "7.3 Algorithms",
    "text": "7.3 Algorithms\n\n7.3.1 Computational Complexity"
  },
  {
    "objectID": "elements-of-compsci.html#data-structures",
    "href": "elements-of-compsci.html#data-structures",
    "title": "7  Elements of Computer Science",
    "section": "7.4 Data Structures",
    "text": "7.4 Data Structures"
  },
  {
    "objectID": "elements-of-compsci.html#information-theory",
    "href": "elements-of-compsci.html#information-theory",
    "title": "7  Elements of Computer Science",
    "section": "7.5 Information Theory",
    "text": "7.5 Information Theory\n\n7.5.1 Signal vs Noise"
  },
  {
    "objectID": "elements-of-compsci.html#formal-verification",
    "href": "elements-of-compsci.html#formal-verification",
    "title": "7  Elements of Computer Science",
    "section": "7.6 Formal Verification",
    "text": "7.6 Formal Verification"
  },
  {
    "objectID": "elements-of-compsci.html#the-discipline-of-software-engineering",
    "href": "elements-of-compsci.html#the-discipline-of-software-engineering",
    "title": "7  Elements of Computer Science",
    "section": "7.7 The Discipline of Software Engineering",
    "text": "7.7 The Discipline of Software Engineering\n\n7.7.1 Patterns"
  },
  {
    "objectID": "hardware.html#in-this-section",
    "href": "hardware.html#in-this-section",
    "title": "8  Hardware and It’s Implications",
    "section": "8.1 In this section",
    "text": "8.1 In this section\nA discussion of why a cursory understanding of modern computing hardware and architecture is important for making the right design decisions within a modeling context. Stack vs heap allocations, pointers, and bit types. A discussion of parallelism and the different kinds of parallelism."
  },
  {
    "objectID": "software.html#in-this-section",
    "href": "software.html#in-this-section",
    "title": "9  Applying Software Engineering Principles",
    "section": "9.1 In this section",
    "text": "9.1 In this section\nWe describe modern software engineering practices such as testing, documentation, and pipelines which can be utlizied by the financial professional to make their own work more robust and automated."
  },
  {
    "objectID": "modeling.html#in-this-chapter",
    "href": "modeling.html#in-this-chapter",
    "title": "10  Modeling",
    "section": "10.1 In This Chapter",
    "text": "10.1 In This Chapter\nWe discuss how to approach a problem and identify the key attributes to include in the model, what are the inherent trade-offs with different approaches, and how to work with data that feeds your model."
  },
  {
    "objectID": "modeling.html#parsimony",
    "href": "modeling.html#parsimony",
    "title": "10  Modeling",
    "section": "10.2 Parsimony",
    "text": "10.2 Parsimony"
  },
  {
    "objectID": "autodiff.html#in-this-chapter",
    "href": "autodiff.html#in-this-chapter",
    "title": "11  Automatic Differentiation",
    "section": "11.1 In This Chapter",
    "text": "11.1 In This Chapter\nHarnessing the chain rule to compute derivatives not just of simple functions, but of complex programs."
  },
  {
    "objectID": "autodiff.html#motivation-for-automatic-derivatives",
    "href": "autodiff.html#motivation-for-automatic-derivatives",
    "title": "11  Automatic Differentiation",
    "section": "11.2 Motivation for (Automatic) Derivatives",
    "text": "11.2 Motivation for (Automatic) Derivatives\nDerivatives are one of the most useful analytical tools we have. Determining the rate of change with respect to an input is effectively sensitivity testing. Knowing the derivative let’s you optimize things faster (see Chapter 12). You can test properties and implications (monotonicy, maxima/minima)."
  },
  {
    "objectID": "autodiff.html#finite-differentiation",
    "href": "autodiff.html#finite-differentiation",
    "title": "11  Automatic Differentiation",
    "section": "11.3 Finite Differentiation",
    "text": "11.3 Finite Differentiation\nFinite differentiation is evaluating a function \\(f(x)\\) at a value \\(x\\) and then at a nearby value \\(x+\\epsilon\\). The line drawn through these two point effectively estimates the line that is tangent to the function \\(f\\) at \\(x\\): effectively the derivative has been found by approximation. That is, we are looking to approximate the derivative using the property:\n\\[\nf'(x) = \\lim_{{\\epsilon \\to 0}} \\frac{{f(x_0 + \\epsilon) - f(x_0)}}{{\\epsilon}}\n\\]\nWe can approximate the result by simply choosing a small \\(\\epsilon\\).\nThere’s also flavors of finite differentiation to approximate derivatives to be aware of:\n\nforward difference is as defined in the above equation, where \\(\\epsilon\\) is added to \\(x_0\\)\nforward difference is as defined in the above equation, where \\(\\epsilon\\) is subtracted from \\(x_0\\)\ncentral difference is where we evaluate at \\(x_0 \\pm \\epsilon\\) and then divide by \\(2\\epsilon\\)\n\nThe benefit of the central difference is that it limits issues around minima and maxima where the trough or peak respectively would seem much steeper if using forward or reverse. Here’s a picture of this:\n\n\n\n\n\nOne benefit of the central difference method is that is often more accurate than forward or reverse. However it comes at the cost of needing to evaluate the function an additional time in many circumstances. Take, for example, the process of optimizing a function to find a maxima or minima. The process usually involves evaluating a function at a gas determining what the derivative of the function is at that point and using both items to update to help better guess. At each step you need to evaluate the function three times for $x4, \\(x+\\epsilon\\), and \\(x-\\epsilon\\). With forward or reverse finite differences, you can reuse the prior function evaluation of the prior guess \\(x\\) As one of the components in the estimation of the derivative, thereby saving an evaluation of the function for each iteration.\nthere are further challenges with the finite differences method. In practice, we are often interested in much more complex functions than \\(x^2\\). For example, we may actually be interested in the sum of a series that is many elements long or contains more complex operations than basic algebra. In the prior example, the \\(\\epsilon\\) is set unusually wide for demonstration purposes. As \\(\\epsilon\\) grow smaller generally, the accuracy of all three finite different methods increases. However, that’s not always the case due to both the complexity of the function that you may be trying to differentiate or due to numerical inaccuracies of floating point math.\nTo demonstrate, here is amore complex example using an arbitrary function\nfor this example we’ll show the results of the three methods calculated at different values of \\(\\epsilon\\):\n\nusing DataFrames\n\n\nf(x) = exp(x)\nϵ = 10 .^ (range(-16, stop=0, length=100))\nx0 = 1\nestimate = @. (f(x0 + ϵ) - f(x0 - ϵ)) / 2ϵ\n1actual = f(x0)\n\nfig = Figure()\nax = Axis(fig[1, 1], xscale=log10, yscale=log10, xlabel=\"ϵ\", ylabel=\"absolute error\")\nscatter!(ax, ϵ, abs.(estimate .- actual))\nfig\n\n\n1\n\nThe derivative of \\(f(x) = exp(x)\\) is itself. That is \\(f'(x) = f(x)\\) in this special case.\n\n\n\n\n\n\n\nFigure 11.1: A log-log plot showing the absolute error of the finite differences. Further to the left, roundoff error dominates while further to the right, truncation error dominates.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe @. in the code example above is a macro that applies broadcasting each function to its right. @. (f(x0 + ϵ) - f(x0 - ϵ)) / 2ϵ is the same as (f.(x0 .+ ϵ) .- f.(x0 .- ϵ)) ./ (2 .* ϵ)\n\n\nA few observations:\n\nAt virtually every value of ϵ we observe some error from the true derivative.\nThat error is the sum of two parts: truncation error is inherent in that we are using a given value for ϵ and not determining the limiting analytic value as \\(\\epsilon \\to 0\\). The other component is roundoff error which arises due to the limited precision of floating point math.\n\nThe implications of this are that we need to often be careful about the choice of ϵ, as the optimal choice will vary depending on the function and the point we are attempting to evaluate. This presents a number of practical diffuclties in various algorithms.\nAdditionally,when computing the finite difference we must evaluate the function multiple times to determine a single estimate of the derivative. When performing something like optimization the process typically involves iteratively making many guesses — plus the number of guesses required to find the right answer can depends on the ability to accurate determine the derivative at a point!\nAdmittedly, despite the accuracy and computational overhead, finite differences can be very useful in many circumstances. However, a more appealing alternative approach will be covered next."
  },
  {
    "objectID": "autodiff.html#automatic-differentiation",
    "href": "autodiff.html#automatic-differentiation",
    "title": "11  Automatic Differentiation",
    "section": "11.4 Automatic Differentiation",
    "text": "11.4 Automatic Differentiation\nAutomatic differentiation (“autodiff” for short) is essentially the practice of defining algorithmically what the derivatives of function should be. We are able to do this through a creative application of the chain rule. Recall that the chain rule allows us to compute the derivative of a composite function using the derivatives of the component functions:\n\\[\nh(x)=f(g(x))\n\\] \\[\nh'(x) = f'(g(x)) g'(x)\n\\]\nUsing this rule, we can define how elementary operations act when differentiated. Combined with the fact that most computer code is building up from a bunch of elementary operations, we can get a very long way in differentiating complex functions.\n\n11.4.1 Dual Numbers\nTo understand where we are going, let’s remind ourselves about complex numbers. Complex numbers are of the form which has an real part (\\(r\\)) and an imaginary part (\\(iq\\)):\n\\[\nr+iq\n\\]\nBy definition we say that \\(i^2 = -1\\). This is useful because it allows us to perform certain types of operations (e.g. finding a square root of a negative number) that is otherwise unsolvable with just the real numbers1. After defining how the normal algebraic operations (addition, multiplication, etc.) work for the imaginary number, we are able to utilize the imaginary numbers for a variety of practical mathematical tasks.\nWhat is meant by extending the algebraic operations for imaginary numbers? For example, stating how addition should work for imaginary numbers:\n\\[\n(r+iq) + (s+iu) = (r+s) + i(q+u)\n\\]\nIn a similar fashion as extending the Real (\\(\\mathbb{R}\\)) numbers with an imaginary part, for automatic differentiation we will extend them with a dual part. A dual number is one of the form:\n\\[\na + \\epsilon b\n\\]\nWhere \\(\\epsilon^2 = 0\\) and \\(\\epsilon \\neq 0\\) by defintion. For our purposes here, one can think of \\(b\\) as the derivative of the function evaluated at the same point as \\(a\\). An intial example should make this clearer. First let’s define a DualNumber:\n\n1struct DualNumber{T,U}\n    a::T\n    b::U\n2    function DualNumber(a::T,b::U=zero(a)) where {T,U}\n        return new{T,U}(a,b)\n    end\nend\n\n\n1\n\nWe define this type parametrically to handle all sorts of &lt;:Real types and allow a and b to vary types in case a mathematical operation causes a type change (e.g. as in the case of integers becoming a floating point number like 10/4 == 2.5)\n\n2\n\nzero(a) is a generic way to create a value equal to zero with the same type of the argument a. zero(12.0) == 0.0 and zero(12) == 0.\n\n\n\n\nNow let’s define how dual numbers work under addition. The mathematical rule is:\n\\[\n(a+\\epsilon b)+(c+\\epsilon d)=(a+c)+(b+d)\\epsilon\n\\]\nWe then need to define how it works for the combinations of numbers that we might receive as arguments to our function (this is an example where multiple dispatch greatly simplifies the code compare to object oriented single dispatch!):\n\nBase.:+(d::DualNumber, e::DualNumber) = DualNumber(d.a + e.a, d.b + e.b)\nBase.:+(d::DualNumber, x) = DualNumber(d.a + x, d.b)\nBase.:+(x, d::DualNumber) = d + x\n\nAnd here’s how we would get the derivative of a very simple function:\n\nf1(x) = 5 + x\n\nf1(DualNumber(10, 1))\n\nDualNumber{Int64, Int64}(15, 1)\n\n\nThat’s not super interesting though - the derivative of f1 is just 1 and we supplied that in the construction of DualNumber. We did at least prove that we can add the 10 and 5!\nLet’s make this more interesting by also defining the multiplication operation on dual numbers. We’ll follow the product rule:\n\\[\n(u \\times v)' = u ' \\times v + u \\times v'\n\\]\n\nBase.:*(d::DualNumber, e::DualNumber) = DualNumber(d.a * g.a, d.b * g.a + d.a * g.b)\nBase.:*(x, d::DualNumber) = DualNumber(d.a * x, d.b * x)\nBase.:*(d::DualNumber, x) = x * d\n\nNow what if we evaluate this function:\n\nf2(x) = 5 + 3x\n\nf2(DualNumber(10, 1))\n\nDualNumber{Int64, Int64}(35, 3)\n\n\nWe have found that the second component is 3, which is indeed the derivative of \\(5+3x\\) with respect to \\(x\\). And in the first part we have the value of f2 evaluated at 10.\n\n\n\n\n\n\nNote\n\n\n\nWhen calcualting the derivative, why do we start with 1 in the dual part of the number? Because the derivative of a variable with respect to itself is 1. From this unitary starting point, the various operations applied accumulate the derivative of the various operations in the \\(b\\) part of \\(a + \\epsilon b\\).\n\n\nWe can also define this for things like transcendental functions:\n\nBase.exp(d::DualNumber) = DualNumber(exp(d.a), exp(d.a) * d.b)\nBase.sin(d::DualNumber) = DualNumber(sin(d.a), cos(d.a)*d.b)\nBase.cos(d::DualNumber) = DualNumber(cos(d.a), -sin(d.a)*d.b)\nexp(DualNumber(1, 1))\n\nDualNumber{Float64, Float64}(2.718281828459045, 2.718281828459045)\n\n\n\nsin(DualNumber(0, 1))\n\nDualNumber{Float64, Float64}(0.0, 1.0)\n\n\n\ncos(DualNumber(0, 1))\n\nDualNumber{Float64, Float64}(1.0, -0.0)"
  },
  {
    "objectID": "autodiff.html#performance-of-automatic-differentiation",
    "href": "autodiff.html#performance-of-automatic-differentiation",
    "title": "11  Automatic Differentiation",
    "section": "11.5 Performance of Automatic Differentiation",
    "text": "11.5 Performance of Automatic Differentiation\nRecall that in the finite difference method, we generally had to evalue the function two or three times to approximate the derivative. Here we have a single function call that provides both the value and the derivative at that value. How does this compare performance-wise to simply evaluating the function a single time?\n\nusing BenchmarkTools\n@btime f2(rand())\n\n  2.792 ns (0 allocations: 0 bytes)\n\n\n5.682095882664635\n\n\n\n@btime f2(DualNumber(rand(), 1))\n\n  2.750 ns (0 allocations: 0 bytes)\n\n\nDualNumber{Float64, Int64}(7.337165351509228, 3)\n\n\nIn performing this computation, the compiler has been able to optimize it such that we effectively are able to compute the function and its derivative at effetcitly the same speed as just the evaluating the function itself! As the function gets more complex, the overhead does increase but is still a much preferred option versus finite differentiation.\n\n\n\n\n\n\nNote\n\n\n\nIn fact, it’s largely due to the advances in applications of automatic differentiation that has led to the explosion of machine learning and artificial intelligence techniques in the 2010s/2020s. The “learning” process relies on solving parameter weights and would be too computationally expensive if using finite differences.\nThese applications of autodiffertiation in specialized C++ libraries underpin the libraries like PyTorch, Tensorfow, and Keras. These libraries specialize in allowing for autodiff on a limited subset of operations. Julia’s available automatic differentiation is more general and can be applied to many more scenarios."
  },
  {
    "objectID": "autodiff.html#forward-mode-and-reverse-mode",
    "href": "autodiff.html#forward-mode-and-reverse-mode",
    "title": "11  Automatic Differentiation",
    "section": "11.6 Forward Mode and Reverse Mode",
    "text": "11.6 Forward Mode and Reverse Mode"
  },
  {
    "objectID": "autodiff.html#automatic-differentiation-in-practice",
    "href": "autodiff.html#automatic-differentiation-in-practice",
    "title": "11  Automatic Differentiation",
    "section": "11.7 Automatic Differentiation in Practice",
    "text": "11.7 Automatic Differentiation in Practice"
  },
  {
    "objectID": "autodiff.html#references",
    "href": "autodiff.html#references",
    "title": "11  Automatic Differentiation",
    "section": "11.8 References",
    "text": "11.8 References\n\nhttps://book.sciml.ai/notes/08-Forward-Mode_Automatic_Differentiation_(AD)_via_High_Dimensional_Algebras/\nhttps://blog.esciencecenter.nl/automatic-differentiation-from-scratch-23d50c699555"
  },
  {
    "objectID": "autodiff.html#footnotes",
    "href": "autodiff.html#footnotes",
    "title": "11  Automatic Differentiation",
    "section": "",
    "text": "Richard Feynman has a wonderful, short lecture on algebra here: https://www.feynmanlectures.caltech.edu/I_22.html↩︎"
  },
  {
    "objectID": "optimization.html#in-this-chapter",
    "href": "optimization.html#in-this-chapter",
    "title": "12  Optimization",
    "section": "12.1 In This Chapter",
    "text": "12.1 In This Chapter\nOptimization as root finding or minimization/maximazation of defined objectives. Differentiable programming and the benefits to optimization problems. Model fitting as an optimization problem."
  },
  {
    "objectID": "sensitivity-analysis.html#in-this-chapter",
    "href": "sensitivity-analysis.html#in-this-chapter",
    "title": "13  Sensitivity Analysis",
    "section": "13.1 In This Chapter",
    "text": "13.1 In This Chapter\nDifferent approaches to understanding the sensitivity of a model to changes in its inputs: derivatives, finite differences, global sensitivity analysis approaches, and statistical approaches."
  },
  {
    "objectID": "sensitivity-analysis.html#setup",
    "href": "sensitivity-analysis.html#setup",
    "title": "13  Sensitivity Analysis",
    "section": "13.2 Setup",
    "text": "13.2 Setup\n\nusing CSV, DataFrames\nusing MortalityTables, Dates\nusing GlobalSensitivity\nusing QuasiMonteCarlo\nusing CairoMakie\n\n\n@enum Sex Female = 1 Male = 2\n@enum Risk Standard = 1 Preferred = 2\n\nmutable struct Policy\n    id::Int\n    sex::Sex\n    benefit_base::Float64\n    COLA::Float64\n    mode::Int\n    issue_date::Date\n    issue_age::Int\n    risk::Risk\nend"
  },
  {
    "objectID": "sensitivity-analysis.html#the-data",
    "href": "sensitivity-analysis.html#the-data",
    "title": "13  Sensitivity Analysis",
    "section": "13.3 The Data",
    "text": "13.3 The Data\n\nsample_csv_data =\n    IOBuffer(\n        raw\"id,sex,benefit_base,COLA,mode,issue_date,issue_age,risk\n         1,M,100000.0,0.03,12,1999-12-05,30,Std\"\n    )\n\nmort = Dict(\n    Male =&gt; MortalityTables.table(988).ultimate,\n    Female =&gt; MortalityTables.table(992).ultimate,\n)\n\nDict{Sex, OffsetArrays.OffsetVector{Float64, Vector{Float64}}} with 2 entries:\n  Male   =&gt; [0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.0225…\n  Female =&gt; [0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.0…\n\n\n\npolicies = let\n\n    # read CSV directly into a dataframe\n    # df = CSV.read(\"sample_inforce.csv\",DataFrame) # use local string for notebook\n    df = CSV.read(sample_csv_data, DataFrame)\n\n    # map over each row and construct an array of Policy objects\n    map(eachrow(df)) do row\n        Policy(\n            row.id,\n            row.sex == \"M\" ? Male : Female,\n            row.benefit_base,\n            row.COLA,\n            row.mode,\n            row.issue_date,\n            row.issue_age,\n            row.risk == \"Std\" ? Standard : Preferred,\n        )\n    end\n\nend\n\n1-element Vector{Policy}:\n Policy(1, Male, 100000.0, 0.03, 12, Date(\"1999-12-05\"), 30, Standard)\n\n\nGiven a basic insurance product, a pure whole of life (WOL) policy with level benefits and level premiums payable within the first 10 years, the reserve at the end of the \\(y^th\\) policy year is defined by\n\\[\nres(y) = \\sum_{t=age+y}^{120} (sur_(t-age-y) * mort_t * B_y * \\sqrt(1 + r)) - (P_y * sur_(t-age-y))\n\\]\nwhere\n\n\\(mort_t\\) is the mortality at age \\(t\\)\n\\(p_y\\) is the survival probability adjusted with COLA, with values of \\(p_(y-1) = 1\\) and \\(p_x = p_(x-1) * (1 - mort_(age+y)) / (1 + COLA)\\) for \\(x &gt;= y\\), and 0 for \\(x &lt; y - 1\\) or \\(age + x &gt;= 120\\), or ultimate age of the current mortality table\n\\(B_y\\) is the level benefit throughout the policy\n\\(P_y\\) is the level premium within the first 10 policy years which is 0 for policy years after 10\n\\(r\\) is the level interest rate throughout the policy\n\n\nfunction sur(y::Int, pol::Policy)\n    if y == 0\n        1\n    elseif y &lt; 0 || 120 - y &lt;= pol.issue_age\n        0\n    else\n        sur(y - 1, pol) * (1 - mort[pol.sex][pol.issue_age+y]) / (1 + pol.COLA)\n    end\nend\n\nfunction res(y::Int, pol::Policy, P::Float64)\n    s = 0.0\n    if y &gt;= 1 && y &lt;= 120 - pol.issue_age\n        for t in (pol.issue_age+y):120\n            prem = 0.0\n            if y &lt;= 9\n                prem = P\n            end\n            s += sur(t - pol.issue_age - y, pol) * mort[pol.sex][t] * pol.benefit_base - prem * sur(t - pol.issue_age - y, pol)\n        end\n    end\n    s\nend\n\nres (generic function with 1 method)"
  },
  {
    "objectID": "sensitivity-analysis.html#common-sensitivity-analysis-methodologies",
    "href": "sensitivity-analysis.html#common-sensitivity-analysis-methodologies",
    "title": "13  Sensitivity Analysis",
    "section": "13.4 Common Sensitivity Analysis Methodologies",
    "text": "13.4 Common Sensitivity Analysis Methodologies\n\n13.4.1 Finite Differences\nDefine a customized finite difference function with respect to the COLA, rippled by a small difference.\n\nfunction res_wrt_r_fd(y::Int, pol::Policy, P::Float64, r::Float64, h=1e-3)\n    p₊, p₋ = deepcopy(pol), deepcopy(pol)\n    p₊.COLA = r + h, p₋.COLA = r - h\n    (res(y, p₊, P) - res(y, p₋, P)) / (2h)\nend\n\nres_wrt_r_fd (generic function with 2 methods)\n\n\n\n\n13.4.2 Scenario Analyses\nScenarios can be generated following scenario generation methodologies to evaluate impacts. Refer to scenario generation chapter.\n\n\n13.4.3 Regression Analyses\n\nfunction r1_wrt_r(r)\n    p = deepcopy(policies[1])\n    p.COLA = r[2]\n    res(Int(floor(r[1])), p, r[3])\nend\n\ngsa(r1_wrt_r, RegressionGSA(), [[1, 1.01], [0.025, 0.035], [10000.0, 10000.1]], samples=1000)\n\nGlobalSensitivity.RegressionGSAResult{Matrix{Float64}, Nothing}([-0.002241307731791374 0.9997042921500664 0.0007396441619925974], [3.854653879225636e-5 0.9997044800328053 -0.0001211228928920829], [0.0015862542026721963 0.9997042955338378 -0.0038074548519504074], nothing, nothing, nothing)\n\n\n\n\n13.4.4 Sobol Indices\nSobol is a variance-based method, and it decomposes the variance of the output of the model or system into fractions which can be attributed to inputs or sets of inputs. This helps to get not just the individual parameter’s sensitivities, but also gives a way to quantify the affect and sensitivity from the interaction between the parameters.\nThe Sobol Indices are “order”ed, the first order indices given by ​ the contribution to the output variance of the main effect of ​ . Therefore, it measures the effect of varying ​ alone, but averaged over variations in other input parameters. It is standardized by the total variance to provide a fractional contribution. Higher-order interaction indices ​ and so on can be formed by dividing other terms in the variance decomposition by Var(Y).\n\nL, U = QuasiMonteCarlo.generate_design_matrices(1000, [1, 0.025, 10000.0], [1, 0.035, 10000.1], SobolSample())\ngsa(r1_wrt_r, Sobol(), L, U)\n\n┌ Warning: The `generate_design_matrices(n, d, sampler, R = NoRand(), num_mats)` method does not produces true and independent QMC matrices, see [this doc warning](https://docs.sciml.ai/QuasiMonteCarlo/stable/design_matrix/) for more context. \n│     Prefer using randomization methods such as `R = Shift()`, `R = MatousekScrambling()`, etc., see [documentation](https://docs.sciml.ai/QuasiMonteCarlo/stable/randomization/)\n└ @ QuasiMonteCarlo ~/.julia/packages/QuasiMonteCarlo/KvLfb/src/RandomizedQuasiMonteCarlo/iterators.jl:255\n\n\nGlobalSensitivity.SobolResult{Vector{Float64}, Nothing, Nothing, Nothing}([-0.0, 1.089514785129876, 3.0140159390210083e-6], nothing, nothing, nothing, [0.0, 1.0013980466757952, 1.689157120266108e-8], nothing)\n\n\n\n\n13.4.5 Morris Method\nThe Morris method also known as Morris’s OAT method where OAT stands for One At a Time can be described in the following steps:\n\\[\nEE_i = \\frac{f(x_1, x_2, ...x_i + \\Delta, ...x_k) - y}{\\Delta}\n\\]\nWe calculate local sensitivity measures known as “elementary effects”, which are calculated by measuring the perturbation in the output of the model on changing one parameter.\nThese are evaluated at various points in the input chosen such that a wide “spread” of the parameter space is explored and considered in the analysis, to provide an approximate global importance measure. The mean and variance of these elementary effects is computed. A high value of the mean implies that a parameter is important, a high variance implies that its effects are non-linear or the result of interactions with other inputs. This method does not evaluate separately the contribution from the interaction and the contribution of the parameters individually and gives the effects for each parameter which takes into consideration all the interactions and its individual contribution.\n\nm = gsa(r1_wrt_r, Morris(), [[1, 1.01], [0.025, 0.035], [10000.0, 10000.1]])\n\nGlobalSensitivity.MorrisResult{Matrix{Float64}, Vector{Any}}([0.0 1.3513320407605895e6 -17.576730275038422], [0.0 1.3513320407605895e6 17.576730275038422], [0.0 4.9896581144644375e9 0.3956906202412113], Any[[0.0, 0.0, 0.0, -0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0, 0.0, -0.0, 0.0, -0.0, 0.0, 0.0, -0.0], [1.3002655895784444e6, 1.3378403824510737e6, 1.2197281537930316e6, 1.2197281537930316e6, 1.3900360410116625e6, 1.3927090731585673e6, 1.3900508527040319e6, 1.4475015001578617e6, 1.4447015464301624e6, 1.3927090731585673e6, 1.392709559980656e6, 1.330222422261318e6, 1.3302228814768621e6, 1.3302233406929825e6], [-16.965650687577757, -16.96565078842253, -16.987959755437235, -16.987959755437235, -16.98795959696688, -16.98795959696688, -16.987959755437235, -17.32909547823331, -17.305967896574494, -17.305967896574494  …  -17.827983154089644, -17.827983154089644, -17.827983139683248, -17.827983067651267, -17.80364193440637, -17.23691904863985, -17.236918991014267, -17.25987977541934, -17.25987977541934, -17.236918991014267]])\n\n\n\n\n13.4.6 Fourier Amplitude Sensitivity Tests\n\ngsa(r1_wrt_r, eFAST(), [[1, 1.01], [0.025, 0.035], [10000.0, 10000.1]], samples=1000)\n\nGlobalSensitivity.eFASTResult{Matrix{Float64}}([4.045447350915167e-11 0.9976726587147 1.677140913998285e-8], [7.028027053657127e-7 0.9999974931512728 0.00233559363392144])"
  },
  {
    "objectID": "sensitivity-analysis.html#benchmarking",
    "href": "sensitivity-analysis.html#benchmarking",
    "title": "13  Sensitivity Analysis",
    "section": "13.5 Benchmarking",
    "text": "13.5 Benchmarking"
  },
  {
    "objectID": "stochastic.html#footnotes",
    "href": "stochastic.html#footnotes",
    "title": "14  Stochastic Modeling",
    "section": "",
    "text": "Kalos was a pioneer in Monte Carlo techniques, quoted via https://doi.org/10.1007/978-3-540-74686-7_3↩︎"
  },
  {
    "objectID": "visualization.html#in-this-chapter",
    "href": "visualization.html#in-this-chapter",
    "title": "15  Visiualizations",
    "section": "15.1 In This Chapter",
    "text": "15.1 In This Chapter\nThe evolved brain and pattern recognition, recommended principles for looking at data, and avoiding common mistakes. Exploratory visualization versus visualizations intended for an audience."
  },
  {
    "objectID": "matrices.html#in-this-chapter",
    "href": "matrices.html#in-this-chapter",
    "title": "16  Matrices and Their Uses",
    "section": "16.1 In This Chapter",
    "text": "16.1 In This Chapter\nMatrices and their myriad uses: reframing problems through the eyes of linear algebra, an intuitive refreshing on applicable maths, and recurring patterns of matrix operations in financial modeling."
  },
  {
    "objectID": "data-learning.html#in-this-chapter",
    "href": "data-learning.html#in-this-chapter",
    "title": "17  Learning from Data",
    "section": "17.1 In this chapter",
    "text": "17.1 In this chapter\nUsing data to inform a model: fitting parameters, forecasting, and fundamental limitations on prediction. Also covered are elements of practical review such as static and dynamic validations, and implied rate analysis."
  },
  {
    "objectID": "stochastic-mortality.html#in-this-chapter",
    "href": "stochastic-mortality.html#in-this-chapter",
    "title": "18  Stochastic Mortality Projections",
    "section": "18.1 In This Chapter",
    "text": "18.1 In This Chapter\nA term life insurance policy is used to illustrate: selecting key model features, design tradeoffs between a few different approaches, and a discussion of the performance impacts of the different approaches to parallelism."
  },
  {
    "objectID": "stochastic-mortality.html#setup",
    "href": "stochastic-mortality.html#setup",
    "title": "18  Stochastic Mortality Projections",
    "section": "18.2 Setup",
    "text": "18.2 Setup\n\nusing CSV, DataFrames\nusing MortalityTables, ActuaryUtilities\nusing Dates\nusing ThreadsX\nusing BenchmarkTools\nusing Random\nusing CairoMakie\n\nDefine a datatype. Not strictly necessary, but will make extending the program with more functions easier.\nType annotations are optional, but providing them is able to coerce the values to be all plain bits (i.e. simple, non-referenced values like arrays are) when the type is constructed. This makes the whole data be stored in the stack and is an example of data-oriented design. It’s much slower without the type annotations (~0.5 million policies per second, ~50x slower).\n\n@enum Sex Female = 1 Male = 2\n@enum Risk Standard = 1 Preferred = 2\n\n\nstruct Policy\n    id::Int\n    sex::Sex\n    benefit_base::Float64\n    COLA::Float64\n    mode::Int\n    issue_date::Date\n    issue_age::Int\n    risk::Risk\nend"
  },
  {
    "objectID": "stochastic-mortality.html#the-data",
    "href": "stochastic-mortality.html#the-data",
    "title": "18  Stochastic Mortality Projections",
    "section": "18.3 The Data",
    "text": "18.3 The Data\n\nsample_csv_data =\n    IOBuffer(\n        raw\"id,sex,benefit_base,COLA,mode,issue_date,issue_age,risk\n         1,M,100000.0,0.03,12,1999-12-05,30,Std\n         2,F,200000.0,0.03,12,1999-12-05,30,Pref\"\n    )\n\nIOBuffer(data=UInt8[...], readable=true, writable=false, seekable=true, append=false, size=152, maxsize=Inf, ptr=1, mark=-1)\n\n\n\npolicies = let\n\n    # read CSV directly into a dataframe\n    # df = CSV.read(\"sample_inforce.csv\",DataFrame) # use local string for notebook\n    df = CSV.read(sample_csv_data, DataFrame)\n\n    # map over each row and construct an array of Policy objects\n    map(eachrow(df)) do row\n        Policy(\n            row.id,\n            row.sex == \"M\" ? Male : Female,\n            row.benefit_base,\n            row.COLA,\n            row.mode,\n            row.issue_date,\n            row.issue_age,\n            row.risk == \"Std\" ? Standard : Preferred,\n        )\n    end\n\n\nend\n\n2-element Vector{Policy}:\n Policy(1, Male, 100000.0, 0.03, 12, Date(\"1999-12-05\"), 30, Standard)\n Policy(2, Female, 200000.0, 0.03, 12, Date(\"1999-12-05\"), 30, Preferred)\n\n\nDefine what mortality gets used:\n\nmort = Dict(\n    Male =&gt; MortalityTables.table(988).ultimate,\n    Female =&gt; MortalityTables.table(992).ultimate,\n)\n\nfunction mortality(pol::Policy, params)\n    return params.mortality[pol.sex]\nend\n\nmortality (generic function with 1 method)\n\n\nThis defines the core logic of the policy projection and will write the results to the given out container (here, a named tuple of arrays).\nThis is using a threaded approach where it could be operating on any of the computer’s available threads, thus acheiving thread-based parallelism - as opposed to multi-processor (multi-machine) or GPU-based computation, which requires formulating the problem a bit differently (array/matrix based). For the scale of computation here, I think I’d apply this model of parallelism.\n\nfunction pol_project!(out, policy, params)\n    # some starting values for the given policy\n    dur = duration(policy.issue_date, params.val_date)\n    start_age = policy.issue_age + dur - 1\n    COLA_factor = (1 + policy.COLA)\n    cur_benefit = policy.benefit_base * COLA_factor^(dur - 1)\n\n    # get the right mortality vector\n    qs = mortality(policy, params)\n\n    # grab the current thread's id to write to results container without conflicting with other threads\n    tid = Threads.threadid()\n\n    ω = lastindex(qs)\n\n    # inbounds turns off bounds-checking, which makes hot loops faster but first write loop without it to ensure you don't create an error (will crash if you have the error without bounds checking)\n    @inbounds for t in 1:min(params.proj_length, ω - start_age)\n\n        q = qs[start_age+t] # get current mortality\n\n        if (rand() &lt; q)\n            return # if dead then just return and don't increment the results anymore\n        else\n            # pay benefit, add a life to the output count, and increment the benefit for next year\n            out.benefits[t, tid] += cur_benefit\n            out.lives[t, tid] += 1\n            cur_benefit *= COLA_factor\n        end\n    end\nend\n\npol_project! (generic function with 1 method)\n\n\nParameters for our projection:\n\nparams = (\n    val_date=Date(2021, 12, 31),\n    proj_length=100,\n    mortality=mort,\n)\n\n(val_date = Date(\"2021-12-31\"), proj_length = 100, mortality = Dict{Sex, OffsetArrays.OffsetVector{Float64, Vector{Float64}}}(Male =&gt; [0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571, 0.022571  …  0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4], Female =&gt; [0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745, 0.00745  …  0.376246, 0.386015, 0.393507, 0.398308, 0.4, 0.4, 0.4, 0.4, 0.4, 1.0]))\n\n\nCheck the number of threads we’re using:\n\nThreads.nthreads()\n\n1\n\n\n\nfunction project(policies, params)\n    threads = Threads.nthreads()\n    benefits = zeros(params.proj_length, threads)\n    lives = zeros(Int, params.proj_length, threads)\n    out = (; benefits, lives)\n    ThreadsX.foreach(policies) do pol\n        pol_project!(out, pol, params)\n    end\n    map(x -&gt; vec(reduce(+, x, dims=2)), out)\nend\n\nproject (generic function with 1 method)"
  },
  {
    "objectID": "stochastic-mortality.html#running-the-projection",
    "href": "stochastic-mortality.html#running-the-projection",
    "title": "18  Stochastic Mortality Projections",
    "section": "18.4 Running the projection",
    "text": "18.4 Running the projection\nExample of a single projection:\n\nproject(repeat(policies, 100_000), params)\n\n(benefits = [5.62809389870499e10, 5.671120575591999e10, 5.710545531792687e10, 5.742479100991753e10, 5.7683856246968056e10, 5.787524078247475e10, 5.7944056315703896e10, 5.8042444069520454e10, 5.8033418423730675e10, 5.7957862597255005e10  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [195219, 190405, 185551, 180604, 175596, 170466, 165145, 160061, 154817, 149559  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\n18.4.1 Stochastic Projection\nLoop through and calculate the reults n times (this is only running the two policies in the sample data” n times).\n\nfunction stochastic_proj(policies, params, n)\n\n    ThreadsX.map(1:n) do i\n        project(policies, params)\n    end\nend\n\nstochastic_proj (generic function with 1 method)\n\n\n\nstoch = stochastic_proj(policies, params, 1000)\n\n1000-element Vector{@NamedTuple{benefits::Vector{Float64}, lives::Vector{Int64}}}:\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 431318.2535087685, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 394717.3022253211, 406558.82129208074, 418755.5859308432, 431318.2535087685, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 394717.3022253211, 406558.82129208074, 418755.5859308432, 431318.2535087685, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 203279.41064604037, 209377.7929654216, 215659.12675438426, 222128.9005570158, 228792.76757372628, 235656.55060093806, 242726.24711896622, 250008.0345325352  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n ⋮\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 431318.2535087685, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 242726.24711896622, 250008.0345325352  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [383220.68177215644, 394717.3022253211, 406558.82129208074, 418755.5859308432, 431318.2535087685, 444257.8011140316, 457585.53514745255, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 471313.1012018761, 485452.49423793243, 500016.0690650704  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 1, 1, 1  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n (benefits = [574831.0226582347, 592075.9533379817, 609838.2319381211, 628133.3788962648, 646977.3802631528, 666386.7016710474, 686378.3027211789, 706969.6518028142, 728178.7413568987, 750024.1035976056  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], lives = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n\n\nlet\n    v = [pv(0.03, s.benefits) for s in stoch]\n    hist(v,\n        bins=15,\n        xlabel=\"Present Value of Benefits\",\n        ylabel=\"Number of scenarios\")\nend"
  },
  {
    "objectID": "stochastic-mortality.html#benchmarking",
    "href": "stochastic-mortality.html#benchmarking",
    "title": "18  Stochastic Mortality Projections",
    "section": "18.5 Benchmarking",
    "text": "18.5 Benchmarking\nUsing a 2022 Macbook Air M2 laptop, about 30 million policies able to be stochastically projected per second:\n\npolicies_to_benchmark = 3_000_000\n# adjust the `repeat` depending on how many policies are already in the array\n# to match the target number for the benchmark\nn = policies_to_benchmark ÷ length(policies)\n\n@benchmark project(p, r) setup = (p = repeat($policies, $n); r = $params)\n\n\nBenchmarkTools.Trial: 16 samples with 1 evaluation.\n Range (min … max):  274.338 ms … 282.284 ms  ┊ GC (min … max): 0.00% … 0.00%\n Time  (median):     276.030 ms               ┊ GC (median):    0.00%\n Time  (mean ± σ):   276.416 ms ±   1.812 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%\n  ▁  ▁  ▁  ██▁  █ █  ▁▁        ▁                              ▁  \n  █▁▁█▁▁█▁▁███▁▁█▁█▁▁██▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█ ▁\n  274 ms           Histogram: frequency by time          282 ms &lt;\n Memory estimate: 9.28 KiB, allocs estimate: 79."
  },
  {
    "objectID": "stochastic-mortality.html#further-optimization",
    "href": "stochastic-mortality.html#further-optimization",
    "title": "18  Stochastic Mortality Projections",
    "section": "18.6 Further Optimization",
    "text": "18.6 Further Optimization\nIn no particular order:\n\nthe RNG could be made faster: https://bkamins.github.io/julialang/2020/11/20/rand.html\nCould make the stochastic set distributed, but at the current speed the overhead of distributed computing is probably more time than it would save. Same thing with GPU projections\n…"
  },
  {
    "objectID": "scenario-generation.html#in-this-chapter",
    "href": "scenario-generation.html#in-this-chapter",
    "title": "19  Scenario Generation",
    "section": "19.1 In This Chapter",
    "text": "19.1 In This Chapter\nHow to generate synthetic data for your model using sub-models, with applications to economic scenario generation and portfolio composition."
  },
  {
    "objectID": "scenario-generation.html#setup",
    "href": "scenario-generation.html#setup",
    "title": "19  Scenario Generation",
    "section": "19.2 Setup",
    "text": "19.2 Setup\n\nusing CSV, DataFrames\nusing Random\nusing StatsBase, Distributions\nusing CairoMakie"
  },
  {
    "objectID": "scenario-generation.html#the-data",
    "href": "scenario-generation.html#the-data",
    "title": "19  Scenario Generation",
    "section": "19.3 The Data",
    "text": "19.3 The Data"
  },
  {
    "objectID": "scenario-generation.html#pseudo-random-number-generators",
    "href": "scenario-generation.html#pseudo-random-number-generators",
    "title": "19  Scenario Generation",
    "section": "19.4 Pseudo Random Number Generators",
    "text": "19.4 Pseudo Random Number Generators\nModern computers utilize Pseudo random number generators (PRNGs) to generate random-like numbers. PRNGs are algorithms used to generate sequences of numbers that appear to be random but are actually determined by an initial value, known as the seed. These generators are called “pseudo-random” because the sequences they produce are deterministic; if you provide the same seed, you’ll get the same sequence of numbers. In addition, they have a finite period, which means that after a certain number of generated values, the sequence will repeat. It’s important to choose or design PRNGs with a long enough period for practical applications.\n\n19.4.1 Common PRNGs\n\n19.4.1.1 Mersenne Twister\nOne of the strengths of the Mersenne Twister is its exceptionally long period. The period is \\(2^(19937)-1\\), which means it can generate \\(2^(19937)-1\\) pseudo random numbers before repeating. This long period is crucial for applications requiring a large number of independent random numbers. It is also known for its good statistical properties. It passes many standard tests for randomness and provides a relatively uniform distribution of random numbers. Moreover, it is designed to allow multiple independent instances to be used concurrently without interfering with each other. This makes it suitable for parallel computing. Although there are faster generators for specific use cases, the Mersenne Twister is still often favored for its balance between speed and quality.\n\n\n19.4.1.2 Xorshift\nXorshift is a family of PRNGs known for their simplicity and relatively fast operation. The name “xorshift” comes from the bitwise XOR (exclusive or) and bit-shifting operations that are the core of the algorithm. Xorshift generators are often used in applications where speed is a priority and cryptographic-strength randomness is not a strict requirement. Xorshift PRNGs use bitwise XOR, left shifts, and right shifts to update the internal state and generate pseudo-random numbers. The basic idea is to repeatedly apply these operations to the state to produce a sequence of numbers. The period of a typical xorshift generator is relatively short compared to some other PRNGs like the Mersenne Twister. However, there are variations of xorshift algorithms that can have longer periods. One of the main advantages of xorshift is its simplicity and speed. The bitwise XOR and bit-shifting operations can be efficiently implemented in hardware, making xorshift generators suitable for applications where fast random number generation is crucial.\n\n\n19.4.1.3 Xoshiro\nXoshiro is a family of PRNGs known for their high performance and good statistical properties. The name “Xoshiro” is derived from the Japanese word “xoroshiro,” meaning “random.” Xoshiro algorithms, including Xoshiro128 and others, use a combination of bitwise XOR, bit-shifting, and addition operations. They often have more complex update rules than basic Xorshift algorithms. In addition, they typically have longer periods, making them suitable for applications that require more pseudo-random numbers before repetition.\n\n\n\n19.4.2 Consistent Interface\nJulia offers a consistent interface for random numbers due to its design and multiple dispatch principles. Consider the following random numbers in different data types.\n\nrng = MersenneTwister(1234)\nrand(Int, (2, 3))\n\n2×3 Matrix{Int64}:\n -8788881637902080812   7805220704166088096   5328119586588015311\n -9170382792654796722  -7222324368677312524  -8710431996781894498\n\n\n\nrng = MersenneTwister(1234)\nrand(Float64, (2, 3))\n\n2×3 Matrix{Float64}:\n 0.473678  0.113855  0.490048\n 0.70316   0.900335  0.0797795\n\n\n\nrng = Xoshiro(1234)\nrand(Bool, (2, 3))\n\n2×3 Matrix{Bool}:\n 1  0  0\n 0  0  0"
  },
  {
    "objectID": "scenario-generation.html#common-economic-scenario-generation-approaches",
    "href": "scenario-generation.html#common-economic-scenario-generation-approaches",
    "title": "19  Scenario Generation",
    "section": "19.5 Common Economic Scenario Generation Approaches",
    "text": "19.5 Common Economic Scenario Generation Approaches\nEconomic scenario generation involves the development of plausible future economic scenarios to assess the potential impact on financial portfolios, investments, or decision-making processes. Various approaches are used to generate economic scenarios, including stochastic differential equations (SDEs) and Monte Carlo simulations.\n\n19.5.1 Interest Rate Models\n\n19.5.1.1 Vasicek and Cox Ingersoll Ross (CIR)\nThe Vasicek model is a one-factor model commonly used for simulating interest rate scenarios. It describes the dynamics of short-term interest rates using a stochastic differential equation (SDE). In a Monte Carlo simulation, we can use the Vasicek model to generate multiple interest rate paths. The CIR model is an extension of the Vasicek model with non-constant volatility. It addresses the issue of negative interest rates by ensuring that interest rates remain positive. Vasicek is defined as\n\\[\ndr(t) = \\kappa (\\theta - r(t)) \\, dt + \\sigma \\, dW(t)\n\\]\nwhere\n\n\\(r(t)\\) is the short-term interest rate at time \\(t\\).\n\\(κ\\) is the speed of mean reversion, representing how quickly the interest rate reverts to its long-term mean.\n\\(θ\\) is the long-term mean or equilibrium level of the interest rate.\n\\(σ\\) is the volatility of the interest rate.\n\\(dW(t)\\) is a Wiener process or Brownian motion, representing a random shock.\n\nAnd CIR is defined as\n\\[\ndr(t) = \\kappa (\\theta - r(t)) \\, dt + \\sigma \\sqrt{r(t)} \\, dW(t)\n\\]\nwhere\n\n\\(r(t)\\) is the short-term interest rate at time \\(t\\).\n\\(κ\\) is the speed of mean reversion, representing how quickly the interest rate reverts to its long-term mean.\n\\(θ\\) is the long-term mean or equilibrium level of the interest rate.\n\\(σ\\) is the volatility of the interest rate.\n\\(dW(t)\\) is a Wiener process or Brownian motion, representing a random shock.\n\nThe following code shows a simplified implementation of a CIR model. The specification of \\(dr\\) can be changed to become a Vasicek model.\n\n# Set seed for reproducibility\nRandom.seed!(1234)\n\n# CIR model parameters\nκ = 0.2       # Speed of mean reversion\nθ = 0.05      # Long-term mean\nσ = 0.1       # Volatility\n\n# Initial short-term interest rate\nr₀ = 0.03\n\n# Number of time steps and simulations\nnum_steps = 252\nnum_simulations = 1_000\n\n# Time increment\nΔt = 1/252\n\n# Function to simulate CIR process\nfunction cir_simulation(κ, θ, σ, r₀, Δt, num_steps, num_simulations)\n    interest_rate_paths = zeros(num_steps, num_simulations)\n    for j in 1:num_simulations\n        interest_rate_paths[1, j] = r₀\n        for i in 2:num_steps\n            dW = randn() * sqrt(Δt)\n            # for Vasicek\n            # dr = κ * (θ - interest_rate_paths[i-1, j]) * Δt + σ * dW\n            dr = κ * (θ - interest_rate_paths[i-1, j]) * Δt + σ * sqrt(interest_rate_paths[i-1, j]) * dW\n            interest_rate_paths[i, j] = max(interest_rate_paths[i-1, j] + dr, 0)  # Ensure non-negativity\n        end\n    end\n    return interest_rate_paths\nend\n\n# Run CIR simulation\ncir_paths = cir_simulation(κ, θ, σ, r₀, Δt, num_steps, num_simulations)\n\n# Plot the simulated interest rate paths\nf = Figure()\nAxis(f[1, 1])\nfor i in 1:num_simulations\n    lines!(1:num_steps, cir_paths[:, i])\nend\nf\n\n\n\n\n\n\n19.5.1.2 Hull White\nThe Hull-White model is a one-factor model that extends the Vasicek model by allowing the mean reversion and volatility parameters to be time-dependent. It is commonly used for pricing interest rate derivatives. Brace-Gatarek-Musiela (BGM) Model extends the Hull-White model to incorporate more factors. It is one of the Libor Market Model (LMM) that describes the evolution of forward rates. It allows for the modeling of both the short-rate and the entire yield curve. It is defined as\n\\[\ndr(t) = (\\theta(t) - a r(t)) \\, dt + \\sigma(t) \\, dW(t)\n\\]\nwhere\n\n\\(r(t)\\) is the short-term interest rate at time \\(t\\).\n\\(θ\\) is the long-term mean or equilibrium level of the interest rate.\n\\(a\\) is the speed of mean reversion.\n\\(σ(t)\\) is the time-dependent volatility of the interest rate.\n\\(dW(t)\\) is a Wiener process or Brownian motion, representing a random shock.\n\n\n# Set seed for reproducibility\nRandom.seed!(1234)\n\n# Hull-White model parameters\nα = 0.1       # Mean reversion speed\nσ = 0.02      # Volatility\nr₀ = 0.03     # Initial short-term interest rate\n\n# Number of time steps and simulations\nnum_steps = 252\nnum_simulations = 1_000\n\n# Time increment\nΔt = 1/252\n\n# Function to simulate Hull-White process\nfunction hull_white_simulation(α, σ, r₀, Δt, num_steps, num_simulations)\n    interest_rate_paths = zeros(num_steps, num_simulations)\n    for j in 1:num_simulations\n        interest_rate_paths[1, j] = r₀\n        for i in 2:num_steps\n            dW = randn() * sqrt(Δt)\n            dr = α * (σ - interest_rate_paths[i-1, j]) * Δt + σ * dW\n            interest_rate_paths[i, j] = interest_rate_paths[i-1, j] + dr\n        end\n    end\n    return interest_rate_paths\nend\n\n# Run Hull-White simulation\nhull_white_paths = hull_white_simulation(α, σ, r₀, Δt, num_steps, num_simulations)\n\n# Plot the simulated interest rate paths\nf = Figure()\nAxis(f[1, 1])\nfor i in 1:num_simulations\n    lines!(1:num_steps, hull_white_paths[:, i])\nend\nf\n\n\n\n\n\n\n\n19.5.2 Stock Models\n\n19.5.2.1 Geometric Brownian Motion (GBM)\nGBM is a stochastic process commonly used to model the price movement of financial instruments, including stocks. It assumes constant volatility and is characterized by a log-normal distribution. It is defined as\n\\[\ndS(t) = \\mu S(t) \\, dt + \\sigma S(t) \\, dW(t)\n\\]\nwhere\n\n\\(S(t)\\) is the stock price at time \\(t\\).\n\\(μ\\) is the drift coefficient (expected return).\n\\(σ\\) is the volatility coefficient.\n\\(dW(t)\\) is a Wiener process or Brownian motion, representing a random shock.\n\n\n# Set seed for reproducibility\nRandom.seed!(1234)\n\n# GBM parameters\nμ = 0.05       # Drift (expected return)\nσ = 0.2        # Volatility\n\n# Initial stock price\nS₀ = 100\n\n# Number of time steps and simulations\nnum_steps = 252\nnum_simulations = 1_000\n\n# Time increment\nΔt = 1/252\n\n# Function to simulate GBM\nfunction gbm_simulation(μ, σ, S₀, Δt, num_steps, num_simulations)\n    stock_price_paths = zeros(num_steps, num_simulations)\n    for j in 1:num_simulations\n        stock_price_paths[1, j] = S₀\n        for i in 2:num_steps\n            dW = randn() * sqrt(Δt)\n            dS = μ * S₀ * Δt + σ * S₀ * dW\n            stock_price_paths[i, j] = stock_price_paths[i-1, j] + dS\n        end\n    end\n    return stock_price_paths\nend\n\n# Run GBM simulation\ngbm_paths = gbm_simulation(μ, σ, S₀, Δt, num_steps, num_simulations)\n\n# Plot the simulated stock price paths\nf = Figure()\nAxis(f[1, 1])\nfor i in 1:num_simulations\n    lines!(1:num_steps, gbm_paths[:, i])\nend\nf\n\n\n\n\n\n\n19.5.2.2 Generalized Autoregressive Conditional Heteroskedasticity (GARCH)\nGARCH models capture time-varying volatility. They are often used in conjunction with other models to forecast volatility. It is defined as\n\\[\n\\sigma^2_t = \\omega + \\alpha_1 r^2_{t-1} + \\beta_1 \\sigma^2_{t-1}\n\\]\n\\[\nr_t = \\varepsilon_t \\sqrt{\\sigma^2_t}\n\\]\n\n\\(σ^2_t\\) is the conditional variance at time \\(t\\)\n\\(r_t\\) is the return at time \\(t\\)\n\\(\\varepsilon_t\\) is a white noise or innovation process\n\\(\\omega\\), \\(\\alpha_1\\), \\(\\beta_1\\) are model parameters\n\n\n# Set seed for reproducibility\nRandom.seed!(1234)\n\n# GARCH(1,1) parameters\nα₀ = 0.01      # Constant term\nα₁ = 0.1       # Coefficient for lagged squared returns\nβ₁ = 0.8       # Coefficient for lagged conditional volatility\n\n# Number of time steps and simulations\nnum_steps = 252\nnum_simulations = 1_000\n\n# Time increment\nΔt = 1/252\n\n# Function to simulate GARCH(1,1) volatility\nfunction garch_simulation(α₀, α₁, β₁, num_steps, num_simulations)\n    volatility_paths = zeros(num_steps, num_simulations)\n    for j in 1:num_simulations\n        ε = randn(num_steps)\n        squared_returns = zeros(num_steps)\n        for i in 2:num_steps\n            squared_returns[i] = α₀ + α₁ * ε[i-1]^2 + β₁ * squared_returns[i-1]\n            volatility_paths[i, j] = sqrt(squared_returns[i])\n        end\n    end\n    return volatility_paths\nend\n\n# Run GARCH simulation\ngarch_paths = garch_simulation(α₀, α₁, β₁, num_steps, num_simulations)\n\n# Plot the simulated volatility paths\nf = Figure()\nAxis(f[1, 1])\nfor i in 1:num_simulations\n    lines!(1:num_steps, garch_paths[:, i])\nend\nf\n\n\n\n\n\n\n\n19.5.3 Copulas\nSimulating data using copulas involves generating multivariate samples with specified marginal distributions and a copula structure.\n\n# Set seed for reproducibility\nRandom.seed!(1234)\n\n# Marginal distributions (e.g., normal)\nmarginal1 = Normal(0, 1)\nmarginal2 = Normal(0, 1)\n\n# Clayton copula parameters\ntheta = 0.5\n\n# Number of data points\nnum_points = 1000\n\n# Generate independent samples from marginals\nu1 = rand(marginal1, num_points)\nu2 = rand(marginal2, num_points)\n\n# Clayton copula simulation\nfunction clayton_copula_simulation(u1, u2, theta)\n    v1 = u1\n    v2 = u2 .* ((theta .* u1).^(-1/theta - 1))\n    return v1, v2\nend\n\n# Simulate Clayton copula\nv1, v2 = clayton_copula_simulation(u1, u2, theta)\n\n# Plot the simulated bivariate data\nf = Figure()\nAxis(f[1, 1])\nscatter!(v1, v2)\nf"
  },
  {
    "objectID": "scenario-generation.html#benchmarking",
    "href": "scenario-generation.html#benchmarking",
    "title": "19  Scenario Generation",
    "section": "19.6 Benchmarking",
    "text": "19.6 Benchmarking"
  },
  {
    "objectID": "similarity-calculation.html#in-this-chapter",
    "href": "similarity-calculation.html#in-this-chapter",
    "title": "20  Similarity Analysis",
    "section": "20.1 In This Chapter",
    "text": "20.1 In This Chapter\nGiven a set of interest, understanding the relative similarity (or not) of features of interest is useful in classification and data compression techniques."
  },
  {
    "objectID": "similarity-calculation.html#setup",
    "href": "similarity-calculation.html#setup",
    "title": "20  Similarity Analysis",
    "section": "20.2 Setup",
    "text": "20.2 Setup\n\nusing CSV, DataFrames\nusing LinearAlgebra\nusing StatsBase, TableTransforms\nusing CairoMakie\nusing NearestNeighbors"
  },
  {
    "objectID": "similarity-calculation.html#the-data",
    "href": "similarity-calculation.html#the-data",
    "title": "20  Similarity Analysis",
    "section": "20.3 The Data",
    "text": "20.3 The Data\nStored data can generally be categorized into two formats: tabular (structured) and non-tabular (unstructured). Structured data format is a structured way of organizing and presenting data in rows and columns, resembling a table. This format is widely used for storing and representing structured datasets, making it easy to read, analyze, and manipulate data. The most common example of structured data is a spreadsheet, where data is organized into rows and columns. Structured data can also be stored in relational databases for easier lookups and matching. On the other hand, unstructured data refers to data that lacks a predefined data model or structure. Unlike structured data, which fits neatly into tables or databases, unstructured data does not have a predefined schema. It can include text documents, images, audio files, video files, social media posts, and more.\nStructured data can be further categorized into numerical and categorical data based on the types of values they represent. The following data tables will be referenced throughout the chapter. Real numerical data can easily be converted or normalized to a series of floating points, and real categorical data to a series of binary literals through one-hot encoding procedures.\n\nsample_csv_data =\n    IOBuffer(\n        raw\"id,sex,benefit_base,education,occupation,issue_age\n         1,M,100000.0,college,1,30.0\n         2,F,200000.0,master,3,20.0\n         3,M,150000.0,high_school,4,40.0\n         4,F,50000.0,college,2,60.0\n         5,M,250000.0,college,1,40.0\n         6,F,200000.0,high_school,2,30.0\"\n    )\n\nIOBuffer(data=UInt8[...], readable=true, writable=false, seekable=true, append=false, size=278, maxsize=Inf, ptr=1, mark=-1)\n\n\n\ndf = CSV.read(sample_csv_data, DataFrame)\ndf_num = apply(MinMax(), df[:, [:benefit_base, :issue_age]])[1]\n\n6×2 DataFrame\n\n\n\nRow\nbenefit_base\nissue_age\n\n\n\nFloat64\nFloat64\n\n\n\n\n1\n0.25\n0.25\n\n\n2\n0.75\n0.0\n\n\n3\n0.5\n0.5\n\n\n4\n0.0\n1.0\n\n\n5\n1.0\n0.5\n\n\n6\n0.75\n0.25\n\n\n\n\n\n\n\narr_cat = hcat(indicatormat(df.sex)', indicatormat(df.education)', indicatormat(df.occupation)')\n\n6×9 Matrix{Bool}:\n 0  1  1  0  0  1  0  0  0\n 1  0  0  0  1  0  0  1  0\n 0  1  0  1  0  0  0  0  1\n 1  0  1  0  0  0  1  0  0\n 0  1  1  0  0  1  0  0  0\n 1  0  0  1  0  0  1  0  0\n\n\nFor unstructured data, due to the nature of their variety, the choice of representation depends on the type of data and the specific task at hand. For text data, a Word2Vec embedding is commonly used, while Convolutional Neural Networks (CNNs) are for image data and wave transforms are for audio data. No matter which transformation is applied, unstructured data can generally be converted to a series of floating points, just like numerical structured data."
  },
  {
    "objectID": "similarity-calculation.html#common-similarity-measures",
    "href": "similarity-calculation.html#common-similarity-measures",
    "title": "20  Similarity Analysis",
    "section": "20.4 Common Similarity Measures",
    "text": "20.4 Common Similarity Measures\nThe following measures are commonly used to calculate similarities.\n\n20.4.1 Euclidean Distance (L2 norm)\nEuclidean distance, also known as the L2 norm, is defined as \\[\nd = \\sqrt{\\sum_{i=1}^{n} (w_i - v_i)^2}\n\\] The distance is usually meaningful when applied to numerical data. The following Julia code shows the Euclidean distance for the first two rows in df_num.\n\n#d₁₂ = √(∑((Array(df_num[1, :]) .- Array(df_num[2, :])) .* (Array(df_num[1, :]) .- Array(df_num[2, :]))))\nd₁₂ = LinearAlgebra.norm(Array(df_num[1, :]) .- Array(df_num[2, :]))\n\n0.5590169943749475\n\n\n\n\n20.4.2 Manhattan Distance (L1 Norm)\nManhattan distance, also known as the L1 norm, is defined as \\[\nd = \\sum_{i=1}^{n} |w_i - v_i|\n\\] The distance is also usually meaningful when applied to numerical data. The following Julia code shows the Euclidean distance for the first two rows in df_num.\n\n#d₁₂ = ∑(abs.(Array(df_num[1, :]) .- Array(df_num[2, :])))\nd₁₂ = LinearAlgebra.norm1(Array(df_num[1, :]) .- Array(df_num[2, :]))\n\n0.75\n\n\n\n\n20.4.3 Cosine Similarity\nCosine similarity is defined as \\[\nd = \\frac{\\sum_{i=1}^{n} w_i \\cdot v_i}{\\sqrt{\\sum_{i=1}^{n} w_i^2} \\cdot \\sqrt{\\sum_{i=1}^{n} v_i^2}}\n\\] The distance would be meaningful when applied to both numerical and categorical data.\nThe following Julia code shows the cosine similarity for the first two rows in df_num.\n\nd₁₂ = (Array(df_num[1, :]) ⋅ Array(df_num[2, :])) / norm(df_num[1, :]) / norm(df_num[2, :])\n\n0.7071067811865475\n\n\nThe following Julia code shows the cosine similarity for the first and the third rows in arr_cat.\n\nd₁₃ = (arr_cat[1, :] ⋅ arr_cat[3, :]) / norm(arr_cat[1, :]) / norm(arr_cat[3, :])\n\n0.33333333333333337\n\n\nNote how similar the syntax of processing for numerical or categorical data is. Multiple dispatch allows Julia to identify most efficient underlying procedure for different types of data. For categorical data, the \\(dot\\) operation on binary vectors is essentially count of 1’s, while for numerical data it is the \\(dot\\) operation for most numerical processing libraries.\n\n\n20.4.4 Jaccard Similarity\nJaccard similarity is defined as \\[\nd = \\frac{|W \\cap V|}{|W \\cup V|}\n\\] The distance is usually meaningful when applied to categorical data. The following Julia code shows the Jaccard similarity for the first and the third rows in arr_cat.\n\nd₁₃ = (arr_cat[1, :] ⋅ arr_cat[3, :]) / sum(arr_cat[1, :] .| arr_cat[3, :])\n\n0.2\n\n\n\n\n20.4.5 Hamming Distance\nHamming distance is defined as d = Number of positions at which w and v differ. The distance is usually meaningful when applied to categorical data. The following Julia code shows the Hamming distance for the first and the third rows in arr_cat.\n\nd₁₃ = sum(arr_cat[1, :] .⊻ arr_cat[3, :])\n\n4"
  },
  {
    "objectID": "similarity-calculation.html#k-nearest-neighbor-knn-clustering",
    "href": "similarity-calculation.html#k-nearest-neighbor-knn-clustering",
    "title": "20  Similarity Analysis",
    "section": "20.5 k-Nearest Neighbor (kNN) Clustering",
    "text": "20.5 k-Nearest Neighbor (kNN) Clustering\nkNN is primarily known as a classification algorithm, but it can also be used for clustering, particularly in the context of density-based clustering. Density-based clustering identifies regions in the data space where the density of data points is higher, and it groups points in these high-density regions. The core idea of kNN clustering is to assign each data point to a cluster based on the density of its neighbors. A data point becomes a core point if it has at least a specified number of neighbors within a certain distance.\n\n# Create a kNN model\nk = 1\nknn_model = KDTree(Array(df_num))\n\n# Query point for prediction\nquery_point = rand(2)\n\n# Find k nearest neighbors\nindices, distances = knn(knn_model, query_point, k)\n\n# Display results\nprintln(\"Query Point: $query_point\")\nprintln(\"Nearest Neighbors Indices: $indices\")\nprintln(\"Distances to Neighbors: $distances\")\n\nf = Figure()\nAxis(f[1, 1])\nscatter!(df_num[:, 1], df_num[:, 2])\nscatter!(query_point[1], query_point[2])\nf\n\nQuery Point: [0.8280323330817347, 0.7613213469309814]\nNearest Neighbors Indices: [1]\nDistances to Neighbors: [0.5781431924568906]"
  },
  {
    "objectID": "similarity-calculation.html#benchmarking",
    "href": "similarity-calculation.html#benchmarking",
    "title": "20  Similarity Analysis",
    "section": "20.6 Benchmarking",
    "text": "20.6 Benchmarking"
  },
  {
    "objectID": "portfolio-optimization.html#in-this-chapter",
    "href": "portfolio-optimization.html#in-this-chapter",
    "title": "21  Portfolio Optimization",
    "section": "21.1 In This Chapter",
    "text": "21.1 In This Chapter\nOptimization in a portfolio context with examples of asset selection under different constraints and objectives."
  },
  {
    "objectID": "other-techniques.html#in-this-chapter",
    "href": "other-techniques.html#in-this-chapter",
    "title": "22  Other Useful Techniques",
    "section": "22.1 In this chapter",
    "text": "22.1 In this chapter\nOther useful techniques are surveyed, such as: memoization to avoid repeated computations, psuedo-monte carlo, creating a model office, and tips on modeling a complete balance sheet."
  },
  {
    "objectID": "other-techniques.html#taking-things-to-the-extreme",
    "href": "other-techniques.html#taking-things-to-the-extreme",
    "title": "22  Other Useful Techniques",
    "section": "22.2 Taking things to the Extreme",
    "text": "22.2 Taking things to the Extreme\nConsider what happens if something is taken to an extreme. For example, what happens in the model if we input negative rates? Where should negative rates be allowed and can the model handle them?"
  },
  {
    "objectID": "other-techniques.html#range-bounding",
    "href": "other-techniques.html#range-bounding",
    "title": "22  Other Useful Techniques",
    "section": "22.3 Range Bounding",
    "text": "22.3 Range Bounding\nSometimes you just need to know that an outcome is within a certain range - if you can develop a “high” and “low” estimate by making assumptions that you know are outside of feasible ranges, then you can determine whether something is reasonable or within tolerances.\nTo take an example from the pages of interview questions: say you need to determine if a mortgaged property’s value is greater than the amount of the outstanding loan (say $100,000). You don’t have an appraisal, but know that it’s in reasonable condition and that (1) a comparable house with many more issues sold for $100 per square foot. You also don’t know the square footage of the house, but know from the number of rooms and layout that it must be at least 1000 square feet. Therefore you know that the value should at least be greater than:\n\\[\n\\frac{\\$100}{\\text{sq. ft}} \\times 1000 \\text{sq. ft} = \\$100,000\n\\]\nWe’d then conclude that the value of the house very likely exceeds the outstanding balance of the loan and resolves our query without complex modeling or expensive appraisals."
  },
  {
    "objectID": "julia.html#installation",
    "href": "julia.html#installation",
    "title": "23  Set up Julia and the Computing Environment",
    "section": "23.1 Installation",
    "text": "23.1 Installation\nJulia is open source and can be downloaded from JuliaLang.org and is available for all major operating systems. After you download and install, then you have Julia installed and can access the REPL, or Read-Eval-Print-Loop, which can run complete programs or function as powerful day-to-day calculator. However, many people find it more comfortable to work in a text editor or IDE (Integrated Development Environment).\nIf you are looking for managed installations with a curated set of packages for use within an organization, there are ways to self-host package repositories and otherwise administratively manage packages. Julia Computing offers managed support with enterprise solutions, including push-button cloud compute capabilities."
  },
  {
    "objectID": "julia.html#package-management",
    "href": "julia.html#package-management",
    "title": "23  Set up Julia and the Computing Environment",
    "section": "23.2 Package Management",
    "text": "23.2 Package Management\nJulia comes with Pkg, a built-in package manger. With it, you can install packages, pin certain versions, recreate environments with the same set of dependencies, and upgrade/remove/develop packages easily. It’s one of the things that just works and makes Julia stand out versus alternative languages that don’t have a de-facto way of managing or installing packages.\nPackage installation is accomplished interactively in the REPL or executing commands.\n\nIn the REPL, you can change to the Package Management Mode by hitting ] and, e.g., add DataFrames CSV to install the two packages. Hit [backspace] to exit that mode in the REPL.\nThe same operation without changing REPL modes would be: using Pkg; Pkg.add([\"DataFrames\", \"CSV\"])\n\nRelated to packages, are environments which are a self-contained workspaces for your code. This lets you install only packages that are relevant to the current work. It also lets you ‘remember’ the exact set of packages and versions that you used. In fact, you can share the environment with others, and it will be able to recreate the same environment as when you ran the code. This is accomplished via a Project.toml file, which tracks the direct dependencies you’ve added, along with details about your project like its version number. The Manifest.toml tracks the entire dependency tree.\nReproducibility via the environment tools above is a really key aspect that will ensure Julia code is consistent across time and users, which is important for financial controls."
  },
  {
    "objectID": "julia.html#editors",
    "href": "julia.html#editors",
    "title": "23  Set up Julia and the Computing Environment",
    "section": "23.3 Editors",
    "text": "23.3 Editors\nBecause Julia is very extensible and amenable to analysis of its own code, you can typically find plugins for whatever tool you prefer to write code in. A few examples:\n\n23.3.1 Visual Studio Code\nVisual Studio Code is a free editor from Microsoft. There’s a full-featured Julia plugin available, which will help with auto-completion, warnings, and other code hints that you might find in a dedicated editor (e.g. PyCharm or RStudio). Like those tools, you can view plots, search documentation, show datasets, debug, and manage version control.\n\n\n23.3.2 Notebooks\nNotebooks are typically more interactive environments than text editors - you can write code in cells and see the results side-by-side.\nThe most popular notebook tool is Jupyter (“Julia, Python, R”). It is widely used and fits in well with exploratory data analysis or other interactive workflows. It can be installed by adding the IJulia.jl package.\nPluto.jl is a newer tool, which adds reactivity and interactivity. It is also more amenable to version control than Jupyter notebooks because notebooks are saved as plain Julia scripts. Pluto is unique to Julia because of the language’s ability to introspect and analyze dependencies in its own code. Pluto also has built-in package/environment management, meaning that Pluto notebooks contains all the code needed to reproduce results (as long as Julia and Pluto are installed)."
  },
  {
    "objectID": "ecosystem.html",
    "href": "ecosystem.html",
    "title": "24  The Julia Ecosystem Today",
    "section": "",
    "text": "A tour of relevant available packages as of 2023.\nThe Julia ecosystem favors composability and interoperability, enabled by multiple dispatch. In other words, because it’s easy to automatically specialize functionality based on the type of data being used, there’s much less need to bundle a lot of features within a single package.\nAs you’ll see, Julia packages tend to be less vertically integrated because it’s easier to pass data around. Counterexamples of this in Python and R:\n\nNumpy-compatible packages that are designed to work with a subset of numerically fast libraries in Python\nspecial functions in Pandas to read CSV, JSON, database connections, etc.\nThe Tidyverse in R has a tightly coupled set of packages that works well together but has limitations with some other R packages\n\nJulia is not perfect in this regard, but it’s neat to see how frequently things just work. It’s not magic, but because of Julia features outside the scope of this article it’s easy for package developers (and you!) to do this.\nJulia also has language-level support for documentation, so packages can follow a consistent style of help-text and have the docs be auto-generated into web pages available locally or online.\nThe following highlighted packages were chosen for their relevance to typical actuarial work, with a bias towards those used regularly by the authors. This is a small sampling of the over 6000 registered Julia Packages[^2]\n\n24.0.1 Data\nJulia offers a rich data ecosystem with a multitude of available packages. Perhaps at the center of the data ecosystem are CSV.jl and DataFrames.jl. CSV.jl is for reading and writing files text files (namely CSVs) and offers top-class read and write performance. DataFrames.jl is a mature package for working with dataframes, comparable to Pandas or dplyr.\nOther notable packages include ODBC.jl, which lets you connect to any database (given you have the right drivers installed), and Arrow.jl which implements the Apache Arrow standard in Julia.\nWorth mentioning also is Dates, a built-in package making date manipulation straightforward and robust.\nCheck out JuliaData org for more packages and information.\n\n\n24.0.2 Plotting\nPlots.jl is a meta-package providing an interface to consistently work with several plotting backends, depending if you are trying to emphasize interactivity on the web or print-quality output. You can very easily add animations or change almost any feature of a plot.\nStatsPlots.jl extends Plots.jl with a focus on data visualization and compatibility with dataframes.\nMakie.jl supports GPU-accelerated plotting and can create very rich, beautiful visualizations, but it’s main downside is that it has not yet been optimized to minimize the time-to-first-plot.\n\n\n24.0.3 Statistics\nJulia has first-class support for missing values, which follows the rules of three-valued logic so other packages don’t need to do anything special to incorporate missing values.\nStatsBase.jl and Distributions.jl are essentials for a range of statistics functions and probability distributions respectively.\nOthers include:\n\nTuring.jl, a probabilistic programming (Bayesian statistics) library, which is outstanding in its combination of clear model syntax with performance.\nGLM.jl for any type of linear modeling (mimicking R’s glm functionality).\nLsqFit.jl for fitting data to non-linear models.\nMultivariateStats.jl for multivariate statistics, such as PCA.\n\nYou can find more packages and learn about them here.\n\n\n24.0.4 Machine Learning\nFlux, Gen, Knet, and MLJ are all very popular machine learning libraries. There are also packages for PyTorch, Tensorflow, and SciKitML available. One advantage for users is that the Julia packages are written in Julia, so it can be easier to adapt or see what’s going on in the entire stack. In contrast to this design, PyTorch and Tensorflow are built primarily with C++.\nAnother advantage is that the Julia libraries can use automatic differentiation to optimize on a wider range of data and functions than those built into libraries in other languages.\n\n\n24.0.5 Differentiable Programming\nSensitivity testing is very common in actuarial workflows: essentially, it’s understanding the change in one variable in relation to another. In other words, the derivative!\nJulia has unique capabilities where almost across the entire language and ecosystem, you can take the derivative of entire functions or scripts. For example, the following is real Julia code to automatically calculate the sensitivity of the ending account value with respect to the inputs:\njulia&gt; using Zygote\n\njulia&gt; function policy_av(pol)\n    COIs = [0.00319, 0.00345, 0.0038, 0.00419, 0.0047, 0.00532]\n    av = 0.0\n    for (i,coi) in enumerate(COIs)\n        av += av * pol.credit_rate\n        av += pol.annual_premium\n        av -= pol.face * coi\n    end\n    return av                # return the final account value\nend\n\njulia&gt; pol = (annual_premium = 1000, face = 100_000, credit_rate = 0.05);\n\njulia&gt; policy_av(pol)        # the ending account value\n4048.08\n\njulia&gt; policy_av'(pol)       # the derivative of the account value with respect to the inputs\n(annual_premium = 6.802, face = -0.0275, credit_rate = 10972.52)\nWhen executing the code above, Julia isn’t just adding a small amount and calculating the finite difference. Differentiation is applied to entire programs through extensive use of basic derivatives and the chain rule. Automatic differentiation, has uses in optimization, machine learning, sensitivity testing, and risk analysis. You can read more about Julia’s autodiff ecosystem here.\n\n\n24.0.6 Utilities\nThere are also a lot of quality-of-life packages, like Revise.jl which lets you edit code on the fly without needing to re-run entire scripts.\nBenchmarkTools.jl makes it incredibly easy to benchmark your code - simply add @benchmark in front of what you want to test, and you will be presented with detailed statistics. For example:\njulia&gt; using ActuaryUtilities, BenchmarkTools\n\njulia&gt; @benchmark present_value(0.05,[10,10,10])\n\nBenchmarkTools.Trial: 10000 samples with 994 evaluations.\n Range (min … max):  33.492 ns … 829.015 ns  ┊ GC (min … max): 0.00% … 95.40%\n Time  (median):     34.708 ns               ┊ GC (median):    0.00%\n Time  (mean ± σ):   36.599 ns ±  33.686 ns  ┊ GC (mean ± σ):  4.40% ±  4.55%\n\n  ▁▃▆▆▆██▇▄▃▂         ▁                                        ▂\n  █████████████▆▆▇█▇████▇██▇█▇█▇▇▆▆▅▅▅▅▅▄▅▄▄▅▅▅▅▄▄▁▅▄▄▅▄▄▅▅▆▅▆ █\n  33.5 ns       Histogram: log(frequency) by time      45.6 ns &lt;\n\n Memory estimate: 112 bytes, allocs estimate: 1.\nTest is a built-in package for performing testsets, while Documenter.jl will build high-quality documentation based on your inline documentation.\nClipData.jl lets you copy and paste from spreadsheets to Julia sessions.\n\n\n24.0.7 Other packages\nJulia is a general-purpose language, so you will find packages for web development, graphics, game development, audio production, and much more. You can explore packages (and their dependencies) at https://juliahub.com/.\n\n\n24.0.8 Actuarial packages\nSaving the best for last, the next article in the series will dive deeper into actuarial packages, such as those published by JuliaActuary for easy mortality table manipulation, common actuarial functions, financial math, and experience analysis."
  }
]