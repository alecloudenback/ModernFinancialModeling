# Automatic Differentiation
```{julia}
#| echo: false
#| output: false
using Pkg
Pkg.activate("env/autodiff")
Pkg.instantiate()
```

## In This Chapter

Harnessing the chain rule to compute derivatives not just of simple functions, but of complex programs.

## Motivation for (Automatic) Derivatives

Derivatives are one of the most useful analytical tools we have. Determining the rate of change with respect to an input is effectively sensitivity testing. Knowing the derivative let's you optimize things faster (see @sec-optimizaiton). You can test properties and implications (monotonicy, maxima/minima).

## Finite Differentiation

Finite differentiation is evaluating a function $f(x)$ at a value $x$ and then at a nearby value $x+\epsilon$. The line drawn through these two point effectively estimates the line that is tangent to the function $f$ at $x$: effectively the derivative has been found by approximation. That is, we are looking to approximate the derivative using the property:

$$
f'(x) = \lim_{{\epsilon \to 0}} \frac{{f(x_0 + \epsilon) - f(x_0)}}{{\epsilon}}
$$

We can approximate the result by simply choosing a small $\epsilon$.

There's also flavors of finite differentiation to approximate derivatives to be aware of:

- forward difference is as defined in the above equation, where $\epsilon$ is added to $x_0$
- forward difference is as defined in the above equation, where $\epsilon$ is subtracted from $x_0$
- central difference is where we evaluate at $x_0 \pm \epsilon$ and then divide by $2\epsilon$

The benefit of the central difference is that it limits issues around minima and maxima where the trough or peak respectively would seem much steeper if using forward or reverse. Here's a picture of this:

```{julia}
#| echo: false
using CairoMakie
let
    x = -0.5:0.001:0.5
    f(x) = x .^ 2

    ϵ = 0.1
    x0 = 0.0

    fwd_tangent = (f(x0 + ϵ) - f(x0)) / ϵ
    rev_tangent = (f(x0) - f(x0 + ϵ)) / ϵ
    central_tangent = (f(x0 + ϵ) - f(x0 - ϵ)) / 2ϵ
    lines(x, fwd_tangent .* x, color=:red, linewidth=1, linestyle=:dash, label="Forward Difference")

    let
        fig = Figure()
        ax = Axis(fig[1, 1], limits=((-0.25, 0.25), (-0.05, 0.1)))
        lines!(ax, x, f.(x), color=:blue, linewidth=2, label="x^2")
        lines!(ax, x, fwd_tangent .* x, color=:red, linewidth=1, linestyle=:dash, label="Forward Difference")
        lines!(ax, x, rev_tangent .* x, color=:green, linewidth=1, linestyle=:dash, label="Reverse Difference")
        lines!(ax, x, central_tangent .* x, color=:orange, linewidth=1, linestyle=:dash, label="Central Difference")
        vlines!(ax, [x0 + ϵ, x0 - ϵ], color=:gray, linewidth=1, linestyle=:dash, label="Epsilon")
        axislegend(ax)

        fig

    end
end
```

One benefit of the central difference method is that is often more accurate than forward or reverse. However it comes at the cost of needing to evaluate the function an additional time in many circumstances. Take, for example, the process of optimizing a function to find a maxima or minima. The process usually involves evaluating a function at a gas determining what the derivative of the function is at that point and using both items to update to help better guess. At each step you need to evaluate the function three times for $x4, $x+\epsilon$, and $x-\epsilon$. With forward or reverse finite differences, you can reuse the prior function evaluation of the prior guess $x$ As one of the components in the estimation of the derivative, thereby saving an evaluation of the function for each iteration.

 there are further challenges with the finite differences method. In practice, we are often interested in much more complex functions than $x^2$. For example, we may actually be interested in the sum of a series that is many elements long or contains more complex operations than basic algebra. In the prior example, the $\epsilon$ is set unusually wide for demonstration purposes. As $\epsilon$ grow smaller generally, the accuracy of all three finite different methods increases. However, that's not always the case due to both the complexity of the function that you may be trying to differentiate or due to numerical inaccuracies of floating point math.

To demonstrate, here is amore complex example using an arbitrary function 

 for this example we'll show the results of the three methods calculated at different values of $\epsilon$:


```{julia}
#| label: fig-finite-diff-error
#| fig-cap: "A log-log plot showing the absolute error of the finite differences. Further to the left, roundoff error dominates while further to the right, truncation error dominates."
using DataFrames


f(x) = exp(x)
ϵ = 10 .^ (range(-16, stop=0, length=100))
x0 = 1
estimate = @. (f(x0 + ϵ) - f(x0 - ϵ)) / 2ϵ
actual = f(x0) #<1>

fig = Figure()
ax = Axis(fig[1, 1], xscale=log10, yscale=log10, xlabel="ϵ", ylabel="absolute error")
scatter!(ax, ϵ, abs.(estimate .- actual))
fig
```
1. The derivative of $f(x) = exp(x)$ is itself. That is $f'(x) = f(x)$ in this special case.

::: callout-note
The `@.` in the code example above is a macro that applies broadcasting each function to its right. `@. (f(x0 + ϵ) - f(x0 - ϵ)) / 2ϵ` is the same as `(f.(x0 .+ ϵ) .- f.(x0 .- ϵ)) ./ (2 .* ϵ)` 
:::

A few observations:

1. At virtually every value of `ϵ` we observe some error from the true derivative.
2. That error is the sum of two parts: **truncation error** is inherent in that we are using a given value for `ϵ` and not determining the limiting analytic value as $\epsilon \to 0$. The other component is **roundoff error** which arises due to the limited precision of floating point math.

The implications of this are that we need to often be careful about the choice of `ϵ`, as the optimal choice will vary depending on the function and the point we are attempting to evaluate. This presents a number of practical diffuclties in various algorithms.

Additionally,when computing the finite difference we must evaluate the function multiple times to determine a single estimate of the derivative. When performing something like optimization the process typically involves iteratively making many guesses — plus the number of guesses required to find the right answer can depends on the ability to accurate determine the derivative at a point! 

Admittedly, despite the accuracy and computational overhead, finite differences can be very useful in many circumstances. However, a more appealing alternative approach will be covered next.

## Automatic Differentiation

**Automatic differentiation** ("autodiff" for short) is essentially the practice of defining algorithmically what the derivatives of function should be. We are able to do this through a creative application of the chain rule. Recall that the **chain rule** allows us to compute the derivative of a composite function using the derivatives of the component functions:

$$
h(x)=f(g(x))
$$
$$
h'(x) = f'(g(x)) g'(x)
$$

Using this rule, we can define how elementary operations act when differentiated. Combined with the fact that most computer code is building up from a bunch of elementary operations, we can get a very long way in differentiating complex functions.

### Dual Numbers

To understand where we are going, let's remind ourselves about complex numbers. Complex numbers are of the form which has an real part ($r$) and an imaginary part ($iq$):

$$
r+iq
$$

By definition we say that $i^2 = -1$. This is useful because it allows us to perform certain types of operations (e.g. finding a square root of a negative number) that is otherwise unsolvable with just the real numbers^[Richard Feynman has a wonderful, short lecture on algebra here: https://www.feynmanlectures.caltech.edu/I_22.html]. After defining how the normal algebraic operations (addition, multiplication, etc.) work for the imaginary number, we are able to utilize the imaginary numbers for a variety of practical mathematical tasks. 

What is meant by extending the algebraic operations for imaginary numbers? For example, stating how addition should work for imaginary numbers:

$$
(r+iq) + (s+iu) = (r+s) + i(q+u)
$$

In a similar fashion as extending the Real ($\mathbb{R}$) numbers with an *imaginary* part, for automatic differentiation we will extend them with a *dual* part. A **dual number** is one of the form:

$$
a + \epsilon b
$$

Where $\epsilon^2 = 0$ and $\epsilon \neq 0$ by defintion. For our purposes here, one can think of $b$ as the derivative of the function evaluated at the same point as $a$. An intial example should make this clearer. First let's define a `DualNumber`:

```{julia}
struct DualNumber{T,U} #<1>
    a::T
    b::U
    function DualNumber(a::T,b::U=zero(a)) where {T,U}# <2>
        return new{T,U}(a,b)
    end
end
```
1. We define this type parametrically to handle all sorts of `<:Real` types and allow `a` and `b` to vary types in case a mathematical operation causes a type change (e.g. as in the case of integers becoming a floating point number like `10/4 == 2.5`)
2. `zero(a)` is a generic way to create a value equal to zero with the same type of the argument `a`. `zero(12.0) == 0.0` and `zero(12) == 0`.

Now let's define how dual numbers work under addition. The mathematical rule is:

$$
(a+\epsilon b)+(c+\epsilon d)=(a+c)+(b+d)\epsilon
$$

We then need to define how it works for the combinations of numbers that we might receive as arguments to our function (this is an example where multiple dispatch greatly simplifies the code compare to object oriented single dispatch!):

```{julia}
Base.:+(d::DualNumber, e::DualNumber) = DualNumber(d.a + e.a, d.b + e.b)
Base.:+(d::DualNumber, x) = DualNumber(d.a + x, d.b)
Base.:+(x, d::DualNumber) = d + x
```

And here's how we would get the derivative of a very simple function:

```{julia}
f1(x) = 5 + x

f1(DualNumber(10, 1))
```

That's not super interesting though - the derivative of `f1` is just `1` and we supplied that in the construction of `DualNumber`. We did at least prove that we can add the `10` and `5`! 

Let's make this more interesting by also defining the multiplication operation on dual numbers. We'll follow the product rule:

$$
(u \times v)' = u ' \times v + u \times v' 
$$

```{julia}
Base.:*(d::DualNumber, e::DualNumber) = DualNumber(d.a * g.a, d.b * g.a + d.a * g.b)
Base.:*(x, d::DualNumber) = DualNumber(d.a * x, d.b * x)
Base.:*(d::DualNumber, x) = x * d
```

Now what if we evaluate this function:

```{julia}
f2(x) = 5 + 3x

f2(DualNumber(10, 1))
```

We have found that the second component is `3`, which is indeed the derivative of $5+3x$ with respect to $x$. And in the first part we have the value of `f2` evaluated at `10`. 

:::callout-note
When calcualting the derivative, why do we start with `1` in the dual part of the number? Because the derivative of a variable with respect to itself is 1. From this unitary starting point, the various operations applied accumulate the derivative of the various operations in the $b$ part of $a + \epsilon b$.
:::


We can also define this for things like transcendental functions:

```{julia}
Base.exp(d::DualNumber) = DualNumber(exp(d.a), exp(d.a) * d.b)
Base.sin(d::DualNumber) = DualNumber(sin(d.a), cos(d.a)*d.b)
Base.cos(d::DualNumber) = DualNumber(cos(d.a), -sin(d.a)*d.b)
exp(DualNumber(1, 1))
```
```{julia}

sin(DualNumber(0, 1))
```

```{julia}
cos(DualNumber(0, 1))
```

## Performance of Automatic Differentiation

Recall that in the finite difference method, we generally had to evalue the function two or three times to *approximate* the derivative. Here we have a single function call that provides both the value and the derivative at that value. How does this compare performance-wise to simply evaluating the function a single time?

```{julia}
using BenchmarkTools
@btime f2(rand())
```

```{julia}
@btime f2(DualNumber(rand(), 1))
```

In performing this computation, the compiler has been able to optimize it such that we effectively are able to compute the function and its derivative at effetcitly the same speed as just the evaluating the function itself! As the function gets more complex, the overhead does increase but is still a *much* preferred option versus finite differentiation.

::: callout-note
In fact, it's largely due to the advances in applications of automatic differentiation that has led to the explosion of machine learning and artificial intelligence techniques in the 2010s/2020s. The "learning" process relies on solving parameter weights and would be too computationally expensive if using finite differences. 

These applications of autodiffertiation in specialized C++ libraries underpin the libraries like PyTorch, Tensorfow, and Keras. These libraries specialize in allowing for autodiff on a limited subset of operations. Julia's available automatic differentiation is more general and can be applied to many more scenarios. 
:::

## Forward Mode and Reverse Mode

## Automatic Differentiation in Practice




## References
 - https://book.sciml.ai/notes/08-Forward-Mode_Automatic_Differentiation_(AD)_via_High_Dimensional_Algebras/
 - https://blog.esciencecenter.nl/automatic-differentiation-from-scratch-23d50c699555
