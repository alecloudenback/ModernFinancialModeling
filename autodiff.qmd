# Automatic Differentiation
```{julia}
#| echo: false
#| output: false
using Pkg
Pkg.activate("env/autodiff")
Pkg.instantiate()
```

## In This Chapter

Harnessing the chain rule to compute derivatives not just of simple functions, but of complex programs.

## Motivation for (Automatic) Derivatives

Derivatives are one of the most useful analytical tools we have. Determining the rate of change with respect to an input is effectively sensitivity testing. Knowing the derivative let's you optimize things faster (see @sec-optimizaiton). You can test properties and implications (monotonicy, maxima/minima).

## Finite Differentiation

Finite differentiation is evaluating a function $f(x)$ at a value $x$ and then at a nearby value $x+\epsilon$. The line drawn through these two point effectively estimates the line that is tangent to the function $f$ at $x$: effecitvely the derivative has been found by approximation. That is, we are looking to approxmate the derivative using the property:

$$
f'(x) = \lim_{{\epsilon \to 0}} \frac{{f(x_0 + \epsilon) - f(x_0)}}{{\epsilon}}
$$

We can approximate the result by simply choosing a small $\epsilon$.

There's also flavors of finite differentiation to approximate derivatives to be aware of:

- forward difference is as defined in the above equation, where $\epsilon$ is added to $x_0$
- forward difference is as defined in the above equation, where $\epsilon$ is substracted from $x_0$
- central difference is where we evaluate at $x_0 \pm \epsilon$ and then divide by $2\epsilon$

The benefit of the central difference is that it limitsissues around minima and maxima where the trough or peak respectively would seem much steeper if using forward or reverse. Here's a picture of this:

```{julia}
#| echo: false
using CairoMakie
let
    x = -0.5:0.001:0.5
    f(x) = x .^ 2

    ϵ = 0.1
    x0 = 0.0

    fwd_tangent = (f(x0 + ϵ) - f(x0)) / ϵ
    rev_tangent = (f(x0) - f(x0 + ϵ)) / ϵ
    central_tangent = (f(x0 + ϵ) - f(x0 - ϵ)) / 2ϵ
    lines(x, fwd_tangent .* x, color=:red, linewidth=1, linestyle=:dash, label="Forward Difference")

    let
        fig = Figure()
        ax = Axis(fig[1, 1], limits=((-0.25, 0.25), (-0.05, 0.1)))
        lines!(ax, x, f.(x), color=:blue, linewidth=2, label="x^2")
        lines!(ax, x, fwd_tangent .* x, color=:red, linewidth=1, linestyle=:dash, label="Forward Difference")
        lines!(ax, x, rev_tangent .* x, color=:green, linewidth=1, linestyle=:dash, label="Reverse Difference")
        lines!(ax, x, central_tangent .* x, color=:orange, linewidth=1, linestyle=:dash, label="Central Difference")
        vlines!(ax, [x0 + ϵ, x0 - ϵ], color=:gray, linewidth=1, linestyle=:dash, label="Epsilon")
        axislegend(ax)

        fig

    end
end
```

In practice, we are often interested in much more complex functions than $x^2$. For example, we may actually be interested in the sum of a series that is many elements long or contains more complex operations than basic algebra. In the prior example, the $\epsilon$ is set unusually wide for demonstration purposes. As $\epsilon$ grow smaller generally, the accuracy of all three finite different methods increases however, that's not always the case due to both the complexity of the function that you may be trying to differentiate or due to numerical inaccuracies of floating point math.

To demonstrate, here is amore complex example using the PDF of a lognormal random variable with $\sigma=0.01$. This is A common distribution to encounter in practice, though it's a bit contrived to be trying to differentiate the probability different stop voice contro backspace 10 times 10 character


```{julia}
using DataFrames

f(x) = 1 / (x * 0.01 * √(2π)) * exp(-log(x)^2 / (2 * 0.01^2))
ϵ = 10.0 .^ -(5:12)
x0 = 1.0 - 0.01^2
let
    fwd = @. (f(x0 + ϵ) - f(x0)) / ϵ
    rev = @. (f(x0) - f(x0 - ϵ)) / ϵ
    central = @. (f(x0 + ϵ) - f(x0 - ϵ)) / 2ϵ
    DataFrame(; ϵ, fwd, rev, central)
end
```

::: callout-note
The `@.` in the code example above is a macro
:::