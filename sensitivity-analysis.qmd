---
author:
  - name: Yun-Tien Lee
---

# Sensitivity Analysis {#sec-sensitivity}

> "Sensitivity analysis is the art of understanding how small changes in assumptions can lead to big changes in outcomes, helping us navigate uncertainty with clarity." — Unknown

## Chapter Overview

Sensitivity analysis tells us how fragile our valuation or risk metrics are once key drivers shift. We will move from **local techniques** (derivatives, finite differences, automatic differentiation) that explain the effect of tiny shocks to **global techniques** (regression-based screening, Sobol, Morris, FAST) that summarize how entire distributions of inputs ripple through a model. The chapter closes with practical scenario-analysis guidance so you can turn the math into board-ready stress narratives.

```{julia}
#| echo: false
#| output: false
using Pkg
Pkg.activate("env/sensitivity")
Pkg.instantiate()
```

## Setup

To keep the examples concrete we will work with a simplified block of participating whole life policies.

```{julia}
using Dates

@enum Sex Female = 1 Male = 2
@enum Risk Standard = 1 Preferred = 2

mutable struct Policy
    id::Int
    sex::Sex
    benefit_base::Float64
    pir::Float64 # the policy interest rate
    mode::Int
    prem::Float64
    pp::Int
    issue_date::Date
    issue_age::Int
    risk::Risk
end
```

The fields give us everything we need to project reserves: demographic attributes, benefit base, the policy interest rate (`pir`), premium pattern, and risk class. Real models would store many more underwriting features, but this toy structure is enough to showcase the techniques.

## The Data

```{julia}
using MortalityTables
sample_csv_data =
    IOBuffer(
        raw"id,sex,benefit_base,pir,mode,prem,pp,issue_date,issue_age,risk
         1,M,100000.0,0.03,1,1000.0,10,1999-12-05,30,Std"
    )

mort = Dict(
    # assuming no issue ages < 2 or > 120
    Male => MortalityTables.table(262).ultimate,
    Female => MortalityTables.table(261).ultimate,
)
```

```{julia}
using CSV, DataFrames

policies = let

    # read CSV directly into a dataframe
    # df = CSV.read("sample_inforce.csv",DataFrame) # use local string for notebook
    df = CSV.read(sample_csv_data, DataFrame)

    # map over each row and construct an array of Policy objects
    map(eachrow(df)) do row
        Policy(
            row.id,
            row.sex == "M" ? Male : Female,
            row.benefit_base,
            row.pir,
            row.mode,
            row.prem,
            row.pp,
            row.issue_date,
            row.issue_age,
            row.risk == "Std" ? Standard : Preferred,
        )
    end

end
```

For example, `res(2, policies[1])` returns the reserve at the end of policy year 2 for our sample contract. We will refer to `policies[1]` when we need a concrete contract for the sensitivity examples.

Given a basic insurance product, a pure whole of life (WOL) policy with level benefits and level premiums payable within the first 10 years, the reserve in-force at the end of the $y^{th}$ ($y$ >= 0) policy year is defined by

$$
res(y) = \sum_{t=age+y}^{120} (sur_{t-age-y} * mort_t * B_y * \sqrt{1 + pir}) - (P_y * sur_{t-age-y})
$$

where

- $mort_t$ is the mortality at age $t$
- $sur_y$ is the survival probability adjusted with the policy interest rate, with values of
    + $sur_0 = 1$,
    + $sur_x = sur_{x-1} * (1 - mort_{age+y}) / (1 + pir)$ for $x >= y$, and
    + 0 for $x < y - 1$ or $age + x >= 120$, or ultimate age of the current mortality table
- $B_y$ is the level benefit throughout the policy
- $P_y$ is the level premium within the first 10 policy years which is 0 for policy years after 10
- $pir$ is the level policy interest rate throughout the policy

```{julia}
# Can be made more efficient by storing values in 
# a pre-calculated table. 
# The recursive calculation here is for illustrative purposes.
function sur(y::Int, pol::Policy; Ω=120)
    if y == 0
        1.0
    elseif y < 0 || Ω - y <= pol.issue_age
        0.0
    else
        sur(y - 1, pol) * (1 - mort[pol.sex][pol.issue_age+y-1]) / (1 + pol.pir)
    end
end

function res(y::Int, pol::Policy; Ω=120)
    s = 0.0
    if y >= 1 && y <= Ω - pol.issue_age
        for t in (pol.issue_age+y):Ω
            prem = 0.0
            if y <= pol.pp
                prem = pol.prem
            end
            s += sur(t - pol.issue_age, pol) * mort[pol.sex][t] * pol.benefit_base /
                 sqrt(1 + pol.pir) - prem * sur(t - pol.issue_age, pol)
        end
    end
    s
end
```

## Common Sensitivity Analysis Methodologies

### Finite Differences

Central finite differences approximate derivatives by shocking one input up and down while holding everything else fixed. The result can be interpreted as a "delta" (absolute change per unit of input) or as an elasticity if we scale by the base reserve.

```{julia}
function res_wrt_r_fd(y::Int, pol::Policy, r::Float64, h=1e-3)
    p₊, p₋ = deepcopy(pol), deepcopy(pol)
    p₊.pir, p₋.pir = r + h, r - h
    (res(y, p₋) - res(y, p₊)) / (2 * res(y, pol))
end

# percentage changes in reserve at year 2 when 
# the interest rate at 3% with a perturbation of 0.1%
res_wrt_r_fd(2, policies[1], 0.03)
```

Use a smaller `h` when `pir` is quoted in decimals (e.g., 3%) and a larger `h` if you work in basis points. Always sanity check that the perturbation stays within practical limits (no negative credited rates).

### Regression Analyses

Regression-based sensitivity (also called standardized regression coefficients) fits a metamodel between sampled inputs and the resulting reserves, then reports correlations and partial correlations as quick importance scores.

```{julia}
using GlobalSensitivity

function r1_wrt_r(r)
    p = deepcopy(policies[1])
    p.pir = r[2]
    p.prem = r[3]
    res(Int(floor(r[1])), p)
end

# reserve @ year 1 or 2, interest rate @ 0.03 ± 0.01, prem @ 1000.0 ± 0.1
reg_anal = gsa(
    r1_wrt_r, RegressionGSA(),
    [[1, 2], [0.029, 0.031], [999.9, 1000.1]],
    samples=1000)
println(reg_anal.pearson)
```

The Pearson coefficients show the correlation coefficient matrix between inputs and outputs.

### Sobol Indices

Sobol is a variance-based method, and it decomposes the variance of the output of the model or system into fractions which can be attributed to inputs or sets of inputs. This helps to get not just the individual parameter's sensitivities, but also gives a way to quantify the effect and sensitivity from the interaction between the parameters.

$$
Y = f_0 + \sum_{i=1}^{d}f_i(X_i) + \sum_{i<j}^{d}f_{ij}(X_i, X_j) + ... + f_{1,2,...,d}(X_1, X_2, ..., X_d)
$$

$$
Var(Y) = \sum_{i=1}^{d}V_i + \sum_{i<j}^{d}V_{ij} + ... + V_{1,2,...,d}
$$

The Sobol Indices are "ordered", the first order indices given by $S_i = \dfrac{V_i}{Var(Y)}$, the contribution to the output variance of the main effect of $X_i$. Therefore, it measures the effect of varying $X_i$ alone, but averaged over variations in other input parameters. It is standardized by the total variance to provide a fractional contribution. Higher-order interaction indices $S_{ij}, S_{ijk}$ and so on can be formed by dividing other terms in the variance decomposition by $Var(Y)$.

```{julia}
using QuasiMonteCarlo, GlobalSensitivity

# reserve @ year 1/2, interest rate @ 0.03 ± 0.01, prem @ 1000.0 ± 0.1
L, U = QuasiMonteCarlo.generate_design_matrices(
    1000,
    [1, 0.029, 999.9],
    [2, 0.031, 1000.1], SobolSample()
)
s = gsa(r1_wrt_r, Sobol(), L, U)
println(s.S1)
println(s.ST)
```

`S1` reports how much of the reserve variance comes from each input alone, while `ST` shows the total contribution including interaction terms. If rate and premium interact strongly, the gap `ST - S1` will be large, signaling non-linear cross-effects.

### Morris Method

The Morris method, also known as Morris's OAT method (where OAT stands for One At a Time), can be described in the following steps:

$$
EE_i = \frac{f(x_1, x_2, ...x_i + \Delta, ...x_k) - y}{\Delta}
$$

We calculate local sensitivity measures known as "elementary effects", which are calculated by measuring the perturbation in the output of the model on changing one parameter.

These are evaluated at various points in the input chosen such that a wide "spread" of the parameter space is explored and considered in the analysis, to provide an approximate global importance measure. The mean and variance of these elementary effects is computed. A high value of the mean implies that a parameter is important, a high variance implies that its effects are non-linear or the result of interactions with other inputs. This method does not evaluate separately the contribution from the interaction and the contribution of the parameters individually and gives the effects for each parameter which takes into consideration all the interactions and its individual contribution.

```{julia}
using GlobalSensitivity

# reserve @ year 1/2, interest rate @ 0.03 ± 0.01, prem @ 1000.0 ± 0.1
m = gsa(r1_wrt_r, Morris(), [[1, 2], [0.029, 0.031], [999.9, 1000.1]])
println(m.means)
println(m.variances)
```

From the means it can be observed which variables are more important, and the variances imply higher degree of nonlinearity or interactions with other variables.

### Fourier Amplitude Sensitivity Tests

FAST offers a robust, especially at low sample size, and computationally efficient procedure to get the first and total order indices as discussed in Sobol. It utilizes monodimensional Fourier decomposition along a curve, exploring the parameter space. The curve is defined by a set of parametric equations,

$$
EE_i = \frac{f(x_1, x_2, ...x_i + \Delta, ...x_k) - y}{\Delta}
$$

where $s$ is a scalar variable varying over the range $−∞ < s < +∞$, $G_i$ are transformation functions and $w_i, ∀i=1,2,…,N$ is a set of different (angular) frequencies, to be properly selected, associated with each factor for all $N$ (samples) number of parameter sets.

```{julia}
using GlobalSensitivity

# reserve @ year 1/2, interest rate @ 0.03 ± 0.01, prem @ 1000.0 ± 0.1
fast = gsa(r1_wrt_r, eFAST(), [[1, 2], [0.029, 0.031], [999.9, 1000.1]], samples=1000)
println(fast.S1)
println(fast.ST)
```

Interpretation mirrors Sobol: `S1` is the main effect and `ST` the total effect. Differences between Sobol and FAST estimates mostly reflect Monte Carlo noise; agreement across both methods boosts confidence in your ranking.

### Automatic Differentiation

By applying the chain rule repeatedly on elementary operations of computer calculations, automatic differentiation can be applied to measure impacts of small differences. More details in @sec-autodiff on automatic differentiation.

### Scenario Analyses

Quantitative sensitivities are only half the story. Boards and regulators still expect named scenarios that link narrative shocks to financial impacts. Use the techniques from @sec-scenario-generation to build the paths, then layer on the following practices.

#### Reverse stress testing

Reverse stress tests start from an unacceptable outcome (e.g., statutory RBC < 250%) and work backward to the combination of market, actuarial, and operational events that would trigger it.

1. Define the failure point in business terms.
2. Enumerate the drivers that could push the system there (rate shocks, mass lapses, cyber loss).
3. Build a trajectory that connects today's state to the failure and quantify the cumulative impact.

Benefits include clearer vulnerability maps, stronger contingency plans, and alignment with supervisory expectations.

#### Stylistic scenarios

Stylistic (narrative) scenarios translate technical shocks into stories that executives recognize ("Higher-for-longer inflation with elevated credit spreads"). Describe the macro setting, policy responses, and customer behavior, then map those story elements to the stochastic blocks in your engine. This makes it easier to compare sensitivities across business units that may not share the same quantitative models.

#### Backtesting scenarios

Before anchoring capital plans on a scenario set, test whether similar historical episodes would have produced the modeled impacts.

1. Define the scenario family (base/up/down, climate pathways, liquidity crunch).
2. Pull historical periods that resemble each narrative and compute the actual KPI movements.
3. Run the scenario through your model for those historical start dates.
4. Compare modeled vs. realized outcomes and reconcile gaps.
5. Adjust assumptions or document why the future is expected to differ (structural breaks, new hedging programs).

Key reminders when using history: validate data quality, resist overfitting scenarios to the past, and note that structural changes (regulation, product mix, technology) can limit comparability.
