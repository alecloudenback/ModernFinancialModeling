```{julia}
#| echo: false
#| output: false
using Pkg
Pkg.activate("env/statistics")
Pkg.instantiate()
```

# Statistical Inference and Information Theory

> "My greatest concern was what to call \[the amount of unpredictabilty in a random outcome\]. I thought of calling it 'information,' but the word was overly used, so I decided to call it 'uncertainty.' When I discussed it with John von Neumann, he had a better idea. Von Neumann told me, 'You should call it entropy, for two reasons. In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, no one really knows what entropy really is, so in a debate you will always have the advantage.'"- Claude Shannon (1971)

## In This Chapter

A brief introduction to infromation theory and its foundational role in statistics. Entropy and probability distributions. Bayes' rule and model selection comparison via likelihoods. A brief tour of modern bayesian statistics.

## Information Theory

Probability, statistics, machine learning, signal processing, and even physics have a foundational link in **information theory** which is the description and analysis of how much useful data is contained within something.

Let's consider the following number that we encounter while reading a report which contains estimates of total amount of assets held. Unfortunately, for one reason or another one of the digits is not visible to you. Here's what you can read, with the `_` indicating that the digit is not visible:

```         
32,000,_00
```

Now you probably already formed an opinion on what the missing number is, but let's look at how we can quantify the analysis.

Given that we know the number was an estimate and the tendency of humans to like nice round numbers, our **prior assumption** for what the probability of the missing digit is may be something like the $p(x_i)$ row of @tbl-digit-information-human. We shall call the individual outcomes $x_i$ and the overall set of probabilities $\{x_0,x_1,...x_9\}$ is called $X$.

The information content of an outcome, $h(x)$ is measured in bits and defined as[^statistics-1]:

[^statistics-1]: Log base two turns out to be the most natural representation of information content as it mimics the fundamental 0 or 1 value bit. A more complete introduction is available in "Information Theory, Inference, and Learning Algorithms" by David MacKay.

$$
h(x_i) = \text{log}_2\frac{1}{p(x_i)}
$$ {#eq-information}

So if we were to find out that the missing digit were indeed `0`, we have gained less information relative to our expectation than if the missing digit were anything other than `0` .

We can characterize the entire distribution $X$ via the **entropy,** $H(X)$, of a probability set is the ensemble's average information content:

$$
H(X) = \sum{p(x_i)\text{log}_2}\frac{1}{p(x_i)} 
$$ {#eq-entropy}

The entropy $H(X)$ of the presumed outcomes in @tbl-digit-information-human distirubtion of outcomes is $0.722 \text{bits}$.

|          |       |       |       |       |       |       |       |       |       |       |
|----------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| $x_i$    | 0     | 1     | 2     | 3     | 4     | 5     | 6     | 7     | 8     | 9     |
| $p(x_i)$ | .91   | .01   | .01   | .01   | .01   | .01   | .01   | .01   | .01   | .01   |
| $h(x_i)$ | 0.136 | 6.644 | 6.644 | 6.644 | 6.644 | 6.644 | 6.644 | 6.644 | 6.644 | 6.644 |

: Probability distribution of missing digit knowing human inclinations to prefer round numbers when estimating. \$\$ {#tbl-digit-information-human}

Note that we have take a view on the probability distribution for the missing digit, and we'll refer to this as the **prior assumption** (or just **prior**). This is an opinionated assumption, so what if we had another colleague who beleived humans are completely rational and without bias for certaim numbers. They would then be arguing for a prior assumed distribution consistent with @tbl-digit-information-unifrom.

With the uniform prior assumption, $H(X) = 3.322 \text{bits}$ and $h(x_i)$ is also unifrom. We will not prove it here, but a uniform probability over a set of outcomes is the highest entropy distribution that can be assumed.

|          |       |       |       |       |       |       |       |       |       |       |
|----------|-------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
| $x_i$    | 0     | 1     | 2     | 3     | 4     | 5     | 6     | 7     | 8     | 9     |
| $p(x_i)$ | .10   | .10   | .10   | .10   | .10   | .10   | .10   | .10   | .10   | .10   |
| $h(x_i)$ | 3.322 | 3.322 | 3.322 | 3.322 | 3.322 | 3.322 | 3.322 | 3.322 | 3.322 | 3.322 |

: Probability distribution of missing digit with uniform, maximal entropy for the assumed probability distribution. {#tbl-digit-information-unifrom}

### Example: Classificaiton

In this example, we will determine the optimal splits for a decision tree[^statistics-2] based on the information gained at each node in the tree.

[^statistics-2]: A decision tree is a classification algorithm which attempts to optimally classify an output based on if/else type branches on the input variables.

```{julia}
#| label: tbl-default-data
#| tbl-cap: "Fictional data regarding loan attributes and whether or not a loan defaulted before it's maturity."
using DataFrames

employed = [true, false, true, true, true, false, false, true]
good_credit = [true, true, false, true, false, false, false, true]
default = [true, false, true, true, true, true, false, true]
default_data = DataFrame(; employed, good_credit, default)
```

The entropy of the default rate data is, per @eq-entropy:

```{julia}
H0 = let
    p1 = sum(default_data.default) / nrow(default_data)
    p2 = 1 - p1
    p1 * log2(1 / p1) + p2 * log(1 / p2)
end
```

Our goal is to determine which attribute (`employed` or `good_credit`) to use as the first split in the decision tree. We will decide this by calculating the information gain, which is the difference in entropy between the prior node and the candidate node. In our case we start with `H0` as calculated above for the output variable `default` and calculate the difference in entropy between it and the average entropy of the data if we split on that node. **Information gain**, $IG(inputs, attributes)$, is:

$$
...
$$

Let's first consider splitting the tree based on the `employed` status. We will calculate the entropy of each subset: with employment and without employment.

If we split the data based on being employed, we'd get two sub-datasets:

```{julia}
df_employed = filter(:employed => ==(true), default_data)
```

and

```{julia}
df_unemployed = filter(:employed => ==(false), default_data)
```

let's call it's entropy `H_employed`, which should be zero because there is no variability in the `default` outcome for this subset.

```{julia}
H_employed = let
    p1 = sum(df_employed.default) / nrow(df_employed)
    p2 = 1 - p1
    # p1 * log2(1 / p1) + p2 * log(1 / p2) 
    p1 * log2(1 / p1) + 0 # <1>
end
```

1.  In the case of $p_i = 0$ the value of $h$ (the second term in the sum above) is taken to be $0$, which is consistent with the $\lim_{p\to0^+}p\log (p) = 0$.

And the corresponding candidate leaf is `H_unemployed`:

```{julia}
H_unemployed = let
    p1 = sum(df_unemployed.default) / nrow(df_unemployed)
    p2 = 1 - p1
    p1 * log2(1 / p1) + p2 * log(1 / p2)
end
```

The average of the two is weighted by the size of the data that would fall into each leaf:

```{julia}
H1_employment = let
    p_emp = nrow(df_employed) / nrow(default_data)
    p_unemp = 1 - p_emp

    p_emp * H_employed + p_unemp * H_unemployed
end
```

The information gain for splitting the tree using employment status is the difference between the root entropy and the entropy of the employment split:

```{julia}
IG_employment = H0 - H1_employment
```

We could repeat the analysis to determine the information gain if we were to split the tree based on having good credit. However, given that there are only two attributes we can already conclude that `employed` is a better attribute to split the data on. This is because the information gain of `IG_employment` (`{julia} round(IG_employment;digits=3)`) is the majority of the overall entropy `H0` (`{julia} round(H0;digits=3)`). Entropy is always additive and you cannot have negative entropy, therefore no other other attribute could have greater information gain. This also matches our intuition when looking at @tbl-default-data as the eye can spot a higher correlation between `employed` and `default` than `good_credit` and `default`.

The above example demonstrates how we can use information theory to create more optimal inferences on data.

### Maxium Entropy Distributions {#sec-maxent}

Why is information theory a useful concept? Many financial models are statistical in nature and concepts of randomness and entropy are foundational. For example, when trying to estimate parameter distributions or assume a distribution for a random process you can lean on information theory to use the most conservative choice: the distribution with the highest entropy given known constraints. These distributions are referred to as **maximum entropy distributions**. Some discussion of maximum entropy distributions in the context of risk assessment is available in an article by Duracz[^statistics-3]. probability distributions and risk asses

[^statistics-3]: <https://www.researchgate.net/publication/239752412_Derivation_of_Probability_Distributions_for_Risk_Assessment>

| Constraint                                                         | Discrete Distribution | Continuous Distribution |     |
|-------------------|------------------|------------------|------------------|
| Bounded range                                                      | Uniform (discrete)    | Uniform (continuous)    |     |
| Bounded range (0 to 1) with information about the mean or variance |                       | Beta                    |     |
| Mean is finite, two possible values                                | Binomial              |                         |     |
| Mean is finite and positive                                        | Geometric             | Exponential             |     |
| Mean is finite and range is \> zero                                |                       | Gamma                   |     |
| Mean and Variance is finite                                        |                       | Guassian (Normal)       |     |
| Positive and equal mean and variance                               | Poisson               |                         |     |
|                                                                    |                       |                         |     |
|                                                                    |                       |                         |     |

: Maximum Entropy Distributions and the conditions under which they are applicable. {#tbl-MEDs}

The distributions in @tbl-MEDs arise again and again in nature because of the second law of thermodynamics - nature likes to have constantly increasing entropy and therefore it should be no surprise (random) processes that maximize entropy pop up all over the place. As an example, let's look at processes that behave like the Gaussian (Normal) distribution.

#### Processes that give rise to certain distributions

A random walk can be viewed as the cumulative impact of nudges pushing in opposite directions. This behavior culminates in the random, terminal position being able to be described by a Gaussian distribution. The center of a Gaussian distribution is "thick" because there are many more ways for the cumulative total nudges to mostly cancel out, while its increasingly rare to end up further and further from the starting point (mean). The distribution then spreads out as flat (randomly) as it can while still maintaining the constraint of having a given, finite variance. Any other continuous distribution that has the same mean and variance has lower entropy than the Guassian.

| Process                                                                                                          | Distribution of Data | Examples                                                                             |
|------------------------|------------------------|------------------------|
| Many *additive* pluses and minus that move an outcome in one dimension                                           | Normal               | Sum of many dice rolls, errors in measurements, sample means (Central Limit Theorem) |
| Many *multiplicative* pluses and minus that move an outcome in one dimension                                     | Log-normal           | Incomes, sizes of cities, stock prices                                               |
| Waiting times between independent events occurring at a constant average rate                                    | Exponential          | Time between radioactive decay events, customer arrivals                             |
| Discrete trials each with the same probability of success, counting the number of successes                      | Binomial             | Coin flips, defective items in a batch                                               |
| Discrete trials each with the same probability of success, counting the number of trials until the first success | Geometric            | Number of job applications until getting hired                                       |
| Continuous trials each with the same probability of success, measuring the time until the first success          | Exponential          | Time until a component fails, time until a sales call results in a sale              |
| Waiting time until the r-th event occurs in a Poisson process                                                    | Gamma                | Time until the 3rd customer arrives, time until the 5th defect occurs                |

: Underlying processes create typical probability distributions. That there is significant overlap with the distributions in @sec-maxent is not a coincidence. {#tbl-prob-processes}

::: callout-tip
## Probability Distributions

There are a *lot* of specialized distributions. There are lists of distributions you can find online or in references such as @Leemis_2008 which has a full-page network diagram of the relationships.

The information-theoretic and Bayesian perspective on it is to eschew memorization of a bunch of special cases and statistical tests. If you pull up the aforementioned diagram in @Leemis_2008, you can see just a handful of distributions that have the most central roles in the universe of distributions. Many distributions are simply transformations, limiting instances, or otherwise special cases of a more fundamental distribution. Instead of trying to memorize a bunch of probability distributions, it's better to think critically about:

1.  The fundamental processes that give rise to the randomness.
2.  Tranformations of the data to make it nicer to work with, such as translations, scaling, or other non-destructive changes.

Then when you encounter a wacky dataset you don't need to comb the depths of Wikipedia to find the perfect probability distributions.
:::

#### Additive and Multiplicative Processes

@tbl-prob-processes describes some examples, let us discuss further what it means to have a process that arises via an additive vs multiplicative effect[^statistics-4].

[^statistics-4]: Mulitiplicative process are often referred to as "geometric", as in "geometric Brownian motion" or "geometric mean". Additive processes are sometimes referred to as "arithmetic". This root of this confusing terminology appears to be due to the fact that series involving repeated multiplication were solved via geometric (triangles, angles, etc.) methods while those using sums and differences were solved via arithmetic.

An outcome is additive if it's the sum or difference of multiple independent processes. One of the simplest examples of this is rolling multiple dice and taking their sum. Or a random walk along the natural numbers wherein with equal probability you take a step left or right. The distribution of the position after $n$ steps converges rapidly to a normal distribution. Another common one is when you are looking at the mean of a sample - since you are summing up the individual measurements you end up with a normal distribution (the Central Limit Theorem).

However, many processes are multiplicative in nature. For example the population density of cities is distributed in a log-normal fashion. If we think about the factors that contribute to choice of place to live, we can see how these factors multiply: an attractive city might make someone 10% more likely to move, a city with water features 15% more likely, high crime 30% less likely, etc. These forces combine in a multiplicative way in the generative process of deciding where to move.

::: {#tip-logarithms .callout-tip} 
## Logarithms

The logarithm of a geometric process transforms the outcomes into "log-space". The information is the same, but is often a more convenient form for the analysis. That is, if:

$$
Y = x_1 \times x_2 \times \ldots \times x_i
$$

Then,

$$
log(Y) = log(x_1) + log(x_2) + \ldots + \log(x_i)
$$

This is effectively the transformation that gives rise to the Normal versus Log-Normal distribution.\
\
Bringing this back to the context of computational thinking:

*First,* we should think about how to transform data or modeling outcomes into a more convenient format. The log transform doesn't eliminate any information but may map the information into a shape that is easier for an optimizer or Monte Carlo simulation to explore.

*Second*, per @sec-elements-programming, floating point math is a *lossy* transformation of real numbers into a digital computer representation. Some information (in the literal Shannon information sense) is lost when computing and this tends to be worst with very small real numbers, such as those we encounter frequently in probabilities and likelihoods. Logarithms map very small numbers into negative numbers that don't encounter the same degree of truncation error that tiny numbers do

*Third,* modern CPUs are generally much faster at adding or subtracting numbers than multiplying or dividing. Therefore working with the logarithm of processes may be computationally faster than the direct process itself. 
:::

## Bayes' Rule

The minister and statistician Thomas Bayes derived a relationship of conditional probabilities that we today know as **Bayes' Rule**, commonly written as:

$$
P(H|D) = \frac{P(D|H) \times P(H)}{P(D)}
$$

The components of this are:

-   $P(H∣D)$ is the conditional probability of event $H$ occurring given that $D$ is true.
-   $P(D∣H)$ is the conditional probability of event $D$ occurring given that $H$ is true.
-   $P(H)$ is the prior probability of event $H$.
-   $P(D)$ is the prior probability of event $D$.

If we take the following:

-   $D$ is the available data
-   $H$ is our hypothesis

Then we can draw conclusions about the probability of a hypothesis being true given the observed data. When thought about this way, Bayes' rule is often described as:

$$
\text{posterior} = \frac{\text{likelihood} \times \text{prior}}{\text{evidence}}
$$

This is a very useful framework, which we'll return to more completely in @sec-modern-bayes. First, let's look at combining information theory and Bayes' rule in an applied example.

### Example: Model Selection via Likelihoods

Let's say that we have competing hypothesis about a data generating process, such as: "given a set of data representing risk outcomes, what distribution best fits the data"?

We can compare these models using Bayes' rules by observing the following: Suppose we have two models, $H_1$ and $H_2$, and we want to compare their likelihoods given the observed data, D. We can use Bayes' rule to calculate the posterior probability of each model: \$\$ P(H_1\|D) = (P(D\|H_1) \* P(H_1)) / P(D)

P(H_2\|D) = (P(D\|H_2) \* P(H_2)) / P(D) \$\$

Where:

-   $P(H_1|D)$ and $P(H_2|D)$ are the posterior probabilities of models $H_1$ and $H_2$, respectively, given the data $D$.
-   $P(D|H_1)$ and $P(D|H_2)$ are the likelihoods of the data $D$ under models $H_1$ and $H_2$, respectively.
-   $P(H_1)$ and $P(H_2)$ are the prior probabilities of models $H_1$ and $H_2$, respectively.
-   $P(D)$ is the marginal likelihood of the data, which serves as a normalizing constant.

To compare the likelihoods of the two models, we can calculate the ratio of their posterior probabilities, known as the Bayes factor, $BF$:

$$
BF = \frac{P(H_1|D)}{P(H_2|D)}
$$

Substituting the expressions for the posterior probabilities from Bayes' rule, we get:

$$
BF = \frac{P(D|H_1) \times P(H_1)} {P(D|H_2) \times P(H_2)}
$$

The marginal likelihood $P(D)$ cancels out since it appears in both the numerator and denominator. If we assume equal prior probabilities for the models, i.e., $P(H_1)$ = $P(H_2)$, then the Bayes factor simplifies to the likelihood ratio: $$
BF = \frac{P(D|H_1)}{P(D|H_2)}
$$

The interpretation of the Bayes factor is as follows:

-   If $BF > 1$, the data favor $H_1$ over $H_2$.
-   If $BF < 1$, the data favor $H_2$ over $H_1$.
-   If $BF = 1$, the data do not provide evidence in favor of either model.

In practice, the likelihoods $P(D|H_1)$ and $P(D|H_2)$ are often calculated using the probability density or mass functions of the models, evaluated at the observed data points. The prior probabilities $P(H_1)$ and $P(H_2)$ can be assigned based on prior knowledge or assumptions about the models. By comparing the likelihoods of the models using the Bayes factor, we can quantify the relative support for each model given the observed data, while taking into account the prior probabilities of the models. 

Another way of interpreting this is the more simplistic evaluation of which model has the higher likilihood given the data: this is simply a matter of comparing the magnitude of the likelihoods.

::: callout-warning
## Null Hypthothesis Statistical Test

Null Hypothesis Statistical Tests (NHST) is the idea of trying to statistically support an alternative hypothesis over a null hypothesis. The support in favor of alternative versus the null is reported via some statistical power, such as the **p-value** (the probability that the test result is as, or more extreme, than the value computed). The idea is that there's some objective way to push science towards greater truths and NHST was seen as a methodology that avoided the subjectivity of the Bayesian approach. However, while pure in concept, the NHST choices of both null hypothesis and model contain significant amounts of subjectivity! We might as well call the null hypothesis a prior and stop trying to disprove it absolutely. Instead: focus on model comparison, model structure, and posterior probabilities of the competiting theories.

Over 100 statistical tests have been developed in service of NHST @Lewis2013, but it's widely viewed now that a focus on NHST has led to *worse* science due to a multitude of factors, such as:

-   "P-hacking" or trying to find subsets of data which can (often only by chance) support rejecting some null
-   Cognitive anchoring to the importance of a p-value of 0.05 or less - why choose that number versus 0.01 or 0.001 or 0.49?
-   Bias in research processes where one may stop data collection or experimentation after achieving a favorable test result
-   Inappropriate application of the myriad of statistical tests
-   Focus on p-values rather than effects that simply matter more or have greater effect
    -   For example, which is of more interest to doctors? A study indicating a 1 in a billion chance of serious side effect (p-value `0.0001`) or a study indicating a 1 in 3 chance (p-value `0.06`)? Many journals would only publish the former study.
-   Difficulty to determine *causal* relationships.

There is subjectivity in the null hypothesis, data collection methodologies, study design, handling of missing data, choice of data *not* to include, which statistical tests to perform, and interpretation of relationships.

The authors of this book recommend against basic NHST and memorization of statistical tests in favor of principled Bayesian approaches. For the actuarial readers, NHST is analogous to traditional credibility methods (of which the authors also prefer more modern statistical approaches).
:::

The example we'll look at relates to the annual rainfall totals for a specific location in California[^statistics-5], which could be useful for insuring flood risk or determining the value of a catastrophe bond. Acknowloging that we are attempting to create a geocentric model[^statistics-6] instead of a scientifically accurate weather model, we narrow the problem to finding a probability distribution that matches the historical rainfall totals. Our goal is to recommend a model that best fits the data and justify that recommendation quantitatively. Before even looking at the data, @tbl-rain-hypothesis shows three competing models based on thinking about the real-world outcome we are trying to model. These three are chosen for the increasingly sophisticated thought process that might lead the modeler to recommend them - but which is supportable by the statistics?

[^statistics-5]: <https://data.ca.gov/dataset/annual-precipitation-data-for-northern-california-1944-current>

[^statistics-6]: See \@sec-predictive-vs-explanatory.

| Hypothesis | Process                          | Possible Rationale                                                                              |
|-----------------|-----------------|--------------------------------------|
| $H_1$      | A Normal (Gaussian) distribution | The sum of independent rainstorms creates annual rainfall totals that are normally distributed  |
| $H_2$      | A LogNormal distribution         | Since it's normal-ish, but skewed and can't be negative                                         |
| $H_3$      | A Gamma Distribution             | Since rainfall totals would be the sum of exponentially-distributed independent rainfall events |

: Three alternative hypothesis about the distribution of annual rainfall totals. {#tbl-rain-hypothesis}

```{julia}
rain = [
    39.51, 42.65, 44.09, 41.92, 28.42, 58.65, 30.18, 64.4, 29.02,
    37.00, 32.17, 36.37, 47.55, 27.71, 58.26, 36.55, 49.57, 39.84,
    82.22, 47.58, 51.18, 32.28, 52.48, 65.24, 51.12, 25.03, 23.27,
    26.11, 47.3, 31.8, 61.45, 94.95, 34.8, 49.53, 28.65, 35.3, 34.8,
    27.45, 20.7, 36.99, 60.54, 22.5, 64.85, 43.1, 37.55, 82.05, 27.9,
    36.55, 28.7, 29.25, 42.32, 31.93, 41.8, 55.9, 20.65, 29.28, 18.4,
    39.31, 20.36, 22.73, 12.75, 23.35, 29.59, 44.47, 20.06, 46.48,
    13.46, 9.34, 16.51, 48.24
];
```

When we do plot it, we can see some of the characteristics that align with our prior assumptions and knowledge about the system itself, such as: the data being constrained to positive values and a skew towards having some extreme weather years with lots of rainfall.

```{julia}
#| label: fig-rain-hist
#| fig-cap: "Annual rainfall totals for a specific location in California."
using CairoMakie
hist(rain)
```

We will show the likelihood of the three models after deriving the **maximum likelihood (MLE)**, which is simply finding the parameters that maximize the calculated likelihood. In general, this can be accomplished by an optimization routine, but here we will just use the functions built into Distributions.jl:

```{julia}
using StatsBase
using Distributions

n = fit_mle(Normal, rain)
ln = fit_mle(Normal, log.(rain))
lg = fit_mle(Gamma, log.(rain))
@show n
@show ln
@show lg;
```

```{julia}
#| label: fig-rain-cdf
let x = rain

    range = 1:0.1:100
    fig, ax, _ = lines(range, cdf.(n, range), label="Normal", axis=(xgridvisible=false, ygridvisible=false,))
    lines!(ax, range, cdf.(ln, log.(range)), label="LogNormal")
    lines!(range, cdf.(lg, log.(range)), label="LogGamma")
    lines!(quantile.(Ref(x), 0.01:0.01:0.99), 0.01:0.01:0.99, label="Data", color=(:black, 0.6), linewidth=3)
    fig[1, 2] = Legend(fig, ax, "Model", framevisible=false)
    fig
end
```

Let's look at the likelihoods. For the practical reasons described in @tip-logarithms, we will compare the the log-likelihoods to maintain convention with what you'd likely see or deal with in practice. Taking the log of the likelihood does not change the ranking of the likelihoods.

```{julia}
 let
    n_lik = sum(log.(pdf.(n, rain)))
    ln_lik = sum(log.(pdf.(ln, log.(rain))))
    lg_lik = sum(log.(pdf.(lg, log.(rain))))

    @show n_lik
    @show ln_lik
    @show lg_lik
end;
```

The results indicate that the LogNormal and the Gamma model for rainfall distribution are very superior to the Normal model, consistent with the visual inspection of the quantiles in @fig-rain-cdf. We reach that conclusion by noting how much more likely the latter two are, as the likelihoods of $-42$ and $-44$ is much greater than $-296$[^statistics-7].

[^statistics-7]: The values are negative because we are taking the logarithm of a number less than $1$. The likelihoods are less than $1$ because the likelihood is the joint (multiplicative) probability of observing each of the individual outcomes.

We evaluated the likelihood at a single point estimate of the parameters, but a true posterior probability of the parameters of the distributions will be represented by a *distribution* rather than a point. Expanding the analysis to account for that point will be the focus of the remainder of this chapter.

::: callout-note
In the literature, $H_3$ (the Gamma distribution) is known as the "Log-Pearson Type III distribution". It's actually recommended by the US Corps of Army Engineers as the recommended way to model rainfall totals.
:::

## Modern Bayesian Statistics {#sec-modern-bayes}

\[DRAFTING NOTE: The content below was pulled from a blog post on Bayesian statistics. The goal for the book is to adapt the concepts and follow the rainfall example by showing an MCMC example of fitting the distribution including the `c` parameter\]

A Bayesian statistical model has four main components to focus on:

1.  **Prior** encoding assumptions about the random variables related to the problem at hand, before conditioning on the data.
2.  A **Model** that defines how the random variables give rise to the observed outcome.
3.  **Data** that we use to update our prior assumptions.
4.  **Posterior** distributions of our random variables, conditioned on the observed data and our model

Having defined the first two components and collected our data, the workflow involves computationally sampling the posterior distribution, often using a technique called Markov Chain Monte-Carlo (MCMC). The result is a series of values that are sampled statistically from the posterior distribution.

### Advantages of the Bayesian Approach

The main advantages of this approach over traditional actuarial techniques are:

1.  **Focus on distributions rather than point estimates of the posterior's mean or mode.** We are often interested in the distribution of the parameters and a focus on a single parameter estimate will understate the risk distribution.
2.  **Model flexibility.** A Bayesian model can be as simple as an ordinary linear regression, but as complex as modeling a full insurance mechanics.
3.  **Simpler mental model.** Fundamentally, Bayes' theorem could be distilled down to an approach where you count the ways that things could occur and update the probabilities accordingly.
4.  **Explicit Assumptions.**: Enumerating the random variables in your model and explicitly parameterizing prior assumptions avoids ambiguity of the assumptions inside the statistical model.

### Challenges with the Bayesian Approach

With the Bayesian approach, there are a handful of things that are challenging. Many of the listed items are not unique to the Bayesian approach, but there are different facets of the issues that arise.

1.  **Model Construction**. One must be thoughtful about the model and how variables interact. However, with the flexibility of modeling, you can apply (actuarial) science to makes better models!
2.  **Model Diagnostics**. Instead of R\^2 values, there are unique diagnostics that one must monitor to ensure that the posterior sampling worked as intended.
3.  **Model Complexity and Size of Data**. The sampling algorithms are computationally intensive - as the amount of data grows and model complexity grows, the runtime demands cluster computing.
4.  **Model Representation**. The statistical derivation of the posterior can only reflect the complexity of the world as defined by your model. A Bayesian model won't automatically infer all possible real-world relationships and constraints.

### Why Now?

There are both philosophical and practical reasons why Bayesian analysis is rapidly changing the statistical landscape.

*Philosophically*, one of the main reasons why Bayesian thinking is appealing is its ability to provide a straightforward interpretation of statistical conclusions.

For example, when estimating an unknown quantity, a Bayesian probability interval can be directly understood as having a high probability of containing that quantity. In contrast, a frequentist confidence interval is typically interpreted only in the context of a series of similar inferences that could be made in repeated practice. In recent years, there has been a growing emphasis on interval estimation rather than hypothesis testing in applied statistics. This shift has strengthened the Bayesian perspective since it is likely that many users of standard confidence intervals intuitively interpret them in a manner consistent with Bayesian thinking.

Another meaningful way to understand the contrast between Bayesian and frequentist approaches is through the lens of decision theory, specifically how each view treats the concept of randomness. This perspective pertains to whether you regard the data being random or the parameters being random.

Frequentist statistics treats parameters as fixed and unknown, and the data as random — this is reflective of the view that data you collect is but one realization of an infinitely repeatable random process. Consequently, frequentist procedures, like hypothesis testing or confidence intervals, are generally based on the idea of long-run frequency or repeatable sampling.

Conversely, Bayesian statistics turns this on its head by treating the data as fixed — after all, once you've collected your data, it's no longer random but a fixed observed quantity. Parameters, which are unknown, are treated as random variables. The Bayesian approach then allows us to use probability to quantify our uncertainty about these parameters.

The Bayesian approach tends to align more closely with our intuitive way of reasoning about problems. Often, you are given specific data and you want to understand what that particular set of data tells you about the world. You're likely less interested in what might happen if you had infinite data, but rather in drawing the best conclusions you can from the data you do have.

*Practically*, recent advances in computational power, algorithm development, and open-source libraries have enabled practitioners to adapt the Bayesian workflow.

Deriving the posterior distribution is analytically intractable so computational methods must be used. Advances in raw computing power only in the 1990's made non-trivial Bayesian analysis possible, and recent advances in algorithms have made the computations more efficient. For example, one of the most popular algorithms, NUTS, was only published in the 2010's.

Many problems require the use of compute clusters to manage runtime, but if there is any place to invest in understanding posterior probability distributions, it's insurance companies trying to manage risk!

Moreover, the availability of open-source libraries, such as Turing.jl, PyMC3, and Stan provide access to the core routines in an accessible interface.

### Subjectivity of the Priors?

There are two ways one might react to subjectivity in a Bayesian context: It's a feature that should be embraced or it’s a flaw that should be avoided.

#### Subjectivity as a Feature

**A Bayesian approach to defining a statistical model is an approach that allows for explicitly incorporating actuarial judgment.** Encoding assumptions into a Bayesian model forces the actuary to be explicit about otherwise fuzzy predilections. The explicit assumption is also more amenable to productive debate about its merits and biases than an implicit judgmental override.

#### Subjectivity as a Flaw

Subjectivity is inherent in all useful statistical methods. Subjectivity in traditional approaches include how the data was collected, which hypothesis to test, what significant levels to use, and assumptions about the data-generating processes.

In fact, the "objective" approach to null hypothesis testing is so prone to abuse and misinterpretation that in 2016, the American Statistical Association issued a statement intended to steer statistical analysis into a "post p\<0.05 era." That "p\<0.05" approach is embedded in most traditional approaches to actuarial credibility[^statistics-8] and therefore should be similarly reconsidered.

[^statistics-8]: Note that the approach discussed here is much more encompassing than the Bühlmann-Straub Bayesian approach described in the actuarial literature.

#### Maximum Entropy Distributions

Further, when assigning a prior assumption to a random variable, there are mathematically most conservative choices to pull from. These are called Maximum Entropy Distributions (MED) and it can be shown that for certain minimal constraints these are the information-theoretic least informative choices. Least informative means that the prior will have the least influence on the resulting posterior distribution.

For example, if all you know is that the mean of a random process is positive, then the Exponential Distribution is your MED. If you know that a mean and variance must exist for the process, then the Normal distribution is your MED. If you know nothing at all, you can use a Uniform distribution for the possible values.

#### Bayesian Versus Machine Learning

Machine learning (ML) is *fully compatible* with Bayesian analysis - one can derive posterior distributions for the ML parameters like any other statistical model and the combination of approaches may be fruitful in practice.

However, to the extent that actuaries have leaned on ML approaches due to the shortcomings of traditional actuarial approaches, Bayesian modeling may provide an attractive alternative without resorting to notoriously finicky and difficult-to-explain ML models. The Bayesian framework provides an explainable model and offers several analytic extensions beyond the scope of this introductory article:

-   Causal Modeling: Identifying not just correlated relationships, but causal ones, in contexts where a traditional experiment is unavailable.
-   Bayes Action: Optimizing a parameter for, e.g., a CTE95 level instead of a parameter mean.
-   Information Criterion: Principled techniques to compare model fit and complexity.
-   Missing data: Mechanisms to handle the different kinds of missing data.
-   Model averaging: Posteriors can be combined from different models to synthesize different approaches.

### Implications for Risk Management

Like Bayes' Formula itself, another aspect of actuarial literature that is taught but often glossed over in practice is the difference between process risk (volatility), parameter risk, and model formulation risk. Often when performing analysis that relies on stochastic result, in practice only process/volatility risk is assessed.

Bayesian statistics provides the tools to help actuaries address parameter risk and model formulation. The posterior distribution of parameters derived is consistent with the observed data and modeled relationships. This posterior distribution of parameters can then be run as an additional dimension to the risk analysis.

Additionally, best practices include skepticism of the model construction itself, and testing different formulation of the modeled relationships and variable combinations to identify models which are best fit for purpose. Tools such as Information Criterion, posterior predictive checks, Bayes factors, and other statistical diagnostics can inform the actuary about tradeoffs between different choices of model.

### Paving the Way Forward for Actuaries

Bayesian approaches to statistical problems are rapidly changing the professional statistical field. To the extent that the actuarial profession incorporates statistical procedures we should consider adopting the same practices. The benefits of this are a better understanding of the distribution of risks, results that are more interpretable and explainable, and techniques that can be applied to a wider range of problems. The combination of these things would serve to enhance actuarial best practices related to understanding and communicating about risk.

For actuaries interested in learning more, there are number of available resources to be found. Textbooks recommended by the author are:

-   Statistical Rethinking (McElreath)
-   Bayes Rules! (Johnson, Ott, Dogucu)
-   Bayesian Data Analysis (Gelman, et. al.)

Additionally, the author has published a few examples of Bayesian analysis in an actuarial context on JuliaActuary.org.