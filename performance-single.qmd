# Writing Performant Single-Threaded Code {#sec-performance-single}



```{julia}
#| echo: false
#| output: false
using Pkg
Pkg.activate("env/performance-single")
Pkg.instantiate()
```

> Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away. - Antoine de Saint-Exupéry, Airman's Odyssey

## In This Chapter

Understanding single-threaded performance, strategies for efficient sequential code, recognizing when parallelization is needed, practical comparisons of single-threaded and parallel approaches, optimizing code in Julia.

## Introduction

With today's hardware, the highest throughput computations utilize GPUs for massive parallelization. However, writing parallel code, let alone performant parallel code, typically relies heavily on understanding patterns of non-parallel code. Secondly, many problems are not "massively parallelizable" and a sequential model architecture is required. For these reasons, it's critical to understand sequential patterns before moving onto parallel code.

For those coming from fundamentally slower languages (such as R or Python), the common advice to speed things up is often to try to parallelize code. With many high level languages the *only* way to achieve reasonable runtime is to utilize parallelism. In contrast, fast languages like Julia can output surprisingly quick single threaded prgrams

Lastly, it may be that a simpler, easier to maintain sequential model is preferable to a more complex parallel version if maximum performance is not required. Like the quote that openend the chapter, you may prefer a simpler sequential version of a model to a more complex parallel one. 

::: .callout-tip
Developer time (your time) is often more expensive than runtime, so be prepared to accept a good-enough but sub-optimal code instead of spending a lot of time optimizing every last nanosecond out of it!
:::

## Patterns of Performant Sequential Code

### Minimize Memory Allocations

Allocating memory onto the Heap takes a lot more time than (1) not using intermediate memory storage at all, or (2) utilizing the Stack. Each allocation requires time for memory management and requires the garbage collector, which can significantly impact performance, especially in tight loops or frequently called functions.

::: callout-note
**Tight loops** or **hot loops** are the performance critical section of the code that are performed many times during a computation. They are often the "inner-most" loop of a nested loop algorithm.
:::

In Julia, a general rule of thumb is that dynamically sizable or mutable objects (arrays, mutable structs) will be heap allocated while small fixed size objects can be stack allocated. For mutable objects, a common technique is to pre-allocate an array and then re-use that array for subsequent calculations. In the following example, note how we pre-allocate the output vector instead of creating vectors for each bond *and then* summing the vectors together at the end:

```{julia}
end_time = 10
cashflow_output = zeros(end_time)

par_bonds = map(1:1000) do i
    (tenor=rand((3, 5, 10)), rate=rand() / 10)
end

for asset in par_bonds
    for t in 1:end_time
        if t == asset.tenor
            cashflow_output[t] += 1 + asset.rate
        else
            cashflow_output[t] += asset.rate
        end
    end
end
cashflow_output

```

Julia's `@allocated` macro will display the number of bytes allocated by an expression, helping you identify and eliminate unnecessary allocations.

```{julia}
random_sum() = sum([rand() for _ in 1:10])
@allocated random_sum()
```

### Optimize Memory Access Patterns

Optimizing memory access patterns is essential for leveraging the CPU's cache hierarchy effectively. Modern CPUs have multiple levels of cache (L1, L2, L3), each with different sizes and access speeds. By structuring your code to access memory in a cache-friendly manner, you can significantly reduce memory latency and improve overall performance.

What is cache-friendly memory access? Essentially it boils down to spatial and temporal locality.

#### Spatial Locality

Spatial locality refers to accessing data that is physically near each other in memory (e..g contiguous blocks of data in an array).

For example, it's better to access data in a linear order rather than random order. For example, if we sum up the elements of an array in order it will be significantly faster than if we do it randomly:

```{julia}
using BenchmarkTools, Random

# Create a large array of structs to emphasize memory access patterns
struct DataPoint
    value::Float64
    # Add padding to make each element 64 bytes (a typical cache line size)
    padding::NTuple{7,Float64}
end

function create_large_array(n)
    [DataPoint(rand(), tuple(rand(7)...)) for _ in 1:n]
end

# Create a large array
const N = 1_000_000
large_array = create_large_array(N)

# Function for sequential access
function sequential_sum(arr)
    sum = 0.0
    for i in eachindex(arr)
        sum += arr[i].value
    end
    sum
end

# Function for random access
function random_sum(arr, indices)
    sum = 0.0
    for i in indices
        sum += arr[i].value
    end
    sum
end

# Create shuffled indices
shuffled_indices = shuffle(1:N)

# Benchmark
println("Sequential access:")
@btime sequential_sum($large_array)

println("\nRandom access:")
@btime random_sum($large_array, $shuffled_indices)
```

When the data is accessed in a linear order, it means that the computer can load chunks of data into the cache and it can operate on that cached data for several cycles before new data needs to be loaded into the cache. In contrast, when accessing the data randomly, then the cache frequently needs to be populated with a different set of bits from a completely different part of our array.

##### Column vs Row Major Order

All multi-dimensional arrays in computer memory are actually stored linearly. When storing the multi-dimensional array, an architectural decision needs to be made at the language-level and Julia is column-major, similar to many performance-oriented languages and libraries (e.g. LAPACK, Fortran, Matlab). Values are stored going down the columns instead of across the rows.

For example, this 2D array would be stored as `[1,2,3,...]` in memory, which is made clear via `vec` (which turns a multi-dimensional array into a 1D vector):

```{julia}
let
    array = [
        1 4 7
        2 5 8
        3 6 9
    ]

    vec(array)
end
```

When working with arrays, prefer accessing elements in column-major order (the default in Julia) to maximize spatial locality. This allows the CPU to prefetch data more effectively.

You can see how summing up values across the first (column) dimension is much faster than summing across rows:

```{julia}
@btime sum(arr, dims=1) setup = arr = rand(1000, 1000)
@btime sum(arr, dims=2) setup = arr = rand(1000, 1000)
```

#### Temporal Locality

The scheduler and branch prediction will recognize data that is accessed together closely in time and prefetch relevant blocks of data. This is an example of keeping "hot" data more readily accessible to the CPU than "cold" data.

Sometimes this happens at the operating system level - if you load a dataset that exceeds the available RAM, some of the "active" memory will actually be kept in a space on the persistent disk (e.g. your SSD) to avoid the computer crashing from being out of memory. Segments of the data that have been accessed recently will be in RAM while section of the data not recently accessed are likely to be in the portion stored on the persistent disk.

### Use Efficient Data Types

The right data type can lead to more compact memory representations, better cache utilization, and more efficient CPU instructions. This is another case of where having a smaller memory footprint allows for higher utilization of the CPU since computers tend to be memory-constrained in speed.

On some CPUs, you may find performance if use the smallest data type that can accurately represent your data. For example, prefer Int32 over Int64 if your values will never exceed 32-bit integer range. For floating-point numbers, use Float32 instead of Float64 if the reduced precision is acceptable for your calculations. These smaller types not only save memory but also allow for more efficient vectorized operations (see @sec-parallelism) on modern CPUs. Sometimes using the smaller datatype isn't beneficial: if you have a 64-bit architecture machine then the overhead of converting to/from 64-bit numbers may outweigh any speedup from higher memory throughput.

For collections, choose appropriate container types based on your use case. Arrays are efficient for calculations that loop through all or most elements, while Dictionaries are better for sparse look-ups or outside of the "hot loop" portion of a computation.

Consider using StaticArrays for small, fixed-size arrays, as they can be allocated on the stack and lead to better performance in certain scenarios than dynamically sizeable arrays. The trade-off is that the static arrays require more up-front compile time and after a certain point (length in the 50-100 element range) it usually isn't worth trying to use StaticArrays.

### Avoid Type Instabilities

Type instabilities occur when the compiler cannot infer a single concrete type for a variable or function return value. These instabilities can significantly hinder Julia's ability to generate optimized machine code, leading to performance degradation. When the compiler is not able to infer the types at compile-time (**compile time dispatch**), then while the program is running a lookup needs to be perfomed to find the most appropriate functions for the given type (**runtime dispatch**). Preferablly, when the types are known at compile-time, Julia is able to create machine code that will point directly to the desired function instead of needing to perform that lookup.

To avoid type instabilities, ensure that functions have consistent return types across all code paths. For example:

```{julia}

function multiple_return_types()
    if rand() > 0.5
        return 1.0 # Float
    else
        return 2   # Int
    end
end

function single_return_type()
    if rand() > 0.5
        return 1.0 # Float
    else
        return 2.0 # Float
    end
end

@btime sum(multiple_return_types() for _ in 1:1000)
@btime sum(single_return_type() for _ in 1:1000)
```

Note that having heterogeneous types as above is not the same thing as **type instability**, which is when Julia cannot determine in advance what the data types will be. In the example above, the return type is not unstable: the compiler recongizes that the single parametrice type `Union{Float64,Int64}` will be returned., even though two different sitypes can be returned the When the types cannot be determined by the compiler, it leads to runtime dispatch.

The following function illustrates a common anti-pattern wherein an `Any` typed array is created and then elements are added to it. Because any type can be added to an `Any` array (we happen to just add floats to it) then Julia's not sure what types to expect inside the container and therefore has to determine it at runtime.

```{julia}
function unstable_function()
    values = [] # Implicitly Any[]
    push!(values, 1.0, 2.0)
    maximum(values)
end
```

Employ Julia's `@code_warntype` macro to identify type instabilities in your code. Code that cannot be inferred will be annotated with red text showing the point of the code that will return unstable types. In this Quarto document, the informative colors will not render with the way it does in a coding environment, but look for the `::Any` annotation which indicates the compiler could not identify the type to return.

```{julia}
@code_warntype unstable_function()
```

More advanced exploration of type inference can be had via [Cthulhu.jl](https://github.com/JuliaDebug/Cthulhu.jl)[^performance-single-1], where you can interactively (via the REPL) navigate through different lines of code to explore inferred code.

[^performance-single-1]: So-named for the "slow descent into madness" when descending into functions to follow the Julia compiler's type inference across many layers of function calls.

When working with parametric types, look to avoid usage of generic type parameters (e.g. `Array{Any}`) whenever possible. For custom types, make use of parametric types to create type-stable abstractions. For example, the latter `struct Bond2` or `struct Bond3`  will allow Julia to create distinct concrete types and methods as opposed to needing generic runtime dispatch due to the unpredicable potential types of the `struct` fields. The difference between `Bond2` and `Bond3` is that the fields in `Bond3` could be any type (such as `Float16` or `String`) as long as both of them equaled the type variable `T`.

```julia
struct Bond1
    par
    coupon
end

struct Bond2
    par::Float64
    coupon::Float64
end

struct Bond3{T}
    par::T
    coupon::T
end
```

### Optimize for Branch Prediction

Modern CPUs use branch prediction to speculatively execute instructions before knowing the outcome of conditional statements. Optimizing your code for branch prediction can significantly improve performance, especially in tight loops or frequently executed code paths.

To optimize for branch prediction:

1. Structure your code to make branching patterns more predictable. For instance, in if-else statements, put the more likely condition first. This allows the CPU to more accurately predict the branch outcome. 

2. Use loop unrolling to reduce the number of branches. This technique involves manually repeating loop body code to reduce the number of loop iterations and associated branch instructions. See @sec-vectorization for more on what this means.

3. Consider using Julia's `@inbounds` macro to eliminate bounds checking in array operations when you're certain the accesses are safe. This reduces the number of conditional checks the CPU needs to perform. 

4. For performance-critical sections with unpredictable branches, consider using branch-free algorithms or bitwise operations instead of conditional statements. This can help avoid the penalties associated with branch mispredictions.

5. In some cases, it may be beneficial to replace branches with arithmetic operations (e.g., using the ternary operator or multiplication by boolean values) to allow for better vectorization and reduce the impact of branch mispredictions.

Here's an example demonstrating the impact of branch prediction:

```{julia}
function sum_if_positive(arr)
    sum = 0.0
    for x in arr
        if isodd(x)
            sum += x
        else
            sum -= x
        end
    end
    sum
end

# Benchmark
arr = rand(Int, 1_000_000)
arr_mostly_odds = fill(3, 999_999)
push!(arr_mostly_odds, 2) # add one even to get to 1M elements
@btime sum_if_positive($arr)
@btime sum_if_positive($arr_mostly_odds);
```

In this example, having consistently seen odd numbers means that the CPU will predict that the branch that will be used is the `sum += x` branch of the `if` statement.

Remember that optimizing for branch prediction often involves trade-offs. The benefits can vary depending on the specific hardware and the nature of your data. If performance critical, profile your code to ensure that your optimizations are actually improving performance in your specific use case. Overoptimizing on one set of hardware (e.g. local computer) may not translate the same on another set of hardware (e.g. server deployment). 

### Further Reading

-   [What scientists must know about hardware to write fast code](https://viralinstruction.com/posts/hardware/)

-   [Optimizing Serial Code, ScIML Book](https://book.sciml.ai/notes/02-Optimizing_Serial_Code/)