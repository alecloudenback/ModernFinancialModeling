---
author:
  - name: Yun-Tien Lee
---

# Matrices and Their Uses {#sec-matrices}

```{julia}
#| echo: false
#| output: false
using Pkg
Pkg.activate("env/matrices")
Pkg.instantiate()
```

> “The essence of mathematics is not to make simple things complicated, but to make complicated things simple. ― Stan Gudder”

## Chapter Overview

Matrices and their myriad uses: reframing problems through the eyes of linear algebra, an intuitive refreshing on applicable maths, and recurring patterns of matrix operations in financial modeling.

## Matrix manipulation

We first review basic matrix manipulation routines before going into more advanced topics.

### Addition and subtraction

Think of each matrix as a data grid (like a spreadsheet). Adding or subtracting values element by element is analogous to combining two sets of financial figures—such as merging cash inflows and outflows.

Example: Combining variations in A and B where each element represents a specific scenario's impact when doing cash flow projections. We would like to see combined variations of A and B, and we also would like to know the difference in variations between A and B.

```{julia}
# Define two matrices with returns across 3 assets over 4 scenarios
A = [0.01 0.02 -0.01;
    0.03 0.01 0.04;
    -0.02 0.01 0.02;
    -0.03 0.03 0.01]
B = [0.02 0.01 -0.01;
    0.02 0.01 -0.01;
    -0.01 0.02 0.01;
    0.01 0.02 -0.02]
# Perform element-wise matrix addition and subtraction
C = A .+ B
D = A .- B
# Display the result
println("Result of matrix addition:")
println(C)
println("Result of matrix subtraction:")
println(D)
```

### Transpose

Transposing a matrix is akin to flipping a dataset over its diagonal—turning rows into columns. This operation is useful when aligning data for regression or matching dimensions in financial models.

Example: Converting a time-series (rows as time points) in A into a format suitable for cross-sectional analysis (columns as different variables) in B.

```{julia}
# Define a matrix with variable values at all time points
A = [1 2 3;
    4 5 6;
    7 8 9]
# Perform matrix transpose
B = A'
# Display the result
println("Result of matrix transpose:")
println(B)
```

### Determinant

The determinant acts as a "volume-scaling" factor. It indicates how much a linear transformation stretches or compresses space. A zero determinant signals that the transformation collapses the space into a lower dimension, implying that the matrix cannot be inverted.

Given a matrix A
$$
\mathbf{A} =
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix}
$$

the determinant of matrix A can be calculated via Laplace expansion along any row e.g. row 1 as
$$
\det(\mathbf{A}) = \sum_{j=1}^{n} (-1)^{1+j} a_{1j} \det(\mathbf{A}_{1j})
$$

where $\mathbf{A}_{1j}$ is a sub-matrix obtained by deleting row $1$ and column $j$.

Example: In portfolio theory, a near-zero determinant of a covariance matrix might indicate multicollinearity among assets.

```{julia}
using LinearAlgebra

# Define a covariance matrix
A = [2.5e-6 2.5e-6 -1.25e-6;
    2.5e-6 2.5e-6 -1.25e-6;
    -1.25e-6 -1.25e-6 1.25e-6]
# Perform matrix determinant calculation
B = det(A)
# Display the result
println("Result of matrix determinant:")
println(B)
```

### Trace

The trace, being the sum of the diagonal elements, offers a quick summary that can reflect the total variance or influence of a matrix.

Example: In risk analysis, the trace of a covariance matrix may provide insights into the overall market volatility captured by the diagonal elements.

```{julia}
using LinearAlgebra

# Define a covariance matrix
A = [2.5e-6 2.5e-6 -1.25e-6;
    2.5e-6 2.5e-6 -1.25e-6;
    -1.25e-6 -1.25e-6 1.25e-6]
# Perform matrix trace calculation
B = tr(A)
# Display the result
println("Result of matrix trace:")
println(B)
```

### Norm

A matrix norm measures the "size" or "energy" of the matrix. It generalizes the concept of vector length to matrices, quantifying the overall magnitude.

The Frobenius norm of a matrix is defined as:
$$
\|\mathbf{A}\|_F = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} |A_{ij}|^2}
$$

Example: A common usage of norms include error analysis, where the norm of the difference between two matrices measures how far an approximation has deviated from the true values. Another common usage in machine learning is during regularization where it allows the training process to know how large an error would be to guide the direction of updating parameters.

```{julia}
using LinearAlgebra

# Define an error matrix
A = [-0.01 0.01 0.0;
    0.01 0.0 0.05;
    -0.01 -0.01 0.01;
    -0.04 0.01 0.03]
# Perform matrix norm calculation
B = norm(A)
# Display the result
println("Result of matrix norm:")
println(B)
```

### Multiplication

Matrix multiplication (non-element-wise) represents the composition of linear transformations. It's like applying a sequence of financial adjustments—first transforming the data with one factor and then modifying it with another. Other applications include:

- Transforming asset returns by a matrix representing factor loadings to obtain risk contributions.

- Neural network construction. Matrix multiplication is fundamental for training and using neural networks.

- Systems of linear equations. Many real-world problems reduce to solving systems of linear equations.

```{julia}
# Define return and factor matrices
A = [0.01 0.02 -0.01;
    0.03 0.01 0.02;
    -0.02 0.01 0.02;
    -0.03 0.03 0.01]
B = [0.8 0.2;
    0.5 0.5;
    0.3 0.7]
# Perform non-element-wise matrix multiplication
C = A * B
# Display the result
println("Result of non-element-wise matrix multiplication:")
println(C)
```

On the other hand, element-wise multiplication multiplies corresponding elements directly (like applying a weight matrix).

Example: Adjusting individual cash flow items by their respective risk weights in stress testing.

```{julia}
# Define cashflow and weight matrices
A = [100 120 150;
    50 60 70;
    200 180 160;
    80 90 100]
B = [0.8 0.8 0.7;
    0.5 0.5 0.4;
    0.3 0.2 0.1;
    0.2 0.5 0.8]
# Perform element-wise matrix multiplication
C = A .* B
# Display the result
println("Result of element-wise matrix multiplication:")
println(C)
```

### Inversion

Matrix inversion "reverses" a transformation. If a matrix transforms one set of financial assets into another state, its inverse would bring them back.

Example: In solving linear systems for equilibrium pricing, obtaining the inverse of the coefficient matrix allows you to revert to the original asset prices.

```{julia}
# Define a linear system
A = [1 -0.2 0; -0.1 1 -0.1; 0 -0.3 1]
# Compute the inverse of the matrix
A_inv = inv(A) # Tip: Prefer solving systems with A \ b instead of inv(A) * b for numerical stability and performance.
# Display the result
println("Inverse of matrix A:")
println(A_inv)
```

::: callout-note
For a matrix to be inverted, it must meet several important criteria.
- Square Matrix. The matrix must be square, meaning it has the same number of rows and columns.
- The determinant of the matrix must be non-zero.
:::

## Matrix decomposition

### Eigenvalues


Eigenvalue decomposition, also known as eigen decomposition, is a matrix factorization that decomposes a matrix into its eigenvectors and eigenvalues. This technique uncovers the intrinsic "modes" or principal directions in a dataset. The eigenvalues indicate the strength of each mode, while eigenvectors show the direction or pattern associated with that strength. Eigenvalues and eigenvectors are fundamental concepts in linear algebra and play key roles include:

- Eigenvalues help in analyzing how linear transformations affect vectors in a vector space.
- Eigenvalues facilitate the diagonalization of matrices and simplify the calculations.
- In systems of differential equations, eigenvalues help determine the stability of equilibrium points.
- Identifying the main factors that cause variance in a set of asset returns, which is critical for risk management or stress testing portfolios.
- In graph theory, eigenvalues of the adjacency matrix provide insights into the properties of the graph, such as connectivity, stability, and clustering.
- Many algorithms in data science, like clustering and factorization methods, rely on eigenvalues to identify patterns and reduce dimensionality, which enhances computational efficiency and interpretability.

```{julia}
using LinearAlgebra

# Use a symmetric matrix so eigenvalues are real and eigenvectors are orthonormal
A = [4.0 1.0 2.0;
    1.0 3.0 0.0;
    2.0 0.0 5.0]
# Perform eigenvalue decomposition
eigen_A = eigen(A)
# Extract eigenvalues and eigenvectors
λ = eigen_A.values
V = eigen_A.vectors

# Display the results
println("Original Matrix:")
println(A)
println("\nEigenvalues:")
println(λ)
println("\nEigenvectors:")
println(V)
```

::: callout-note
For a matrix to get eigenvalues, it must be square, meaning it has the same number of rows and columns.
:::

### Singular values

Singular value decomposition (SVD) breaks a matrix into three matrices U, Σ, and V, representing the left singular vectors (analogous to the primary features), the singular values (diagonal matrix capturing the importance), and the right singular vectors (detail on how features interact), respectively. Singular values are key to:

- Matrix factorization, which simplifies many matrix operations, making it easier to analyze and manipulate data.
- Dimensionality reduction. This is particularly useful in high-dimensional data scenarios, where reducing dimensions helps eliminate noise and improve computational efficiency.
- SVD can be used for data compression, particularly in image processing.
- SVD helps filter out noise in data analysis.
- SVD provides a robust method for solving linear equations, particularly when the matrix is ill-conditioned or singular.
- In machine learning, SVD helps extract important features from datasets.
- SVD provides insights into the relationships within data. The singular values indicate the strength of the relationship, while the singular vectors offer a way to visualize and interpret those relationships.

```{julia}
using LinearAlgebra

# Create a random matrix
A = rand(4, 3)
# Perform Singular Value Decomposition (SVD)
U, Σ, V = svd(A)
# U: Left singular vectors
# Σ: Singular values (diagonal matrix)
# V: Right singular vectors (transpose)
# Reconstruct original matrix
A_reconstructed = U * Diagonal(Σ) * V'

# Display the results
println("Original Matrix:")
println(A)
println("\nLeft Singular Vectors:")
println(U)
println("\nSingular Values:")
println(Σ)
println("\nRight Singular Vectors:")
println(V)
println("\nReconstructed Matrix:")
println(A_reconstructed)
```

### Matrix Factorization and Factorization Machines

Matrix factorization is a popular technique in recommendation systems for modeling user-item interactions and making personalized recommendations. The core idea behind matrix factorization is to decompose the user-item interaction matrix into two lower-dimensional matrices, capturing latent factors that represent user preferences and item characteristics. By learning these latent factors, the recommendation system can make predictions for unseen user-item pairs.

Factorization Machines (FM) are a type of supervised machine learning model designed for tasks such as regression and classification, especially in the context of recommendation systems and predictive modeling with sparse data. FM models extend traditional linear models by incorporating interactions between features, allowing them to capture complex relationships within the data.

Example: In credit scoring or recommendation systems for financial products, these techniques reveal latent factors that influence customer behavior.

```{julia}
using Recommendation
using SparseArrays
using Random

# 1. Synthetic events
num_users = 100
num_items = 50
num_ratings = 500
user_ids = rand(1:num_users, num_ratings)
item_ids = rand(1:num_items, num_ratings)
ratings  = rand(1:5, num_ratings)

events = [Event(user_ids[i], item_ids[i], float(ratings[i])) for i in 1:num_ratings]

# 2. Train/test split
Random.seed!(1234)
shuffle!(events)
train_size = round(Int, 0.8 * num_ratings)
train_events = events[1:train_size]
test_events  = events[train_size+1:end]

# 3. DataAccessor
da_train = DataAccessor(train_events, num_users, num_items)

# 4. Instantiate MatrixFactorization recommender
recommender = MatrixFactorization(da_train)

# 5. Train with hyperparameters
build!(recommender;
       reg=0.05,
       learning_rate=0.007,
       eps=1e-4,
       max_iter=20,
       random_init=true)

# 6. Recommend for a user
user = 1
k = 5
candidates = collect(1:num_items)
rec_list = recommend(recommender, user, k, candidates)
println("Recommendations for user $user: ", rec_list)

# 7. Evaluate RMSE
predicted = Float64[]
actual = Float64[]
for ev in test_events
    pred_rating = Recommendation.predict(recommender, ev.user, ev.item)
    push!(predicted, pred_rating)
    push!(actual, ev.value)
end

rmse = measure(RMSE(), predicted, actual)
println("Test RMSE = ", rmse)
```

### Principal component analysis

Principal Component Analysis (PCA) is a widely used technique in various fields for dimensionality reduction, data visualization, feature extraction, and noise reduction. PCA can also be applied to detect anomalies or outliers in the data by identifying data points that deviate significantly from the normal patterns captured by the principal components. Anomalies may appear as data points with large reconstruction errors or as outliers in the low-dimensional space spanned by the principal components.

Example: Compressing various economic indicators into a handful of principal components to illustrate predominant trends in market dynamics or risk factors.

```{julia}
using MultivariateStats

# Generate some synthetic data
Random.seed!(1234)
data = randn(5, 100)  # 100 samples, 5 features
# Perform PCA
pca_model = fit(PCA, data; maxoutdim=2)  # Project to 2 principal components
# Transform the data
transformed_data = transform(pca_model, data)
# Access principal components and explained variance ratio
principal_components = pca_model.prinvars
explained_variance_ratio = pca_model.prinvars ./ sum(pca_model.prinvars)

# Print results
println("Principal Components:")
println(principal_components)
println("Explained Variance Ratio:")
println(explained_variance_ratio)
```

