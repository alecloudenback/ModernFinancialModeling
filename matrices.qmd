---
author:
  - name: Yun-Tien Lee
---

# Matrices and Their Uses {#sec-matrices}

```{julia}
#| echo: false
#| output: false
using Pkg
Pkg.activate("env/matrices")
Pkg.instantiate()
```

> “The essence of mathematics is not to make simple things complicated, but to make complicated things simple. ― Stan Gudder”

## Chapter Overview

Linear algebra is the workhorse behind portfolio construction, risk decomposition, scenario generation, and many other financial workflows. This chapter revisits essential matrix operations and connects each to a realistic finance task.

## Matrix manipulation

We first review basic matrix manipulation routines before going into more advanced topics.

### Addition and subtraction

Think of each matrix as a data grid (like a spreadsheet). Adding or subtracting values element by element is analogous to combining two sets of financial figures—such as merging cash inflows and outflows.

Example: Combining variations in `A` and `B` where each element represents a specific scenario's impact when doing cash flow projections. We would like to know the difference in variations between `A` and `B`.

```{julia}
# Define two matrices with returns across 3 assets over 4 scenarios
A = [0.01 0.02 -0.01;
    0.03 0.01 0.04;
    -0.02 0.01 0.02;
    -0.03 0.03 0.01]

B = [0.02 0.01 -0.01;
    0.02 0.01 -0.01;
    -0.01 0.02 0.01;
    0.01 0.02 -0.02]

# Perform element-wise subtraction
D = A .- B
```

### Transpose

Transposing a matrix is akin to flipping a dataset over its diagonal—turning rows into columns. This operation is useful when aligning data for regression or matching dimensions in financial models.

Example: Converting a time-series (rows as time points) in `A` into a format suitable for cross-sectional analysis (columns as different variables) in `B`.

```{julia}
# Define a matrix with variable values at all time points
A = [1 2 3;
    4 5 6;
    7 8 9]
# Perform matrix transpose
B = A'
```

::: callout-tip
For real-valued data `transpose(A)` and `A'` give the same result. The explicit `transpose` keeps intent clear and avoids accidentally taking the complex conjugate when your data later includes complex numbers (for example when working with Fourier transforms).
:::

### Determinant

The determinant acts as a "volume-scaling" factor. It indicates how much a linear transformation stretches or compresses space. A zero determinant signals that the transformation collapses the space into a lower dimension, implying that the matrix cannot be inverted.

Given a matrix A
$$
\mathbf{A} =
\begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{pmatrix}
$$

the determinant of matrix A can be calculated via Laplace expansion along any row, e.g., row 1 as
$$
\det(\mathbf{A}) = \sum_{j=1}^{n} (-1)^{1+j} a_{1j} \det(\mathbf{A}_{1j})
$$

where $\mathbf{A}_{1j}$ is a sub-matrix obtained by deleting row $1$ and column $j$.

Example: In portfolio theory, a near-zero determinant of a covariance matrix might indicate multicollinearity among assets.

```{julia}
using LinearAlgebra

# Define a covariance matrix
A = [2.5e-6 2.5e-6 -1.25e-6;
    2.5e-6 2.5e-6 -1.25e-6;
    -1.25e-6 -1.25e-6 1.25e-6]

# Perform matrix determinant calculation
B = det(A)
```

### Trace

The trace, being the sum of the diagonal elements, offers a quick summary that can reflect the total variance or influence of a matrix.

Example: In risk analysis, the trace of a covariance matrix may provide insights into the overall market volatility captured by the diagonal elements.

```{julia}
using LinearAlgebra

# Define a covariance matrix
A = [2.5e-6 2.5e-6 -1.25e-6;
    2.5e-6 2.5e-6 -1.25e-6;
    -1.25e-6 -1.25e-6 1.25e-6]
# Perform matrix trace calculation
B = tr(A)
```

### Norm

A matrix norm measures the "size" or "energy" of the matrix. It generalizes the concept of vector length to matrices, quantifying the overall magnitude.

The Frobenius norm of a matrix is defined as:
$$
\|\mathbf{A}\|_F = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} |A_{ij}|^2}
$$

Example: A common usage of norms includes error analysis, where the norm of the difference between two matrices measures how far an approximation has deviated from the true values. Another common usage in machine learning is during regularization where it allows the training process to know how large an error would be to guide the direction of updating parameters.

```{julia}
using LinearAlgebra

# Define an error matrix
A = [-0.01 0.01 0.0;
    0.01 0.0 0.05;
    -0.01 -0.01 0.01;
    -0.04 0.01 0.03]

# Perform matrix norm calculation
B = norm(A)
```

Julia's `norm` defaults to the Frobenius norm for matrices. Specify `norm(A, 2)` for the operator (spectral) norm if you need the largest singular value instead.

### Multiplication

Matrix multiplication (non-element-wise) represents the composition of linear transformations. It's like applying a sequence of financial adjustments — first transforming the data with one factor and then modifying it with another.

Other applications include:

- Transforming asset returns by a matrix representing factor loadings to obtain risk contributions.

- Neural network construction. Matrix multiplication is fundamental for training and using neural networks.

- Systems of linear equations. Many real-world problems reduce to solving systems of linear equations.

```{julia}
# Define return and factor matrices
A = [0.01 0.02 -0.01;
    0.03 0.01 0.02;
    -0.02 0.01 0.02;
    -0.03 0.03 0.01]
B = [0.8 0.2;
    0.5 0.5;
    0.3 0.7]

# Perform non-element-wise matrix multiplication
C = A * B
```

On the other hand, element-wise multiplication multiplies corresponding elements directly (like applying a weight matrix).

Example: Adjusting individual cash flow items by their respective risk weights in stress testing.

```{julia}
# Define cashflow and weight matrices
A = [100 120 150;
    50 60 70;
    200 180 160;
    80 90 100]

B = [0.8 0.8 0.7;
    0.5 0.5 0.4;
    0.3 0.2 0.1;
    0.2 0.5 0.8]

# Perform element-wise matrix multiplication
C = A .* B
```

### Inversion

Matrix inversion "reverses" a transformation. If a matrix transforms one set of financial assets into another state, its inverse would bring them back.

Example: In solving linear systems for equilibrium pricing, obtaining the inverse of the coefficient matrix allows you to revert to the original asset prices.

That said, production code typically avoids forming the inverse explicitly; instead, we solve systems with the matrix factorization that underlies the inverse.

```{julia}
# Define a linear system
A = [1 -0.2 0; -0.1 1 -0.1; 0 -0.3 1]

# Compute the inverse of the matrix
A_inv = inv(A)
```

::: callout-tip
In numerical work we usually solve systems with `A \ b` instead of computing `inv(A)` explicitly. Forming the inverse is slower and amplifies rounding error, whereas the backslash operator reuses a factorization and gives better numerical stability.
:::

::: callout-note
For a matrix to be inverted, it must meet several important criteria.
- Square Matrix. The matrix must be square, meaning it has the same number of rows and columns.
- The determinant of the matrix must be non-zero.
:::

## Matrix decomposition

### Eigenvalues

Eigenvalue decomposition, also known as eigen decomposition, is a matrix factorization that decomposes a matrix into its eigenvectors and eigenvalues. This technique uncovers the intrinsic "modes" or principal directions in a dataset. The eigenvalues indicate the strength of each mode, while eigenvectors show the direction or pattern associated with that strength. Eigenvalues and eigenvectors are fundamental concepts in linear algebra and play key roles include:

- Eigenvalues help in analyzing how linear transformations affect vectors in a vector space.
- Eigenvalues facilitate the diagonalization of matrices and simplify the calculations.
- In systems of differential equations, eigenvalues help determine the stability of equilibrium points.
- Identifying the main factors that cause variance in a set of asset returns, which is critical for risk management or stress testing portfolios.
- In graph theory, eigenvalues of the adjacency matrix provide insights into the properties of the graph, such as connectivity, stability, and clustering.
- Many algorithms in data science, like clustering and factorization methods, rely on eigenvalues to identify patterns and reduce dimensionality, which enhances computational efficiency and interpretability.

```{julia}
using LinearAlgebra

# Use a symmetric matrix so eigenvalues are real and eigenvectors are orthonormal
A = [4.0 1.0 2.0;
    1.0 3.0 0.0;
    2.0 0.0 5.0]

# Perform eigenvalue decomposition
eigen_A = eigen(A)
```

::: callout-note
For a matrix to get eigenvalues, it must be square, meaning it has the same number of rows and columns.
:::

### Singular values

Singular value decomposition (SVD) breaks a matrix into three matrices U, Σ, and V, representing the left singular vectors (analogous to the primary features), the singular values (diagonal matrix capturing the importance), and the right singular vectors (detail on how features interact), respectively. Singular values are key to:

- Matrix factorization, which simplifies many matrix operations, making it easier to analyze and manipulate data.
- Dimensionality reduction. This is particularly useful in high-dimensional data scenarios, where reducing dimensions helps eliminate noise and improve computational efficiency.
- SVD can be used for data compression, particularly in image processing.
- SVD helps filter out noise in data analysis.
- SVD provides a robust method for solving linear equations, particularly when the matrix is ill-conditioned or singular.
- In machine learning, SVD helps extract important features from datasets.
- SVD provides insights into the relationships within data. The singular values indicate the strength of the relationship, while the singular vectors offer a way to visualize and interpret those relationships.

```{julia}
using Random, LinearAlgebra

# Create a random matrix
A = rand(4, 3)
```

```{julia}
# Perform Singular Value Decomposition (SVD)
U, Σ, V = svd(A);
```

Left Singular Vectors (U):

```{julia}
U
```

Singular Values (Σ):

```{julia}
Σ
```

Right Singular Vectors (V):

```{julia}
V
```

Reconstructed Matrix:

```{julia}
A_reconstructed = U * Diagonal(Σ) * V'
```

Reconstructed Matrix matches Original Matrix:

```{julia}
A_reconstructed ≈ A
```

### Matrix Factorization and Factorization Machines

Matrix factorization is a popular technique in recommendation systems for modeling user-item interactions and making personalized recommendations. The core idea behind matrix factorization is to decompose the user-item interaction matrix into two lower-dimensional matrices, capturing latent factors that represent user preferences and item characteristics. By learning these latent factors, the recommendation system can make predictions for unseen user-item pairs.

Factorization Machines (FM) are a type of supervised machine learning model designed for tasks such as regression and classification, especially in the context of recommendation systems and predictive modeling with sparse data. FM models extend traditional linear models by incorporating interactions between features, allowing them to capture complex relationships within the data.

Example: In credit scoring or recommendation systems for financial products, these techniques reveal latent factors that influence customer behavior.

```{julia}
using Recommendation
using SparseArrays
using Random

# 1. Synthetic events
num_users = 100
num_items = 50
num_ratings = 500
user_ids = rand(1:num_users, num_ratings)
item_ids = rand(1:num_items, num_ratings)
ratings = rand(1:5, num_ratings)

events = [Event(user_ids[i], item_ids[i], Float64(ratings[i])) for i in 1:num_ratings]

# 2. Train/test split
Random.seed!(1234)
shuffle!(events)
train_size = round(Int, 0.8 * num_ratings)
train_events = events[1:train_size]
test_events = events[train_size+1:end]

# 3. DataAccessor
da_train = DataAccessor(train_events, num_users, num_items)

# 4. Instantiate MatrixFactorization recommender
recommender = MatrixFactorization(da_train)

# 5. Train with hyperparameters
build!(recommender;
    reg=0.05,
    learning_rate=0.007,
    eps=1e-4,
    max_iter=20,
    random_init=true)

# 6. Recommend for a user
user = 1
k = 5
candidates = collect(1:num_items)

# Recommendations for user
rec_list = recommend(recommender, user, k, candidates)
```

Next we evaluate the RMSE:

```{julia}

# 7. Evaluate RMSE
predicted = Float64[]
actual = Float64[]
for ev in test_events
    pred_rating = Recommendation.predict(recommender, ev.user, ev.item)
    push!(predicted, pred_rating)
    push!(actual, ev.value)
end

rmse = measure(RMSE(), predicted, actual)
```

Lower RMSE indicates better out-of-sample accuracy. In production you would also monitor business metrics — such as product uptake or credit conversion — but RMSE is a good sanity check that the latent factors are capturing the signal.

### Principal component analysis

Principal Component Analysis (PCA) is a widely used technique in various fields for dimensionality reduction, data visualization, feature extraction, and noise reduction. PCA can also be applied to detect anomalies or outliers in the data by identifying data points that deviate significantly from the normal patterns captured by the principal components. Anomalies may appear as data points with large reconstruction errors or as outliers in the low-dimensional space spanned by the principal components.

Example: Compressing various economic indicators into a handful of principal components to illustrate predominant trends in market dynamics or risk factors.

```{julia}
using Random, MultivariateStats

# Generate some synthetic data
Random.seed!(1234)
data = randn(5, 100)  # 100 samples, 5 features
# Perform PCA
pca_model = fit(PCA, data; maxoutdim=2)  # Project to 2 principal components
# Transform the data
transformed_data = transform(pca_model, data)
# Access principal components and explained variance ratio
principal_components = pca_model.prinvars
explained_variance_ratio = pca_model.prinvars ./ sum(pca_model.prinvars)

# Print results
println("Principal Components:")
println(principal_components)
println("Explained Variance Ratio:")
println(explained_variance_ratio)
```

`MultivariateStats` expects variables on the rows and observations in the columns. The reported variance ratios guide how many components to retain; in many market datasets the first few components align with broad risk factors such as market, curve, and credit spreads.


